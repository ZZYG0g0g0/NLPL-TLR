<p> A Visitor which handles nodes in Derby's abstract syntax trees. In addition to this contract, it is expected that an ASTVisitor will have a 0-arg constructor. You use an ASTVisitor like this: </p> <blockquote><pre> // initialize your visitor MyASTVisitor myVisitor = new MyASTVisitor(); myVisitor.initializeVisitor(); languageConnectionContext.setASTVisitor( myVisitor ); // then run your queries. ... // when you're done inspecting query trees, release resources and // remove your visitor languageConnectionContext.setASTVisitor( null ); myVisitor.teardownVisitor(); </pre></blockquote>
An abstract implementation of LockFactory that allows different implementations of the lock table to be plugged in. All the methods of <code>LockFactory</code> are implemented. Subclasses must implement the <code>createLockTable()</code> method and make it return the desired <code>LockTable</code> object. <BR> MT - Mutable - Container Object : Thread Aware
This class will perform the database connection establishment, querying the database, shut downing the database. Created under DERBY-4587-PlanExporter tool
Module interface for an access manager.  An access manager provides transactional access via access methods to data in a single storage manager. <p> An AccessFactory is typically obtained from the Monitor: <p> <blockquote><pre> // Get the current transaction controller. AccessFactory af; af = (AccessFactory) Monitor.findServiceModule(this, AccessFactory.MODULE); </pre></blockquote>
Global constants provided by the Access Interface.
AccessPath represents a proposed access path for an Optimizable. An Optimizable may have more than one proposed AccessPath.

This allows us to get to the byte array to go back and edit contents or get the array without having a copy made. <P> Since a copy is not made, users must be careful that no more writes are made to the stream if the array reference is handed off. <p> Users of this must make the modifications *before* the next write is done, and then release their hold on the array.
An activation contains all the local state information necessary to execute a re-entrant PreparedStatement. The way it will actually work is that a PreparedStatement will have an executable plan, which will be a generated class. All of the local state will be variables in the class. Creating a new instance of the executable plan will create the local state variables. This means that an executable plan must implement this interface, and that the PreparedStatement.getActivation() method will do a "new" operation on the executable plan. <p> The fixed implementations of Activation in the Execution impl package are used as skeletons for the classes generated for statements when they are compiled. <p> There are no fixed implementations of Activation for statements; a statement has an activation generated for it when it is compiled.
ActivationClassBuilder provides an interface to satisfy generation's common tasks in building an activation class, as well as a repository for the JavaFactory used to generate the basic language constructs for the methods in the class. Common tasks include the setting of a static field for each expression function that gets added, the creation of the execute method that gets expanded as the query tree is walked, setting the superclass. <p> An activation class is defined for each statement. It has the following basic layout: TBD See the document \\Jeeves\Unversioned Repository 1\Internal Technical Documents\Other\GenAndExec.doc for details. <p> We could also verify methods as they are added, to have 0 parameters, ...
A Lock represents a granted or waiting lock request. <BR> MT - Mutable - Immutable identity : Thread Aware

Address of a customer, warehouse or district. <P> Fields map to definition in TPC-C for the CUSTOMER, WAREHOUSE and DISTRICT tables. The Java names of fields do not include the C_,W_ or D_ prefixes and are in lower case. <BR> All fields have Java bean setters and getters.
A bundle's authority to perform specific privileged administrative operations on or to get sensitive information about a bundle. The actions for this permission are: <pre> Action               Methods class                Bundle.loadClass execute              Bundle.start Bundle.stop StartLevel.setBundleStartLevel extensionLifecycle   BundleContext.installBundle for extension bundles Bundle.update for extension bundles Bundle.uninstall for extension bundles lifecycle            BundleContext.installBundle Bundle.update Bundle.uninstall listener             BundleContext.addBundleListener for SynchronousBundleListener BundleContext.removeBundleListener for SynchronousBundleListener metadata             Bundle.getHeaders Bundle.getLocation resolve              PackageAdmin.refreshPackages PackageAdmin.resolveBundles resource             Bundle.getResource Bundle.getResources Bundle.getEntry Bundle.getEntryPaths Bundle.findEntries Bundle resource/entry URL creation startlevel           StartLevel.setStartLevel StartLevel.setInitialBundleStartLevel context              Bundle.getBundleContext </pre> <p> The special action &quot;*&quot; will represent all actions. <p> The name of this permission is a filter expression. The filter gives access to the following parameters: <ul> <li>signer - A Distinguished Name chain used to sign a bundle. Wildcards in a DN are not matched according to the filter string rules, but according to the rules defined for a DN chain.</li> <li>location - The location of a bundle.</li> <li>id - The bundle ID of the designated bundle.</li> <li>name - The symbolic name of a bundle.</li> </ul>

Describe a G (Aggregate) alias. The AggregateAliasInfo maintains a version stamp so that it can evolve its persistent form over time.
An AggregateDefinition defines an aggregate. It is used by Derby during query compilation to determine what Aggregator is used to aggregate a particular data type and what datatype the Aggregator will emit.  A single AggregateDefinition may map to one or more Aggregators depending on the input type.  For example, a user defined STDEV aggregate may use one aggregator implementation for the INTEGER type and another for a user defined type that implements a point.  In this case, both the aggregators would have a single AggregateDefinition that would chose the appropriate aggregator based on the input type.  On the other hand, if only a single aggregator is needed to aggregate over all of the input types (e.g. COUNT()), then it may be convenient to implement both the AggregateDefinition and the Aggregator interfaces by the same class.
An Aggregate Node is a node that represents a set function/aggregate. It used for all system aggregates as well as user defined aggregates.
This sort observer performs aggregation.
Represents aggregate function calls on a window
<p> Behavior of a user-defined Derby aggregator. Aggregates values of type V and returns a result of type R. In addition to the methods in the interface, implementing classes must have a 0-arg public constructor. </p>
This is a simple class used to store the run time information needed to invoke an aggregator.  This class is serializable because it is stored with the plan.  It is serializable rather than externalizable because it isn't particularly complicated and presumbably we don't need version control on plans.
Vector of AggergatorInfo objects. /////////////////////////////////////////////////////////////  OBJECT INTERFACE  /////////////////////////////////////////////////////////////
This class represents an Alias Descriptor. The public methods for this class are: <ol> <li>getUUID</li> <li>getJavaClassName</li> <li>getAliasType</li> <li>getNameSpace</li> <li>getSystemAlias</li> <li>getAliasId</li> </ol>
An interface for describing an alias in Derby systems. In a Derby system, an alias can be one of the following: <ul> <li>method alias <li>UDT alias <li>class alias <li>synonym <li>user-defined aggregate </ul>

All package suites for the function tests. Suites added: <UL> <LI> tests.lang <LI> tests.jdbcapi <LI> tests.tools <LI> tests.jdbc4 (Java SE 6  only) </UL>
An AllResultColumn represents a "*" result column in a SELECT statement.  It gets replaced with the appropriate set of columns at bind time.
A <code>ServiceEvent</code> listener that does not filter based upon package wiring. <code>AllServiceListener</code> is a listener interface that may be implemented by a bundle developer. When a <code>ServiceEvent</code> is fired, it is synchronously delivered to an <code>AllServiceListener</code>. The Framework may deliver <code>ServiceEvent</code> objects to an <code>AllServiceListener</code> out of order and may concurrently call and/or reenter an <code>AllServiceListener</code>. <p> An <code>AllServiceListener</code> object is registered with the Framework using the <code>BundleContext.addServiceListener</code> method. <code>AllServiceListener</code> objects are called with a <code>ServiceEvent</code> object when a service is registered, modified, or is in the process of unregistering. <p> <code>ServiceEvent</code> object delivery to <code>AllServiceListener</code> objects is filtered by the filter specified when the listener was registered. If the Java Runtime Environment supports permissions, then additional filtering is done. <code>ServiceEvent</code> objects are only delivered to the listener if the bundle which defines the listener object's class has the appropriate <code>ServicePermission</code> to get the service using at least one of the named classes under which the service was registered. <p> Unlike normal <code>ServiceListener</code> objects, <code>AllServiceListener</code> objects receive all <code>ServiceEvent</code> objects regardless of whether the package source of the listening bundle is equal to the package source of the bundle that registered the service. This means that the listener may not be able to cast the service object to any of its corresponding service interfaces if the service object is retrieved. This is a marker interface
An allocation extent row manages the page status of page in the extent. AllocExtent is externalizable and is written to the AllocPage directly, without being converted to a row first. <P> <PRE>
An allocation page of the file container. <P> This class extends a normal Stored page, with the exception that a hunk of space may be 'borrowed' by the file container to store the file header. <P> The borrowed space is not visible to the alloc page even though it is present in the page data array.  It is accessed directly by the FileContainer.  Any change made to the borrowed space is not managed or seen by the allocation page. <P> The reason for having this borrowed space is so that the container header does not need to have a page of its own. <P><B>Page Format</B><BR> An allocation page extends a stored page, the on disk format is different from a stored page in that N bytes are 'borrowed' by the container and the page header of an allocation page will be slightly bigger than a normal stored page.  This N bytes are stored between the page header and the record space. <P> The reason why this N bytes can't simply be a row is because it needs to be statically accessible by the container object to avoid a chicken and egg problem of the container object needing to instantiate an alloc page object before it can be objectified, and an alloc page object needing to instantiate a container object before it can be objectified.  So this N bytes must be stored outside of the normal record interface yet it must be settable because only the first alloc page has this borrowed space.  Other (non-first) alloc page have N == 0. <PRE> [ borrowed ] +----------+-------------+---+---------+-------------------+-------------+--------+ | FormatId | page header | N | N bytes | alloc extend rows | slot offset |checksum| +----------+-------------+---+---------+-------------------+-------------+--------+ </PRE> N is a byte that indicates the size of the borrowed space.  Once an alloc page is initialized, the value of N cannot change. <P> The maximum space that can be borrowed by the container is 256 bytes. <P> The allocation page are of the same page size as any other pages in the container. The first allocation page of the FileContainer starts at the first physical byte of the container.  Subsequent allocation pages are chained via the nextAllocPageOffset.  Each allocation page is expected to manage at least 1000 user pages (for 1K page size) so this chaining may not be a severe performance hit.  The logical -&gt; physical mapping of an allocation page is stored in the previous allocation page.  The container object will need to maintain this mapping. <P> The following fields are stored in the page header <PRE>
Allocation page operation - to allocate, deallocate or free a page
This interface describe the operations that has to do with page allocation/deallocation.  This interface is used for a special allocation page that records the allocation information and dispense the allocation policy.
An auxiliary object to cache the allocation information for a file container. <B>Only a FileContainer should use this object</B> <P> The allocation cache contains an array of AllocExtents and 3 arrays of longs: <OL><LI>ExtentPageNums[i] is the page number of the i'th extent <LI>lowRange[i] is the smallest page number managed by extent i <LI>hiRange[i] is the largest page number managed by extent i </OL> <P> Note thate extentPageNums and lowRange does not change once the extent has been created, but hiRange will change for the last extent as more pages are allocated. <P> Extents can be individually invalidated or the entire cache (all extends) can be invalidated at once. <P> MT - unsafe Synrhonized access to all methods must be enforced by the caller of AllocationCache
This class  describes actions that are ALWAYS performed for a alter constraint at Execution time.
This class  describes actions that are ALWAYS performed for an ALTER TABLE Statement at Execution time.
A AlterTableNode represents a DDL statement that alters a table. It contains the name of the object to be created. class interface
Used for deferrable CHECK constraint. When we evaluate check constraints for a row where at least one constraint is deferrable, we need to know exactly which set of constraints violated the checks.  The normal evaluation of check constraints is generated as one big (NOT c1) AND (NOT c2) AND ...  AND (NOT cn) using short-circuited (McCarthy) boolean evaluation. <p> This kind of evaluation of the expression can only tell us the first failing constraint, so we use full evaluation instead, as embodied in this node. See also {@link org.apache.derby.iapi.types.BooleanDataValue#throwExceptionIfImmediateAndFalse}.

<p> These are methods for testing ANSI routine resolution. The resolution rules are described in DERBY-3652. Methods which return -1 are methods which we expect will never be matched. </p>
Takes a quantified predicate subquery's result set. NOTE: A row with a single column containing null will be returned from getNextRow() if the underlying subquery ResultSet is empty.
<p> This is an XML-reading VTI which has been tweaked to handle the formatting of timestamps and nulls found in Apache server logs. </p>
AppRequester stores information about the application requester. It is used so that multiple sessions can share information when they are started from the same version of the application requester.
An object input stream that implements resolve class in order to load the class through the ClassFactory.loadApplicationClass method.
<p> This table function acts like a union view on a set of archive tables. The idea is that the old contents of a main table are periodically moved to archive tables whose names start with $tableName$suffix. Each bulk move of rows results in the creation of a new archive table. The archive tables live in the same schema as the main table and have its shape. This table function unions the main table together with all of its archived snapshots. So, for instance, you might have the following set of tables, which this table function unions together: </p> <pre> T1 T1_ARCHIVE_1 T1_ARCHIVE_2 ... T1_ARCHIVE_N </pre> <p> This table function may appear in user documentation. If you change the behavior of this table function, make sure that you adjust the user documentation linked from DERBY-6117. </p>
An InputStream that allows reading from an array of bytes. The array of bytes that is read from can be changed without having to create a new instance of this class.

Utility class for constructing and reading and writing arrays from/to formatId streams and for performing other operations on arrays.


AssertFailure is raised when an ASSERT check fails. Because assertions are not used in production code, are never expected to fail, and recovering from their failure is expected to be hard, they are under RuntimeException so that no one needs to list them in their throws clauses. An AssertFailure at the outermost system level will result in system shutdown. An AssertFailure also contains a string representation of a full thread dump for all the live threads at the moment it was thrown if the JVM supports it and we have the right permissions. If the JVM doesn't have the method Thread.getAllStackTraces i.e, we are on a JVM &lt; 1.5, or if we don't have the permissions java.lang.RuntimePermission "getStackTrace" and "modifyThreadGroup", a message saying so is stored instead. The thread dump string is printed to System.err after the normal stack trace when the error is thrown, and it is also directly available by getThreadDump().
import java.io.PrintStream;
<p> Does asynchronous shipping of log records from the master to the slave being replicated to. The implementation does not ship log records as soon as they become available in the log buffer (synchronously), instead it does log shipping in the following two-fold scenarios 1) Periodically (i.e.) at regular intervals of time. 2) when a request is sent from the master controller (force flushing of the log buffer). 3) when a notification is received from the log shipper about a log buffer element becoming full and the load on the log buffer so warrants a ship. </p>
List of all connection (JDBC) attributes by the system. <P> This class exists for two reasons <Ol> <LI> To act as the internal documentation for the attributes. <LI> To remove the need to declare a java static field for the attributes name in the protocol/implementation class. This reduces the footprint as the string is final and thus can be included simply as a String constant pool entry. </OL> <P> This class should not be shipped with the product. <P> This class has no methods, all it contains are String's which by are public, static and final since they are declared in an interface. <P> At some point this class should be replaced by org.apache.derby.shared.common.reference.Attribute. The issue is that this class is used by ij to check attributes, ij uses reflection on this class to get the list of valid attributes. The expanded class in shared has the client attributes as well. Ideally ij would work of an explicit list of attributes and not infer the set from reflection. See DERBY-1151



The AuthenticationService provides a mechanism for authenticating users willing to access JBMS. <p> There can be different and user defined authentication schemes, as long the expected interface here below is implementing and registered as a module when JBMS starts-up. <p>
<p> This is the authentication service base class. </p> <p> There can be 1 Authentication Service for the whole Derby system and/or 1 authentication per database. In a near future, we intend to allow multiple authentication services per system and/or per database. </p> <p> It should be extended by the specialized authentication services. </p> <p><strong>IMPORTANT NOTE:</strong></p> <p> User passwords are hashed using a message digest algorithm if they're stored in the database. They are not hashed if they were defined at the system level. </p> <p> The passwords can be hashed using two different schemes: </p> <ul> <li>The SHA-1 authentication scheme, which was the only available scheme in Derby 10.5 and earlier. This scheme uses the SHA-1 message digest algorithm.</li> <li>The configurable hash authentication scheme, which allows the users to specify which message digest algorithm to use.</li> </ul> <p> In order to use the configurable hash authentication scheme, the users have to set the {@code derby.authentication.builtin.algorithm} property (on system level or database level) to the name of an algorithm that's available in one of the security providers registered on the system. If this property is not set, or if it's set to NULL or an empty string, the SHA-1 authentication scheme is used. </p> <p> Which scheme to use is decided when a password is about to be stored in the database. One database may therefore contain passwords stored using different schemes. In order to determine which scheme to use when comparing a user's credentials with those stored in the database, the stored password is prefixed with an identifier that tells which scheme is being used. Passwords stored using the SHA-1 authentication scheme are prefixed with {@link PasswordHasher#ID_PATTERN_SHA1_SCHEME}. Passwords that are stored using the configurable hash authentication scheme are prefixed with {@link PasswordHasher#ID_PATTERN_CONFIGURABLE_HASH_SCHEME} and suffixed with the name of the message digest algorithm. </p>
The Authorizer verifies a connected user has the authorization to perform a requested database operation using the current connection. <P> Today no object based authorization is supported.
This Stream is a wrapper for PositionedStoreStream to set the position correctly before performing any operation on it. All the read and skip methods ensure that the PositionedStoreStream is set to right position before actually performing these operations. PositionedStoreStream is accessed within synchronized block to ensure exclusive access to it. This class must be constructed while synchronizing on ConnectionChild.getConnectionSynchronization
AutoincrementCounter is a not so general counter for the specific purposes of autoincrement columns. It can be thought of as an in-memory autoincrement column. The counting or incrementing is done in fashion identical to the AUTOINCREMENTVALUE in SYSCOLUMNS. <p> To create a counter, the user must call the constructor with a start value, increment and optionally a final value. In addition the caller must specify the schema name, table name and column name uniquely identifying the counter. <p> When a counter is created it is in an invalid state-- to initialize it, the user must call either  <i>update</i> or <i>reset(false)</i>. The value of a counter can be changed by either calling reset or update.
This is the dummy driver which is registered with the DriverManager and which is autoloaded by JDBC4. Loading this class will NOT automatically boot the Derby engine, but it will register this class as a valid Driver with the DriverManager. Instead, the engine boots lazily when you ask for a Connection. Alternatively, you can force the engine to boot as follows: <PRE> Class.forName("org.apache.derby.jdbc.EmbeddedDriver").newInstance(); // or new org.apache.derby.jdbc.EmbeddedDriver(); </PRE>
The interface of objects which can be associated with a page while it's in cache.
Aggregator for AVG(). Extends the SumAggregator and implements a count. Result is then sum()/count(). To handle overflow we catch the exception for value out of range, then we swap the holder for the current sum to one that can handle a larger range. Eventually a sum may end up in a SQLDecimal which can handle an infinite range. Once this type promotion has happened, it will not revert back to the original type, even if the sum would fit in a lesser type.
<p> Interface describing a table function which can be given information about the context in which it runs. </p>
@derby.formatId ACCESS_B2I_V3_ID @derby.purpose   The tag that describes the on disk representation of the B2I conglomerate object.  Access contains no "directory" of conglomerate information.  In order to bootstrap opening a file it encodes the factory that can open the conglomerate in the conglomerate id itself.  There exists a single B2IFactory which must be able to read all btree format id's. This format was used for all Derby database B2I's in version 10.2 and previous versions. @derby.upgrade   The format id of this object is currently always read from disk as the first field of the conglomerate itself.  A bootstrap problem exists as we don't know the format id of the B2I until we are in the "middle" of reading the B2I.  Thus the base B2I implementation must be able to read and write all formats based on the reading the "format_of_this_conglomerate". soft upgrade to ACCESS_B2I_V4_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default collation id needed by new format.  No code other than readExternal and writeExternal need know about old format. write: will never write out new format id in soft upgrade mode. Code in readExternal and writeExternal handles writing correct version.  Code in the factory handles making sure new conglomerates use the B2I_v10_2 class to that will write out old format info. hard upgrade to ACCESS_B2I_V4_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default collation id needed by new format. write: Only "lazy" upgrade will happen.  New format will only get written for new conglomerate created after the upgrade.  Old conglomerates continue to be handled the same as soft upgrade. @derby.diskLayout format_of_this_conlgomerate(byte[]) containerid(long) segmentid(int) number_of_key_fields(int) number_of_unique_columns(int) allow_duplicates(boolean) maintain_parent_links(boolean) array_of_format_ids(byte[][]) baseConglomerateId(long) rowLocationColumn(int) ascend_column_info(FormatableBitSet) @derby.formatId ACCESS_B2I_V4_ID @derby.purpose   The tag that describes the on disk representation of the B2I conglomerate object.  Access contains no "directory" of conglomerate information.  In order to bootstrap opening a file it encodes the factory that can open the conglomerate in the conglomerate id itself.  There exists a single B2IFactory which must be able to read all btree format id's. This format was used for all Derby database B2I's in version 10.3. @derby.upgrade   The format id of this object is currently always read from disk as the first field of the conglomerate itself.  A bootstrap problem exists as we don't know the format id of the B2I until we are in the "middle" of reading the B2I.  Thus the base B2I implementation must be able to read and write all formats based on the reading the "format_of_this_conglomerate". soft upgrade to ACCESS_B2I_V5_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default isUniqueWithDuplicateNulls value of false. No code other than readExternal and writeExternal need know about old format. write: will never write out new format id in soft upgrade mode. Code in readExternal and writeExternal handles writing correct version.  Code in the factory handles making sure new conglomerates use the B2I_v10_3 class that will write out old format info. hard upgrade to ACCESS_B2I_V5_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default isUniqueWithDuplicateNulls value of false. write: Only "lazy" upgrade will happen.  New format will only get written for new conglomerate created after the upgrade.  Old conglomerates continue to be handled the same as soft upgrade. @derby.diskLayout format_of_this_conlgomerate(byte[]) containerid(long) segmentid(int) number_of_key_fields(int) number_of_unique_columns(int) allow_duplicates(boolean) maintain_parent_links(boolean) array_of_format_ids(byte[][]) baseConglomerateId(long) rowLocationColumn(int) ascend_column_info(FormatableBitSet) collation_ids(compressed array of ints) @derby.formatId ACCESS_B2I_V5_ID @derby.purpose   The tag that describes the on disk representation of the B2I conglomerate object.  Access contains no "directory" of conglomerate information.  In order to bootstrap opening a file it encodes the factory that can open the conglomerate in the conglomerate id itself.  There exists a single B2IFactory which must be able to read all btree format id's. This format is the current version id of B2I and has been used in versions of Derby after the 10.3 release. @derby.upgrade   This is the current version, no upgrade necessary. @derby.diskLayout format_of_this_conlgomerate(byte[]) containerid(long) segmentid(int) number_of_key_fields(int) number_of_unique_columns(int) allow_duplicates(boolean) maintain_parent_links(boolean) array_of_format_ids(byte[][]) baseConglomerateId(long) rowLocationColumn(int) ascend_column_info(FormatableBitSet) collation_ids(compressed array of ints) isUniqueWithDuplicateNulls(boolean) Implements an instance of a B-Tree secondary index conglomerate. A B2I object has two roles. <ol> <li> The B2I object is stored on disk, and holds the store specific information needed to access/describe the conglomerate. This includes information such as the format ids of the columns, the conglomerate id of the base table, the location of row location column. </li> <li> Access to all the interfaces start by making a call off the Conglomerate interface. So for instance to get a scan on the conglomerate method {@link #openScan openScan} should be called. </li> </ol>
Controller used to insert rows into a secondary index. Implements the ConglomerateController interface for the B-Tree index access method. Note most work of this class is inherited from the generic btree implementation.  This class initializes the top level object and deals with locking information specific to a secondary index implementation of a btree.
Controller used to provide cost estimates to optimizer about secondary index data access. Implements the StoreCostController interface for the B-Tree index implementation.  The primary use of this interface is to provide costs used by the query optimizer to use when choosing query plans. Provides costs of things like fetch one row, how many rows in conglomerate, how many rows between these 2 keys. Note most work of this class is inherited from the generic btree implementation.  This class initializes the top level object and deals with locking information specific to a secondary index implementation of a btree.
The "B2I" (acronym for b-tree secondary index) factory manages b-tree conglomerates implemented	on the raw store which are used as secondary indexes. <p> Most of this code is generic to all conglomerates.  This class might be more easily maintained as an abstract class in Raw/Conglomerate/Generic. The concrete ConglomerateFactories would simply have to supply the IMPLEMENTATIONID, FORMATUUIDSTRING, and implement createConglomerate and defaultProperties.  Conglomerates which support more than one format would have to override supportsFormat, and conglomerates which support more than one implementation would have to override supportsImplementation.
The btree secondary index implementation of ScanManager which provides reading and deleting of entries in the btree secondary index. This supports setting up and iterating through a set of rows while providing a start key, stop key, and a set of AND and OR qualifiers to skip unwanted rows.  Currently derby only supports forward scans (but individual columns can have descending order).  This interface is also used to delete rows from the conglomerate.  Note that update is not supported, it must be implemented as a delete, followed by an insert. Note most work of this class is inherited from the generic btree implementation. This class initializes the top level object and deals with locking information specific to a secondary index implementation of a btree.
Scan used to find maximum value in the secondary index. This class implements an optimized interface to find the maximum row, for a set of rows between an input start and stop key. Note most work of this class is inherited from the generic btree implementation. This class initializes the top level object and deals with locking information specific to a secondary index implementation of a btree.
Secondary index locking policy that does no locking. <p> This is used when the caller knows that logical locks are already obtained so need not be requested again.  For instance when inserting a row into an index, a X row lock has already been obtained when the row was inserted into the base table, so there is no need to get another lock in the secondary index. <p> This class overrides all interfaces of BTreeLockingPolicy making them no-ops.
The btree locking policy which implements read uncommitted isolation level. It inherits all functionality from B2IRowLocking2 except that it does not get any read row locks (and thus does not release them).  Note that table level and table level intent locking remains the same as B2IRowLocking2 as this is currently the way we prevent concurrent ddl from happening while a read uncommitted scanner is operating in the btree. ************************************************************************ Abstract Protected lockNonScan*() locking methods of BTree: lockNonScanPreviousRow   - lock the row previous to the current (inherit from B2IRowLocking2, we still get page control locks) - only called by insert. lockNonScanRow           - lock the input row (inherit from B2IRowLocking2, we still get page control locks) - only called by insert. *************************************************************************
The btree locking policy which implements read committed isolation level. It inherits all functionality from B2IRowLockingRR (repeatable read) except that it releases read locks after obtaining them.  It provides a single implementation of unlockScanRecordAfterRead() which releases a read lock after it has been locked and processed.
Implements the jdbc serializable isolation level using row locks. <p> Holds read and write locks until end of transaction. Obtains previous key locks to protect from phantom reads.

This class implements the static compiled information relevant to a btree secondary index.  It is what is returned by B2I.getStaticCompiledOpenConglomInfo(). <p> Currently the only interesting information stored is Conglomerate for this index and the Conglomerate for the base table of this conglomerate.

The B2IUndo interface packages up the routines which the rawstore needs to call to perform logical undo of a record in a B2i.  The rawstore will determine that a page has changed since the record was written, and if it has it will call the findUndo() interface, to find the page where the record exists (as it may have moved). <p> This class must not contain any persistent state, as this class is stored in the log record of the insert/delete.
@derby.formatId ACCESS_B2I_V4_ID @derby.purpose   The tag that describes the on disk representation of the B2I conglomerate object.  Access contains no "directory" of conglomerate information.  In order to bootstrap opening a file it encodes the factory that can open the conglomerate in the conglomerate id itself.  There exists a single B2IFactory which must be able to read all btree format id's. This format was used for all Derby database B2I's in version 10.3. @derby.upgrade   The format id of this object is currently always read from disk as the first field of the conglomerate itself.  A bootstrap problem exists as we don't know the format id of the B2I until we are in the "middle" of reading the B2I.  Thus the base B2I implementation must be able to read and write all formats based on the reading the "format_of_this_conglomerate". soft upgrade to ACCESS_B2I_V5_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default isUniqueWithDuplicateNulls value of false. No code other than readExternal and writeExternal need know about old format. write: will never write out new format id in soft upgrade mode. Code in readExternal and writeExternal handles writing correct version.  Code in the factory handles making sure new conglomerates use the B2I_v10_3 class that will write out old format info. hard upgrade to ACCESS_B2I_V5_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default isUniqueWithDuplicateNulls value of false. write: Only "lazy" upgrade will happen.  New format will only get written for new conglomerate created after the upgrade.  Old conglomerates continue to be handled the same as soft upgrade. @derby.diskLayout format_of_this_conlgomerate(byte[]) containerid(long) segmentid(int) number_of_key_fields(int) number_of_unique_columns(int) allow_duplicates(boolean) maintain_parent_links(boolean) array_of_format_ids(byte[][]) baseConglomerateId(long) rowLocationColumn(int) ascend_column_info(FormatableBitSet) collation_ids(compressed array of ints) Class used to instantiate 10.3 version of the B2I object. This class implements the format of the B2I object as existed in the 10.3 release of Derby.  In subsequent releases the format was enhanced to store the uniqueWithDuplicateNulls attribute of the index. For upgrade purpose all 10.3 and prior versions are assumed to have false for the uniqueWithDuplicateNulls attribute. This class reads and writes the V4 version to/from disk and reads/writes current in-memory version of the data structure.
@derby.formatId ACCESS_B2I_V3_ID @derby.purpose   The tag that describes the on disk representation of the B2I conglomerate object.  Access contains no "directory" of conglomerate information.  In order to bootstrap opening a file it encodes the factory that can open the conglomerate in the conglomerate id itself.  There exists a single B2IFactory which must be able to read all btree format id's. This format was used for all Derby database B2I's in version 10.2 and previous versions. @derby.upgrade   The format id of this object is currently always read from disk as the first field of the conglomerate itself.  A bootstrap problem exists as we don't know the format id of the B2I until we are in the "middle" of reading the B2I.  Thus the base B2I implementation must be able to read and write all formats based on the reading the "format_of_this_conglomerate". soft upgrade to ACCESS_B2I_V4_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default collation id needed by new format.  No code other than readExternal and writeExternal need know about old format. write: will never write out new format id in soft upgrade mode. Code in readExternal and writeExternal handles writing correct version.  Code in the factory handles making sure new conglomerates use the B2I_v10_2 class to that will write out old format info. hard upgrade to ACCESS_B2I_V4_ID: read: old format is readable by current B2I implementation, with automatic in memory creation of default collation id needed by new format. write: Only "lazy" upgrade will happen.  New format will only get written for new conglomerate created after the upgrade.  Old conglomerates continue to be handled the same as soft upgrade. @derby.diskLayout format_of_this_conlgomerate(byte[]) containerid(long) segmentid(int) number_of_key_fields(int) number_of_unique_columns(int) allow_duplicates(boolean) maintain_parent_links(boolean) array_of_format_ids(byte[][]) baseConglomerateId(long) rowLocationColumn(int) ascend_column_info(FormatableBitSet) Class used to instantiate 10.2 version of the B2I object. This class implements the format of the B2I object as existed in the 10.2 and previous releases of Derby.  In subsequent releases the format was enhanced to store the Collation Id of the columns in the index. Collation can be configured on a per column basis to allow for alter sort ordering of each column.  One use of this is to allow a column to be sorted according to language based rules rather than the default numerical ordering of the binary value. For upgrade purpose all columns stored with ACCESS_B2I_V3_ID format are assumed to be USC_BASIC collation id (ie. the default numerical ordering, rather than any alternate collation). This class reads and writes the V3 version to/from disk and reads/writes current in-memory version of the data structure.
ClassBuilder is used to construct a java class's byte array representation. Limitations: No checking for language use violations such as invalid modifiers or duplicate field names. All classes must have a superclass; java.lang.Object must be supplied if there is no superclass. <p> When a class is first created, it has: <ul> <li> a superclass <li> modifiers <li> a name <li> a package <li> no superinterfaces, methods, fields, or constructors <li> an empty static initializer <li> an empty initializer </ul> <p> MethodBuilder implementations are required to supply a way for Generators to give them code.  Most typically, they may have a stream to which the Generator writes the code that is of the type to satisfy what the Generator is writing. <p> BCClass is a ClassBuilder implementation for generating java bytecode directly.
To be able to identify the expressions as belonging to this implementation, and to be able to generate code off of it if so.
<p> <b>Debugging problems with generated classes</b> <p> When the code has been generated incorrectly, all sorts of odd things can go wrong.  This is one recommended approach to finding the problem. <p> First, turn on ByteCodeGenInstr and DumpClassFile. Look for missing files (right now they are consecutively numbered by the activation class builder; later on they won't be, but BytCodeGenInstr dumps messages about the classes it has). Look at the log to make sure that all "GEN starting class/method" messages are paired with a "GEN ending class/method" message. If a file is missing or the pairing is missing, then something went wrong when the system tried to generate the bytecodes. Resort to your favorite debugging tool to step through the faulty statement. <p> If you get class files but the system crashes on you (I had an OS segmentation fault once) or you get funny messages like JDBC Excpetion: ac5 where ac5 is just the name of a generated class, then one of the following is likely: <ul> <li> you are calling INVOKEVIRTUAL when you are supposed to call INVOKEINTERFACE <li> you have an inexact match on a method argument or return type. <li> you are trying to get to a superclass's field using a subclass. </ul> The best way to locate the problem here is to do this (replace ac5.class with the name of your class file): <ol> <li> javap -c -v ac5 &gt; ac5.gp<br> if javap reports "Class not found", and the file ac5.class does exist in the current directory, then the .class file is probably corrupt.  Try running mocha on it to see if that works. The problem will be in the code that generates the entries for the class file -- most likely the ConstantPool is bad, an attribute got created incorrectly, or perhaps the instruction streams are goofed up. <li> java mocha.Decompiler ac5.class<br> if mocha cannot create good java source, then you really need to go back and examine the calls creating the java constructs; a parameter might have been null when it should have, a call to turn an expression into a statement may be missing, or something else may be wrong. <li> mv ac5.mocha ac5.java <li> vi ac5.java ; you will have to fix any new SQLBoolean(1, ...) calls to be new SQLBoolean(true, ...).  Also mocha occasionally messes up other stuff too.  Just iterate on it until it builds or you figure out what is wrong with the generated code. <li> javac ac5.java <li> javap -v -c ac5 &gt; ac5.jp <li> sed '1,$s/#[0-9]* &lt;/# &lt;/' ac5.gp &gt; ac5.gn <li> sed '1,$s/#[0-9]* &lt;/# &lt;/' ac5.jp &gt; ac5.jn<br> These seds are to get rid of constant pool entry numbers, which will be wildly different on the two files. <li> vdiff32 ac5.gn ac5.jn<br> this tool shows you side-by-side diffs.  If you change to the window that interleaves the diffs, you can see the length of the line.  Look for places where there are invokevirtual vs. invokeinterface differences, differences in the class name of a field, differences in the class name of a method parameter or return type.  The generated code will* have some unavoidable differences from the compiled code, such as: <ul> <li> it will have goto's at the end of try blocks rather than return's. <li> it will do a getstatic on a static final field rather than inlining the static final field's value <li> it will have more checkcast's in it, since it doesn't see if the checkcast will always succeed and thus remove it. </ul> Once you find a diff, you need to track down where the call was generated and modify it appropriately: change newMethodCall to newInterfaceMethodCall; add newCastExpression to get a argument into the right type for the parameter; ensure the return type given for the method is its declared return type. </ol>

MethodBuilder is used to piece together a method when building a java class definition. <p> When a method is first created, it has: <ul> <li> a return type <li> modifiers <li> a name <li> an empty parameter list <li> an empty throws list <li> an empty statement block </ul> <p> MethodBuilder implementations are required to supply a way for Statements and Expressions to give them code.  Most typically, they may have a stream to which their contents writes the code that is of the type to satisfy what the contents represent. MethodBuilder implementations also have to have a way to supply ClassBuilders with their code, that satisfies the type of class builder they are implemented with.  This is implementation-dependent, so ClassBuilders, MethodBuilders, Statements, and Expressions all have to be of the same implementation in order to interact to generate a class. <p> Method Builder implementation for generating bytecode.

A method descriptor. Ie. something that describes the type of a method, parameter types and return types. It is not an instance of a method. <BR> This has no generated class specific state.
Sets up a data model with very large BLOBs. The table created will have three fields: 1. a value field (val), which is the value for every byte in the BLOB. 2. a length (length) field which is the actual size of the BLOB 3. the data field (data), which is the actual BLOB data.
A b-tree object corresponds to an instance of a b-tree conglomerate.  It contains the static information about a conglomerate which is built at create conglomerate time. <p> This generic implementation is expected to be extended by the concreate implementations. <P> The fields are set when the conglomerate is created and never changed thereafter.  When alter table is supported then it will change under the control of a table level lock. <p> They have package scope because they're read by the scans and controllers. <p> A table of all conglomerates in the system is maintained by the accessmanager. A cache of conglomerates is maintained in the accessmanager, and references to the read only objects are handed out.  A copy of the Conglomerate object is kept in the control row of the root page, so that during logical undo this information can be read without needing to access the possibly corrupt table maintained by the access manager.
A b-tree controller corresponds to an instance of an open b-tree conglomerate. <P> <B>Concurrency Notes</B> <P> The concurrency rules are derived from OpenBTree. <P>
The StoreCostController interface provides methods that an access client (most likely the system optimizer) can use to get store's estimated cost of various operations on the conglomerate the StoreCostController was opened for. <p> It is likely that the implementation of StoreCostController will open the conglomerate and will leave the conglomerate open until the StoreCostController is closed.  This represents a significant amount of work, so the caller if possible should attempt to open the StoreCostController once per unit of work and rather than close and reopen the controller.  For instance if the optimizer needs to cost 2 different scans against a single conglomerate, it should use one instance of the StoreCostController. <p> The locking behavior of the implementation of a StoreCostController is undefined, it may or may not get locks on the underlying conglomerate.  It may or may not hold locks until end of transaction. An optimal implementation will not get any locks on the underlying conglomerate, thus allowing concurrent access to the table by a executing query while another query is optimizing. <p>
A b-tree scan controller corresponds to an instance of an open b-tree scan. <P> <B>Concurrency Notes</B> <P> The concurrency rules are derived from OpenBTree. <P>
The generic.BTree directory wants to know as little about locking as possible, in order to make the code usuable by multiple implementations.  But the generic code will make calls to abstract lock calls implemented by concrete btree implementations.  Concrete implementations like B2I understand locking, and needs informatation specific to the implementation to make the lock calls. <p> This class is created and owned by the concrete application, but is passed into and returned from the generic code when lock calls are made. Concrete implementations which do not need lock calls can just pass a null pointer where a BTreeLockingPolicy is requested. <p> There are 2 types of lock interfaces, lockScan*() and lockNonScan*(). <p> The lockScan*() interfaces save the key for the current scan position before giving up the latch on the page if they have to wait for a row lock. The callers can reposition the scan using the saved key by calling {@code BTreeScan.reposition()} if such a situation occurs. Then the latches are reobtained, possibly on a different page if the current key has been moved to another page in the meantime. Upon return from these interfaces the row lock requested is guaranteed to have been obtained on the correct key for the row requested.  These interfaces handle the special case of unique indexes where the RowLocation can change while waiting on the lock (see implementation for details), basically the lock is retryed after waiting if the RowLocation has changed. <p> The lockNonScan*() interfaces do not save the current scan position. If these routines return that the latch was released while waiting to obtain the lock, then the caller must requeue the lock request after taking appropriate action.  This action usually involves researching the tree to make sure that the correct key is locked with latches held. These interfaces do not handle the special case of unique indexes where the RowLocation can change while waiting on the lock, as the row may disappear when the latch is released to wait on the lock - thus it is necessary that the caller retry the lock if the interface returns that the latch was released.
A b-tree scan controller corresponds to an instance of an open b-tree scan. <P> <B>Concurrency Notes</B> <P> The concurrency rules are derived from OpenBTree. <P> A BTreeScan implementation that provides the 95% solution to the max on btree problem.  If the row is the last row in the btree it works very efficiently.  This implementation will be removed once backward scan is fully functional. The current implementation only exports to the user the ability to call fetchMax() and get back one row, none of the generic scan ablities are exported. To return the maximum row this implementation does the following: 1) calls positionAtStartPosition() which returns with the a latch on the rightmost leaf page and a lock on the rightmost leaf row on that page. It will loop until it can get the lock without waiting while holding the latch.  At this point the slot position is just right of the locked row. 2) in fetchMax() it loops backward on the last leaf page, locking rows as it does so, until it finds the first non-deleted, non-NULL row. 3) If it is not successful in this last page search it attempts to latch the left sibling page, without waiting to avoid deadlocks with forward scans, and continue the search on that page. 4) If the sibling page couldn't be latched without waiting, save the current position, release all latches, and restart the scan from the saved position.
The BTreePostCommit class implements the Serviceable protocol. In it's role as a Serviceable object, it stores the state necessary to find a page in a btree that may have committed delete's to reclaim. In it's role as a PostCommitProcessor it looks up the page described, and reclaims space in the btree.  It first trys to clean up any deleted commits on the page.  It then will shrink the tree if it is going to delete all rows from the page (RESOLVE - not done yet).
************************************************************************ Public Methods of XXXX class: *************************************************************************
A b-tree scan controller corresponds to an instance of an open b-tree scan. <P> <B>Concurrency Notes</B> <P> The concurrency rules are derived from OpenBTree. <P>
This object provides performance information related to an open scan. The information is accumulated during operations on a ScanController() and then copied into this object and returned by a call to ScanController.getStatistic().
Load generator which creates back-to-back load. This means that you have a number of threads running in parallel, where each thread continuously performs operations with no pauses in between.
A background cleaner that {@code ConcurrentCache} can use to clean {@code Cacheable}s asynchronously in a background instead of synchronously in the user threads. It is normally used by the replacement algorithm in order to make dirty {@code Cacheable}s clean and evictable in the future. When the background cleaner is asked to clean an item, it puts the item in a queue and requests to be serviced by a <code>DaemonService</code> running in a separate thread.


Extend BackingStoreHashtable with the ability to maintain the underlying openScan() until the hashtable has been closed.  This is necessary for long row access.  Access to long row delays actual objectification until the columns are accessed, but depends on the underlying table to be still open when the column is accessed. <P> Transactions are obtained from an AccessFactory. ************************************************************************ Public Methods of XXXX class: *************************************************************************
<p> A BackingStoreHashtable is a utility class which will store a set of rows into an in memory hash table, or overflow the hash table to a tempory on disk structure. </p> <p> All rows must contain the same number of columns, and the column at position N of all the rows must have the same format id.  If the BackingStoreHashtable needs to be overflowed to disk, then an arbitrary row will be chosen and used as a template for creating the underlying overflow container. </p> <p> The hash table will be built logically as follows (actual implementation may differ).  The important points are that the hash value is the standard java hash value on the row[key_column_numbers[0], if key_column_numbers.length is 1, or row[key_column_numbers[0, 1, ...]] if key_column_numbers.length &gt; 1, and that duplicate detection is done by the standard java duplicate detection provided by java.util.Hashtable. </p> <pre> import java.util.Hashtable; hash_table = new Hashtable(); Object row; // is a DataValueDescriptor[] or a LocatedRow boolean  needsToClone = rowSource.needsToClone(); while((row = rowSource.getNextRowFromRowSource()) != null) { if (needsToClone) row = clone_row_from_row(row); Object key = KeyHasher.buildHashKey(row, key_column_numbers); if ((duplicate_value = hash_table.put(key, row)) != null) { Vector row_vec; // inserted a duplicate if ((duplicate_value instanceof vector)) { row_vec = (Vector) duplicate_value; } else { // allocate vector to hold duplicates row_vec = new Vector(2); // insert original row into vector row_vec.addElement(duplicate_value); // put the vector as the data rather than the row hash_table.put(key, row_vec); } // insert new row into vector row_vec.addElement(row); } } </pre> <p> What actually goes into the hash table is a little complicated. That is because the row may either be an array of column values (i.e. DataValueDescriptor[]) or a LocatedRow (i.e., a structure holding the columns plus a RowLocation). In addition, the hash value itself may either be one of these rows or (in the case of multiple rows which hash to the same value) a bucket (List) of rows. To sum this up, the values in a hash table which does not spill to disk may be the following: </p> <ul> <li>DataValueDescriptor[] and ArrayList<DataValueDescriptor></li> <li>or LocatedRow and ArrayList<LocatedRow></li> </ul> <p> If rows spill to disk, then they just become arrays of columns. In this case, a LocatedRow becomes a DataValueDescriptor[], where the last cell contains the RowLocation. </p>

This class creates and populates tables that can be used by the bank transactions test clients. It attempts to create tables that follow the rules defined by the TPC-B benchmark specification.
This class implements a client thread which performs bank transactions. The transactions are intended to perform the same operations as the transactions specified by the TPC-B benchmark.
In the absence of java.util.concurrent.CyclicBarrier on some of the platforms we test, create our own barrier class. This class allows threads to wait for one another on specific locations, so that they know they're all in the expected state.
BaseActivation provides the fundamental support we expect all activations to have. Doesn't actually implement any of the activation interface, expects the subclasses to do that.
A BaseColumnNode represents a column in a base table.  The parser generates a BaseColumnNode for each column reference.  A column refercence could be a column in a base table, a column in a view (which could expand into a complex expression), or a column in a subquery in the FROM clause.  By the time we get to code generation, all BaseColumnNodes should stand only for columns in base tables.
BaseContainer is an abstract class that provides the locking bahaviour for an object representing an active container, that is the actual storage container, not the ContainerHandle interface. This class is designed so that it can change the container it represents to avoid creating a new object for every container. <P> This object implements lockable to provide an object to lock while a page is being allocated. <BR> MT - Mutable - mutable identity :
A handle to an open container, implememts RawContainerHandle. <P> This class is a DerbyObserver to observe RawTransactions and is also a DerbyObservable to handle the list of pages accessed thorough this handle. <BR> This class implements Lockable (defined to be ContainerHandle) and is the object used to logically lock the container. <BR> MT - Mutable - Immutable identity - Thread Aware
Provides the abstract class with most of the implementation of DataFactory and ModuleControl shared by all the different filesystem implementations. <p> RESOLVE (mikem - 2/19/98) - Currently only getContainerClass() is abstract, there are probably more routines which should be abstract.  Also the other implementations should probably inherit from the abstract class, rather than from the DataFileFactory class.  Also there probably should be a generic directory and the rest of the filesystem implementations parallel to it. I wanted to limit the changes going into the branch and then fix inheritance stuff in main. <p> The code in this class was moved over from DataFileFactory.java and then that file was made to inherit from this one.
This class overloads BaseDataFileFactory to produce RAFContainer4 objects instead of RAFContainer objects. It makes no other change to its superclass' behavior.
BaseExpressionActivation Support needed by Expression evaluators (Filters) and by ResultSet materializers (Activations)

Implementation of the monitor that uses the class loader that the its was loaded in for all class loading. end of class BaseMonitor
This class implements all the the generic locking behaviour for a Page. It leaves method used to log and store the records up to sub-classes. It is intended that the object can represent multiple pages from different containers during its lifetime. <P> A page contains a set of records, which can be accessed by "slot", which defines the order of the records on the page, or by "id" which defines the identity of the records on the page.  Clients access records by both slot and id, depending on their needs. <P> BasePage implements Observer to watch the ContainerHandle which notifies its Observers when it is closing. <BR> MT - mutable
This class provides a base for implementations of the StorageFactory interface. It is used by the database engine to access persistent data and transaction logs under the directory (default) subsubprotocol.
Walk through a subtree and build a list of the assigned numbers for all tables that exist in that subtree.  We do this by looking for any column references in the subtree and, for each column reference, we walk down the ColumnReference-ResultColumn chain until we find the the bottom-most table number, which should correspond to a base table.
This is the base implementation of TypeCompiler
This class is the base class for all type ids that are written to the system tables.
This authentication service is the basic Derby user authentication level support. It is activated upon setting derby.authentication.provider database or system property to 'BUILTIN'. <p> It instantiates and calls the basic User authentication scheme at runtime. <p> In 2.0, users can now be defined as database properties. If derby.database.propertiesOnly is set to true, then in this case, only users defined as database properties for the current database will be considered.
This data source is suitable for client/server use of Derby, running on Java 8 Compact Profile 2 or higher. <p/> BasicClientConnectionPoolDataSource40 is similar to ClientConnectionPoolDataSource except that it does not support JNDI, i.e. it does not implement {@code javax.naming.Referenceable}.
This data source is suitable for client/server use of Derby, running on Java 8 Compact Profile 2 or higher. <p/> BasicClientDataSource40 is similar to ClientDataSource except it can not be used with JNDI, i.e. it does not implement {@code javax.naming.Referenceable}. <p/> * The standard attributes provided are, cf. e.g. table 9.1 in the JDBC 4.2 specification. <ul> <li>databaseName</li> <li>dataSourceName</li> <li>description</li> <li>password</li> <li>user</li> </ul> These standard attributes are not supported: <ul> <li>networkProtocol</li> <li>roleName</li> </ul> The Derby client driver also supports these attributes: <ul> <li>loginTimeout</li> @see javax.sql.CommonDataSource set/get <li>logWriter</li> @see javax.sql.CommonDataSource set/get <li>createDatabase</li> <li>connectionAttributes</li> <li>shutdownDatabase</li> <li>attributesAsPassword</li> <li>retrieveMessageText</li> <li>securityMechanism</li> <li>traceDirectory</li> <li>traceFile</li> <li>traceFileAppend</li> <li>traceLevel<li> </ul>
This data source is suitable for client/server use of Derby, running on Java 8 Compact Profile 2 or higher. <p/> Similar to ClientXADataSource except it does not support JNDI, i.e. it does not implement {@code javax.naming.Referenceable}. compile-time check of 41 extensions
A BasicDaemon is a background worker thread which does asynchronous I/O and general clean up.  It should not be used as a general worker thread for parallel execution. One cannot count on the order of request or count on when the daemon will wake up, even with serviceNow requests.  Request are not persistent and not recoverable, they are all lost when the system crashes or is shutdown. System shutdown, even orderly ones, do not wait for daemons to finish its work or empty its queue.  Furthermore, any Serviceable subscriptions, including onDemandOnly, must tolerate spurious services.  The BasicDaemon will setup a context manager with no context on it.  The Serviceable object's performWork must provide useful context on the context manager to do its work.  The BasicDaemon will wrap performWork call with try / catch block and will use the ContextManager's error handling to clean up any error.  The BasicDaemon will guarentee serviceNow request will not be lost as long as the jbms does not crash - however, if N serviceNow requests are made by the same client, it may only be serviced once, not N times. Many Serviceable object will subscribe to the same BasicDaemon.  Their performWork method should be well behaved - in other words, it should not take too long or hog too many resources or deadlock with anyone else.  And it cannot (should not) error out. The BasicDaemon implementation manages the DaemonService's data structure, handles subscriptions and enqueues requests, and determine the service schedule for its Serviceable objects.  The BasicDaemon keeps an array (Vector) of Serviceable subscriptions it also keeps 2 queues for clients that uses it for one time service - the 1st queue is for a serviceNow enqueue request, the 2nd queue is for non serviceNow enqueue request. This BasicDaemon services its clients in the following order: 1. any subscribed client that have made a serviceNow request that has not been fulfilled 2. serviceable clients on the 1st queue 3. all subscribed clients that are not onDemandOnly 4. serviceable clients 2nd queue
The Database interface provides control over the physical database (that is, the stored data and the files the data are stored in), connections to the database, operations on the database such as backup and recovery, and all other things that are associated with the database itself. <p> The Database interface does not provide control over things that are part of the Domain, such as users. <p> I'm not sure what this will hold in a real system, for now it simply provides connection-creation for us.  Perhaps when it boots, it creates the datadictionary object for the database, which all users will then interact with?
A dependency represents a reliance of the dependent on the provider for some information the dependent contains or uses.  In Language, the usual case is a prepared statement using information about a schema object in its executable form. It needs to be notified if the schema object changes, so that it can recompile against the new information.
The dependency manager tracks needs that dependents have of providers. <p> A dependency can be either persistent or non-persistent. Persistent dependencies are stored in the data dictionary, and non-persistent dependencies are stored within the dependency manager itself (in memory). <p> <em>Synchronization:</em> The need for synchronization is different depending on whether the dependency is an in-memory dependency or a stored dependency. When accessing and modifying in-memory dependencies, Java synchronization must be used (specifically, we synchronize on {@code this}). When accessing and modifying stored dependencies, which are stored in the data dictionary, we expect that the locking protocols will provide the synchronization needed. Note that stored dependencies should not be accessed while holding the monitor of {@code this}, as this may result in deadlocks. So far the need for synchronization across both in-memory and stored dependencies hasn't occurred.
This data source is suitable for an application using embedded Derby, running on Java 8 Compact Profile 2 or higher. <p/> BasicEmbeddedConnectionPoolDataSource40 is similar to EmbeddedConnectionPoolDataSource40 except it does not support JNDI naming, i.e. it does not implement {@code javax.naming.Referenceable}.
This data source is suitable for an application using embedded Derby, running on Java 8 Compact Profile 2 or higher. <p/> BasicEmbeddedDataSource40 is similar to EmbeddedDataSource, but does not support JNDI naming, i.e. it does not implement {@code javax.naming.Referenceable}. <p/> The standard attributes provided are, cf. e.g. table 9.1 in the JDBC 4.2 specification. <ul> <li>databaseName</li> <li>dataSourceName</li> <li>description</li> <li>password</li> <li>user</li> </ul> These standard attributes are not supported: <ul> <li>networkProtocol</li> <li>portNumber</li> <li>roleName</li> <li>serverName</li> </ul> The embedded Derby driver also supports these attributes: <ul> <li>loginTimeout</li> @see javax.sql.CommonDataSource set/get <li>logWriter</li> @see javax.sql.CommonDataSource set/get <li>createDatabase</li> <li>connectionAttributes</li> <li>shutdownDatabase</li> <li>attributesAsPassword</li> </ul> <br> See the specific Derby DataSource implementation for details on their meaning. <p/> See also the JDBC specifications for more details.
This data source is suitable for an application using embedded Derby, running on Java 8 Compact Profile 2 or higher. <p/> BasicEmbeddedXADataSource40 is similar to EmbeddedXADataSource40, except that it does not support JNDI naming, i.e. it does not implement {@code javax.naming.Referenceable}. compile time check of 41 extensions
Get a header to prepend to a line of output. * A HeaderPrintWriter requires an object which implements this interface to construct line headers.
Basic class to print lines with headers. <p> STUB: Should include code to emit a new line before a header which is not the first thing on the line.
Abstract ResultSet for for operations that return rows but do not allow the caller to put data on output pipes. This basic implementation does not include support for an Activiation. See NoPutResultSetImpl.java for an implementaion with support for an activiation. <p> This abstract class does not define the entire ResultSet interface, but leaves the 'get' half of the interface for subtypes to implement. It is package-visible only, with its methods being public for exposure by its subtypes. <p>
This is the implementation of ProviderInfo in the DependencyManager.
Basic fixtures and setup for the upgrade test, not tied to any specific release.
This is the most basic sort observer.  It handles distinct sorts and non-distinct sorts.

A hack implementation of something similar to a DCE UUID generator.  Generates unique 128-bit numbers based on the current machine's internet address, the current time, and a sequence number.  This implementation should be made to conform to the DCE specification. ("DEC/HP, Network Computing Architecture, Remote Procedure Call Runtime Extensions Specification, version OSF TX1.0.11," Steven Miller, July 23, 1992.  This is part of the OSF DCE Documentation. Chapter 10 describes the UUID generation algorithm.) <P> Some known deficiencies: <ul> <li> Rather than using the 48-bit hardware network address, it uses the 32-bit IP address. IP addresses are not guaranteed to be unique. <li> There is no provision for generating a suitably unique number if no IP address is available. <li> Two processes running on this machine which start their respective UUID services within a millisecond of one another may generate duplicate UUIDS. </ul> <P> However, the intention is that UUIDs generated from this class will be unique with respect to UUIDs generated by other DCE UUID generators.

Test that the two new encryption properties DATA_ENCRYPT_ALGORITHM_VERSION="data_encrypt_algorithm_version" LOG_ENCRYPT_ALGORITHM_VERSION="log_encrypt_algorithm_version" exist and verify the version. Note, these values start off with 1.
This operation indicates the beginning of a transaction.
A BetweenOperatorNode represents a BETWEEN clause. The between values are represented as a 2 element list in order to take advantage of code reuse.
BigDecimalHandler provides wrappers for JDBC API methods which use BigDecimal. When writing tests which use BigDecimal, the methods in this class can be called instead of directly calling JDBC methods. This way the same test can be used in JVMs like J2ME/CDC/Foundation Profile 1.0, which do not have BigDecimal class, or JSR169 Profile, which does not support method calls using BigDecimal (such as ResultSet.getBigDecimal(..).
This node represents a binary arithmetic operator, like + or *.
This node is the superclass  for all binary comparison operators, such as =, &lt;&gt;, &lt;, etc.
A BinaryListOperatorNode represents a built-in "binary" operator with a single operand on the left of the operator and a list of operands on the right. This covers operators such as IN and BETWEEN.

A BinaryOperatorNode represents a built-in binary operator as defined by the ANSI/ISO SQL standard.  This covers operators like +, -, *, /, =, &lt;, etc. Java operators are not represented here: the JSQL language allows Java methods to be called from expressions, but not Java operators.
The Orderable interface represents a value that can be linearly ordered. <P> Currently only supports linear (&lt;, =, &lt;=) operations. Eventually we may want to do other types of orderings, in which case there would probably be a number of interfaces for each "class" of ordering. <P> The implementation must handle the comparison of null values.  This may require some changes to the interface, since (at least in some contexts) comparing a value with null should return unknown instead of true or false.
The BinaryOrderableWrapper is a wrapper class which intercepts the readExternal() callback made by raw store during a fetch, and does a comparison instead.
This class represents the 6 binary operators: LessThan, LessThanEquals, Equals, NotEquals, GreaterThan and GreaterThanEquals.
Converts a stream containing the Derby stored binary form to one that just contains the application's data. Simply read and save the length information.

The BitDataValue interface corresponds to a SQL BIT
This class implements TypeCompiler for the SQL BIT datatype.
This class provides utility methods for converting byte arrays to hexidecimal Strings and manipulating BIT/BIT VARYING values as a packed vector of booleans. <P> The BIT/BIT VARYING methods are modeled after some methods in the <I>java.util.BitSet</I> class. An alternative to using a SQL BIT (VARYING) column in conjunction with the methods provided herein to provide bit manipulation would be to use a serialized <I>java.util.BitSet</I> column instead. <p> This class contains the following static methods: <UL> <LI> void <B>set</B>(byte[] bytes, int position) to set a bit</LI> <LI> void <B>clear</B>(byte[] bytes, int position) to clear a bit</LI> <LI> boolean <B>get</B>(byte[] bytes, int position) to get the bit status </LI> </UL> <p> Since these methods effectively allow a SQL BIT to be considered as an array of booleans, all offsets (position parameters) are zero based.  So if you want to set the first bit of a BIT type, you would use <I> set(MyBitColumn, 0) </I>. <p> Examples: <UL> <LI> SELECT BitUtil::get(bitcol, 2) FROM mytab </LI> <LI> UPDATE mytab SET bitcol = BitUtil::set(bitcol, 2)  </LI> <LI> UPDATE mytab SET bitcol = BitUtil::clear(bitcol, 2)  </LI> </UL>
An <code>InputStream</code> that will use an locator to fetch the Blob value from the server. <p> Closing a <code>ByteArrayInputStream</code> has no effect. The methods in this class can be called after the stream has been closed without generating an <code>IOException</code>. <p> This <code>InputStream</code> implementation is pretty basic.  No buffering of data is done.  Hence, for efficiency #read(byte[]) should be used instead of #read().  Marks are not supported, but it should be pretty simple to extend the implementation to support this.  A more efficient skip implementation should also be straight-forward.
An <code>OutputStream</code> that will use an locator to write bytes to the Blob value on the server. <p> Closing a <code>BlobLocatorOutputStream</code> has no effect. The methods in this class can be called after the stream has been closed without generating an <code>IOException</code>. <p> This <code>OutputStream</code> implementation is pretty basic.  No buffering of data is done.  Hence, for efficiency #write(byte[]) should be used instead of #write(int).

Stores data in blocks, and supports reading/writing data from/into these blocks. <p> The blocked array is expanded and shrunk as required.
An input stream reading from a blocked byte array.
Output stream writing bytes into an underlying blocked byte array.


This class implements TypeCompiler for the SQL BOOLEAN datatype.
Create and boot the supplied db argument. This auxiliary program is used by {@code BootLockTest.java} to boot a db in a different jvm and subsequently attempt a boot to from the original VM to detect dual boot attempt. <p/> Started as: {@code java org.apache.derbyTesting.functionTests.tests.store.BootLockMinion <dbname> <port>}
A branch row contains key fields and the pointer to the child page.
Implements row which is stored in the branch pages of a btree.  A non-suffix compressed branch row contains all of the columns of the leaf rows of a btree and contains an additional field at the end.  The extra field of a branch row in a branch page at level N, is the child page field pointing the page at level N-1 which has keys which follow or equal the branch row entry. There are 3 ways to use this class to produce a branch row: createEmptyTemplate() creates a empty row template createBranchRowFromOldBranchRow() creates a new row with reference to an old branch row. createBranchRowFromOldLeafRow() creates a new row with reference to an old leaf row.
Brokered CallableStatement. This class implements the JDBC 4.1 interface.

This is a rudimentary connection that delegates EVERYTHING to Connection.

Provides control over a BrokeredConnection
A brokered {@code PreparedStatement} that forwards calls off to a real {@code PreparedStatement} obtained through the {@link BrokeredStatementControl#getRealPreparedStatement} method. This class implements the JDBC 4.1 interface.

A Statement implementation that forwards all of its requests to an underlying Statement. This class implements the JDBC 4.1 interface.
Provides control over a BrokeredStatement, BrokeredPreparedStatement or BrokeredCallableStatement


Thin wrapper around a sequence generator to support the bulk-insert optimization used by InsertResultSet.
Read a base table or index in bulk.  Most of the work for this method is inherited from TableScanResultSet. This class overrides getNextRowCore (and extends re/openCore) to use a row array and fetch rows from the Store in bulk  (using fetchNextGroup). <p> Since it retrieves rows in bulk, locking is not as is usual -- locks may have already been released on rows as they are returned to the user.  Hence, this ResultSet is not suitable for a query running Isolation Level 1, cursor stability. <p> Note that this code is only accessable from an optimizer override.  If it makes sense to have the optimizer select bulk reads, then this should probably be rolled into TableScanResultSet.
An installed bundle in the Framework. <p> A <code>Bundle</code> object is the access point to define the lifecycle of an installed bundle. Each bundle installed in the OSGi environment must have an associated <code>Bundle</code> object. <p> A bundle must have a unique identity, a <code>long</code>, chosen by the Framework. This identity must not change during the lifecycle of a bundle, even when the bundle is updated. Uninstalling and then reinstalling the bundle must create a new unique identity. <p> A bundle can be in one of six states: <ul> <li>{@link #UNINSTALLED} <li>{@link #INSTALLED} <li>{@link #RESOLVED} <li>{@link #STARTING} <li>{@link #STOPPING} <li>{@link #ACTIVE} </ul> <p> Values assigned to these states have no specified ordering; they represent bit values that may be ORed together to determine if a bundle is in one of the valid states. <p> A bundle should only execute code when its state is one of <code>STARTING</code>,<code>ACTIVE</code>, or <code>STOPPING</code>. An <code>UNINSTALLED</code> bundle can not be set to another state; it is a zombie and can only be reached because references are kept somewhere. <p> The Framework is the only entity that is allowed to create <code>Bundle</code> objects, and these objects are only valid within the Framework that created them.
Customizes the starting and stopping of a bundle. <p> <code>BundleActivator</code> is an interface that may be implemented when a bundle is started or stopped. The Framework can create instances of a bundle's <code>BundleActivator</code> as required. If an instance's <code>BundleActivator.start</code> method executes successfully, it is guaranteed that the same instance's <code>BundleActivator.stop</code> method will be called when the bundle is to be stopped. The Framework must not concurrently call a <code>BundleActivator</code> object. <p> <code>BundleActivator</code> is specified through the <code>Bundle-Activator</code> Manifest header. A bundle can only specify a single <code>BundleActivator</code> in the Manifest file. Fragment bundles must not have a <code>BundleActivator</code>. The form of the Manifest header is: <p> <code>Bundle-Activator: <i>class-name</i></code> <p> where <code><i>class-name</i></code> is a fully qualified Java classname. <p> The specified <code>BundleActivator</code> class must have a public constructor that takes no parameters so that a <code>BundleActivator</code> object can be created by <code>Class.newInstance()</code>.
A bundle's execution context within the Framework. The context is used to grant access to other methods so that this bundle can interact with the Framework. <p> <code>BundleContext</code> methods allow a bundle to: <ul> <li>Subscribe to events published by the Framework. <li>Register service objects with the Framework service registry. <li>Retrieve <code>ServiceReferences</code> from the Framework service registry. <li>Get and release service objects for a referenced service. <li>Install new bundles in the Framework. <li>Get the list of bundles installed in the Framework. <li>Get the {@link Bundle} object for a bundle. <li>Create <code>File</code> objects for files in a persistent storage area provided for the bundle by the Framework. </ul> <p> A <code>BundleContext</code> object will be created and provided to the bundle associated with this context when it is started using the {@link BundleActivator#start} method. The same <code>BundleContext</code> object will be passed to the bundle associated with this context when it is stopped using the {@link BundleActivator#stop} method. A <code>BundleContext</code> object is generally for the private use of its associated bundle and is not meant to be shared with other bundles in the OSGi environment. <p> The <code>Bundle</code> object associated with a <code>BundleContext</code> object is called the <em>context bundle</em>. <p> The <code>BundleContext</code> object is only valid during the execution of its context bundle; that is, during the period from when the context bundle is in the <code>STARTING</code>, <code>STOPPING</code>, and <code>ACTIVE</code> bundle states. If the <code>BundleContext</code> object is used subsequently, an <code>IllegalStateException</code> must be thrown. The <code>BundleContext</code> object must never be reused after its context bundle is stopped. <p> The Framework is the only entity that can create <code>BundleContext</code> objects and they are only valid within the Framework that created them.
An event from the Framework describing a bundle lifecycle change. <p> <code>BundleEvent</code> objects are delivered to <code>SynchronousBundleListener</code>s and <code>BundleListener</code>s when a change occurs in a bundle's lifecycle. A type code is used to identify the event type for future extendability. <p> OSGi Alliance reserves the right to extend the set of types.
A Framework exception used to indicate that a bundle lifecycle problem occurred. <p> <code>BundleException</code> object is created by the Framework to denote an exception condition in the lifecycle of a bundle. <code>BundleException</code>s should not be created by bundle developers. <p> This exception is updated to conform to the general purpose exception chaining mechanism.

A <code>BundleEvent</code> listener. <code>BundleListener</code> is a listener interface that may be implemented by a bundle developer. When a <code>BundleEvent</code> is fired, it is asynchronously delivered to a <code>BundleListener</code>. The Framework delivers <code>BundleEvent</code> objects to a <code>BundleListener</code> in order and must not concurrently call a <code>BundleListener</code>. <p> A <code>BundleListener</code> object is registered with the Framework using the {@link BundleContext#addBundleListener} method. <code>BundleListener</code>s are called with a <code>BundleEvent</code> object when a bundle has been installed, resolved, started, stopped, updated, unresolved, or uninstalled.
A bundle's authority to require or provide a bundle or to receive or attach fragments. <p> A bundle symbolic name defines a unique fully qualified name. <p> For example: <pre> <code> org.osgi.example.bundle </code> </pre> <p> <code>BundlePermission</code> has four actions: <code>PROVIDE</code>, <code>REQUIRE</code>,<code>HOST</code>, and <code>FRAGMENT</code>. The <code>PROVIDE</code> action implies the <code>REQUIRE</code> action. Stores a set of <code>BundlePermission</code> permissions.
A looping alphabet, returning bytes in a specified encoding. The alphabet loops over a list of bytes representing characters. The alphabet-object is used by looping stream, which in turn is used for testing methods requiring streaming inputs. The following alphabets have been defined: <ul><li><em>Modern latin, lowercase</em> ; letters a - z (26) <li><em>Norwegian/Danish, lowercase</em> ; letters a - z, plus three additional letters (29) <li><em>Tamil</em> ; 46 Tamil letters from UNICODE U0B80 <li><em>CJK subset</em> ; 12 letter from UNICODE CJK U4E00 </ul> End class ByteAlphabet
ByteArray wraps java byte arrays (byte[]) to allow byte arrays to be used as keys in hashtables. This is required because the equals function on byte[] directly uses reference equality. <P> This class also allows the trio of array, offset and length to be carried around as a single object.
A stream whose source is a list of byte arrays. This class was created when first implementing the JDBC 4 length less overloads in the client driver. The reason was missing support for streaming data with unknown length from the client to the server. The purpose of the stream is to avoid having to repeatedly copy data to grow the byte buffer, or doing a single big copy to combine the byte arrays in the end. This is important for the temporary solution, since we must materialize the stream to find the length anyway. If there is less data available than the specified length, an exception is thrown. Available data is determined by the length of the byte arrays, not the contents of them. A byte array with all 0's is considered valid data. Besides from truncation, this stream does not change the underlying data in any way. End of class ByteArrayCombinerStream
Holder for a growing sequence of bytes. The ByteHolder supports a writing phase in which a caller appends bytes to the ByteHolder. Later the caller may read the bytes out of the ByteHolder in the order they were written.

This class implements TypeCompiler for the SQL LOB types.
Double Constant - page 97 - Section 4.4.5
Float Constant - page 96
A generic constant pool entry for entries that simply hold indexes into other entries. <BR> Ref Constant Pool Entry  - page 94 - Section 4.4.2	- Two indexes <BR> NameAndType Constant Pool Entry  - page 99 - Section 4.4.6 - Two indexes <BR> String Constant Pool Entry - page 96 - Section 4.4.3 - One index <BR> Class Reference Constant Pool Entry - page 93 - Section 4.4.1 - One index
Integer Constant - page 96
Long Constant - page 97 - Section 4.4.5
Constant Pool class - pages 92-99 Utf8- page 100 - Section 4.4.7
This class provides a class path based implementation of the StorageFile interface. It is used by the database engine to access persistent data and transaction logs under the classpath subsubprotocol.
This class provides a class path based implementation of the StorageFactory interface. It is used by the database engine to access persistent data and transaction logs under the classpath subsubprotocol.
Constants for the LockFactory
The purpose of this interface is to hold the constant definitions of the different node type identifiers, for use with NodeFactory. The reason this class exists is that it is not shipped with the product, so it saves footprint to have all these constant definitions here instead of in NodeFactory.
Class representing an entry in the cache. It is used by <code>ConcurrentCache</code>. When a thread invokes any of the methods in this class, except <code>lock()</code>, it must first have called <code>lock()</code> to ensure exclusive access to the entry. <p> When no thread holds the lock on the entry, it must be in one of the following states: <dl> <dt>Uninitialized</dt> <dd>The entry object has just been constructed, but has not yet been initialized. In this state, <code>isValid()</code> returns <code>false</code>, whereas <code>isKept()</code> returns <code>true</code> in order to prevent removal of the entry until it has been initialized. When the entry is in this state, calls to <code>lockWhenIdentityIsSet()</code> will block until <code>settingIdentityComplete()</code> has been called.</dd> <dt>Unkept</dt> <dd>In this state, the entry object contains a reference to a <code>Cacheable</code> and the keep count is zero. <code>isValid()</code> returns <code>true</code> and <code>isKept()</code> returns <code>false</code> in this state. <code>getCacheable()</code> returns a non-null value.<dd> <dt>Kept</dt> <dd>Same as the unkept state, except that the keep count is positive and <code>isKept()</code> returns <code>true</code>.</dd> <dt>Removed</dt> <dd>The entry has been removed from the cache. In this state, <code>isValid()</code> and <code>isKept()</code> return <code>false</code>, and <code>getCacheable()</code> returns <code>null</code>. When an entry has entered the removed state, it cannot be transitioned back to any of the other states.</dd> </dl> <p> To prevent deadlocks, each thread should normally lock only one entry at a time. In some cases it is legitimate to hold the lock on two entries, for instance if an entry must be evicted to make room for a new entry. If this is the case, exactly one of the two entries must be in the uninitialized state, and the uninitialized entry must be locked before the lock on the other entry can be requested.
A factory for handing out caches.

This is an MBean that provides information about one of Derby's cache managers.
Any object that implements this interface can be cached using the services of the CacheManager/CacheFactory. In addition to implementing this interface the class must be public and it must have a public no-arg constructor. This is because the cache manager will construct objects itself and then set their identity by calling the setIdentity method. <P> A Cacheable object has five states: <OL> <OL> <LI> No identity - The object is only accessable by the cache manager <LI> Identity, clean, unkept - The object has an identity, is clean but is only accessable by the cache manager <LI> Identity, clean, kept - The object has an identity, is clean, and is in use by one or more threads <LI> Identity, kept, dirty - The object has an identity, is dirty, and is in use by one or more threads <LI> Identity, unkept, dirty - The object has an identity, is dirty but is only accessable by the cache manager </OL> </OL> <BR> While the object is kept it is guaranteed not to change identity. While it is unkept no-one outside of the cache manager can have a reference to the object. The cache manager returns kept objects and they return to the unkept state when all the current users of the object have released it. <BR> It is required that the object can only move into a dirty state while it is kept. <BR> MT - Mutable : thread aware - Calls to Cacheable method must only be made by the cache manager or the object itself.
The CacheableConglomerate implements a single item in the cache used by the Conglomerate directory to cache Conglomerates.  It is simply a wrapper object for the conglomid and Conglomerate object that is read from the Conglomerate Conglomerate.   It is a wrapper rather than extending the conglomerate implementations because we want to cache all conglomerate implementations: (ie. Heap, B2I, ...). References to the Conglomerate objects cached by this wrapper will be handed out to callers.  When this this object goes out of cache callers may still have references to the Conglomerate objects, which we are counting on java to garbage collect.  The Conglomerate Objects never change after they are created.
Any object that implements this interface can be cached using the services of the CacheManager/CacheFactory. In addition to implementing this interface the class must be public and it must have a public no-arg constructor. This is because the cache manager will construct objects itself and then set their identity by calling the setIdentity method. <P> A Cacheable object has five states: <OL> <OL> <LI> No identity - The object is only accessable by the cache manager <LI> Identity, clean, unkept - The object has an identity, is clean but is only accessable by the cache manager <LI> Identity, clean, kept - The object has an identity, is clean, and is in use by one or more threads <LI> Identity, kept, dirty - The object has an identity, is dirty, and is in use by one or more threads <LI> Identity, unkept, dirty - The object has an identity, is dirty but is only accessable by the cache manager </OL> </OL> <BR> While the object is kept it is guaranteed not to change identity. While it is unkept no-one outside of the cache manager can have a reference to the object. The cache manager returns kept objects and they return to the unkept state when all the current users of the object have released it. <BR> It is required that the object can only move into a dirty state while it is kept. <BR> MT - Mutable : thread aware - Calls to Cacheable method must only be made by the cache manager or the object itself.
A base page that is cached. Since there are multiple page formats, use this abstract class to implement cacheable interface.
Cacheable interface
<p> A wrapper class for a {@code ValueNode} that is referenced multiple places in the abstract syntax tree, but should only be evaluated once. This node will cache the return value the first time the expression is evaluated, and simply return the cached value the next time. </p> <p>For example, an expression such as</p> <pre> CASE expr1 WHEN expr2 THEN expr3 WHEN expr4 THEN expr5 END </pre> <p>is rewritten by the parser to</p> <pre> CASE WHEN expr1 = expr2 THEN expr3 WHEN expr1 = expr4 THEN expr5 END </pre> <p> In this case, we want {@code expr1} to be evaluated only once, even though it's referenced twice in the rewritten tree. By wrapping the {@code ValueNode} for {@code expr1} in a {@code CachedValueNode}, we make sure {@code expr1} is only evaluated once, and the second reference to it will use the cached return value from the first evaluation. </p>
A logical connection used in a connection pool with capabilities for caching prepared statements. <p> An instance of this class is what is passed out the the client. It uses a JDBC statement object cache to avoid re-preparing commonly used queries. The cache has scope of a physical connection, and is lost when the pooled connection is closed (which includes closing the physical connection).
An CallStatementNode represents a CALL <procedure> statement. It is the top node of the query tree for that statement. A procedure call is very simple. CALL [<schema>.]<procedure>(<args>) <args> are either constants or parameter markers. This implementation assumes that no subqueries or aggregates can be in the argument list. A procedure is always represented by a MethodCallNode.
Call a Java procedure. This calls a generated method in the activation which sets up the parameters and then calls the Java method that the procedure resolved to. <P> Valid dynamic results returned by the procedure will be closed as inaccessible when this is closed (e.g. a CALL within a trigger). <BR> Any code that requires the dynamic results to be accessible (such as the JDBC Statement object executing the CALL) must obtain the dynamic results from Activation.getDynamicResults() and remove each ResultSet it will be handling by clearing the reference in the object returned.
Contains the necessary methods to call the stored procedure that operate on LOBs identified by locators.  An instance of this class will be initialized with a <code>Connection</code> parameter and all calls will be made on that connection. <p> The class makes sure that each procedure call is only prepared once per instance.  Hence, it will keep references to <code>CallableStatement</code> objects for procedures that have been called through this instance.  This makes it possible to prepare each procedure call only once per <code>Connection</code>. <p> Since LOBs can not be parameters to stored procedures, the framework should make sure that calls involving a byte[] or String that does not fit in a VARCHAR (FOR BIT DATA), are split into several calls each operating on a fragment of the LOB.
This is a decorator (in Design Patterns Terminology) class to enhance the functionality of a RowLocationRetRowSource. It assumes that the rows are coming in sorted order from the row source and it simply keeps track of the cardinality of all the leading columns.
An CastNode represents a cast expression.
Superclass of all row factories.
Peforms character conversions.
Allocation page operation - to allocate, deallocate or free a page

A decorator that changes the default ssl mode for the current configuration. Its tearDown method restores the previous configuration.
A decorator that changes the default user and password for the current configuration. Its tearDown method restores the previous configuration. Optionally the passwordToken can be changed as well.
Upgrade test cases for changes made in 10.1. If the old version is 10.1 or later then these tests will not be run. <BR> 10.1 Upgrade issues <UL> <LI> testProcedureSignature - Routines with explicit Java signatures. </UL>
Upgrade test cases for 10.10.
Upgrade test cases for 10.11.
Upgrade test cases for 10.12.
Upgrade test cases for 10.13.
Upgrade test cases for 10.14.
Upgrade test cases for changes made in 10.2. If the old version is 10.2 or later then these tests will not be run. <BR> 10.2 Upgrade issues <UL> <LI> testTriggerInternalVTI - Check internal re-write of triggers does not break triggers in soft upgrade mode. <LI> testReusableRecordIdSequenceNumber - Test reuseable record identifiers does not cause issues in soft upgrade <LI> testGrantRevokeStatements - Check G/R not allowed in soft upgrade. <LI> testDatabaseOwner - test that on a hard upgrade database owner is set. </UL>
Upgrade test cases for changes made in 10.3. If the old version is 10.3 or later then these tests will not be run. <BR> 10.3 Upgrade issues
Upgrade test cases for 10.4. If the old version is 10.4 or later then these tests will not be run. <BR> 10.4 Upgrade issues <UL> <LI> testMetaDataQueryRunInSYScompilationSchema - DERBY-2946 Make sure that metadata queries get run with SYS schema as the current compilation schema rather than a user schema as the current compilation schema. This is because if the user is inside a user schema in a collated database and if the meta data query gets run inside the user schema, then we will run into collation mismatch errors for a subclause like following in the WHERE clause. P.SELECTPRIV = 'Y' The reason for error is that the left hand side of the = operation will have collation type of UCS_BASIC because that column belongs to a table from system schema. But the collation type of the right hand will be territory based if the current compilation schema is user schema. But if the current compilation schema is set to SYS schema, then right hand side will also have collation of UCS_BASIC and hence there won't be any collation mismatch. Background info : character string constants pick up the collation of the current compilation schema. </UL>
Upgrade test cases for 10.5. If the old version is 10.5 or later then these tests will not be run. <BR> 10.5 Upgrade issues <UL> <LI> testUpdateStatisticsProcdure - DERBY-269 Make sure that SYSCS_UTIL.SYSCS_UPDATE_STATISTICS can only be run in Derby 10.5 and higher. </UL>
Upgrade test cases for 10.6. If the old version is 10.6 or later then these tests will not be run. <BR> 10.6 Upgrade issues <UL> <LI> testSetXplainSchemaProcedure - DERBY-2487 Make sure that SYSCS_UTIL.SYSCS_SET_XPLAIN_SCHEMA can only be run in Derby 10.5 and higher. </UL>
Upgrade test cases for 10.7. If the old version is 10.7 or later then these tests will not be run. <BR> 10.7 Upgrade issues <UL> <LI>BOOLEAN data type support expanded.</LI> </UL>
Upgrade test cases for 10.9.
A looping alphabet, returning characters. The alphabet loops over a list of characters. The alphabet-object is used by looping readers, which in turn is used for testing methods requiring streaming inputs. The following alphabets have been defined: <ul><li><em>Modern latin, lowercase</em> ; letters a - z (26) <li><em>Norwegian/Danish, lowercase</em> ; letters a - z, plus three additional letters (29) <li><em>Tamil</em> ; 46 Tamil letters from UNICODE U0B80 <li><em>CJK subset</em> ; 12 letter from UNICODE CJK U4E00 </ul> Enc class CharAlphabet

This interface describes a character stream that maintains line and column number positions of the characters.  It also has the capability to backup the stream to some extent.  An implementation of this interface is used in the TokenManager implementation generated by JavaCCParser. All the methods except backup can be implemented in any fashion. backup needs to be implemented correctly for the correct operation of the lexer. Rest of the methods are all used to get information like line number, column number and the String that constitutes a token and are not used by the lexer. Hence their implementation won't affect the generated lexer's operation.
Generates stream headers for non-Clob string data types. <p> The stream header encodes the byte length of the stream. Since two bytes are used for the header, the maximum encodable length is 65535 bytes. There are three special cases, all handled by encoding zero into the header and possibly appending an EOF-marker to the stream: <ul> <li>Unknown length - with EOF marker</li> <li>Length longer than maximum encodable length - with EOF marker</li> <li>Length of zero - no EOF marker</li> </ul> The length is encoded like this: <pre> out.writeByte((byte)(byteLength &gt;&gt;&gt; 8)); out.writeByte((byte)(byteLength &gt;&gt;&gt; 0)); </pre> @Immutable
This class implements TypeCompiler for the SQL char datatypes.

A description of a byte stream representing characters. The description is used by decoders to properly configure themselves. Note that encoding is not included in the description, because all internal byte streams are expected to be using the modified UTF-8 encoding (see DataInput). <p> The information in the description is only guaranteed to be valid at the moment it is passed to the decoder object. As the decoder works on the stream, the information in the descriptor will be outdated. <p> To create a stream descriptor, obtain a {@code Builder} instance and set the required parameters. @Immutable
This class represents a check constraint descriptor.
This tests to see if the security manager is running.
A Log Operation that represents a checkpoint.
Do some checks on the Order Entry database.
A Log Operation that represents a checksum for a group of log records that are written to the tranaction log file. <PRE>
A CipherFactory can create new CipherProvider, which is a wrapper for a javax.crypto.Cipher This service is only available when run on JDK1.2 or beyond. To use this service, either the SunJCE or an alternative clean room implementation of the JCE must be installed. To use a CipherProvider to encrypt or decrypt, it needs 3 things: 1) A CipherProvider that is initialized to ENCRYPT or DECRYPT 2) A secret Key for the encryption/decryption 3) An Initialization Vector (IvParameterSpec) that is used to create some randomness in the encryption See $WS/docs/funcspec/mulan/configurableEncryption.html See http://java.sun.com/products/JDK/1.1/docs/guide/security/CryptoSpec.html See http://java.sun.com/products/JDK/1.2/docs/guide/security/CryptoSpec.html See http://java.sun.com/products/jdk/1.2/jce/index.html
Interface to create instances of the cipher factory based on the user specified encryption properties.
A CipherProvider is a wrapper for a Cipher class in JCE. This service is only available when run on JDK1.2 or beyond. To use this service, either the SunJCE or an alternative clean room implementation of the JCE must be installed. To use a CipherProvider to encrypt or decrypt, it needs 3 things: 1) A CipherProvider that is initialized to ENCRYPT or DECRYPT 2) A secret Key for the encryption/decryption 3) An Initialization Vector (IvParameterSpec) that is used to create some randomness in the encryption See $WS/docs/funcspec/mulan/configurableEncryption.html See http://java.sun.com/products/JDK/1.1/docs/guide/security/CryptoSpec.html See http://java.sun.com/products/JDK/1.2/docs/guide/security/CryptoSpec.html See http://java.sun.com/products/jdk/1.2/jce/index.html
ClassBuilder is used to construct a java class's byte array representation. Limitations: No checking for language use violations such as invalid modifiers or duplicate field names. All classes must have a superclass; java.lang.Object must be supplied if there is no superclass. <p> When a class is first created, it has: <ul> <li> a superclass <li> modifiers <li> a name <li> a package <li> no superinterfaces, methods, fields, or constructors <li> an empty static initializer </ul> <p> MethodBuilder implementations are required to get code out of the constructs within their bodies in some manner. Most typically, they may have a stream to which the statement and expression constructs write the code that they represent, and they walk over the statements and expressions in the appropriate order.
An enumeration that filters only classes from the enumeration of the class pool. Code has been added to also include classes referenced in method and field signatures.
A class factory module to handle application classes and generated classes.
Context that provides the correct ClassFactory for the current service. Allows stateless code to obtain the correct class loading scheme.
A wrapper around DataOutputStream to provide input functions in terms of the types defined on pages 83 of the Java Virtual Machine spec. For this types use these methods of DataOutputStream <UL> <LI>float - writeFloat <LI>long - writeLong <LI>double - writeDouble <LI>UTF/String - writeUTF <LI>U1Array - write(byte[]) </UL>
Based upon "THE class FILE FORMAT" chapter of "The Java Virtual Machine Specification" corresponding to version 1.0.2 of the Java Virtual Machine and 1.0.2 of the Java Language Specification. ISBN  0-201-63452-X, September 1996.

A wrapper around DataInputStream to provide input functions in terms of the types defined on pages 83.
Methods to find out relationships between classes and methods within a class. All class names within this interface are treated as java language class names, e.g. int, COM.foo.Myclass, int[], java.lang.Object[]. That is java internal class names as defined in the class file format are not understood.


List of strings representing class names, which are typically found for classes with implement the Formatable interface. These strings are removed from the code to separate them from the strings which need to be internationalized. It also reduces footprint. <P> This class has no methods, all it contains are String's which by default are public, static and final since they are declared in an interface.

Map from class names to size coefficients. The size coefficients can be used to estimate how much memory an instance of the class takes.
This class implements a program that catalogs the size estimate coefficients of various classes. end of ClassSizeCrawler
<p> This decorator adds another resource to the classpath, removing it at tearDown(). </p>
Interface that must be implemented by performance clients. The implementations of this interface normally perform a single operation, and the selected implementation of {@code LoadGenerator} repeatedly invokes this operation in order to get the desired distribution.



This class implements the JDBC {@code java.sql.Clob} interface.
Spawns the JVM process running the compatibility tests for the given client version.
Returns the test suite run by each client in the compatibility test. <p> This is where one would add the tests from a new test class to be included in the compatibility suite.

This datasource is suitable for a client/server use of Derby, running on full Java SE 6 and higher, corresponding to JDBC 4.0 and higher. <p/> ClientConnectionPoolDataSource is a factory for PooledConnection objects. An object that implements this interface will typically be registered with a naming service that is based on the Java Naming and Directory Interface (JNDI).
<P> This is a vacuous, deprecated class. At one time, it had real behavior and helped us support separate datasources for Java 5 and Java 6. Now that we no longer support Java 5, all functionality has migrated into the superclass, ClientConnectionPoolDataSource. This class is preserved for backward compatibility reasons. </P> compile-time check for 4.1 extension
Specifies Derby extensions to the {@code java.sqlx.ConnectionPoolDataSource}.
This data source is suitable for a client/server use of Derby, running on full Java SE 6 and higher, corresponding to JDBC 4.0 and higher. <p/> ClientDataSource is a simple data source implementation that can be used for establishing connections in a non-pooling, non-distributed environment. The class ClientConnectionPoolDataSource can be used in a connection pooling environment, and the class ClientXADataSource can be used in a distributed, and pooling environment. <p>The example below registers a DNC data source object with a JNDI naming service. <pre> org.apache.derby.client.ClientDataSource dataSource = new org.apache.derby.client.ClientDataSource (); dataSource.setServerName ("my_derby_database_server"); dataSource.setDatabaseName ("my_derby_database_name"); javax.naming.Context context = new javax.naming.InitialContext(); context.bind ("jdbc/my_datasource_name", dataSource); </pre> The first line of code in the example creates a data source object. The next two lines initialize the data source's properties. Then a Java object that references the initial JNDI naming context is created by calling the InitialContext() constructor, which is provided by JNDI. System properties (not shown) are used to tell JNDI the service provider to use. The JNDI name space is hierarchical, similar to the directory structure of many file systems. The data source object is bound to a logical JNDI name by calling Context.bind(). In this case the JNDI name identifies a subcontext, "jdbc", of the root naming context and a logical name, "my_datasource_name", within the jdbc subcontext. This is all of the code required to deploy a data source object within JNDI. This example is provided mainly for illustrative purposes. We expect that developers or system administrators will normally use a GUI tool to deploy a data source object. <p/> Once a data source has been registered with JNDI, it can then be used by a JDBC application, as is shown in the following example. <pre> javax.naming.Context context = new javax.naming.InitialContext (); javax.sql.DataSource dataSource = (javax.sql.DataSource) context.lookup ("jdbc/my_datasource_name"); java.sql.Connection connection = dataSource.getConnection ("user", "password"); </pre> The first line in the example creates a Java object that references the initial JNDI naming context. Next, the initial naming context is used to do a lookup operation using the logical name of the data source. The Context.lookup() method returns a reference to a Java Object, which is narrowed to a javax.sql.DataSource object. In the last line, the DataSource.getConnection() method is called to produce a database connection. <p/> This simple data source subclass of BasicClientDataSource40 maintains it's own private <code>password</code> property. <p/> The specified password, along with the user, is validated by DERBY. This property can be overwritten by specifying the password parameter on the DataSource.getConnection() method call. <p/> This password property is not declared transient, and therefore may be serialized to a file in clear-text, or stored to a JNDI server in clear-text when the data source is saved. Care must taken by the user to prevent security breaches. <p/>
<P> This is a vacuous, deprecated class. At one time, it had real behavior and helped us support separate datasources for Java 5 and Java 6. Now that we no longer support Java 5, all functionality has migrated into the superclass, ClientDataSource. This class is preserved for backward compatibility reasons. </P> compile-time check for 4.1 extension
The data source factory for Derby client driver data sources. <p> This factory reconstructs a Derby data source object when it is retrieved from JNDI. References are needed since many naming services don't have the ability to store Java objects in their serialized form. When a data source object is bound in this type of naming service the {@link javax.naming.Reference} for that object is actually stored by the JNDI implementation, not the data source object itself. <p> A JNDI administrator is responsible for making sure that both the object factory and data source implementation classes provided by a JDBC driver vendor are accessible to the JNDI service provider at runtime. <p> An object factory implements the {@link javax.naming.spi.ObjectFactory} interface. This interface contains a single method, {@code getObjectInstance} which is called by a JNDI service provider to reconstruct an object when that object is retrieved from JNDI. A JDBC driver vendor should provide an object factory as part of their JDBC 2.0 product.
Specifies Derby extensions to the {@code java.sqlx.DataSource} API common to all Derby client driver data sources.
Note: Tag members using the strictest visibility. Note: Mark methods synchronized if and only if they update object state and are public. Not yet done: Application heap data should be copied for shiraz. Save for future pass to avoid clutter during development. Not yet done: Apply meaning-preserving program transformations for performance, including the replacement of slow ADTs with faster unsynchronized ADTs. Save for future pass to avoid clutter during development. Not yet done: Assign an ErrorKey, ResourceKey, and Resource for each throw statement. Save for future pass to avoid maintenance during development.
The client JDBC driver (type 4) for Derby.
<p> Adds driver functionality which is only visible from JDBC 4.0 onward. </p> <p> This class was part of Derby's public API up to Derby 10.10. Even though it doesn't provide any more functionality than {@code ClientDriver}, it is preserved for backward compatibility. </p>
The methods of this interface are used to return JDBC interface implementations to the user depending on the JDBC version supported by the JDK.
Implements the the ClientJDBCObjectFactory interface and returns the classes that implement the JDBC3.0/2.0 interfaces For example, newCallableStatement would return ClientCallableStatement
Implements the ClientJDBCObjectFactory interface and returns the JDBC 4.2 specific classes.
A very simple wrapper around a message id.  This is needed so that the new constructors for SqlException using message ids don't conflict with the old constructors. Once all messages have been internationalized, we could conceivably get rid of this class.
Parameter meta data as used internally by the driver is always a column meta data instance. We will only create instances of this class when getParameterMetaData() is called. This class simply wraps a column meta data instance.  Once we go to JDK 1.4 as runtime pre-req, we can extend ColumnMetaData and new up ParameterMetaData instances directly, and we won't have to wrap column meta data instances directly.
A physical connection to a data source, to be used for creating logical connections to the same data source.



FIXME! Use ClientRunner in e.g. Shutdown.... etc.



This enumeration of types represents the typing scheme used by our jdbc driver. Once this is finished, we need to review our switches to make sure they are exhaustive

<p> This datasource is suitable for a client/server use of Derby, running on full Java SE 6 and higher, corresponding to JDBC 4.0 and higher. </p> An XADataSource is a factory for XAConnection objects.  It represents a RM in a DTP environment.  An object that implements the XADataSource interface is typically registered with a JNDI service provider. <P> ClientXADataSource automatically supports the correct JDBC specification version for the Java Virtual Machine's environment. <UL> <LI> JDBC 4.2 - Java SE 8 </LI> <LI> JDBC 4.1 - Java SE 7 </LI> <LI> JDBC 4.0 - Java SE 6 </LI> </UL> <P>ClientXADataSource is serializable and referenceable.</p> <P>See ClientDataSource for DataSource properties.</p>
<P> This is a vacuous, deprecated class. At one time, it had real behavior and helped us support separate datasources for Java 5 and Java 6. Now that we no longer support Java 5, all functionality has migrated into the superclass, ClientXADataSource. This class is preserved for backward compatibility reasons. </P> compile-time check for 4.1 extension
Specifies Derby extensions to the {@code java.sqlx.XADataSource}.
class Xid
Wrap a Writer as an OutputStream to support Clob.setAsciiStream(). Any value written to the OutputStream is a valid ASCII value (0-255 from JDBC 4 spec appendix C2) thus this class simply passes the written values onto the Writer.
An <code>InputStream</code> that will use an locator to fetch the Clob value from the server. <p> Closing a <code>ByteArrayInputStream</code> has no effect. The methods in this class can be called after the stream has been closed without generating an <code>IOException</code>. <p> This <code>InputStream</code> implementation is pretty basic.  No buffering of data is done.  Hence, for efficiency #read(byte[]) should be used instead of #read().  Marks are not supported, but it should be pretty simple to extend the implementation to support this.  A more efficient skip implementation should also be straight-forward.
An <code>OutputStream</code> that will use an locator to write bytes to the Clob value on the server. <p> Closing a <code>ByteArrayInputStream</code> has no effect. The methods in this class can be called after the stream has been closed without generating an <code>IOException</code>. <p> This <code>OutputStream</code> implementation is pretty basic.  No buffering of data is done.  Hence, for efficiency #write(byte[]) should be used instead of #write(int).
An <code>Reader</code> that will use an locator to fetch the Clob value from the server. <p> This <code>Reader</code> implementation is pretty basic.  No buffering of data is done.  Hence, for efficiency #read(char[]) should be used instead of #read().  Marks are not supported, but it should be pretty simple to extend the implementation to support this.  A more efficient skip implementation should also be straight-forward.
An {@code Writer} that will use an locator to write the Clob value into the server. <p> This {@code Writer} implementation is pretty basic.  No buffering of data is done.  Hence, for efficiency {@code #write(char[])} should be used instead of {@code #write(int)}.

Generates stream headers for Clob data values. <p> <em>THREAD SAFETY NOTE</em>: This class is considered thread safe, even though it strictly speaking isn't. However, with the assumption that an instance of this class cannot be shared across databases with different versions, the only bad thing that can happen is that the mode is obtained several times. @ThreadSafe
{@code ClobUpdatableReader} is used to create a {@code Reader} capable of detecting changes to the underlying source. <p> This class is aware that the underlying stream can be modified and reinitializes itself if it detects any change in the stream. This invalidates the cache so the changes are reflected immediately. <p> The task of this class is to detect changes in the underlying Clob. Repositioning is handled by other classes.
Writer implementation for <code>Clob</code>.

Implementation of a replacement policy which uses the clock algorithm. All the cache entries are stored in a circular buffer, called the clock. There is also a clock hand which points to one of the entries in the clock. Each time an entry is accessed, it is marked as recently used. If a new entry is inserted into the cache and the cache is full, the clock hand is moved until it is over a not recently used entry, and that entry is evicted to make space for the new entry. Each time the clock hand sweeps over a recently used entry, it is marked as not recently used, and it will be a candidate for removal the next time the clock hand sweeps over it, unless it has been marked as recently used in the meantime. <p> To allow concurrent access from multiple threads, the methods in this class need to synchronize on a number of different objects: <ul> <li><code>CacheEntry</code> objects must be locked before they can be used</li> <li>accesses to the clock structure (circular buffer + clock hand) should be synchronized on the <code>ArrayList</code> representing the circular buffer</li> <li>accesses to individual <code>Holder</code> objects in the clock structure should be protected by synchronizing on the holder</li> </ul> To avoid deadlocks, we need to ensure that all threads obtain synchronization locks in the same order. <code>CacheEntry</code>'s class javadoc dictates the order when locking <code>CacheEntry</code> objects. Additionally, we require that no thread should obtain any other synchronization locks while it is holding a synchronization lock on the clock structure or on a <code>Holder</code> object. The threads are however allowed to obtain synchronization locks on the clock structure or on a holder while they are locking one or more <code>CacheEntry</code> objects.
This is a simple interface that is used by streams that can clone themselves. <p> The purpose is for the implementation of BLOB/CLOB (and potentially other types whose value is represented by a stream), for which their size makes it impossible or very expensive to materialize the value.

Simple utility to compute/recover the parameters of a mixture-of-Gaussian distribution from independent samples.
Simple utility to compute/recover the parameters of a mixture-of-Gaussian distribution from independent samples, using SQL.
This node represents coalesce/value function which returns the first argument that is not null. The arguments are evaluated in the order in which they are specified, and the result of the function is the first argument that is not null. The result can be null only if all the arguments can be null. The selected argument is converted, if necessary, to the attributes of the result. SQL Reference Guide for DB2 has section titled "Rules for result data types" at the following url http://publib.boulder.ibm.com/infocenter/db2help/index.jsp?topic=/com.ibm.db2.udb.doc/admin/r0008480.htm I have constructed following table based on various tables and information under "Rules for result data types" This table has FOR BIT DATA TYPES broken out into separate columns for clarity Note that are few differences between Derby and DB2 1)there are few differences between what data types are considered compatible In DB2, CHAR FOR BIT DATA data types are compatible with CHAR data types ie in addition to following table, CHAR is compatible with CHAR FOR BIT DATA, VARCHAR FOR BIT DATA and LONG VARCHAR FOR BIT DATA ie in addition to following table, VARCHAR is compatible with CHAR FOR BIT DATA, VARCHAR FOR BIT DATA and LONG VARCHAR FOR BIT DATA ie in addition to following table, LONG VARCHAR is compatible with CHAR FOR BIT DATA, VARCHAR FOR BIT DATA and LONG VARCHAR FOR BIT DATA ie in addition to following table, CHAR FOR BIT DATA is compatible with DATE, TIME, TIMESTAMP ie in addition to following table, VARCHAR FOR BIT DATA is compatible with DATE, TIME, TIMESTAMP 2)few data types do not have matching precision in Derby and DB2 In DB2, precision of TIME is 8. In Derby, precision of TIME is 0. In DB2, precision,scale of TIMESTAMP is 26,6. In Derby, precision of TIMESTAMP is 0,0. In DB2, precision of DOUBLE is 15. In Derby, precision of DOUBLE is 52. In DB2, precision of REAL is 23. In Derby, precision of REAL is 7. In DB2, precision calculation equation is incorrect when we have int and decimal arguments. The equation should be p=x+max(w-x,10) since precision of integer is 10 in both DB2 and Derby. Instead, DB2 has p=x+max(w-x,11) Types.             S  I  B  D  R  D  C  V  L  C  V  L  C  D  T  T  B M  N  I  E  E  O  H  A  O  H  A  O  L  A  I  I  L A  T  G  C  A  U  A  R  N  A  R  N  O  T  M  M  O L  E  I  I  L  B  R  C  G  R  C  G  B  E  E  E  B L  G  N  M     L     H  V  .  H  V           S I  E  T  A     E     A  A  B  A  A           T N  R     L           R  R  I  R  R           A T                       C  T  .  .           M H     B  B           P A     I  I R     T   T SMALLINT         { "SMALLINT", "INTEGER", "BIGINT", "DECIMAL", "DOUBLE", "DOUBLE", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, INTEGER          { "INTEGER", "INTEGER", "BIGINT", "DECIMAL", "DOUBLE", "DOUBLE", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, BIGINT           { "BIGINT", "BIGINT", "BIGINT", "DECIMAL", "DOUBLE", "DOUBLE", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, DECIMAL          { "DECIMAL", "DECIMAL", "DECIMAL", "DECIMAL", "DOUBLE", "DOUBLE", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, REAL             { "DOUBLE", "DOUBLE", "DOUBLE", "DOUBLE", "REAL", "DOUBLE", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, DOUBLE           { "DOUBLE", "DOUBLE", "DOUBLE", "DOUBLE", "DOUBLE", "DOUBLE", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, CHAR             { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "CHAR", "VARCHAR", "LONG VARCHAR", "ERROR", "ERROR", "ERROR", "CLOB", "DATE", "TIME", "TIMESTAMP", "ERROR" }, VARCHAR          { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "VARCHAR", "VARCHAR","LONG VARCHAR", "ERROR", "ERROR", "ERROR", "CLOB", "DATE", "TIME", "TIMESTAMP", "ERROR" }, LONGVARCHAR      { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "LONG VARCHAR", "LONG VARCHAR", "LONG VARCHAR", "ERROR", "ERROR", "ERROR", "CLOB", "ERROR", "ERROR", "ERROR", "ERROR" }, CHAR FOR BIT     { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "BIT", "BIT VARYING", "LONG BIT VARYING", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, VARCH. BIT       { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "BIT VARYING", "BIT VARYING", "LONG BIT VARYING", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, LONGVAR. BIT     { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "LONG BIT VARYING", "LONG BIT VARYING", "LONG BIT VARYING", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR" }, CLOB             { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "CLOB", "CLOB", "CLOB", "ERROR", "ERROR", "ERROR", "CLOB", "ERROR", "ERROR", "ERROR", "ERROR" }, DATE             { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "DATE", "DATE", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "DATE", "ERROR", "ERROR", "ERROR" }, TIME             { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "TIME", "TIME", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "TIME", "ERROR", "ERROR" }, TIMESTAMP        { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "TIMESTAMP", "TIMESTAMP", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "TIMESTAMP", "ERROR" }, BLOB             { "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "ERROR", "BLOB" }
This class represents a chunk of code in a CodeAttribute. Typically, a CodeAttribute represents the code in a method. If there is a try/catch block, each catch block will get its own code chunk.  This allows the catch blocks to all be put at the end of the generated code for a method, which eliminates the need to generate a jump around each catch block, which would be a forward reference. final int splitNonZeroStack(BCMethod mb, ClassHolder ch, final int codeLength, final int optimalMinLength, int maxStack) { // program counter for the instruction that // made the stack reach the given stack depth. int[] stack_pcs = new int[maxStack+1]; Arrays.fill(stack_pcs, -1); int stack = 0; // maximum possible split seen that is less than // the minimum. int possibleSplitLength = -1; System.out.println("NZ SPLIT + " + mb.getName()); // do not split until at least this point (inclusive) // used to ensure no split occurs in the middle of // a conditional. int outerConditionalEnd_pc = -1; int end_pc = 0 + codeLength; for (int pc = 0; pc < end_pc;) { short opcode = getOpcode(pc); int stackDelta = stackWordDelta(ch, pc, opcode); stack += stackDelta; // Cannot split a conditional but need to calculate // the stack depth at the end of the conditional. // Each path through the conditional will have the // same stack depth. int[] cond_pcs = findConditionalPCs(pc, opcode); if (cond_pcs != null) { // an else block exists, skip the then block. if (cond_pcs[3] != -1) { pc = cond_pcs[3]; continue; } if (SanityManager.DEBUG) { if (outerConditionalEnd_pc != -1) { if (cond_pcs[5] >= outerConditionalEnd_pc) SanityManager.THROWASSERT("NESTED CONDITIONALS!"); } } if (outerConditionalEnd_pc == -1) { outerConditionalEnd_pc = cond_pcs[5]; } } pc += instructionLength(opcode); // Don't split in the middle of a conditional if (outerConditionalEnd_pc != -1) { if (pc > outerConditionalEnd_pc) { // passed the outermost conditional outerConditionalEnd_pc = -1; } continue; } if (stackDelta == 0) continue; // Only split when the stack is having items popped if (stackDelta > 0) { // pushing double word, clear out a if (stackDelta == 2) stack_pcs[stack - 1] = pc; stack_pcs[stack] = pc; continue; } int opcode_pc = pc - instructionLength(opcode); // Look for specific opcodes that have the capability // of having a significant amount of code in a self // contained block. switch (opcode) { // this.method(A) construct //  ...         -- stack N //  push this -- stack N+1 //  push args -- stack N+1+A //  call method -- stack N+R (R=0,1,2) // //  stackDelta = (N+R) - (N+1+A) = R-(1+A) //  stack = N+R //  Need to determine N+1 // // // //  this.a(<i2>, <i2>, <i3>) //  returning int // //  stackDelta = -3 (this & 3 args popped, ret pushed) //  initial depth N = 10 //  pc        - stack //  100 ...       - stack 10 //  101 push this - stack 11 //  109 push i1   - stack 12 //  125 push i2   - stack 13 //  156 push i3   - stack 14 //  157 call      - stack 11 // //  need stack_pcs[11] = stack_pcs[11 + -3] // // ref.method(args).method(args) ... method(args) // case VMOpcode.INVOKEINTERFACE: case VMOpcode.INVOKESPECIAL: case VMOpcode.INVOKEVIRTUAL: { String vmDescriptor = getTypeDescriptor(ch, opcode_pc); int r = CodeChunk.getDescriptorWordCount(vmDescriptor); // PC of the opcode that pushed the reference for // this method call. int ref_pc = stack_pcs[stack - r + 1]; if (getOpcode(ref_pc) == VMOpcode.ALOAD_0) { System.out.println("POSS SPLIT " + (pc - ref_pc) + " @ " + ref_pc); } break; } case VMOpcode.INVOKESTATIC: String vmDescriptor = getTypeDescriptor(ch, opcode_pc); int r = CodeChunk.getDescriptorWordCount(vmDescriptor); int p1_pc = stack_pcs[stack - r + 1]; System.out.println("POSS STATIC SPLIT " + (pc - p1_pc) + " @ " + p1_pc); } stack_pcs[stack] = opcode_pc; } return -1; }
This is a simple interface that houses externally visible statics for code generation.
TBD: organize into separate kinds of code points; impose organizational scheme. TBD: reconsider the various SECCHKCD_xx constants, perhaps we should hardwire.
This class has a hashtable of CodePoint values.  It is used by the tracing code and by the protocol testing code It is arranged in alphabetical order.
This class describes a row in the SYS.SYSCOLPERMS system table, which keeps the column permissions that have been granted but not revoked.
CollationElementsInterface is an interface which will be implemented by all the Collator sensitive char data types. These methods will be called by WorkHorseForCollatorDatatypes's collation sensitive methods "like, stringcompare" etc.
CollatorSQLChar class differs from SQLChar based on how the 2 classes use different collations to collate their data. SQLChar uses Derby's default collation which is UCS_BASIC. Whereas, this class uses the RuleBasedCollator object that was passed to it in it's constructor and that RuleBasedCollator object decides the collation. In Derby 10.3, this class will be passed a RuleBasedCollator which is based on the database's territory. In future releases of Derby, this class can be used to do other kinds of collations like case-insensitive collation etc by just passing an appropriate RuleBasedCollator object for that kind of collation.
CollatorSQLClob class differs from SQLClob based on how the 2 classes use different collations to collate their data. SQLClob uses Derby's default collation which is UCS_BASIC. Whereas, this class uses the RuleBasedCollator object that was passed to it in it's constructor and that RuleBasedCollator object decides the collation. In Derby 10.3, this class will be passed a RuleBasedCollator which is based on the database's territory. In future releases of Derby, this class can be used to do other kinds of collations like case-insensitive collation etc by just passing an appropriate RuleBasedCollator object for that kind of collation.
CollatorSQLLongvarchar class differs from SQLLongvarchar based on how the 2 classes use different collations to collate their data. SQLLongvarchar uses Derby's default collation which is UCS_BASIC. Whereas, this class uses the RuleBasedCollator object that was passed to it in it's constructor and that RuleBasedCollator object decides the collation. In Derby 10.3, this class will be passed a RuleBasedCollator which is based on the database's territory. In future releases of Derby, this class can be used to do other kinds of collations like case-insensitive collation etc by just passing an appropriate RuleBasedCollator object for that kind of collation.
CollatorSQLVarchar class differs from SQLVarchar based on how the 2 classes use different collations to collate their data. SQLVarchar uses Derby's default collation which is UCS_BASIC. Whereas, this class uses the RuleBasedCollator object that was passed to it in it's constructor and that RuleBasedCollator object decides the collation. In Derby 10.3, this class will be passed a RuleBasedCollator which is based on the database's territory. In future releases of Derby, this class can be used to do other kinds of collations like case-insensitive collation etc by just passing an appropriate RuleBasedCollator object for that kind of collation.
Collect all nodes of the designated type to be returned in a list. <p> Can find any type of node -- the class or class name of the target node is passed in as a constructor parameter.
A ColumnDefinitionNode represents a column definition in a DDL statement. There will be a ColumnDefinitionNode for each column in a CREATE TABLE statement, and for the column in an ALTER TABLE ADD COLUMN statement.
This class represents a column descriptor. public methods in this class are: <ol> <li>long getAutoincStart()</li> <li>java.lang.String getColumnName()</li> <li>DefaultDescriptor getDefaultDescriptor(DataDictionary dd)</li> <li>DefaultInfo getDefaultInfo</li> <li>UUID getDefaultUUID</li> <li>DataValueDescriptor getDefaultValue</li> <li>int getPosition()</li> <li>UUID getReferencingUUID()</li> <li>TableDescriptor getTableDescriptor</li> <li>DTD getType()</li> <li>hasNonNullDefault</li> <li>isAutoincrement</li> <li>setColumnName</li> <li>setPosition</li> </ol>
This represents a list of column descriptors.
This is the Column descriptor that is passed from Compilation to Execution for CREATE TABLE statements.
Under JDBC 2, we must new up our parameter meta data as column meta data instances Once we move to JDK 1.4 pre-req, create a ResultSetMetaData class and make this class abstract
The column ordering interface defines a column that is to be ordered in a sort or index, and how it is to be ordered.  Column instances are compared by calling the compare(Orderable) method of Orderable.
A ColumnReference represents a column in the query tree.  The parser generates a ColumnReference for each column reference.  A column reference could be a column in a base table, a column in a view (which could expand into a complex expression), or a column in a subquery in the FROM clause.

<p> This interface must be implemented by objects returned from <code>LockFactory.createCompatibilitySpace()</code>. </p> <p> A <code>CompatibilitySpace</code> can have an owner (for instance a transaction). Currently, the owner is used by the virtual lock table to find out which transaction a lock belongs to. Some parts of the code also use the owner as a group object which guarantees that the lock is released on a commit or an abort. The owner has no special meaning to the lock manager and can be any object, including <code>null</code>. </p>
A Compensation operation can compensate for the action of a log operation. A Compensation operation itself is not undo-able, i.e., it is loggable but not undoable. A Compensation operation is generated by the logging system when it calls undoable.generateUndo().  GenerateUndo should be the <B>only</B> way a compensation operation can be made.
CompilerContext stores the parser and type id factory to be used by the compiler.  Stack compiler contexts when a new, local parser is needed (if calling the compiler recursively from within the compiler, for example). CompilerContext objects are private to a LanguageConnectionContext. History: 5/22/97 Moved getExternalInterfaceFactory() to LanguageConnectionContext because it had to be used at execution. - Jeff
CompilerContextImpl, implementation of CompilerContext. CompilerContext and hence CompilerContextImpl objects are private to a LanguageConnectionContext. end of class CompilerContextImpl
Log operation to implement compressing space from a container and returning it to the operating system.
This class overrides the CompressSpacePageOperation class to write CompressSpaceOperation Log Records that do not support negative values for new_highest_page. No other changes are added to the superclass behavior. This class ensures backward compatibility for Soft upgrades.
This class is used to compress the table to retrieve the space after deletion
Static methods to write and read compressed forms of numbers to DataOut and DataIn interfaces. Format written is platform independent like the Data* interfaces and must remain fixed once a product is shipped. If a different format is required then write a new set of methods, e.g. writeInt2. The formats defined by stored format identifiers are implicitly dependent on these formats not changing.
The ConcatableDataValue interface corresponds to the SQL 92 string value data type.  It is implemented by datatypes that have a length, and can be concatenated. It is implemented by the character datatypes and the bit datatypes. The following methods are defined herein: charLength() The following is defined by the sub classes (bit and char) concatenate()
This node represents a concatenation comparison operator varying.
A cache manager based on the utilities found in the <code>java.util.concurrent</code> package. It allows multiple threads to access the cache concurrently without blocking each other, given that they request different objects and the requested objects are present in the cache. <p> All methods of this class should be thread safe. When exclusive access to an entry is required, it is achieved by calling the <code>lock()</code> method on the <code>CacheEntry</code> object. To ensure that the entry is always unlocked, all calls to <code>CacheEntry.lock()</code> should be followed by a <code>try</code> block with a <code>finally</code> clause that unlocks the entry.
Factory class which creates cache manager instances based on the <code>ConcurrentCache</code> implementation.
This class provides monitoring capabilities for ConcurrentCache through Java Management Extension (JMX).
Test for several threads creating tables in the same originally non-existing schema.  This will cause an implicit creation of the schema.  The test was created for the fix of JIRA issue DERBY-230 where an error occurred if two threads try to create the schema in parallel.
A ConcurrentLockSet is a complete lock table which maps <code>Lockable</code>s to <code>LockControl</code> objects. <P> A LockControl contains information about the locks held on a Lockable. <BR> MT - Mutable : All public methods of this class, except addWaiters, are thread safe. addWaiters can only be called from the thread which performs deadlock detection. Only one thread can perform deadlock detection at a time. <BR> The class creates ActiveLock and LockControl objects. LockControl objects are never passed out of this class, All the methods of LockControl are called while holding a ReentrantLock associated with the Lockable controlled by the LockControl, thus providing the single threading that LockControl required. Methods of Lockables are only called by this class or LockControl, and always while holding the corresponding ReentrantLock, thus providing the single threading that Lockable requires.
A <code>LockFactory</code> which allows multiple threads to enter without blocking each other out.
A Conditional represents an if/then/else block. When this is created the code  will already have the conditional check code. The code is optimized for branch offsets that fit in 2 bytes, though will handle 4 byte offsets. <code> if condition then code else code </code> what actually gets built is <code> if !condition branch to eb: then code goto end:  // skip else eb: else code end: </code> If no else condition was provided then the code is: <code> if !condition branch to end: then code end: </code> Note all branches here are using relative offsets, not absolute program counters. If the then code leads to the conditional branch offset being too big (&gt;32k) because the then code is larger than 32767 bytes then this is built: <code> // when else code is present if condition branch to tb: (relative offset +8) goto_w eb: // indirect for else block (5 bytes) tb: then code (&gt; 32767 bytes) goto end: eb: else code end: </code> <code> // when only then code is present if condition branch to tb: (relative offset +8) goto_w end: // indirect for else block (5 bytes) tb: then code (&gt; 32767 bytes) end: </code> If there is an else branch and only it is larger than 32767 bytes then the code is: <code> if !condition branch to eb: (offset increased by two over previous value) then code goto_w end:  // skip else eb: else code (&gt; 32767 bytes) end: </code> This has one special case where the size of conditional branch to eb: now must change from a 16bit value to a 32 bit value. The generated code for this is the same as when both the then code and the else code require 32bit offsets for the branches. This code is: <code> if condition branch to tb: (relative offset +8) goto_w eb: // indirect for else block (5 bytes) tb: then code (&gt; 32767 bytes) goto_w end: eb: else code (&gt; 32767 bytes) end: </code> In theory, at the moment this should not happen as this would mean a total code size that exceeds the limit on the code size for a method (64k). This code handles this case as it does occur if the limit for a branch is lowered for testing purposes, to ensure the complete set of branch re-write code works. This lowering of the limit can be done by changing the constant BRANCH16LIMIT.
A ConditionalNode represents an if/then/else operator with a single boolean expression on the "left" of the operator and a list of expressions on the "right". This is used to represent the java conditional (aka immediate if).
Supports a configuration object. <p> <code>Configurable</code> is an interface that should be used by a bundle developer in support of a configurable service. Bundles that need to configure a service may test to determine if the service object is an <code>instanceof Configurable</code>.

ConglomPropertyable provides the interfaces to read properties from a conglomerate. <p> RESOLVE - If language ever wants these interfaces on a ScanController it should not be too difficult to add them.
A conglomerate is an abstract storage structure (they correspond to access methods).  The Conglomerate interface corresponds to a single instance of a conglomerate. In other words, for each conglomerate in the system, there will be one object implementing Conglomerate. <P> The Conglomerate interface is implemented by each access method. The implementation must maintain enough information to properly open the conglomerate and scans, and to drop the conglomerate. This information typically will include the id of the container or containers in which the conglomerate is stored, and my also include property information. <P> Conglomerates are created by a conglomerate factory.  The access manager stores them in a directory (which is why they implement Storable).
A conglomerate is an abstract storage structure (they correspond to access methods).  The ConglomerateController interface is the interface that access manager clients can use to manipulate the contents of the underlying conglomerate. <p> Each conglomerate holds a set of rows.  Each row has a row location. The conglomerate provides methods for: <ul> <li> Inserting rows, <li> Fetching, deleting, and replacing entire rows by row location, and <li> fetching and updating individual columns of a row identified by row location. </ul> <p> Conglomerates do not provide any mechanism for associative access to rows within the conglomerate; this type of access is provided by scans via the ScanController interface. <p> Although all conglomerates have the same interface, they have different implementations.  The implementation of a conglomerate determines some of its user-visible semantics; for example whether the rows are ordered or what the types of the rows' columns must be.  The implementation is specified by an implementation id.  Currently there are two implementations, "heap", and "btree".  The details of their behavior are specified in their implementation documentation.  (Currently, only "heap" is implemented). <p> All conglomerate operations are subject to the transactional isolation of the transaction they were opened from.  Transaction rollback will close all conglomerates.  Transaction commit will close all non-held conglomerates. <p> Scans are opened from a TransactionController. <P> A ConglomerateController can handle partial rows. Partial rows are described in RowUtil.
The ConglomerateDescriptor class is used to get information about conglomerates for the purpose of optimization. A ConglomerateDescriptor can map to a base table, an index or a index backing a constraint. Multiple ConglomerateDescriptors can map to a single underlying store conglomerate, such as when multiple index definitions share a physical file. NOTE: The language module does not have to know much about conglomerates with this architecture. To get the cost of using a conglomerate, all it has to do is pass the ConglomerateDescriptor to the access methods, along with the predicate. What the access methods need from a ConglomerateDescriptor remains to be seen.

The factory interface for all conglomerate access methods.
Static utility routine package for all Conglomerates. <p> A collection of static utility routines that are shared by multiple Conglomerate implementations. <p>
Methods implemented by the common Connection class to handle certain events that may originate from the material or common layers.  Reply implementations may update connection state via this interface.
Any class in the embedded JDBC driver (ie this package) that needs to refer back to the EmbedConnection object extends this class.
Interface-ized from EmbedConnectionContext.  Some basic connection attributes that can be obtained from jdbc.
To enable multi-user use of ij.Main2
Test opening connections until a failure due to out of memory and then continue with 500 connection requests to see if the system reacts well of falls over.
This class tests Derby's ability to handle multiple connection attempts against one or more databases, which may or may not exist. Such repeated connection attempts have been known to cause OutOfMemoryErrors in the past, see for example DERBY-2480.

Connection factory using javax.sql.ConnectionPoolDataSource. <p> Connections are obtained by calling <code>getPooledConnection.getConnection()</code>, and statement pooling is enabled.


In addition, Connection objects are passed for convenient access to any material connection caches.

Factory for getting connections within the tests that is designed for the simple working case for most tests. Most tests just need to connect or shutdown the database, this hides through BaseJDBCTestCase and TestConfiguration the details of how those operations are performed. <P> Tests that need finer control over the connection handling should use the JDBC classes directly, such as DriverManager or DataSource. <P> This is split out into an interface and sub-classes to ensure that no ClassNotFoundExceptions are thrown when running in an JSR 169 environment and DriverManager is not available.
Change the Connector implementation at setup time and restore at tearDown time.
This interface describes the columns in a referenced constraint. Added to be the protocol version of ConstraintInfo.
The ConsistencyChecker class provides static methods for verifying the consistency of the data stored within a database. <p>This class can only be used within an SQL-J statement, a Java procedure or a server side Java method. <p>This class can be accessed using the class alias <code> CONSISTENCYCHECKER </code> in SQL-J statements.
Class which represents an RDB Package Consistency Token.
This interface describes actions that are ALWAYS performed for a Statement at Execution time. For instance, it is used for DDL statements to describe what they should stuff into the catalogs. <p> An object satisfying this interface is put into the PreparedStatement and run at Execution time. Thus ConstantActions may be shared across threads and must not store connection/thread specific information in any instance field.
A pre-compiled activation that supports a single ResultSet with a single constant action. All the execution logic is contained in the constant action. <P> At compile time for DDL statements this class will be picked as the implementation of Activation. The language PreparedStatement will contain the ConstantAction created at compiled time. At execute time this class then fetches a language ResultSet using ResultSetFactory.getDDLResultSet and executing the ResultSet will invoke the execute on the ConstantAction.
<p> This visitor replaces a {@code ValueNode} with a node representing a constant value, if the {@code ValueNode} is known to always evaluate to the same value. It may for instance replace a sub-tree representing {@code 1=1} with a constant {@code TRUE}. </p> <p> The actual evaluation of the {@code ValueNode}s is performed by invoking {@link ValueNode#evaluateConstantExpressions()} on every {@code ValueNode} in the query tree. </p> <p> In contrast to most other visitors, this visitor walks the tree bottom-up. Top-down processing of the tree would only evaluate constant expressions at the leaf level, so for instance {@code (1=1)=(1=2)} would only be simplified to {@code TRUE=FALSE}. With bottom-up processing, the top-level = node will be processed after the leaves, and it sees the intermediate tree {@code TRUE=FALSE} which it is able to transform into the even simpler tree {@code FALSE}. </p>
ConstantNode holds literal constants as well as nulls. <p> A NULL from the parser may not yet know its type; that must be set during binding, as it is for parameters. <p> the DataValueDescriptor methods want to throw exceptions when they are of the wrong type, but to do that they must check typeId when the value is null, rather than the instanceof check they do for returning a valid value. <p> For code generation, we generate a static field.  Then we set the field be the proper constant expression (something like <code> getDatavalueFactory().getCharDataValue("hello", ...)) </code>) in the constructor of the generated method.  Ideally we would have just
Constant Pool class - pages 92-99 implements PoolEntry

This class  describes actions that are ALWAYS performed for a constraint creation at Execution time.
A ConstraintDefintionNode is a class for all nodes that can represent constraint definitions.
This class is used to get information from a ConstraintDescriptor. A ConstraintDescriptor can represent a constraint on a table or on a column.

This is a simple class used to store the run time information about a constraint.
Contained roles shows all roles contained in the given identifier, or if the second argument, if given, is not 0, the inverse relation; all roles who contain the given role identifier. <p>To use it, query it as follows: </p> <pre> SELECT * FROM TABLE(SUSCS_DIAG.CONTAINED_ROLES('FOO')) t; </pre> <pre> SELECT * FROM TABLE(CONTAINED_ROLES('FOO', 1)) t; </pre> <p>The following columns will be returned: <ul><li>ROLEID -- VARCHAR(128) NOT NULL </ul> </p>
An Observer that can be attached to a transaction to implement some action when the transaction commits or rollsback in some way.
A Container Operation change the state of the container. A ContainerBasicOperation is the base class for all container operations.
A Container contains a contigious address space of pages, the pages start at page number Container.FIRST_PAGE_NUMBER and are numbered sequentially. The page size is set at addContainer() time. RESOLVE: this style of coding is not currently enforced If the caller calls getPage (or one of its variants) more than once on the same page, the caller must call unlatch a corresponding number of times in order to ensure that the page is latched.  For example: <p> <blockquote><pre> Container c; Page p1 = c.getPage(Container.FIRST_PAGE_NUMBER); Page p2 = c.getPage(Container.FIRST_PAGE_NUMBER); p1.unlatch();  -- Page is still latched. p2.unlatch();  -- Page is now unlatched. </pre></blockquote> <p> There is no restriction on the order in which latching and unlatching is done.  In the example, p1 could have been unlatched after p2 with no ill effects. <P>	<B>Open container modes</B> ContainerHandle.MODE are used to open or create the container. Unlike TableProperties, MODEs are not permanantely associated with the container, it is effective only for the lifetime of the containerHandle itself. <BR>A container may use any of these mode flags when it is opened. <UL> <LI>MODE_READONLY - Open the container in read only mode. <LI>MODE_FORUPDATE - Open the container in update mode, if the underlying storage does not allow updates then the container will be opned in read only mode. <LI>MODE_UNLOGGED - If Unset, any changes to the container are logged. If set, any user changes to the container are unlogged. It is guaranteed at commit time that all changes made during the transaction will have been flushed to disk. Using this mode automatically opens the container in container locking, isolation 3 level. The state of the container following an abort or any type of rollback is unspecified. <LI>MODE_CREATE_UNLOGGED - If set, not only are user changes to the container are unlogged, page allocations are also unlogged.  This MODE is only useful for container is created in the same statement and no change on the container (other than the create) is ever logged.  The difference between MODE_UNLOGGED and MODE_CREATE_UNLOGGED is that page allocation is also unlogged and commit of nested transaction will not cause the container to be forced from the cache.  Unlike MODE_UNLOGGED, MODE_CREATE_UNLOGGED does not force the cache.  It is up to the client of raw store to force the cache at the appropriate time - this allows a statement to create and open the container serveral times for bulk loading without logging or doing any synchronous I/O. <LI>MODE_LOCK_NOWAIT - if set, then don't wait for the container lock, else wait for the container lock.  This flag only dictates whether the lock should be waited for or not.  After the container is successfully opened, whether this bit is set or not has no effect on the container handle. </UL> If neither or both of the {MODE_READONLY, MODE_FORUPDATE} modes are specified then the behaviour of the container is unspecified. <BR> MODE_UNLOGGED must be set for MODE_CREATE_UNLOGGED to be set. <P> <B>Temporary Containers</B><BR> If when creating a container the segment used is ContainerHandle.TEMPORARY_SEGMENT then the container is a temporary container. Temporary containers are not logged or locked and do not live across re-boots of the system. In addition any abort or rollback including rollbacks to savepoints truncate the container if it has been opened for update since the last commit or abort.  Temporary containers are private to a transaction and must only be used a single thread within the transaction at any time, these restrictions are not currently enforced. <BR> When opening a temporary container for update access these additional mode flags may be used <UL> <LI> MODE_TRUNCATE_ON_COMMIT - At commit/abort time container is truncated. <LI> MODE_DROP_ON_COMMIT - At commit/abort time the container is dropped. <LI> MODE_TEMP_IS_KEPT - At commit/abort time the container is kept around. </UL> If a temporary container is opened multiple times in the same transaction with different modes then the most severe mode is used, ie. none &lt; truncate on commit &lt; drop on commit. The MODE_UNLOGGED, MODE_CREAT_UNLOGGED flags are ignored when opening a temporary container, not logged is always assumed.
An abstract class that opens the container at commit and delegates the actual work to a sub-class.
A key that identifies a Container within the RawStore. <BR> MT - Immutable
A ContainerLock represents a qualifier that is to be used when locking a container through a ContainerHandle. <BR> MT - Immutable
A locking policy that implements container level locking with isolation degree 2. * We can inherit all the others methods of NoLocking since we hold the * container lock until the end of transaction, and we don't obtain row * locks.
A locking policy that implements container level locking with isolation degree 3. * We can inherit all the others methods of NoLocking since we hold the * container lock until the end of transaction, and we don't obtain row * locks.
Log operation to create, drop or remove a container. Both the doMe or the undoMe of a create actually caused the container header to be modified and flushed before the log record is flushed.  This is necessary for 2 reasons, one is that of ensuring enough disk space, and the other is because unlike any other operation, the log record create container is in the log stream before the container is in the container cache.  What this mean is that if a checkpoint started after the container operation but before the container is kept or is dirtied in the container cache, then checkpoint will not know to wait for the container to be kept or cleaned.  The checkpoint will erroneous assume that the operation does not need to be redone since its log instant is before the checkpoint but in fact the change has not been flushed to disk. A drop or remove container does not have this problem.  The container exist and is in kept state when the operation is logged so the checkpoint will not overlook it and it doesn't need to flush the container header.  In the case of remove, the stub is flushed for a different reason - that of ensuring disk space.
A Container undo operation rolls back the change of a Container operation
Contexts are created and used to manage the execution environment. They provide a convenient location for storing globals organized by the module using the globals. <p> A basic context implementation is provided as an abstract class; this implementation satisfies the interface and should in general be used as the supertype of all context types.  Otherwise, context classes must satisfy the semantics of the interface through their own distinct implementations. <p> Contexts assist in cleanup when errors are caught in the outer block. <p> Use of context cleanup is preferred over using try/catch blocks throughout the code. <p> Use of context pushing and popping is preferred over using many instance or local variables, even when try/catch is present. when the instance or local variables would be holding resources. <P> Usually Context's have a reference based equality, ie. they do not provide an implementation of equals(). Contexts may implement a value based equality but this usually means there is only one instance of the Context on the stack, This is because the popContext(Context) will remove the most recently pushed Context that matches via the equals method, not by a reference check. Implementing equals is useful for Contexts used in notifyAllThreads() that is not aimed at a single thread.

Contexts are created and used to manage the execution environment. They provide a convenient location for storing globals organized by the module using the globals. <p> We provide this abstract class for other implementations to use so that they can simply add fields and operations on them. To be usable by the context manager, the subclasses must define CleanupOnError and call super() in any constructor. <p> Contexts assist in cleanup when errors are caught in the outer block. <p> Contexts implement the sanity interface to check and provide information about their contents.
The ContextManager collects contexts as they are created. It maintains stacks of contexts by named ids, so that the top context of a given type can be returned. It also maintains a global stack so that contexts can be traversed in the order they were created. <p> The first implementation of the context manager assumes there is only one thread to worry about and that the user(s) of the class only create one instance of ContextManager.
A set of static methods to supply easier access to contexts. OLD extends Hashtable

read the control file properties. If the passed parameter for control file name is null, assigns default values to the properties. Also, if the control file has message property in it, it sends the errors to that file by redirecting system err to that message file
Base class for leaf and branch control rows. <P> <B>Concurrency Notes</B> <P> All access through control rows is serialized by an exclusive latch on the page the control row is for.  The page is latched when the control row is "gotten" (ControlRow#Get), and unlatched when the control row is released (ControlRow#release). <P> <B>To Do List</B> <UL> <LI> <I>[NOTE1]</I> The code is arranged to fault in fields from the row as necessary. many of the fields of a control row are rarely used (left sibling, parent). The accessors fault in the underlying column only when requested by allocating the appropriate object and calling fetchFromSlot and only fetching the requested field. <LI> <I>[NOTE2]</I> Currently, all the fields of the control row are stored as StorableU8s for simplicity.  This is too few bits to hold the long page numbers, and too many to hold the version, level, and isRoot flag.  Some consideration will have to be given to the appropriate storage format for these values. <LI> <I>[NOTE3]</I> The implementation relies on the existance of page "auxiliary" pointers which keep Object versions of the control row. <P>
Represents copying num_rows from one page to another page. <PRE>
For tests which require support files. Copy them to the output directory for the test.
FormatableInstanceGetter to load stored instances of DependableFinder. Class is registered in RegisteredFormatIds
This class provides a proxy base implementation of the WritableStorageFactory interface to instrument I/O operations for testing purposes. Some methods in this class adds support for corrupting the I/O operation sent by the engine before invoking the real storage factory underneath. By deault all the calls will go to the real storage factory defined by the concrete class, unless corruption is enabled through CorruptibleIo instance.
This class provides proxy implementation of the StorageFactory interface for testing. Storage Factory is used by the database engine to access persistent data and transaction logs. By default all the method calls delegate the work to the real disk storage factory (org.apache.derby.impl.io.DirStorageFactory) based on the classes in the java.io packgs. In some cases this  factory instruments some methods to corrupt the io to simulate disk corruptions for testing. For example to simulate out of order partial writes to disk before the crash. Derby by default uses the storage factory implementation in DirStorageFactory/DirStorageFactory4 when a database is accessed with "jdbc:derby:<databaseName>". This factory can be specified instead using derby.subSubProtocol.<sub protocol name>  For example: derby.subSubProtocol.csf=org.apache.derbyTesting.functionTests. util.corruptio.CorruptDiskStorageFactory database need to be accessed by specifying the subporotocol name like 'jdbc:derby:csf:wombat'. Interaction between the tests that requires instrumenting the i/o and this factory is through the flags in CorruptibleIo class. Tests should not call the methods in this factory directly. Database engine invokes the methods in this factory, so they can instrumented to do whatever is required for testing.
This class provides proxy implementation of the StorageFile interface. It is used by CorruptDiskStorageFactory to instrument the database engine i/o for testing.
This class provides a proxy implementation of the StorageRandomAccess File interface.  It is used by CorruptDiskStorageFactory to instrument the database engine i/o for testing puproses. How the i/o operation are corrupted is based on the values set in the instance of the Singleton CorruptibleIo class by the tests. Methods in this class functon similar to java.io.RandomAccessFile except when modified to perform the corruptios.

This is a helper class to instrument the CorruptDiskStorageFactory to modify the i/o opertions before the request are sent to a real storage factory. Tests can specify what type of corruption is required like log/data files and the at what and offset and the length of the corruption to be done in the write requests. Only one instance of this class will exist in the system, Tests should hold onto the instance of this class until they are done sending the i/o requests by executing statement that will actuall will trigger i/o , for example a commit will flush the log buffers. Otherwise class garbage collector can reinitialize the values. @version 1.0 @see WritableStorageFactory @see StorageFactory
A CostEstimate represents the cost of getting a ResultSet, along with the ordering of rows in the ResultSet, and the estimated number of rows in this ResultSet.

Definition for the COUNT()/COUNT(*) aggregates.
Aggregator for COUNT()/COUNT(*).
An OutputStream that simply provides methods to count the number of bytes written to an underlying stream.
Index scan tests.
This class performs actions that are ALWAYS performed for a CREATE FUNCTION, PROCEDURE or SYNONYM Statement at execution time. These SQL objects are stored in the SYS.SYSALIASES table and represented as AliasDescriptors.
A CreateAliasNode represents a CREATE ALIAS statement.
This class  describes actions that are ALWAYS performed for a constraint creation at Execution time.
This class is used by PlanExporter tool (DERBY-4587) in order to create HTML output of a query plan using a plain XSL style sheet and a XML data of a query plan.
ConstantAction to create an index either through a CREATE INDEX statement or as a backing index to a constraint.
A CreateIndexNode is the root of a QueryTree that represents a CREATE INDEX statement.
This class performs actions that are ALWAYS performed for a CREATE ROLE statement at execution time. These SQL objects are stored in the SYS.SYSROLES table.
A CreateRoleNode is the root of a QueryTree that represents a CREATE ROLE statement.
This class describes actions that are ALWAYS performed for a CREATE SCHEMA Statement at Execution time.
A CreateSchemaNode is the root of a QueryTree that represents a CREATE SCHEMA statement.
This class performs actions that are ALWAYS performed for a CREATE SEQUENCE statement at execution time. These SQL objects are stored in the SYS.SYSSEQUENCES table.
A CreateSequenceNode is the root of a QueryTree that represents a CREATE SEQUENCE statement.
This class  describes actions that are ALWAYS performed for a CREATE TABLE Statement at Execution time.
A CreateTableNode is the root of a QueryTree that represents a CREATE TABLE or DECLARE GLOBAL TEMPORARY TABLE statement.
This class  describes actions that are ALWAYS performed for a CREATE TRIGGER Statement at Execution time.
A CreateTriggerNode is the root of a QueryTree that represents a CREATE TRIGGER statement.
This class  describes actions that are ALWAYS performed for a CREATE VIEW Statement at Execution time. A view is represented as: <UL> <LI> TableDescriptor with the name of the view and type VIEW_TYPE <LI> Set of ColumnDescriptor's for the column names and types <LI> ViewDescriptor describing the SQL query that makes up the view. </UL> Dependencies are created as: <UL> <LI> ViewDescriptor depends on the Providers that its compiled query depends on. <LI> ViewDescriptor depends on the privileges required to execute the view. </UL> Note there are no dependencies created between the ViewDescriptor, TableDecriptor and the ColumnDescriptor's.
A CreateViewNode is the root of a QueryTree that represents a CREATE VIEW statement.
This class is to create the final xml file, that will be used by the Graphical Query Explainer. This is called from org.apache.derby.tools.PlanExporter.
All currently supported derby types are mapped to one of the following jdbc types: Types.SMALLINT; Types.INTEGER; Types.BIGINT; Types.REAL; Types.DOUBLE; Types.DECIMAL; Types.DATE; Types.TIME; Types.TIMESTAMP; Types.CHAR; Types.VARCHAR; Types.LONGVARCHAR; Types.CLOB; Types.BLOB;
CurrentDatetime provides execution support for ensuring that the current datetime is evaluated only once for a statement. The same value is returned for every CURRENT_DATE, CURRENT_TIME, and CURRENT_TIMESTAMP in the statement. <p> This is expected to be used by an activation and its result set, and so 'forget' must be called whenever you want to reuse the CurrentDatetime object for additional executions of the statement.
The CurrentDatetimeOperator operator is for the builtin CURRENT_DATE, CURRENT_TIME, and CURRENT_TIMESTAMP operations.
The CurrentOf operator is used by positioned DELETE and UPDATE to get the current row and location for the target cursor.  The bind() operations for positioned DELETE and UPDATE add a column to the select list under the statement for the row location accessible from this node. This node is placed in the from clause of the select generated for the delete or update operation. It acts much like a FromBaseTable, using the information about the target table of the cursor to provide information.
Takes a cursor name and returns the current row of the cursor; for use in generating the source row and row location for positioned update/delete operations. <p> This result set returns only one row.
The CurrentRowLocation operator is used by DELETE and UPDATE to get the RowLocation of the current row for the target table.  The bind() operations for DELETE and UPDATE add a column to the target list of the SelectNode that represents the ResultSet to be deleted or updated.
Return the current system time as a String Used to print a timestamp for suite/test runs
When we calculate column offsets make sure we calculate the correct offsets for double byte charactr5er data length from server is number of chars, not bytes Direct byte-level converters are called directly by this class, cross converters are deferred to the CrossConverters class.
CursorActivation includes an additional method used on cursors.
A basic holder for information about cursors for execution.
A CursorNode represents a result set that can be returned to a client. A cursor can be a named cursor created by the DECLARE CURSOR statement, or it can be an unnamed cursor associated with a SELECT statement (more precisely, a table expression that returns rows to the client).  In the latter case, the cursor does not have a name.
The CursorResultSet interface is used to provide additional operations on result sets that can be used in cursors. <p> Since the ResulSet operations must also be supported by cursor result sets, we extend that interface here as well.

No Contents we just need our own CustomMbeanServerBuilder for the DERBY-3887 test
An Order Entry customer. <P> Fields map to definition in TPC-C for the CUSTOMER table. The Java names of fields do not include the C_ prefix and are in lower case. <BR> For clarity these fields are renamed in Java <UL> <LI>w_id =&gt; warehouse (SQL column C_W_ID) <LI>d_id =&gt; district (SQL column C_D_ID) </UL> <BR> The columns that map to an address are extracted out as a Address object with the corresponding Java field address. <BR> All fields have Java bean setters and getters. <BR> Fields that are DECIMAL in the database map to String in Java (rather than BigDecimal) to allow running on J2ME/CDC/Foundation. <P> Primary key maps to {warehouse,district,id}. <P> A Customer object may sparsely populated, when returned from a business transaction it is only guaranteed to contain  the information required to display the result of that transaction.
This node represents a unary DB2 compatible length operator
Interface for classes that populate a database for a certain test.
<p> OptionalTool to create wrapper functions which allow you to invoke DatabaseMetaData methods via SQL. The wrapper functions slightly change the signature of the metadata methods as follows: </p> <ul> <li>Arguments of type int[] and String[] have been eliminated--they are automatically wildcarded.</li> <li>The method getRowIdLifetime() has been commented out--Derby does not support object types.</li> <li>The method getSchemas() has been commented out--it can be uncommented when the registration logic is made smarter to handle the dropping of different overloads.</li> <li>The method supportsConvert() has been commented out because Derby only allows one function by a given name and the supportsConvert( int, int ) overload is more general.</li> </ul> <p> Methods which return ResultSet are mapped to table functions. You can join the metadata table functions like this: </p> <pre> -- list all of the columns in the connected Derby database select t.table_schem, t.table_name, c.column_name, c.type_name from table( getTables( null, null, null ) ) t, table( getColumns( null, null, null, null ) ) c where c.table_schem = t.table_schem and c.table_name = t.table_name and t.table_type = 'TABLE' ; -- now list metadata in a foreign database call setDatabaseURL( 'com.mysql.jdbc.Driver', 'jdbc:mysql://localhost/world?user=root&amp;password=' ); select t.table_schem, t.table_name, c.column_name, c.type_name from table( getTables( 'WORLD', null, null ) ) t, table( getColumns( 'WORLD', null, null, null) ) c where c.table_name = t.table_name and t.table_type = 'TABLE' ; -- release the foreign connection call setDatabaseURL( '', '' ); </pre>









Dblook implementation for SEQUENCEs.



Class for implementation of DependableFinder in the core DataDictionary for referenced columns in a table.
Abstract class that has actions that are across all DDL actions.
Abstract class that has actions that are across all DDL actions that are tied to a table.  An example of DDL that affects a table is CREATE INDEX or DROP VIEW.  An example of DDL that does not affect a table is CREATE STATEMENT or DROP SCHEMA.
A DDLStatementNode represents any type of DDL statement: CREATE TABLE, CREATE INDEX, ALTER TABLE, etc.
The DDMReader is used to read DRDA protocol.   DRDA Protocol is divided into three layers corresponding to the DDM three-tier architecture. For each layer, their is a DSS (Data Stream Structure) defined. Layer A     Communications management services Layer B     Agent services Layer C     Data management services <P> At layer A are request, reply and data correlation, structure chaining, continuation or termination of chains when errors are detected, interleaving and multi-leaving request, reply, and data DSSs for multitasking environments. For TCP/IP, the format of the DDM envelope is 2 bytes     Length of the data 1 byte      'D0' - indicates DDM data 1 byte      DDM format byte(DSSFMT) - type of DSS(RQSDSS,RPYDSS), whether it is chained, information about the next chained DSS 2 bytes     request correlation identifier <P> The correlation identifier ties together a request, the request data and the reply.  In a chained DSS, each request has a correlation identifier which is higher than the previous request (all correlation identifiers must be greater than 0). <P> At layer B are object mapping, object validation and command routing. Layer B objects with data 5 bytes less than 32K bytes consist of 2 bytes     Length 2 bytes     Type of the object (code point) Object data Object data is either SCALAR or COLLECTION data.  Scalar data consists of a string of bytes formatted as the class description of the object required. Collections consist of a set of objects in which the entries in the collection are nested within the length/ code point of the collection. <P> Layer B objects with data &gt;=32763 bytes long format is 2 bytes     Length - length of class, length, and extended total length fields (high order bit set, indicating &gt;=32763) 2 bytes     Type of the object (code point) n bytes     Extended total length - length of the object (n = Length - 4) Object data <P> At layer C are services each class of DDM object provides. |-------------------------------------------| Layer C | Specific  |   Specific    |   Specific    | | Commands  |   Replies     | Scalars and   | | and their |  and their    | Collections   | |-------------------------------------------|----------------| Layer B | Commands  |    Reply      | Scalars and   | Communications | |           |   Messages    | Collections   |                | |-----------|---------------|---------------|----------------| Layer A |  RQSDSS   |   RPYDSS      | OBJDSS        | CMNDSS         | |           |               |               | Mapped Data    | |-----------|---------------|---------------|----------------| |                DDM Data Stream Structures                  | |------------------------------------------------------------| DSS's may be chained so that more than one can be transmitted at a time to improve performance. For more details, see DRDA Volume 3 (Distributed Data Management(DDM) Architecture (DDS definition)
The DDMWriter is used to write DRDA protocol.   The DRDA Protocol is described in the DDMReader class. For more details, see DRDA Volume 3 (Distributed Data Management(DDM) Architecture (DDS definition)
Static Data dictionary utilities.
Generic code for upgrading data dictionaries. Currently has all minor version upgrade logic. <p> A word about minor vs. major upgraded.  Minor upgrades must be backwards/forwards compatible. So they cannot version classes or introduce new classes.  Major releases are only backwards compatible; they will run against an old database, but not the other way around.  So they can introduce new classes, etc.
Class for most DependableFinders in the core DataDictionary. This class is stored in SYSDEPENDS for the finders for the provider and dependent. It stores no state, its functionality is driven off its format identifier.
Test DML statement called from within static initializer
A DMLModGeneratedColumnsStatementNode for a table(with identity columns) modification: to wit, INSERT, UPDATE. The code below used to reside in InsertNode but when we fixed DERBY-6414, rather than duplicating the code in UpdateNode, we moved the common code for insert and update of identity columns to this class.
A DMLStatement for a table modification: to wit, INSERT UPDATE or DELETE.
A DMLStatementNode represents any type of DML statement: a cursor declaration, an INSERT statement, and UPDATE statement, or a DELETE statement.  All DML statements have result sets, but they do different things with them.  A SELECT statement sends its result set to the client, an INSERT statement inserts its result set into a table, a DELETE statement deletes from a table the rows corresponding to the rows in its result set, and an UPDATE statement updates the rows in a base table corresponding to the rows in its result set.
Base class for Insert, Delete and UpdateVTIResultSet
This class includes code for auto generated columns that can be shared by insert and update statements in the execution phase.
For INSERT/UPDATE/DELETE impls.  Used to tag them.
This class translates DRDA protocol from an application requester to JDBC for Derby and then translates the results from Derby to DRDA for return to the application requester.

Class used to transport that fact that we had a timeout and should *NOT* disconnect the connection. See DRDAConnThread.run()

DRDAResultSet holds result set information
Class that starts the network server in its own daemon thread. Works in two situations. <BR> As a module in the engine's Monitor, booted if the property derby.drda.startNetworkServer is set to true. In this case the boot and shutdown is through the standard ModuleControl methods. <BR> Direct calls from the NetworkServerControlImpl start methods. This is to centralize the creation of the daemon thread in this class in the engine code, since the Monitor provides the thread. This means that NetworkServerControlImpl calls this class to create a thread which in turn calls back to NetworkServerControlImpl.runServer to start the server.
DRDAStatement stores information about the statement being executed
This class provides functionality for reusing buffers and strings when parsing DRDA packets. A byte array representing a string is stored internally. When the string is requested as a <code>String</code> object, the byte array is converted to a string, and the string is cached to avoid unnecessary conversion later.
This class translates DRDA XA protocol from an application requester to XA calls for Derby and then translates the results from Derby to DRDA for return to the application requester. This class requires the use of javax.transaction.xa classes from j2ee, so is separated from DRDAConnThread, because of the additional library requirements


Debugging class used to print debug information about a B2I. Code here can be used in SANE development builds but the class is not necessary for a release so does not add footprint to a customer release. See the DiagnosticableGeneric interface for more information. Note that all the real work currently is inherited from it's parent, but this class needs to exist as the diagnostic interface requires a class names D_XXX to provide information about XXX.
A BTreeDiag class is a "helper" class for the rest of the btree generic code.  It is separated into a separate class so that it can be compiled out if necessary (or loaded at runtime if necessary). <p> more info.
The D_BaseContainerHandle class provides diagnostic information about the BaseContainerHandle class.  Currently this info is a single string of the form TABLE(conglomerate_id, container_id)
The D_BaseContainerHandle class provides diagnostic information about the BaseContainerHandle class.  Currently this info is a single string of the form TABLE(conglomerate_id, container_id)
The D_ContainerLock class provides diagnostic information about the ContainerLock qualifer, and is used for output in lock debugging.
import java.util.Properties; DEBUGGING: This class provides some utility functions used to debug on disk structures of the store.

The HeapController_D class implements the Diagnostics protocol for the HeapController class.


The D_RecordId class provides diagnostic information about the BaseContainerHandle class.  Currently this info is a single string of the form ROW(conglomerate_id, page_number, record_id)
The D_RowLock class provides diagnostic information about the RowLock qualifer, and is used for output in lock debugging.
The D_StoredPage class provides diagnostic information about the StoredPage class.  Currently this info includes: o a dump of the page. o page size of the page. o bytes free on the page. o bytes reserved on the page.
The D_Xact class provides diagnostic information about the Xact class.
Daemon Factory can create new DaemonService, which runs on seperate background threads.  One can use these DaemonService to handle background clean up task by implementing Serviceable and subscribing to a DaemonService. A DaemonService is a background worker thread which does asynchronous I/O and general clean up.  It should not be used as a general worker thread for parallel execution.  A DaemonService can be subscribe to by many Serviceable objects and a daemon will call that object's performWork from time to time.  These performWork method should be well behaved - in other words, it should not take too long or hog too many resources or deadlock with anyone else.  And it cannot (should not) error out. The best way to use a daemon is to have an existing DaemonService and subscribe to it. If you can't find an existing one, then make one thusly: DaemonService daemon = DaemonFactory.createNewDaemon(); After you have a daemon, you can subscribe to it by int myClientNumber = daemon.subscribe(serviceableObject); and ask it to run performWork for you ASAP by daemon.serviceNow(myClientNumber); Or, for one time service, you can enqueue a Serviceable Object by daemon.enqueue(serviceableObject, true);  - urgent service daemon.enqueue(serviceableObject, false); - non-urgent service
A DaemonService provides a background service which is suitable for asynchronous I/O and general clean up.  It should not be used as a general worker thread for parallel execution.  A DaemonService can be subscribe to by many Serviceable objects and a DaemonService will call that object's performWork from time to time.  The performWork method is defined by the client object and should be well behaved - in other words, it should not take too long or hog too many resources or deadlock with anyone else.  And it cannot (should not) error out. <P>It is up to each <code>DaemonService</code> implementation to define its level of service, including <UL> <LI>how quickly and how often the clients should expect to be be serviced <LI>how the clients are prioritized <LI>whether the clients need to tolerate spurious services </UL> <P>MT - all routines on the interface must be MT-safe.

This is an implementation of the DataDescriptorGenerator interface that lives in the DataDictionary protocol.  See that interface for a description of what this class is supposed to do.
The DataDictionary interface is used with the data dictionary to get descriptors for binding and compilation. Some descriptors (such as table and column descriptors) are added to and deleted from the data dictionary by other modules (like the object store). Other descriptors are added and deleted by the language module itself (e.g. the language module adds and deletes views, because views are too high-level for modules like the object store to know about).
Standard database implementation of the data dictionary that stores the information in the system catalogs.

Table function for reading a data file in the seg0 directory of a Derby database. This is based on the 10.11 version of data records.
A util class for DataInput.
Connection factory using javax.sql.DataSource. Should work for an Derby data source, including JSR169 support.
A virtual data store, keeping track of all the virtual files existing and offering a set of high-level operations on virtual files. <p> A newly created data store doesn't contain a single existing directory.
A data store entry representing either a file or a directory. <p> If the entry is a directory, it doesn't create a data object.
DataType is the superclass for all data types. It provides common behavior for datavalue descriptors -- it throws exceptions for all of the get* and setvalue(*)  methods of DataValueDescriptor; the subtypes need only override the one for the type they represent and all types it can also be returned as, and the methods dealing with nulls. Since all types satisfy getString DataType does not define that interfaces of DataValueDescriptor. DataType is a little glue for columns to hold values with.
DataTypeDescriptor describes a runtime SQL type. It consists of a catalog type (TypeDescriptor) and runtime attributes. The list of runtime attributes is: <UL> <LI> Collation Derivation </UL> <P> A DataTypeDescriptor is immutable.
A set of static utility methods for data types.
Class DataUtils: Utility class to drop/create database objects and populate data
The DataValueDescriptor interface provides methods to get the data from a column returned by a statement. <p> This interface matches the getXXX methods on java.sql.ResultSet. This means everyone satisfies getString and getObject; all of the numeric types, within the limits of representation, satisfy all of the numeric getXXX methods; all of the character types satisfy all of the getXXX methods except getBytes and getBinaryStream; all of the binary types satisfy getBytes and all of the getXXXStream methods; Date satisfies getDate and getTimestamp; Time satisfies getTime; and Timestamp satisfies all of the date/time getXXX methods. The "preferred" method (one that will always work, I presume) is the one that matches the type most closely. See the comments below for "preferences".  See the JDBC guide for details. <p> This interface does not include the getXXXStream methods. <p> The preferred methods for JDBC are: <p> CHAR and VARCHAR - getString() <p> BIT - getBoolean() <p> TINYINT - getByte() <p> SMALLINT - getShort() <p> INTEGER - getInt() <p> BIGINT - getLong() <p> REAL - getFloat() <p> FLOAT and DOUBLE - getDouble() <p> DECIMAL and NUMERIC - getBigDecimal() <p> BINARY and VARBINARY - getBytes() <p> DATE - getDate() <p> TIME - getTime() <p> TIMESTAMP - getTimestamp() <p> No JDBC type corresponds to getObject().  Use this for user-defined types or to get the JDBC types as java Objects.  All primitive types will be wrapped in their corresponding Object type, i.e. int will be wrapped in an Integer. <p> getStream()
This interface is how we get data values of different types. For any method that takes a 'previous' argument it is required that the caller pass in an object of the same class that would be returned by the call if null was passed for previous.
Core implementation of DataValueFactory.
Database stores information about the current database It is used so that a session may have more than one database
Change the current configuration's database name at setup. Previous configuration is restored on tearDown.
An abstract implementation of the ClassFactory. This package can be extended to fully implement a ClassFactory. Implementations can differ in two areas, how they load a class and how they invoke methods of the generated class. <P> This class manages a hash table of loaded generated classes and their GeneratedClass objects.  A loaded class may be referenced multiple times -- each class has a reference count associated with it.  When a load request arrives, if the class has already been loaded, its ref count is incremented.  For a remove request, the ref count is decremented unless it is the last reference, in which case the class is removed.  This is transparent to users.
A context for a database.
A context that shutdowns down the database on a databsae exception.
A DatabaseInstant is a quantity which the database associates with events to collate them. This interface is used in the column SYS.SYSSYNCINSTANTS.INSTANT. <P> Assume a database associates a DatabaseInstant to an event E1. We call this I(E1). Also assume the same Database associates a DatabaseInstant to a second event E2. We call this I(E2). By definition <OL> <LI> If I(E1) &lt; I(E2) event E1 occurred before event E2 <LI> If I(E2) = I(E2) event E1 is the same event as E2 <LI> If I(E1) &gt; I(E2) event E1 occurred after event E2 </OL> <P>It is not meaningful to compare a DatabaseInstant from one database with a DatabaseInstant from another. The result of such a comparison is undefined. Because a database may construct, store and compare huge numbers of DatabaseInstants, this interface does not require an implementation to notice when a caller compares a DatabaseInstants from different databases. <P> Any implementation of this interface must implement value equality, thus implementing equals() and hashCode() methods.
This class represents access to database-scoped privileges. An example of database-scoped privileges is the permission to create a database under a specified directory path. <p> A DatabasePermission is defined by two string attributes, similar to a java.io.FilePermission: <ul> <li> <i>URL</i> - a location description of or for a Derby database <li> <i>Actions</i> - a list of granted administrative actions </ul> The database location URL may contain certain wildcard characters. The currently only supported database action is <i>create</i>.
This class is used to insert, delete and updated the rows
High performance converters from date/time byte encodings to JDBC Date, Time and Timestamp objects. <p/> Using this class for direct date/time conversions from bytes offers superior performance over the alternative method of first constructing a Java String from the encoded bytes, and then using {@link java.sql.Date#valueOf java.sql.Date.valueOf()}, {@link java.sql.Time#valueOf java.sql.Time.valueOf()} or {@link java.sql.Timestamp#valueOf java.sql.Timestamp.valueOf()}. <p/>

This class provides a simple regular expression parser for standard format dates, times, and timestamps
This class represents a date or time value as it is represented in the database. In contrast to {@code java.sql.Date}, {@code java.sql.Time} and {@code java.sql.Timestamp}, which are based on {@code java.util.Date}, this class does <b>not</b> represent the time as an offset from midnight, January 1, 1970 GMT. Instead, it holds each component (year, month, day, hour, minute, second, nanosecond) as it would have been represented in a given calendar. Since it does not hold information about the time zone for the time it represents, it does not point to a well-defined point in time without being used together with a {@code java.util.Calendar} object.

Utility class for testing files stored in the database.
DbSetup: Creates database and builds single user table with indexes end of class definition

DbUtil - a database utility class for all IUD and Select operations
<p> Code to support deadlock detection. </p> <p> This class implements deadlock detection by searching for cycles in the wait graph. If a cycle is found, it means that (at least) two transactions are blocked by each other, and one of them must be aborted to allow the other one to continue. </p> <p> The wait graph is obtained by asking the {@code LockSet} instance to provide a map representing all wait relations, see {@link #getWaiters}. The map consists of two distinct sets of (key, value) pairs: </p> <ol> <li>(space, lock) pairs, where {@code space} is the compatibility space of a waiting transaction and {@code lock} is the {@code ActiveLock} instance on which the transaction is waiting</li> <li>(lock, prevLock) pairs, where {@code lock} is an {@code ActiveLock} and {@code prevLock} is the {@code ActiveLock} or {@code LockControl} for the first waiter in the queue behind {@code lock}</li> </ol> <p> The search is performed as a depth-first search starting from the lock request of a waiter that has been awoken for deadlock detection (either because {@code derby.locks.deadlockTimeout} has expired or because some other waiter had picked it as a victim in order to break a deadlock). From this lock request, the wait graph is traversed by checking which transactions have already been granted a lock on the object, and who they are waiting for. </p> <p> The state of the search is maintained by pushing compatibility spaces (representing waiting transactions) and granted locks onto a stack. When a dead end is found (that is, a transaction that holds locks without waiting for any other transaction), the stack is popped and the search continues down a different path. This continues until a cycle is found or the stack is empty. Detection of cycles happens when pushing a new compatibility space onto the stack. If the same space already exists on the stack, it means the graph has a cycle and we have a deadlock. </p> <p> When a deadlock is found, one of the waiters in the deadlock cycle is awoken and it will terminate itself, unless it finds that the deadlock has been broken in the meantime, for example because one of the involved waiters has timed out. </p>
A deadlock watch utlity.  An instance of this class can be created and started and it will look for a Java level deadlock at the time given  if not stopped. A deadlock is detected using JMX and the ThreadMXBean

Converters from fixed point decimal bytes to <code>java.math.BigDecimal</code>, <code>double</code>, or <code>long</code>.

Utility class that provides static methods to decorate tests. Used as a central collection point for decorators than cannot be simply expressed as a TestSetup class. Typically the decorators will be collections of other decorators
A DecryptInputStream is used by stream container to access an encrypted stream of bytes.
This class is used to decrypt password and/or userid. It uses Diffie_Hellman algorithm to get the publick key and secret key, and then DES encryption is done using certain token (based on security mechanism) and this side's own public key. Basically, this class is called when using a security mechanism that encrypts user ID and password (eusridpwd). This class uses IBM JCE to do Diffie_Hellman algorithm and DES encryption.
This interface is used to get information from a DefaultDescriptor.
<p>An interface for describing a default for a column or parameter in Derby systems.</p>


DefaultNode represents a column/parameter default.
This is the default optimizer tracing logic for use when a custom tracer wasn't specified.
This class implements the default policy for defering modifications to virtual tables.
This interface is implemented by a read/write VTI class that wants to control when modifications to the VTI are deferred, or to be notified that a it is to be modified. Consider the following statement:<br> UPDATE NEW myVTI(...) SET cost = cost + 10 WHERE cost &lt; 15 <p> Updating a column that is used in the WHERE clause might or might not give the VTI implementation trouble; the update might cause the same row to be selected more than once. This problem can be solved by building the complete list of rows to be updated and the new column values before updating any rows. The updates are applied after the list is built. This process is called "deferred update". <p> By default, updates on a VTI are deferred when the VTI ResultSet is scrollable (ResultSet.TYPE_SCROLL_SENSITIVE or TYPE_SCROLL_INSENSITIVE), and one or more of the following is true. <ol> <li>One or more of the columns in the SET clause is also used in the WHERE clause and the VTI ResultSet is sensitive. We do not defer updates when the ResultSet is TYPE_SCROLL_INSENSITIVE because it is not necessary. <li>The where clause contains a subselect on a VTI from the same class as the target VTI. We do not look at the VTI parameters, just the VTI class name. </ol> <p> By default, deletes on a VTI are deferred in a similar situation: when the VTI ResultSet is scrollable (ResultSet.TYPE_SCROLL_SENSITIVE or TYPE_SCROLL_INSENSITIVE), and the where clause contains a subselect on a VTI from the same class as the target VTI. We do not look at the VTI parameters, just the VTI class name. <p> By default, inserts into a VTI are deferred when the same VTI class is used as both the source and target. It does not depend on the scrollability of the VTI ResultSet because inserts can be deferred without scrolling the ResultSet. <p> If these defaults are not appropriate then the class implementing the VTI should also implement this interface (org.apache.derby.vti.DeferModification). <p> (A read/write VTI is implemented by a class that implements the java.sql.PreparedStatement interface, often by extending the UpdatableVTITemplate interface. @see UpdatableVTITemplate). <p> Update and delete statement deferral is implemented by scrolling the VTI's ResultSet. Therefore, updates and deletes on a VTI are never deferred unless the VTI's ResultSets are scrollable, even if the DeferModification interface methods return <b>true</b>. Therefore for an update or delete to be deferred the VTI getResultSetType() method must return ResultSet.TYPE_SCROLL_SENSITIVE or TYPE_SCROLL_INSENSITIVE and the VTI must produce scrollable java.sql.ResultSets that implement the getRow() and absolute() methods. If your VTI is implemented as an extension to UpdatableVTITemplate then you must override the getResultSetMethod: UpdatableVTITemplate.getResultSetType() throws an exception. If your VTI's ResultSets are implemented as extensions to VTITemplate then you must override the getRow() and absolute() methods: VTITemplate.getRow() and absolute() throw exceptions. <p> This interface is not used when the VTI is referenced only in a subselect; it is only used when a VTI appears as the target of an INSERT, UPDATE, or DELETE statement.
This class provides support for deferrable constraints. When the constraint mode is deferred, any violation of the constraint should not be flagged until the constraint mode is switched back to immediate, which may happen by explicitly setting the constraint mode to immediate, or implicitly at commit time. It may also happen implicitly when returning from a stored procedure if the constraint mode is immediate in the caller context. <p> The approach taken in Derby to support deferred constraints is to make a note when the violation happens (at insert or update time), and then remember that violation until the mode switches back as described above.  We note exactly which rows cause violations, so checking can happen as quickly as possible when we get there. The core mechanism used to remember the violations as well as the deferred checking is embodied in this class.
Delete the rows from the specified  base table and executes delete/update on dependent tables depending on the referential actions specified. Note:(beetle:5197) Dependent Resultsets of DeleteCascade Resultset can  in any one of the multiple resultsets generated for the same table because of multiple foreign key relationship to  the same table. At the bind time , dependents are binded only once per table. We can not depend on mainNodeTable Flag to fire actions on dependents, it should be done based on whether the resultset has dependent resultsets or not.
This class  describes compiled constants that are passed into DeleteResultSets. KeyToBaseRowConstantAction METHODS
A DeleteNode represents a DELETE statement. It is the top-level node for the statement. For positioned delete, there may be no from table specified. The from table will be derived from the cursor specification of the named cursor.
Represents a delete (or undelete) of a record in a page. <PRE>
Delete the rows from the specified base table. This will cause constraints to be checked and triggers to be executed based on the c's and t's compiled into the insert plan.
Delete the rows from the specified base table. This will cause constraints to be checked and triggers to be executed based on the c's and t's compiled into the insert plan.
lifted from TypeFactoryImpl.DTSClassInfo
A Dependable is an in-memory representation of an object managed by the Dependency System. There are two kinds of Dependables: Providers and Dependents. Dependents depend on Providers and are responsible for executing compensating logic when their Providers change. <P> The fields represent the known Dependables. <P> Persistent dependencies (those between database objects) are stored in SYS.SYSDEPENDS.
A DependableFinder is an object that can find an in-memory Dependable, given the Dependable's ID. <P> The DependableFinder is able to write itself to disk and, once read back into memory, locate the in-memory Dependable that it represents. <P> DependableFinder objects are stored in SYS.SYSDEPENDS to record dependencies between database objects.
A dependency represents a reliance of the dependent on the provider for some information the dependent contains or uses.  In Language, the usual case is a prepared statement using information about a schema object in its executable form. It needs to be notified if the schema object changes, so that it can recompile against the new information.
DependencyDescriptor represents a persistent dependency between SQL objects, such as a TRIGGER being dependent on a TABLE. A DependencyDescriptor is stored in SYSDEPENDS as four separate columms corresponding to the getters of this class.
Dependency Manager Interface <p> The dependency manager tracks needs that dependents have of providers. This is a general purpose interface which is associated with a DataDictinary object; infact the dependencymanager is really the datadictionary keeping track of dependencies between objects that it handles (descriptors) as well as prepared statements. <p> The primary example of this is a prepared statement's needs of schema objects such as tables. <p> Dependencies are used so that we can determine when we need to recompile a statement; compiled statements depend on schema objects like tables and constraints, and may no longer be executable when those tables or constraints are altered. For example, consider an insert statement. <p> An insert statement is likely to have dependencies on the table it inserts into, any tables it selects from (including subqueries), the authorities it uses to do this, and any constraints or triggers it needs to check. <p> A prepared insert statement has a dependency on the target table of the insert. When it is compiled, that dependency is registered from the prepared statement on the data dictionary entry for the table. This dependency is added to the prepared statement's dependency list, which is also accessible from an overall dependency pool. <p> A DDL statement will mark invalid any prepared statement that depends on the schema object the DDL statement is altering or dropping.  We tend to want to track at the table level rather than the column or constraint level, so that we are not overburdened with dependencies.  This does mean that we may invalidate when in fact we do not need to; for example, adding a column to a table may not actually cause an insert statement compiled for that table to stop working; but our level of granularity may force us to invalidate the insert because it has to invalidate all statements that depend on the table due to some of them actually no longer being valid. It is up to the user of the dependency system at what granularity to track dependencies, where to hang them, and how to identify when objects become invalid.  The dependency system is basically supplying the ability to find out who is interested in knowing about other, distinct operations.  The primary user is the language system, and its primary use is for invalidating prepared statements when DDL occurs. <p> The insert will recompile itself when its next execution is requested (not when it is invalidated). We don't want it to recompile when the DDL is issued, as that would increase the time of execution of the DDL command unacceptably.  Note that the DDL command is also allowed to proceed even if it would make the statement no longer compilable.  It can be useful to have a way to recompile invalid statements during idle time in the system, but our first implementation will simply recompile at the next execution. <p> The start of a recompile will release the connection to all dependencies when it releases the activation class and generates a new one. <p> The Dependency Manager is capable of storing dependencies to ensure that other D.M.s can see them and invalidate them appropriately. The dependencies in memory only the current D.M. can see; the stored dependencies are visible to other D.M.s once the transaction in which they were stored is committed. <p> REVISIT: Given that statements are compiled in a separate top-transaction from their execution, we may need/want some intermediate memory storage that makes the dependencies visible to all D.M.s in the system, without requiring that they be stored. <p> To ensure that dependencies are cleaned up when a statement is undone, the compiler context needs to keep track of what dependent it was creating dependencies for, and if it is informed of a statement exception that causes it to throw out the statement it was compiling, it should also call the dependency manager to have the dependencies removed. <p> Several expansions of the basic interface may be desirable: <ul> <li> to note a type of dependency, and to invalidate or perform an invalidation action based on dependency type <li> to note a type of invalidation, so the revalidation could actually take some action other than recompilation, such as simply ensuring the provider objects still existed. <li> to control the order of invalidation, so that if (for example) the invalidation action actually includes the revalidation attempt, revalidation is not attempted until all invalidations have occurred. <li> to get a list of dependencies that a Dependent or a Provider has (this is included in the above, although the basic system does not need to expose the list). <li> to find out which of the dependencies for a dependent were marked invalid. </ul> <p> To provide a simple interface that satisfies the basic need, and yet supply more advanced functionality as well, we will present the simple functionality as defaults and provide ways to specify the more advanced functionality. <pre> interface Dependent { boolean isValid(); InvalidType getInvalidType(); // returns what it sees // as the "most important" // of its invalid types. void makeInvalid( ); void makeInvalid( DependencyType dt, InvalidType it ); void makeValid(); } interface Provider() { } interface Dependency() { Provider getProvider(); Dependent getDependent(); DependencyType getDependencyType(); boolean isValid(); InvalidType getInvalidType(); // returns what it sees // as the "most important" // of its invalid types. } interface DependencyManager() { void addDependency(Dependent d, Provider p, ContextManager cm); void invalidateFor(Provider p); void invalidateFor(Provider p, DependencyType dt, InvalidType it); void clearDependencies(Dependent d); void clearDependencies(Dependent d, DependencyType dt); Enumeration getProviders (Dependent d); Enumeration getProviders (Dependent d, DependencyType dt); Enumeration getInvalidDependencies (Dependent d, DependencyType dt, InvalidType it); Enumeration getDependents (Provider p); Enumeration getDependents (Provider p, DependencyType dt); Enumeration getInvalidDependencies (Provider p, DependencyType dt, InvalidType it); } </pre> <p> The simplest things for DependencyType and InvalidType to be are integer id's or strings, rather than complex objects. <p> In terms of ensuring that no makeInvalid calls are made until we have identified all objects that could be, so that the calls will be made from "leaf" invalid objects (those not in turn relied on by other dependents) to dependent objects upon which others depend, the dependency manager will need to maintain an internal queue of dependencies and make the calls once it has completes its analysis of the dependencies of which it is aware.  Since it is much simpler and potentially faster for makeInvalid calls to be made as soon as the dependents are identified, separate implementations may be called for, or separate interfaces to trigger the different styles of invalidation. <p> In terms of separate interfaces, the DependencyManager might have two methods, <pre> void makeInvalidImmediate(); void makeInvalidOrdered(); </pre> or a flag on the makeInvalid method to choose the style to use. <p> In terms of separate implementations, the ImmediateInvalidate manager might have simpler internal structures for tracking dependencies than the OrderedInvalidate manager. <p> The language system doesn't tend to suffer from this ordering problem, as it tends to handle the impact of invalidation by simply deferring recompilation until the next execution.  So, a prepared statement might be invalidated several times by a transaction that contains several DDL operations, and only recompiled once, at its next execution.  This is sufficient for the common use of a system, where DDL changes tend to be infrequent and clustered. <p> There could be ways to push this "ordering problem" out of the dependency system, but since it knows when it starts and when it finished finding all of the invalidating actions, it is likely the best home for this. <p> One other problem that could arise is multiple invalidations occurring one after another.  The above design of the dependency system can really only react to each invalidation request as a unit, not to multiple invalidation requests. <p> Another extension that might be desired is for the dependency manager to provide for cascading invalidations -- that is, if it finds and marks one Dependent object as invalid, if that object can also be a provider, to look for its dependent objects and cascade the dependency on to them.  This can be a way to address the multiple-invalidation request need, if it should arise.  The simplest way to do this is to always cascade the same invalidation type; otherwise, dependents need to be able to say what a certain type of invalidation type gets changed to when it is handed on. <p> The basic language system does not need support for cascaded dependencies -- statements do not depend on other statements in a way that involves the dependency system. <p> I do not know if it would be worthwhile to consider using the dependency manager to aid in the implementation of the SQL DROP statements or not. Past implementations of database systems have not used the dependency system to implement this functionality, but have instead hard-coded the lookups like so: <pre> in DropTable: scan the TableAuthority table looking for authorities on this table; drop any that are found. scan the ColumnAuthority table looking for authorities on this table; drop any that are found. scan the View table looking for views on this table; drop any that are found. scan the Column table looking for rows for columns of this table; drop any that are found. scan the Constraint table looking for rows for constraints of this table; drop any that are found. scan the Index table looking for rows for indexes of this table; drop the indexes, and any rows that are found. drop the table's conglomerate drop the table's row in the Table table. </pre> <p> The direct approach such as that outlined in the example will probably be quicker and is definitely "known technology" over the use of a dependency system in this area.
A dependent has the ability to know whether or not it is valid and to mark itself as valid or invalid. Marking itself as invalid usually means it cannot be used in the system until it is revalidated, but this is in no way enforced by this interface.
DependentResultSet should be used by only ON DELETE CASCADE/ON DELETE SET NULL ref actions implementation to gather the rows from the dependent tables. Idea is to scan the foreign key index for the rows in the source table materialized temporary result set. Scanning of foreign key index gives us the rows that needs to be deleted on dependent tables. Using the row location we got from the index , base row is fetched.
Derby related utility methods for the JUnit tests. The class assumes the tests are either being run from the build classes folder or from the standard jar files (or a subset of the standard jars). <BR> If the tests are being run from the classes then it is assumed all the functionality is available, otherwise the functionality will be driven from which jar files are on the classpath. E.g. if only derby.jar is on the classpath then the hasXXX() methods will return false except hasEmbedded().

Test case for DERBY-6131: select from view with "upper" and "in" list throws a ClassCastException null value functionality.



DerbyConstants used by the Derby JUnit framework, for instance when assuming something about default settings of a property.
Holds information required to run a Derby distribution and make choices based on the version of the Derby distribution. <p> <em>Implementation note</em>: For simplicity distributions off the classes directory have been forbidden. The main reason for this is that it is sometimes a hard requirement that you must include only a single JAR from a distribution on the classpath. One such example is the compatibility test, where you need the testing code from one distribution and the client driver from another. While it is possible to support such a configuration running off the {@code classes}-directory in many scenarios, it complicates the creation and handling of classpath string. Generating the JARs when testing on trunk seems like an acceptable price to pay.
This class provides the functionality to create the build related related properties, which are used for creating the Derby plug-in for Eclipse by the ANT script. - The Eclipse plugin will be called 'Apache Derby Core Plug-in for Eclipse' - The plugin can be build from the main build.xml using 'ant' with the 'plugin' argument. - The package name for the Derby plug-in will be: org.apache.derby.core_<major>.<minor>.<interim> (example: org.apache.derby.core_10.1.0). - The plugin.xml in the Derby plug-in will show the actual version of the the Derby build (example: 10.1.0.0 (111545M) ). This can be viewed from Help - About Eclipse Platform - Plug-in Details of Eclipse of the Eclipse IDE - The zip file created for the DerbyEclipse under the jars directory will have the name: derby_core_plugin_<major>.<minor>.<interim>.zip (example:derby_core_plugin_10.1.0.zip) - The current packaging includes derby.jar, derbynet.jar and derbytools.jar. The locale jars for Derby are not included yet.
A subclass of <code>IOException</code> that carries a SQL state. The original reason for adding it was to separate between <code>IOException</code>s generated by the application stream and the ones generated by the Derby wrapper streams, see for instance <code>RawToBinaryFormatStream</code>. Without this distinction, the user would not be able to easily write <code>catch</code>-blocks to handle specific errors happening when reading streams. End class DerbyIOException
<p> Wrapper for a StorageRandomAccessFile which can serve as a Lucene IndexInput. </p>
<p> Wrapper for a StorageRandomAccessFile which can serve as a Lucene IndexOutput. </p>

<p> This is an XML-reading VTI which has been tweaked to handle the formatting of JIRA ids found in Derby JIRA reports. </p> /////////////////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////////////////
<p> Derby implementation of Lucene Directory. </p>

<p> Created to provide the Observable behavior which Derby has depended on since Java 1.2 but which as deprecated in JDK 9 build 118. A DerbyObservable is an object whose state changes are being tracked. </p>
<p> Created to provide the Observable behavior which Derby has depended on since Java 1.2 but which as deprecated in JDK 9 build 118. A DerbyObserver is an object which registers it interest in being notified when events occur. </p>
The main plugin class to be used in the desktop.





Representation of a Derby version on the form major.minor.fixpack.point, for instance "10.8.1.2". <p> This class doesn't consider the alpha/beta flag nor the revision number.

Utility methods for the package of diagnostic vtis.

Public Methods of This class: Public Methods of XXXX class:
The Diagnosticable class implements the Diagnostics protocol, and can be used as the parent class for all other Diagnosticable objects.
The Diagnosticable class implements the Diagnostics protocol, and can be used as the parent class for all other Diagnosticable objects.
This class provides a disk based implementation of the StorageFile interface. It is used by the database engine to access persistent data and transaction logs under the directory (default) subsubprotocol.
This class provides a disk based implementation of the StIRandomAccess File interface. It is used by the database engine to access persistent data and transaction logs under the directory (default) subsubprotocol.
This class provides a disk based implementation of the StorageFactory interface. It is used by the database engine to access persistent data and transaction logs under the directory (default) subsubprotocol.



This class is used by BackingStoreHashtable when the BackingStoreHashtable must spill to disk.  It implements the methods of a hash table: put, get, remove, elements, however it is not implemented as a hash table. In order to minimize the amount of unique code it is implemented using a Btree and a heap conglomerate. The Btree indexes the hash code of the row key. The actual key may be too long for our Btree implementation. Created: Fri Jan 28 13:58:03 2005

Interface to display the results of the business operations. Methods are called by implementations of Operations. There is no requirement for implementations to follow the layout dictated by the TPC-C specification. All the information required by the TPC-C specification for display will be provided through the passed in parameters. <BR> Objects passed in from the data model (Customer etc.) may not be fully populated, but they will contain all the information required for that specific operation. <BR> Any display method must not retain references to any objects it is passed, the caller may be re-using the objects across transactions. <P> DECIMAL values are represented as String objects to allow Order Entry to be run on J2ME/CDC/Foundation which does not support BigDecimal.
Helper class encapsulating logic used in the upgrade test for testing functionality dropping, and skipping generation of, disposable statistics entries.
This ResultSet evaluates grouped aggregates when there is 1 or more distinct aggregate. It will scan the entire source result set and calculate the grouped aggregates when scanning the source during the first call to next(). RESOLVE - This subclass is essentially empty.  Someday we will need to write additional code for distinct grouped aggregates, especially when we support multiple distinct aggregates. /////////////////////////////////////////////////////////////////////////////  ResultSet interface (leftover from NoPutResultSet)  ///////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////  CursorResultSet interface  ///////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////  SCAN ABSTRACTION UTILITIES  ///////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////  AGGREGATION UTILITIES  /////////////////////////////////////////////////////////////////////////////
A DistinctNode represents a result set for a distinct operation on a select.  It has the same description as its input result set. For the most part, it simply delegates operations to its childResultSet, which is currently expected to be a ProjectRestrictResultSet generated for a SelectNode. NOTE: A DistinctNode extends FromTable since it can exist in a FromList.
This ResultSet evaluates scalar aggregates where 1 (or more, in the future) of the aggregates are distinct. It will scan the entire source result set and calculate the scalar aggregates when scanning the source during the first call to next().
Eliminates duplicates while scanning the underlying conglomerate. (Assumes no predicates, for now.)
An Order Entry district. <P> Fields map to definition in TPC-C for the DISTRICT table. The Java names of fields do not include the D_ prefix and are in lower case. For clarity this field are renamed in Java <UL> <LI>w_id =&gt; warehouse (SQL column D_W_ID) </UL> <BR> The columns that map to an address are extracted out as a Address object with the corresponding Java field address. <BR> All fields have Java bean setters and getters. <BR> Fields that are DECIMAL in the database map to String in Java (rather than BigDecimal) to allow running on J2ME/CDC/Foundation. <P> Primary key maps to {warehouse, id}. <P> Implemented by extending Warehouse as they share the same basic format. <P> A District object may sparsely populated, when returned from a business transaction it is only guaranteed to contain  the information required to display the result of that transaction.
A properties object that links two independent properties together. The read property set is always searched first, with the write property set being second. But any put() calls are always made directly to the write object. Only the put(), propertyNames() and getProperty() methods are supported by this class.
<p> Embedded JDBC driver for JDBC 4.2. </p>
Connection factory using DriverManager.
Helper class used by the upgrade tests to unload JDBC drivers loaded in separate class loaders. This class must live in the same class loader as the drivers it attempts to unload.
Driver to do the load phase for the Order Entry benchmark. This class takes in following arguments currently: Usage: java org.apache.derbyTesting.system.oe.run.DriverUtility options Options: <OL> <LI>-scale warehouse scaling factor. Takes a short value. If not specified defaults to 1 <LI>-doChecks check consistency of data, takes a boolean value. If not specified, defaults to true <LI>-driver jdbc driver class to use <LI>-dbUrl  database connection url <LI>-help prints usage </OL> To load database with scale of 2 and to not do any checks, the command to run the test is as follows: <BR> java org.apache.derbyTesting.system.oe.run.DriverUtility -driver org.apache.derby.jdbc.ClientDriver -dbUrl 'jdbc:derby://localhost:1527/db' -scale 2 -doChecks false <BR>
This class performs actions that are ALWAYS performed for a DROP FUNCTION/PROCEDURE/SYNONYM statement at execution time. All of these SQL objects are represented by an AliasDescriptor.
A DropAliasNode  represents a DROP ALIAS statement.
This class  describes actions that are ALWAYS performed for a drop constraint at Execution time.
Shutdown and drop the database identified by the logical name passed in when creating this decorator.
A Filter to qualify tuples coming from a scan of SYSDEPENDS. Tuples qualify if they have the right providerID.
This class  describes actions that are ALWAYS performed for a DROP INDEX Statement at Execution time.
A DropIndexNode is the root of a QueryTree that represents a DROP INDEX statement.
Drop a table on a commit or abort
This class  describes actions that are ALWAYS performed for a DROP ROLE Statement at Execution time.
A DropRoleNode is the root of a QueryTree that represents a DROP ROLE statement.
This class  describes actions that are ALWAYS performed for a DROP SCHEMA Statement at Execution time.
A DropSchemaNode is the root of a QueryTree that represents a DROP SCHEMA statement.
This class  describes actions that are ALWAYS performed for a DROP SEQUENCE Statement at Execution time.
A DropSequenceNode  represents a DROP SEQUENCE statement.
this class drops all statistics for a particular table or index.
This class  describes actions that are ALWAYS performed for a DROP TABLE Statement at Execution time.
A DropTableNode is the root of a QueryTree that represents a DROP TABLE statement.
This class  describes actions that are ALWAYS performed for a DROP TRIGGER Statement at Execution time.
A DropTriggerNode is the root of a QueryTree that represents a DROP TRIGGER statement.
This class  describes actions that are ALWAYS performed for a DROP VIEW Statement at Execution time.
A DropViewNode is the root of a QueryTree that represents a DROP VIEW statement.
This class defines DSS constants that are shared in the classes implementing the DRDA protocol.
Generic process and error tracing encapsulation. This class also traces a DRDA communications buffer. The value of the hex bytes are traced along with the ascii and ebcdic translations.
A VTI which reports its context
A crude Blob implementation for datatype testing.
A crude Clob implementation for datatype testing.
Dummy implementation of OptTrace to test the loading of custom trace logic for the Optimizer.

A DynamicByteArrayOutputStream allows writing to a dynamically resizable array of bytes.   In addition to dynamic resizing, this extension allows the user of this class to have more control over the position of the stream and can get a direct reference of the array.
Information that can be "compiled" and reused per transaction per open operation.  This information is read only by the caller and written by user.  Likely information kept in this object is a set of scratch buffers which will be used by openScan() and thus must not be shared across multiple threads/openScan()'s/openConglomerate()'s.  The goal is to optimize repeated operations like btree inserts, by allowing a set of scratch buffers to be reused across a repeated execution of a statement like an insert/delete/update. This information is obtained from the getDynamicCompiledConglomInfo(conglomid) method call.  It can then be used in openConglomerate() and openScan() calls for increased performance.  The information is only valid until the next ddl operation is performed on the conglomerate.  It is up to the caller to provide an invalidation methodology. The dynamic info is a set of variables to be used in a given ScanController or ConglomerateController.  It can only be used in one controller at a time. It is up to the caller to insure the correct thread access to this info.  The type of info in this is a scratch template for btree traversal, other scratch variables for qualifier evaluation, ...
EXTDTAObjectHolder provides Externalized Large Object representation that does not hold locks until the end of the transaction (DERBY-255) It serves as a holder for lob data and is only valid as long as the original result set from which it came is on the same row.
Implementation of InputStream which get EXTDTA from the DDMReader. <p> This class can be used to stream LOBs from Network client to the Network server. <p> To be able to correctly stream data from the client without reading the while value up front, a trailing Derby-specific status byte was introduced (version 10.6). It is used by the client to tell the server if the data it received was valid, or if it detected an error while streaming the data. The DRDA protocol, or at least Derby's implementation of it, doesn't enable the client to inform the server about the error whilst streaming (there is a mechanism in DRDA to interrupt a running request, but it didn't seem like a feasible approach in this case).
A concrete implementation of a CcsidMgr used to convert between Java UCS2 and Ebcdic as needed to handle character DDM Parameters.  This implementation only supports converting from the ASCII invariant of UNICODE to Ebcdic.  This should be fine since this class is intended for converting DDM Parameter data only.
A convenience wrapper around an XML Document Element. Provides some utility methods for common operations on Element trees.
Implements java.sql.Blob (see the JDBC 2.0 spec). A blob sits on top of a BINARY, VARBINARY or LONG VARBINARY column. If its data is small (less than 1 page) it is a byte array taken from the SQLBit class. If it is large (more than 1 page) it is a long column in the database. The long column is accessed as a stream, and is implemented in store as an OverflowInputStream.  The Resetable interface allows sending messages to that stream to initialize itself (reopen its container and lock the corresponding row) and to reset itself to the beginning. NOTE: In the case that the data is large, it is represented as a stream. This stream is returned to the user in the getBinaryStream() method. This means that we have limited control over the state of the stream, since the user can read bytes from it at any time.  Thus all methods here reset the stream to the beginning before doing any work. CAVEAT: The methods may not behave correctly if a user sets up multiple threads and sucks data from the stream (returned from getBinaryStream()) at the same time as calling the Blob methods. <P><B>Supports</B> <UL> <LI> JSR169 - no subsetting for java.sql.Blob <LI> JDBC 2.0 <LI> JDBC 3.0 - no new dependencies on new JDBC 3.0 or JDK 1.4 classes, new update methods can safely be added into implementation. </UL>
Local implementation.

Implements java.sql.Clob (see the JDBC 2.0 spec). A clob sits on top of a CHAR, VARCHAR or LONG VARCHAR column. If its data is small (less than 1 page) it is a byte array taken from the SQLChar class. If it is large (more than 1 page) it is a long column in the database. The long column is accessed as a stream, and is implemented in store as an OverflowInputStream. The Resetable interface allows sending messages to that stream to initialize itself (reopen its container and lock the corresponding row) and to reset itself to the beginning. <p> NOTE: In the case that the data is large, it is represented as a stream. This stream can be returned to the user in the getAsciiStream() method. This means that we have limited control over the state of the stream, since the user can read bytes from it at any time.  Thus all methods here reset the stream to the beginning before doing any work. CAVEAT: The methods may not behave correctly if a user sets up multiple threads and sucks data from the stream (returned from getAsciiStream()) at the same time as calling the Clob methods. <P><B>Supports</B> <UL> <LI> JSR169 - no subsetting for java.sql.Clob <LI> JDBC 2.0 <LI> JDBC 3.0 - no new dependencies on new JDBC 3.0 or JDK 1.4 classes, new update methods can safely be added into implementation. </UL>
Local implementation of Connection for a JDBC driver in the same process as the database. <p> There is always a single root (parent) connection.  The initial JDBC connection is the root connection. A call to <I>getCurrentConnection()</I> or with the URL <I>jdbc:default:connection</I> yields a nested connection that shares the same root connection as the parent.  A nested connection is implemented using this class.  The nested connection copies the state of the parent connection and shares some of the same objects (e.g. ContextManager) that are shared across all nesting levels.  The proxy also maintains its own state that is distinct from its parent connection (e.g. autocommit or warnings). <p> <B>SYNCHRONIZATION</B>: Just about all JDBC actions are synchronized across all connections stemming from the same root connection.  The synchronization is upon the a synchronized object return by the rootConnection. <P><B>Supports</B> <UL> <LI> JDBC 4.2 </UL>

This class provides information about the database as a whole. <P>Many of the methods here return lists of information in ResultSets. You can use the normal ResultSet methods such as getString and getInt to retrieve the data from these ResultSets.  If a given form of metadata is not available, these methods should throw a SQLException. <P>Some of these methods take arguments that are String patterns.  These arguments all have names such as fooPattern.  Within a pattern String, "%" means match any substring of 0 or more characters, and "_" means match any one character. Only metadata entries matching the search pattern are returned. If a search pattern argument is set to a null ref, it means that argument's criteria should be dropped from the search. <P>A SQLException will be thrown if a driver does not support a meta data method.  In the case of methods that return a ResultSet, either a ResultSet (which may be empty) is returned or a SQLException is thrown. <p> This implementation gets instructions from the Database for how to satisfy most requests for information.  Each instruction is either a simple string containing the desired information, or the text of a query that may be executed on the database connection to gather the information.  We get the instructions via an "InstructionReader," which requires the database Connection for initialization. <p> Those few pieces of metadata that are related to the driver, rather than the database, come from a separate InstructionReader.  Note that in that case it probably doesn't make sense to allow an instruction to specify a query.
This class implements the ParameterMetaData interface from JDBC 3.0.
A PooledConnection object is a connection object that provides hooks for connection pool management. <P>This is Derby's implementation of a PooledConnection for use in the following environments: <UL> <LI> JDBC 4.2 - Java SE 8 </LI> <LI> JDBC 4.1 - Java SE 7 </LI> <LI> JDBC 4.0 - Java SE 6 </LI> </UL>
EmbedPreparedStatement is a local JDBC statement. It supports JDBC 4.1.
<p> PreparedStatement methods added by JDBC 4.2 which require Java 8. </p>
A EmbedResultSet for results from the EmbedStatement family. Supports JDBC 4.1.
JDBC 4.2 specific methods that cannot be implemented in superclasses.
A ResultSetMetaData object can be used to find out about the types and properties of the columns in a ResultSet. <p> We take the (Derby) ResultDescription and examine it, to return the appropriate information. <P> This class can be used outside of this package to convert a ResultDescription into a ResultSetMetaData object. <P> EmbedResultSetMetaData objects are shared across multiple threads by being stored in the ResultDescription for a compiled plan. If the required api for ResultSetMetaData ever changes so that it has a close() method, a getConnection() method or any other Connection or ResultSet specific method then this sharing must be removed.
This class implements the Savepoint interface from JDBC 3.0. This allows to set, release, or rollback a transaction to designated Savepoints. Savepoints provide finer-grained control of transactions by marking intermediate points within a transaction. Once a savepoint has been set, the transaction can be rolled back to that savepoint without affecting preceding work. <P><B>Supports</B> <UL> <LI> JSR169 - no subsetting for java.sql.Savepoint <LI> JDBC 3.0 - class introduced in JDBC 3.0 </UL>
We would import these, but have name-overlap import java.sql.Statement; import java.sql.ResultSet; EmbedStatement is a local JDBC statement. It supports JDBC 4.1.

Implements XAResource

<P> This datasource is suitable for an application using embedded Derby, running on full Java SE 6 and higher, corresponding to 4.0 and higher. EmbeddedConnectionPoolDataSource is a ConnectionPoolDataSource implementation. </P> <P>A ConnectionPoolDataSource is a factory for PooledConnection objects. An object that implements this interface will typically be registered with a JNDI service.</P> <P> EmbeddedConnectionPoolDataSource automatically supports the correct JDBC specification version for the Java Virtual Machine's environment.</P> <UL> <LI>JDBC 4.0 - Java SE 6</LI> <LI>JDBC 4.1 - Java SE 7</LI> <LI>JDBC 4.2 - full Java SE 8</LI> </UL> <P> Use BasicEmbeddedConnectionPoolDataSource40 if your application runs on Java 8 Compact Profile 2. </P> <P>EmbeddedConnectionPoolDataSource is serializable and referenceable.</P> <P>See EmbeddedDataSource for DataSource properties.</P>
<P> This is a vacuous, deprecated class. At one time, it had real behavior and helped us support separate data sources for Java 5 and Java 6. Now that we no longer support Java 5, all functionality has migrated into the superclass, EmbeddedConnectionPoolDataSource. This class is preserved for backward compatibility reasons. </P> compile-time check for 4.1 extension
Common interface of Derby embedded connection pooling data sources.
<P> This data source is suitable for an application using embedded Derby, running on full Java SE 6 and higher, corresponding to 4.0 and higher. </P> <P>A DataSource  is a factory for Connection objects. An object that implements the DataSource interface will typically be registered with a JNDI service provider.</P> <P> EmbeddedDataSource automatically supports the correct JDBC specification version for the Java Virtual Machine's environment.</P> <UL> <LI>JDBC 4.0 - Java SE 6</LI> <LI>JDBC 4.1 - Java SE 7</LI> <LI>JDBC 4.2 - full Java SE 8</LI> </UL> <P> Use BasicEmbeddedDataSource40 if your application runs on Java 8 Compact Profile 2. </P> <P>The following is a list of properties that can be set on a Derby DataSource object: <P><B>Standard DataSource properties</B> (from JDBC 3.0 specification). <UL><LI><B><code>databaseName</code></B> (String): <I>Mandatory</I> <BR>This property must be set and it identifies which database to access.  If a database named wombat located at g:/db/wombat is to be accessed, then one should call <code>setDatabaseName("g:/db/wombat")</code> on the data source object.</LI> <LI><B><code>dataSourceName</code></B> (String): <I>Optional</I> <BR> Name for DataSource.  Not used by the data source object.  Used for informational purpose only.</LI> <LI><B><code>description</code></B> (String): <I>Optional</I> <BR>Description of the data source.  Not used by the data source object.  Used for informational purpose only.</LI> <LI><B><code>password</code></B> (String): <I>Optional</I> <BR>Database password for the no argument <code>DataSource.getConnection()</code>, <code>ConnectionPoolDataSource.getPooledConnection()</code> and <code>XADataSource.getXAConnection()</code> methods. <LI><B><code>user</code></B> (String): <I>Optional</I> <BR>Database user for the no argument <code>DataSource.getConnection()</code>, <code>ConnectionPoolDataSource.getPooledConnection()</code> and <code>XADataSource.getXAConnection()</code> methods. </UL> <BR><B>Derby specific DataSource properties.</B> <UL> <LI><B><code>attributesAsPassword</code></B> (Boolean): <I>Optional</I> <BR>If true, treat the password value in a <code>DataSource.getConnection(String user, String password)</code>, <code>ConnectionPoolDataSource.getPooledConnection(String user, String password)</code> or <code>XADataSource.getXAConnection(String user, String password)</code> as a set of connection attributes. The format of the attributes is the same as the format of the attributes in the property connectionAttributes. If false the password value is treated normally as the password for the given user. Setting this property to true allows a connection request from an application to provide more authentication information that just a password, for example the request can include the user's password and an encrypted database's boot password.</LI> <LI><B><code>connectionAttributes</code></B> (String): <I>Optional</I> <BR>Defines a set of Derby connection attributes for use in all connection requests. The format of the String matches the format of the connection attributes in a Derby JDBC URL. That is a list of attributes in the form <code><I>attribute</I>=<I>value</I></code>, each separated by semi-colon (';'). E.g. <code>setConnectionAttributes("bootPassword=erd3234dggd3kazkj3000");</code>. <BR>The database name must be set by the DataSource property <code>databaseName</code> and not by setting the <code>databaseName</code> connection attribute in the <code>connectionAttributes</code> property. <BR> Any attributes that can be set using a property of this DataSource implementation (e.g user, password) should not be set in connectionAttributes. Conflicting settings in connectionAttributes and properties of the DataSource will lead to unexpected behaviour. <BR>Please see the Derby documentation for a complete list of connection attributes. </LI> <LI><B><code>createDatabase</code></B> (String): <I>Optional</I> <BR>If set to the string "create", this will cause a new database of <code>databaseName</code> if that database does not already exist.  The database is created when a connection object is obtained from the data source. </LI> <LI><B><code>shutdownDatabase</code></B> (String): <I>Optional</I> <BR>If set to the string "shutdown", this will cause the database to shutdown when a java.sql.Connection object is obtained from the data source.  E.g., If the data source is an XADataSource, a getXAConnection().getConnection() is necessary to cause the database to shutdown. </UL> <P><B>Examples.</B> <P>This is an example of setting a property directly using Derby's EmbeddedDataSource object.  This code is typically written by a system integrator : <PRE> import org.apache.derby.jdbc.*; // dbname is the database name // if create is true, create the database if necessary javax.sql.DataSource makeDataSource (String dbname, boolean create) throws Throwable { EmbeddedDataSource ds = new EmbeddedDataSource(); ds.setDatabaseName(dbname); if (create) ds.setCreateDatabase("create"); return ds; } </PRE> <P>Example of setting properties thru reflection.  This code is typically generated by tools or written by a system integrator: <PRE> javax.sql.DataSource makeDataSource(String dbname) throws Throwable { Class[] parameter = new Class[1]; parameter[0] = dbname.getClass(); DataSource ds =  new EmbeddedDataSource(); Class cl = ds.getClass(); Method setName = cl.getMethod("setDatabaseName", parameter); Object[] arg = new Object[1]; arg[0] = dbname; setName.invoke(ds, arg); return ds; } </PRE> <P>Example on how to register a data source object with a JNDI naming service. <PRE> DataSource ds = makeDataSource("mydb"); Context ctx = new InitialContext(); ctx.bind("jdbc/MyDB", ds); </PRE> <P>Example on how to retrieve a data source object from a JNDI naming service. <PRE> Context ctx = new InitialContext(); DataSource ds = (DataSource)ctx.lookup("jdbc/MyDB"); </PRE>
<P> This is a vacuous, deprecated class. At one time, it had real behavior and helped us support separate data sources for Java 5 and Java 6. Now that we no longer support Java 5, all functionality has migrated into the superclass, EmbeddedDataSource. This class is preserved for backward compatibility reasons. </P> compile-time check for 4.1 extension
Methods that extend the API of {@code javax.sql.DataSource} common for all Derby embedded data sources.
The embedded JDBC driver (Type 4) for Derby. <P> The driver automatically supports the correct JDBC specification version for the Java Virtual Machine's environment. <UL> <LI> JDBC 4.0 - Java SE 6 <LI> JDBC 3.0 - Java 2 - JDK 1.4, J2SE 5.0 </UL> <P> Loading this JDBC driver boots the database engine within the same Java virtual machine. <P> The correct code to load the Derby engine using this driver is (with approriate try/catch blocks): <PRE> Class.forName("org.apache.derby.jdbc.EmbeddedDriver").newInstance(); // or new org.apache.derby.jdbc.EmbeddedDriver(); </PRE> When loaded in this way, the class boots the actual JDBC driver indirectly. The JDBC specification recommends the Class.ForName method without the .newInstance() method call, but adding the newInstance() guarantees that Derby will be booted on any Java Virtual Machine. <P> Note that you do not need to manually load the driver this way if you are running on Jave SE 6 or later. In that environment, the driver will be automatically loaded for you when your application requests a connection to a Derby database. <P> Any initial error messages are placed in the PrintStream supplied by the DriverManager. If the PrintStream is null error messages are sent to System.err. Once the Derby engine has set up an error logging facility (by default to derby.log) all subsequent messages are sent to it. <P> By convention, the class used in the Class.forName() method to boot a JDBC driver implements java.sql.Driver. This class is not the actual JDBC driver that gets registered with the Driver Manager. It proxies requests to the registered Derby JDBC driver.
<P> This data source is suitable for an application using embedded Derby, running on full Java SE 6 or higher, corresponding to JDBC 4.0 and higher. EmbeddedXADataSource is an XADataSource implementation. <P/> <P>An XADataSource is a factory for XAConnection objects.  It represents a RM in a DTP environment.  An object that implements the XADataSource interface is typically registered with a JNDI service provider. </P> <P> EmbeddedXADataSource automatically supports the correct JDBC specification version for the Java Virtual Machine's environment. </P> <UL> <LI>JDBC 4.0 - Java SE 6 </LI> <LI>JDBC 4.1 - Java SE 7</LI> <LI>JDBC 4.2 - full Java SE 8</LI> </UL> <P> Use BasicEmbeddedXADataSource40 if your application runs on Java 8 Compact Profile 2. </P> <P>EmbeddedXADataSource object only works on a local database.  There is no client/server support.  An EmbeddedXADataSource object must live in the same jvm as the database. </P> <P>EmbeddedXADataSource is serializable and referenceable.</P> <P>See EmbeddedDataSource for DataSource properties.</P>
<P> This is a vacuous, deprecated class. At one time, it had real behavior and helped us support separate datasources for Java 5 and Java 6. Now that we no longer support Java 5, all functionality has migrated into the superclass, EmbeddedXADataSource. This class is preserved for backward compatibility reasons. </P> compile-time check for 4.1 extension
Common interface of Derby embedded XA data sources.
DataDictionary implementation that does nothing! Used for the storeless system.
ResultSetStatisticsFactory provides a wrapper around all of objects associated with run time statistics. <p> This implementation of the protocol is for stubbing out the RunTimeStatistics feature at execution time..
Create an encoded stream from a <code>Reader</code>. This is an internal class, used to pass readers of characters as streams of bytes. The characters will be represented according to the specified encoding. It is up to the caller to ensure the specified encoding is available, and in general only encodings available as default from Java 1.3 and up should be used. Currently, the encodings 'UTF8' and 'UTF-16BE' are used. Streams are obtained by calling the static methods of this class, for instance <code>createUTF8Stream</code>.
Log operation to encrypt a container with a new encryption key or to encrypt an unencrypted container while configuring the database for encryption. Container is synced to the disk when encryption is is successful, there is nothing to do on a redo. If there is crash/error while configuring a database for encryption; original version of the container is put back during undo. <PRE>
A Encrypt Container undo operation rolls back the change of a Encrypt Container operation
This class is used to encrypt all the containers in the data segment with a new encryption key when password/key is changed or when an existing database is reconfigured for encryption. Encryption of existing data in the data segments is done by doing the following: Find all the containers in data segment (seg0) and encrypt all of them with the new  encryption key, the process for each container is: 1.Write a log record to indicate that the container is getting encrypted. 2.Read all the pages of the container through the page cache and encrypt each page with new encryption key and then write to a temporary file(n<cid>.dat) in the data segment itself. 3. Rename the current container file (c<cid>.dat) to another file (o<cid>.dat) 4. Rename the new encrypted version of the file (n&lt;cid).dat) to be the current container file (c<cid>.dat). 5. All the old version of  the container (o<cid>.dat) files are removed after a successful checkpoint with a new key or on a rollback.
This class is a wrapper class on top of StorageRandomAccess to provide common methods to write in encrypted file. This class is NOT thread safe. The user class should take care of synchronization if being used in multi threaded environment.
This class is get used when using encrypted password and/or userid mechanism. The <b>EncryptionManager</b> classs uses Diffie_Hellman algorithm to get the publick key and secret key, and then DES encryption is done using certain token (based on security mechanism) and server side's public key. Basically, this class is called when using security mechanism User ID and encrypted password (usrencpwd) and Encrypted user ID and password (eusridpwd). This class uses JCE provider to do Diffie_Hellman algorithm and DES encryption, obtainPublicKey(), calculateEncryptionToken(int, byte[]) and encryptData(byte[], int, byte[], byte[]) The agreed public value for the Diffie-Hellman prime is 256 bits and hence the encrytion will work only if the jce provider supports a 256 bits prime  This class also have methods for the SECMEC_USRSSBPWD security mechanism.
A suite that runs a set of tests using encrypted databases with a number of algorithms. This is a general encryption test to see if tests run without any problems when encryption is enabled. <BR> It is not for testing of encryption functionality, e.g. testing that bootPassword must be a certain length etc. That should be in a specific JUnit test that probably needs to control database creation more carefully than this. <BR> The same set of tests is run for each algorithm, and each algorithm (obviously) uses a single use database with the required encryption setup.

This operation indicates the End of a transaction.
Additional methods the engine exposes on its CallableStatement object implementations, whose signatures are not compatible with older platforms.
Additional methods the embedded engine exposes on its Connection object implementations. An internal api only, mainly for the network server. Allows consistent interaction between EmbedConnections and BrokeredConnections.
Additional methods the embedded engine exposes on all of its large object (LOB) implementations. <p> An internal API only, mainly for the network server. <p> <b>Implementation note</b>: If a new method is needed, that only applies to one specific large object type (for instance a Blob), one should consider creating a new interface that extends from this one.
Additional methods the embedded engine exposes on its PreparedStatement object implementations. An internal api only, mainly for the network server. Allows consistent interaction between embedded PreparedStatement and Brokered PreparedStatements. (DERBY-1015)
Additional methods the embedded engine exposes on its ResultSet object implementations. An internal api only, mainly for the network server
Additional methods the embedded engine exposes on its Statement object implementations. An internal api only, mainly for the network server. Allows consistent interaction between emebdded statements and brokered statements.
Derby engine types. Enumerate different modes the emmbedded engine (JDBC driver, SQL langauge layer and store) can run in. A module can query the monitor to see what type of service is being requested in terms of its engine type and then use that in a decision as to if it is suitable.
Getting error information for SQLData/serializable data streams.
ErrorLogReader is a virtual table interface (VTI) which contains all the statements of "interest" in db2j.<!-- -->log or a specified file when db2j.<!-- -->language.<!-- -->logStatementText=true. <P>One use of this VTI is to determine the active transactions and the SQL statements in those transactions at a given point in time, say when a deadlock or lock timeout occurred.  In order to do that, you must first find the timestamp (timestampConstant) of interest in the error log. The SQL to view the active transactions at a given in time is: <PRE>SELECT vti.ts, threadid, cast(xid as int) as xid_int, cast(lccid as int) as lccid_int, logtext FROM new org.apache.derby.diag.ErrorLogReader() vti, (VALUES timestampConstant) t(ts) WHERE vti.ts &lt;= t.ts AND vti.ts &gt; (SELECT MAX(ts) IS NULL ? '2000-01-01 00:00:00.1' : MAX(ts) FROM new org.apache.derby.diag.ErrorLogReader() vti_i WHERE (logtext LIKE 'Committing%' OR logtext LIKE 'Rolling%') AND vti.xid = vti_i.xid AND ts &lt; t.ts) ORDER BY xid_int, vti.ts </PRE> <P>The ErrorLogReader virtual table has the following columns: <UL><LI>TS varchar(26) - the timestamp of the statement.</LI> <LI>THREADID varchar(40) - the thread name.</LI> <LI>XID varchar(15) - the transaction ID.</LI> <LI>LCCID varchar(15) - the connection ID.</LI> <LI>DATABASE varchar(128) -  Database name <LI>DRDAID  varchar(50) - nullable. DRDA ID for network server session. <LI>LOGTEXT long varchar - text of the statement or commit or rollback.</LI> </UL>
ErrorMessage shows all the SQLStates, locale-sensitive error messages, and exception severities for a database. <p>To use it, query it as follows:</p> <PRE> SELECT* FROM NEW org.apache.derby.diag.ErrorMessages() AS EQ; </PRE> <P>The following columns will be returned: <UL><LI>SQL_STATE--VARCHAR(5) - nullable.  The SQLState of the SQLException.<br> (The code returned by getSQLState() in SQLException.)</LI> <LI>MESSAGE--VARCHAR(32672) - nullable.  The error message<br> (The code returned by getMessage() in SQLException.)</LI> <LI>SEVERITY--INTEGER - nullable.  The Derby code for the severity.<br> (The code returned by getErrorCode() in SQLException.)</LI> </UL>
Limit and ObjectInput capabilities. Combin
Class used to form error messages.  Primary reason for existence is to allow a way to call printStackTrace() w/o automatically writting to a stream.

An exception factory is used to create SQLExceptions of the correct type.

This is a refactoring wrapper around the common ExceptionSeverity class and may be removed at some point in the future. See org.apache.derby.common.error.ExceptionSeverity
This class provides utility routines for exceptions
An ExecAggregator is the interface that execution uses to an aggregate.  System defined aggregates will implement this directly. <P> The life time of an ExecAggregator is as follows. <OL> <LI> An ExecAggregator instance is created using the defined class name. <LI> Its setup() method is called to define its role (COUNT(*), SUM, etc.). <LI> Its newAggregator() method may be called any number of times to create new working aggregators as required. These aggregators have the same role and must be created in an initialized state. <LI> accumlate and merge will be called across these set of aggregators <LI> One of these aggregators will be used as the final one for obtaining the result </OL> <P>
This is a table name reference that can be retrieved from an active cursor.
This is an extension of ExecRow for use with indexes and sorting.
Execution extends prepared statement to add methods it needs for execution purposes (that should not be on the Database API).
Execution sees this extension of Row that provides connectivity to the Storage row interface and additional methods for manipulating Rows in execution's ResultSets.
<p> A class used for storing information on how to build {@code ExecRow} instances. Typically created by the compiler and used during execution to produce and reset row templates. </p> <p> This class must be {@code Formatable} so that it can be stored in the database as part of a stored prepared statement generated for trigger actions or metadata queries. The stored format does not need to be stable across different versions, since the stored prepared statements are discarded on upgrade and will never be read by other Derby versions than the one that originally wrote them. </p>
A ExecSPSNode is the root of a QueryTree that represents an EXECUTE STATEMENT statement.  It is a tad abnormal.  During a bind, it locates and retrieves the SPSDescriptor for the particular statement.  At generate time, it generates the prepared statement for the stored prepared statement and returns it (i.e. it effectively replaces itself with the appropriate prepared statement).
ExecutionContext stores the factories that are to be used by the current connection. It also provides execution services for statement atomicity.
This is the factory for creating a factories needed by execution per connection, and the context to hold them. <p> There is expected to be one of these configured per database. <p> If a factory is needed outside of execution (say, data dictionary or compilation), then it belongs in the LanguageConnectionContext.
An ExecutionStatementValidator is an object that is handed a ConstantAction and asked whether it is ok for this result set to execute.  When something like a trigger is executing, one of these gets pushed. Before execution, each validator that has been pushed is invoked on the result set that we are about to execution.  It is up to the validator to look at the result set and either complain (throw an exception) or let it through.
This class provides ways to export data from a table or a view into a file. Export functions provided in this  class are called through Systement Procedures.
<P>
uses the passed connection and table/view name to make the resultset on that entity. If the entity to be exported has non-sql types in it, an exception will be thrown
this class takes the passed row and writes it into the data file using the properties from the control file FIXED FORMAT: if length of nullstring is greater than column width, throw execption

ExpressionClassBuilder provides an interface to satisfy generation's common tasks in building classes that involve expressions. This is the common superclass of ActivationClassBuilder and FilterClassBuilder. See the documentation on ActivationClassBuilder.
This is a simple interface to hide the impl of ExpressionClassBuilder from the protocol side. RESOLVE - This interface needs to be filled in and the impl code changed to go through the interface as much as possible.


This node represents a unary extract operator, used to extract a field from a date/time. The field value is returned as an integer.
A FKConstraintDefintionNode represents table constraint definitions.
This is a simple class used to store the run time information about a foreign key.  Used by DML to figure out what to check.
Convience functions for performing file manipulations in ij scripts.
<P> Callers of these methods must be within the context of a Derby statement execution otherwise a SQLException will be thrown. <BR> There are two basic ways to call these methods. <OL> <LI> Within a SQL statement. <PRE> -- checkpoint the database CALL org.apache.derby.iapi.db.Factory:: getDatabaseOfConnection().checkpoint(); </PRE> <LI> In a server-side JDBC method. <PRE> import org.apache.derby.iapi.db.*; ... // checkpoint the database Database db = Factory.getDatabaseOfConnection(); db.checkpoint(); </PRE> </OL> This class can only be used within an SQL-J statement, a Java procedure or a server side Java method. <p>This class can be accessed using the class alias <code> FACTORY </code> in SQL-J statements.
Class <code>FailedProperties40</code> is a helper class for <code>java.sql.SQLClientInfoException</code>. It provides convenient access to data that is needed when constructing those exceptions. Should be kept in sync with its embedded counter part.
A stream class that throws an exception on the first read request.



FetchDescriptor is used to package up all the arguments necessary to describe what rows and what row parts should be returned from the store back to language as part of a fetch. <p> The FetchDescriptor may also contain scratch space used to process the qualifiers passed in the scan.  This scratch space will be used to cache information about the qualifiers, valid column list, row size so that calculations need only be done once per scan rather than every iteration.

FileContainer is an abstract base class for containers which are based on files. This class extends BaseContainer and implements Cacheable and TypedFormat
A Descriptor for a file that has been stored in the database.
Write log records to a log file as a stream (ie. log records added to the end of the file, no concept of pages). <P> The format of a log record that is not a compensation operation is <PRE>
Implementation of the monitor that uses the class loader that the its was loaded in for all class loading.
Management of file resources within	a database. Suitable for jar files, images etc. <P>A file resource is identified by the pair (name,generationId). Name is an arbitrary String supplied by the caller. GenerationId is a non-repeating sequence number constructed by the database. Within a database a	(name,generationId) pair uniquely identifies a version of a file resource for all time. Newer generation numbers reflect newer versions of the file. <P>A database supports the concept of a designated current version of a fileResource. The management of the current version is transactional. The following rules apply <OL> <LI>Adding a FileResource makes the added version the current version <LI>Removing a FileResource removes the current version of the resource. After this operation the database holds no current version of the FileResoure. <LI>Replacing a FileResource removes the current version of the resource. </OL> <P>For the benefit of replication, a database optionally retains historic versions of stored files. These old versions are useful when processing old transactions in the stage.
A set of public static methods for dealing with File objects.
An RFC 1960-based Filter. <p> <code>Filter</code> objects can be created by calling {@link BundleContext#createFilter} with the chosen filter string. <p> A <code>Filter</code> object can be used numerous times to determine if the match argument matches the filter string that was used to create the <code>Filter</code> object. <p> Some examples of LDAP filters are: <pre> &quot;(cn=Babs Jensen)&quot; &quot;(!(cn=Tim Howes))&quot; &quot;(&amp;(&quot; + Constants.OBJECTCLASS + &quot;=Person)(|(sn=Jensen)(cn=Babs J*)))&quot; &quot;(o=univ*of*mich*)&quot; </pre>
Client talking to the Apache JIRA instance to retrieve and derive information required to generate releases notes for a Derby release. <p> The purpose of this client is to carry out some of the tasks a release manager has to do when generating the release notes.
Wrapper for invoking {@code FilteredIssueLister} from ant.
<p> This VTI makes a table out of the records in a flat file. This is an abstract class. Child classes are responsible for implementing the following methods which parse and advance the file: </p> <ul> <li>parseRow() - Parses the next record of the file into an array of Strings, one for each column.</li> </ul>
Converters from floating point bytes to Java <code>float</code>, <code>double</code>, or <code>java.math.BigDecimal</code>.
Scan the the log which is implemented by a series of log files.n This log scan knows how to move across log file if it is positioned at the boundary of a log file and needs to getNextRecord. <PRE> 4 bytes - length of user data, i.e. N 8 bytes - long representing log instant N bytes of supplied data 4 bytes - length of user data, i.e. N </PRE>

<p> OptionalTool to create wrapper functions and views for all of the user tables in a foreign database. </p>
A foreign key.
A Referential Integrity checker for a foreign key constraint.  It makes sure the foreign key is intact.  This is used for a change to a foreign key column.  see ReferencedKeyRIChecker for the code that validates changes to referenced keys.
<p> This class contains a table function which can be used to bulk-import data from a foreign database. Because the table function is a RestrictedVTI, it can also be used to periodically and efficiently integrate data streams from a foreign database. </p> <p> If you need to siphon data out of the foreign database on an ongoing basis, you can restrict the data you SELECT. Note that the local views are backed by RestrictedVTIs. That means that the actual query sent to the foreign database will only involve the columns you SELECT. In addition, the query will include the WHERE clause, provided that it is simple enough (see the javadoc for RestrictedVTI): </p> <p> The following script shows how to use this table function: </p> <pre> -- create a foreign database with a table in it connect 'jdbc:derby:memory:db;create=true;user=test_dbo;password=test_dbopassword'; call syscs_util.syscs_create_user( 'test_dbo', 'test_dbopassword' ); create table employee ( firstName   varchar( 50 ), lastName    varchar( 50 ), employeeID  int primary key ); insert into employee values ( 'Billy', 'Goatgruff', 1 ); insert into employee values ( 'Mary', 'Hadalittlelamb', 2 ); connect 'jdbc:derby:memory:db;shutdown=true'; -- now create the database where we will do our work connect 'jdbc:derby:memory:db1;create=true'; -- register a table function with the shape of the foreign table create function employeeFunction ( schemaName  varchar( 32672 ), tableName   varchar( 32672 ), connectionURL        varchar( 32672 ) ) returns table ( firstName   varchar( 50 ), lastName    varchar( 50 ), employeeID  int ) language java parameter style derby_jdbc_result_set no sql external name 'org.apache.derby.vti.ForeignTableVTI.readForeignTable' ; -- create a convenience view to factor out the function parameters create view foreignEmployee as select firstName, lastName, employeeID from table ( employeeFunction ( 'TEST_DBO', 'EMPLOYEE', 'jdbc:derby:memory:db;user=test_dbo;password=test_dbopassword' ) ) s; -- now select from the view as though it were a local table select * from foreignEmployee; select lastName from foreignEmployee where employeeID = 2; </pre>
A stream for reading objects with format id tags which was produced by a FormatIdOutputStream. <P>Please see the documentation for FormatIdOutputStream for information about the streams format and capabilites.
A stream for serializing objects with format id tags. <P>An ObjectOutput (henceforth 'out') preceeds objects it writes with a format id. The companion FormatIdInputStream (henceforth 'in') uses these format ids in parsing the stored data. The stream can be thought of as containing a sequence of (formatId,object) pairs interspersed with other data. The assumption is that out.writeObject() produces these pairs and in.readObject() uses the format ids to construct objects from the pairs that out.writeObject produced. The description below describes each supported pair and how in.readObject() processes it. <OL> <LI> (NULL_FORMAT_ID, nothing) in.readObject() returns null. <LI> (SRING_FORMAT_ID, UTF8 encoded string)in.readObject reads and returns this string. <LI> (SERIALIZABLE_FORMAT_ID,serialized object) in.readObject() reads the object using java serialization and returns it. <LI> (A format id for a Storable, isNull flag and object if isNull == false) (see note 1) in.readObject() reads the boolean isNull flag. If is null is true, in.readObject() returns a Storable object of the correct class which is null. If ifNull is false, in.readObject() restores the object using its readExternal() method. <LI> (A format id for a Formatable which is not Storable, the stored object) (see note 1) in.readObject restores the object using its readExternal() method. </OL> <P>Note 1: The FormatIdInputStream uses Monitor.newInstanceFromIdentifier(format id) to get the class. <P>Note 2: An object may support more than one of the following interfaces Storable, Formatable, Serializable. In this case out.writeObject use the first of these interfaces which the object supports (based on the order listed here) to determine how to write the object.

Utility class with static methods for constructing and reading the byte array representation of format id's. <P>This utility supports a number of families of format ids. The byte array form of each family is a different length. In all cases the first two bits of the first byte indicate the family for an id. The list below describes each family and gives its two bit identifier in parens. <UL> <LI> (0) - The format id is a one byte number between 0 and 63 inclusive. The byte[] encoding fits in one byte. <LI> (1) - The format id is a two byte number between 16384 to 32767 inclusive. The byte[] encoding stores the high order byte first. <LI> (2) - The format id is four byte number between 2147483648 and 3221225471 inclusive. The byte[] encoding stores the high order byte first. <LI> (3) - Future expansion. </UL>
Derby interface for creating a stored form for an object and re-constructing an equivalent object from this stored form. The object which creates the stored form and the re-constructed object need not be the same or related classes. They must share the same TypedFormat.
A formatable holder for an array of formatables. Used to avoid serializing arrays.
FormatableBitSet is implemented as a packed array of bytes.
A formatable holder for a java.util.Hashtable. Used to avoid serializing Properties.
Class that loads Formattables (typically from disk)through one level of indirection. A concrete implementation of this class is registered as the class to handle a number of format identifiers in RegisteredFormatIds. When the in-memory representation of RegisteredFormatIds is set up an instance of the concrete class will be created for each format identifier the class is registered for, and each instances will have its setFormatId() called once with the appropriate format identifier. <BR> When a Formattable object is read from disk and its registered class is an instance of FormatableInstanceGetter the getNewInstance() method will be called to create the object. The implementation can use the fmtId field to determine the class of the instance to be returned. <BR> Instances of FormatableInstanceGetter are system wide, that is there is a single set of RegisteredFormatIds per system.
A formatable holder for an int.
A formatable holder for an long.
A formatable holder for a java.util.Properties. Used to avoid serializing Properties.

<p> This class contains a table function which forwards its behavior to another ResultSet wrapped inside it. </p>
A general event from the Framework. <p> <code>FrameworkEvent</code> objects are delivered to <code>FrameworkListener</code>s when a general event occurs within the OSGi environment. A type code is used to identify the event type for future extendability. <p> OSGi Alliance reserves the right to extend the set of event types.
A <code>FrameworkEvent</code> listener. <code>FrameworkListener</code> is a listener interface that may be implemented by a bundle developer. When a <code>FrameworkEvent</code> is fired, it is asynchronously delivered to a <code>FrameworkListener</code>. The Framework delivers <code>FrameworkEvent</code> objects to a <code>FrameworkListener</code> in order and must not concurrently call a <code>FrameworkListener</code>. <p> A <code>FrameworkListener</code> object is registered with the Framework using the {@link BundleContext#addFrameworkListener} method. <code>FrameworkListener</code> objects are called with a <code>FrameworkEvent</code> objects when the Framework starts and when asynchronous errors occur.
Framework Utility class. <p> This class contains utility methods which access Framework functions that may be useful to bundles.
A FromBaseTable represents a table in the FROM list of a DML statement, as distinguished from a FromSubquery, which represents a subquery in the FROM list. A FromBaseTable may actually represent a view.  During parsing, we can't distinguish views from base tables. During binding, when we find FromBaseTables that represent views, we replace them with FromSubqueries. By the time we get to code generation, all FromSubqueries have been eliminated, and all FromBaseTables will represent only true base tables. <p> <B>Positioned Update</B>: Currently, all columns of an updatable cursor are selected to deal with a positioned update.  This is because we don't know what columns will ultimately be needed from the UpdateNode above us.  For example, consider:<pre><i> get c as 'select cint from t for update of ctinyint' update t set ctinyint = csmallint </pre></i> Ideally, the cursor only selects cint.  Then, something akin to an IndexRowToBaseRow is generated to take the CursorResultSet and get the appropriate columns out of the base table from the RowLocation returned by the cursor.  Then the update node can generate the appropriate NormalizeResultSet (or whatever else it might need) to get things into the correct format for the UpdateResultSet. See CurrentOfNode for more information.
A FromList represents the list of tables in a FROM clause in a DML statement.  It extends QueryTreeNodeVector.
A FromSubquery represents a subquery in the FROM list of a DML statement. The current implementation of this class is only sufficient for Insert's need to push a new select on top of the one the user specified, to make the selected structure match that of the insert target table.
A FromTable represents a table in the FROM clause of a DML statement. It can be either a base table, a subquery or a project restrict.
A FromVTI represents a VTI in the FROM list of a DML statement.
A Comparable UDT for tests.
<p> Functions used by the Scores application. </p>
This is a common superclass for the various impls. Saving class files is a common thing to do.
Filter to only accept interesting files for generating reports.
Simple utility to generate a mixture-of-Gaussian configuration.
Generate summary information from a RunSuite run. Can be called separately, if given the suite name. Will be called from RunSuite if System property genrep=true. Condenses run information down, prints out result stats, and shows details of failures (.diff files).
Generated classes must implement this interface.
A meta-class that represents a generated class. (Similar to java.lang.Class).
<p> Helper routines for testing generated columns. See DERBY-481. </p>
Handle for a method within a generated class.
This node describes a Generation Clause in a column definition. /////////////////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////////////////

This class holds an Activation, and passes through most of the calls to the activation.  The purpose of this class is to allow a PreparedStatement to be recompiled without the caller having to detect this and get a new activation. In addition to the Activation, this class holds a reference to the PreparedStatement that created it, along with a reference to the GeneratedClass that was associated with the PreparedStatement at the time this holder was created.  These references are used to validate the Activation, to ensure that an activation is used only with the PreparedStatement that created it, and to detect when recompilation has happened. We detect recompilation by checking whether the GeneratedClass has changed. If it has, we try to let the caller continue to use this ActivationHolder. We create a new instance of the new GeneratedClass (that is, we create a new Activation), and we compare the number and type of parameters.  If these are compatible, we copy the parameters from the old to the new Activation. If they are not compatible, we throw an exception telling the user that the Activation is out of date, and they need to get a new one.
Generic aggregation utilities.
Adaptor that sits between execution layer and aggregates.


This is a stripped down implementation of a column descriptor that is intended for generic use.  It can be seralized and attached to plans.
A class that implements the methods shared across all implementations of the Conglomerate interface.

Factory for creating ConstantActions. <P>Implemetation note: For most operations, the ResultSetFactory determines if the operation is allowed in a readonly/target database. Because we perform JAR add/drop/replace with a utility rather than using normal language processing we never get a result set for these operations. For this reason, the ConstantActionFactory rather than the ResultSetFactory checks if the these operations are allowed.
************************************************************************ Public Methods implementing ConglomerateController: *************************************************************************
A Generic class which implements the basic functionality needed for a cost controller.

ExecutionContext stores the result set factory to be used by the current connection, and manages execution-level connection activities. <p> An execution context is expected to be on the stack for the duration of the connection.
This Factory is for creating the execution items needed by a connection for a given database.  Once created for the connection, they should be pushed onto the execution context so that they can be found again by subsequent actions during the session.
LanguageConnectionContext keeps the pool of prepared statements, activations, and cursors in use by the current connection. <p> The generic impl does not provide statement caching.
LanguageConnectionFactory generates all of the items a language system needs that is specific to a particular connection. Alot of these are other factories.
The LanguageFactory provides system-wide services that are available on the Database API.
<p> This is a generic mode aggregator for testing with many types. </p>
A parameter.  Originally lifted from ParameterValueSet.
Implementation of ParameterValueSet
Basic implementation of prepared statement. relies on implementation of ResultDescription and Statement that are also in this package. <p> These are both dependents (of the schema objects and prepared statements they depend on) and providers.  Prepared statements that are providers are cursors that end up being used in positioned delete and update statements (at present). <p> This is impl with the regular prepared statements; they will never have the cursor info fields set. <p> Stored prepared statements extend this implementation

This is the implementation for Qualifier.  It is used for generated scans.
Class GenericQuery: The generic class that is extended by the Query classes or instantiated when the 'query.list' of custom queries is provided
Generic implementation of a Referential Integrity checker.  Abstract.
GenericResultDescription: basic implementation of result description, used in conjunction with the other implementations in this package.  This implementation of ResultDescription may be used by anyone.
ResultSetFactory provides a wrapper around all of the result sets used in this execution implementation. This removes the need of generated classes to do a new and of the generator to know about all of the result sets.  Both simply know about this interface to getting them. <p> In terms of modularizing, we can create just an interface to this class and invoke the interface.  Different implementations would get the same information provided but could potentially massage/ignore it in different ways to satisfy their implementations.  The practicality of this is to be seen. <p> The cost of this type of factory is that once you touch it, you touch *all* of the possible result sets, not just the ones you need.  So the first time you touch it could be painful ... that might be a problem for execution. ///////////////////////////////////////////////////////////////  PUBLIC MINIONS  ///////////////////////////////////////////////////////////////
The set of interfaces implemented by all types of ScanControllers. <P> A scan is the mechanism for iterating over the rows in a conglomerate, the scan controller is the interface through which access clients control the underlying scan.  An instance of a scan controller can be thought of as an open scan. <p> Scans are opened from a TransactionController. <P> A ScanController can handle partial rows. Partial rows are described in RowUtil. <BR> A scan controller is opened with a FormatableBitSet that describes the columns that need to be returned on a fetch call. This FormatableBitSet need not include any columns referenced in the qualifers, start and/or stop keys.
This is the implementation for ScanQualifier.  It is used for system and user scans.

GenericStatementContext is pushed/popped around a statement prepare and execute so that any statement specific clean up can be performed.
Prepared statement that can be made persistent.
A trigger executor is an object that executes a trigger.  It is subclassed by row and statement executors.
This node represents a unary getCurrentConnection operator RESOLVE - parameter will always be null for now.  Someday we may want to allow user to specify which of their connections they want.  Assume that we will use a String.
<P>Use of VirtualTableInterface to provide support for DatabaseMetaData.getProcedureColumns(). <P>This class is called from a Query constructed in java/org.apache.derby.impl.jdbc/metadata.properties: <PRE> <P>The VTI will return columns 3-14, an extra column to the specification METHOD_ID is returned to distinguish between overloaded methods. <OL> <LI><B>PROCEDURE_CAT</B> String =&gt; procedure catalog (may be null) <LI><B>PROCEDURE_SCHEM</B> String =&gt; procedure schema (may be null) <LI><B>PROCEDURE_NAME</B> String =&gt; procedure name <LI><B>COLUMN_NAME</B> String =&gt; column/parameter name <LI><B>COLUMN_TYPE</B> Short =&gt; kind of column/parameter: <UL> <LI> procedureColumnUnknown - nobody knows <LI> procedureColumnIn - IN parameter <LI> procedureColumnInOut - INOUT parameter <LI> procedureColumnOut - OUT parameter <LI> procedureColumnReturn - procedure return value <LI> procedureColumnResult - result column in ResultSet </UL> <LI><B>DATA_TYPE</B> int =&gt; SQL type from java.sql.Types <LI><B>TYPE_NAME</B> String =&gt; SQL type name, for a UDT type the type name is fully qualified <LI><B>PRECISION</B> int =&gt; precision <LI><B>LENGTH</B> int =&gt; length in bytes of data <LI><B>SCALE</B> short =&gt; scale <LI><B>RADIX</B> short =&gt; radix <LI><B>NULLABLE</B> short =&gt; can it contain NULL? <UL> <LI> procedureNoNulls - does not allow NULL values <LI> procedureNullable - allows NULL values <LI> procedureNullableUnknown - nullability unknown </UL> <LI><B>REMARKS</B> String =&gt; comment describing parameter/column <LI><B>METHOD_ID</B> Short =&gt; Derby extra column (overloading) <LI><B>PARAMETER_ID</B> Short =&gt; Derby extra column (output order) </OL>
A transaction identifier that is unique among all raw stores and all transactions The equals() method for TransactionId implements by value equality. MT - immutable need to write a value based HashCode() method.
This abstract class represents a global transaction id which can be tested for equality against other transaction ids, which can be hashed into a hash table, and which can be output as a string. <P> This class has 2 direct subclasses. <UL> <LI> org.apache.derby.iapi.store.access.xa.XAXactId : this class is a specific implementation of the JTA Xid interface <LI> org.apache.derby.impl.store.access.GlobalXactId : this class represents internal Derby transaction ids </UL> <P> The main reason for this class is to ensure that equality etc. works in a consistent way across both subclasses.

This class represents a GRANT statement.

This class performs actions that are ALWAYS performed for a GRANT role statement at execution time. Descriptors corresponding to the grants are stored in the SYS.SYSROLES table, along with the role definitions, cf CreateRoleConstantAction.
This class represents a GRANT role statement.
Test client which performs iterated GROUP BY statements on the {@code ONEKTUP} tables generated by {@code WisconsinFiller}. Based on the parameters specified when run, we perform a particular GROUP BY statement, and fetch and check the number of rows returned, as part of a performance run controlled by perf.clients.Runner. For example, you could cause this benchmark's GROUP BY to be: - one which returns 10 groups, with 1000 rows in each group, or - one which returns 100 groups, with 100 rows in each group, or - one which returns 1000 groups, with 10 rows in each group, etc. With correspondingly larger numbers of groups as the scale factor grows. You can use more rows by passing '-load_opts numRows=100000', e.g. Note that this only has an effect when you run -init. Note that changing the number of rows in the table also changes the expected size of each group; we issue a select count(*) query at the start to figure out the expected group size, but this hack only works with table sizes that are multiples of 1000. If you use a substantially larger number of rows (say, 100000 or more), you should specify '-rt 300' or higher so that a valid number of executions can occur, as the benchmark starts to slow down dramatically with large numbers of rows. To prepare the database for this little benchmark: java org.apache.derbyTesting.perf.clients.Runner -init -load group_by -load_opts numRows=NNNNNN (if you want more than 10,000 rows in DB) (this will also run the default GROUP BY, which is GROUP BY TEN) On subsequent runs you can skip the '-init', and should instead specify a particular GROUP BY to run, which you do by specifying: - the number of GROUP_BY columns (-load_opts numGroupingCols=N), and - the number of groups for each column (-load_opts numGroupsK=NNNN) (NOTE: we count from 1, not from 0, with these parameters!) I've tried this benchmark up to 5 grouping columns, which seemed like plenty for the benchmarking I wanted to do. The code supports more, but I'm not sure if it works or not. For example, this runs a 2-column group by: -load group_by -load_opts numGroupingCols=2,numGroups1=10,numGroups2=100 The resulting SQL will be: SELECT TEN, ONEPERCENT, COUNT(*) FROM TENKTUP1 GROUP BY TEN,ONEPERCENT Note that due to the way that the data in the TEN and ONEPERCENT columns are loaded, they are not independent, so this actually produces 100 groups. If numGroupingCols == 1, and thus the code can predict the number of rows that ought to be in each group, and the total number of groups, then it checks those values in the result as well.
A GroupByColumn is a column in the GROUP BY clause.
A GroupByList represents the list of expressions in a GROUP BY clause in a SELECT statement.
A GroupByNode represents a result set for a grouping operation on a select.  Note that this includes a SELECT with aggregates and no grouping columns (in which case the select list is null) It has the same description as its input result set. <p> For the most part, it simply delegates operations to its bottomPRSet, which is currently expected to be a ProjectRestrictResultSet generated for a SelectNode. <p> NOTE: A GroupByNode extends FromTable since it can exist in a FromList. <p> There is a lot of room for optimizations here: <UL> <LI> agg(distinct x) group by x =&gt; agg(x) group by x (for min and max) </LI> <LI> min()/max() use index scans if possible, no sort may be needed. </LI> </UL>
This scan controller can only be used for group fetch, no update operations are supported, use ScanController if you need scan interfaces other than group fetch. <p> In general group fetch will be more efficient than using the ScanController fetchNext() interface to get rows one at a time.  The performance comes from reducing the per call overhead of getting a row.  Also this interface can, depending on the requested isolation level, possibly do more efficient locking. <p> Group fetch scans are opened from a TransactionController.
This ResultSet evaluates grouped, non distinct aggregates. It will scan the entire source result set and calculate the grouped aggregates when scanning the source during the first call to next(). The implementation is capable of computing multiple levels of grouping in a single result set (this is requested using GROUP BY ROLLUP). This implementation has 3 variations, which it chooses according to the following rules: - If the data are guaranteed to arrive already in sorted order, we make a single pass over the data, computing the aggregates in-line as the data are read. - If the statement requests either multiple ROLLUP levels, or a DISTINCT grouping, then the data are first sorted, then we make a single pass over the data as above. - Otherwise, the data are sorted, and a SortObserver is used to compute the aggregations inside the sort, and the results are read back directly from the sorter. Note that, as of the introduction of the ROLLUP support, we no longer ALWAYS compute the aggregates using a SortObserver, which is an arrangement by which the sorter calls back into the aggregates during the sort process each time it consolidates two rows with the same sort key. Using aggregate sort observers is an efficient technique, but it was complex to extend it to the ROLLUP case, so to simplify the code we just have one path for both already-sorted and un-sorted data sources in the ROLLUP case.
An HalfOuterJoinNode represents a left or a right outer join result set. Right outer joins are always transformed into left outer joins during preprocessing for simplicity.
Simple class to isolate OEChecks from junit but allow its checks to be used in JUnit tests. This implementation just reports errors to System.out. In a JUnit environment an implementation can use Assert.fail() to report failures.
Class: HandleResult Purpose: To capture stdout and stderr to a file (PrintWriter is used for writing the output)
Copied from the Harmony project's implementation of javax.sql.rowset.serial.SerialBlob at subversion revision 946981.
Copied from the Harmony project's implementation of javax.sql.rowset.serial.SerialClob at subversion revision 946981.
Find out if we have an correlated column reference anywhere below us.  Stop traversal as soon as we find one.
Find out if we have a particular node anywhere in the tree.  Stop traversal as soon as we find one. <p> Can find any type of node -- the class or class name of the target node is passed in as a constructor parameter.
Find out if we have a user-defined table function anywhere in the tree.  Stop traversal as soon as we find one.
Find out if we have a value node with variant type less than what the caller desires, anywhere below us.  Stop traversal as soon as we find one. This is used in two places: one to check the values clause of an insert statement; i.e <pre> insert into <table> values (?, 1, foobar()); </pre> If all the expressions in the values clause are QUERY_INVARIANT (and an exception is made for parameters) then we can cache the results in the RowResultNode. This is useful when we have a prepared insert statement which is repeatedly executed. <p> The second place where this is used is to check if a subquery can be materialized or not.
Hash join of 2 arbitrary result sets. Simple subclass of nested loop, differentiated to ease RunTimeStatistics output generation.

Left outer join using hash join of 2 arbitrary result sets. Simple subclass of nested loop left outer join, differentiated to ease RunTimeStatistics output generation.
Takes a conglomerate and a table filter builds a hash table on the specified column of the conglomerate on the 1st open.  Look up into the hash table is done on the hash key column.  The hash table consists of either <code>DataValueDescriptor[]</code>s or <code>List</code>s of <code>DataValueDescriptor[]</code>. The store builds the hash table. When a collision occurs, the store builds a <code>List</code> with the colliding <code>DataValueDescriptor[]</code>s.
A HashTableNode represents a result set where a hash table is built.
Builds a hash table on the underlying result set tree.
A HeaderPrintWriter is like a PrintWriter with support for including a header in the output. It is expected users will use HeaderPrintWriters to prepend headings to trace and log messages.
A heap object corresponds to an instance of a heap conglomerate.  It caches information which makes it fast to open heap controllers from it.

A heap scan object represents an instance of a scan on a heap conglomerate. * Methods of ScanManager
The heap conglomerate factory manages heap conglomerates implemented on the raw store.

The StoreCostController interface provides methods that an access client (most likely the system optimizer) can use to get store's estimated cost of various operations on the conglomerate the StoreCostController was opened for. <p> It is likely that the implementation of StoreCostController will open the conglomerate and will leave the conglomerate open until the StoreCostController is closed.  This represents a significant amount of work, so the caller if possible should attempt to open the StoreCostController once per unit of work and rather than close and reopen the controller.  For instance if the optimizer needs to cost 2 different scans against a single conglomerate, it should use one instance of the StoreCostController. <p> The locking behavior of the implementation of a StoreCostController is undefined, it may or may not get locks on the underlying conglomerate.  It may or may not hold locks until end of transaction. An optimal implementation will not get any locks on the underlying conglomerate, thus allowing concurrent access to the table by a executing query while another query is optimizing. <p> The StoreCostController gives 2 kinds of cost information
The HeapPostCommit class implements the Serviceable protocol. In it's role as a Serviceable object, it stores the state necessary to find a page in a heap that may have committed delete's to reclaim. It looks up the page described, and reclaims space in the conglomerate. It first trys to clean up any deleted commits on the page.  It will then deallocate the page if no rows remain on the page.  All work is done while holding the latch on the page, and locks are never "waited" on while holding this latch. This implementation uses record level locking to reclaim the space. For the protocols to work correctly all other heap methods must be prepared for a record or a page to "disappear" if they don't hold a latch and/or a lock.  An example of the problem case is a scan which does not hold locks on it's current position (group scan works this way), which is positioned on a row deleted by another xact, it must be prepared to continue the scan after getting an error if the current page/row disapppears.
A heap row location represents the location of a row in the heap. <P> It is implementad as a wrapper around a raw store record handle.

This object provides performance information related to an open scan. The information is accumulated during operations on a ScanController() and then copied into this object and returned by a call to ScanController.getStatistic().


An internal api for VTIs to allow VTI's written in terms of the datatype system, e.g. returning rows. This allows passing of data from the VTI into the query engine without a conversion through a JDBC ResultSet.



Utility class for parsing and producing string representations of ids. This class supports both delimited and un-delimited ids. <P>The syntax for an id follows. <PRE> id := delim-id | unDelim-id delim-id := "[""|[any char but quote]]+" undelim-id := (a-z|A-Z|anyunicodeletter)[a-z|A-Z|_|0-9|anyunicodeletter|anyunicodedigit]* In the syntax braces show grouping. '*' means repeat 0 or more times. '|' means or. '+' means repeat 1 or more times. </PRE> <P>In addition this class provides support for qualified names. A qualified name is a dot (.) separated list of ids. <P>Limitations: <OL> <LI>Unicode escape sequences in ids are not supported. <LI>Escape sequences (\n...) are not supported. </OL>
Filter which fails all Visitables.
This class implements import of data from a URL into a table. Import functions provided here in this class shouble be called through Systement Procedures. Import uses VTI , which is supprted only through Systemem procedures mechanism.
<P>
This class implements  <code > java.sql.BLOB interface </code>. Objects created using the <code> ImportBlob </code> class  are intended to be be used to create a blob object of the data  stored in an import file or as an hex string.  Only the routines that are needed read the blob data for the blob columns by the inserts done through the VTI  have real implementations, Other routines are dummy ones to satisfy <code> java.sql.Blob </code> interface.
This class implements  <code > java.sql.CLOB interface </code>. Objects created using the <code> ImportClob </code> class  are intended to be be used to create a clob object of the data  stored in an import file.  Only the routines that are needed  to read the clob data for the clob columns by the  inserts done through the VTI have real implementations,  Other routines are dummy ones to satisfy <code> java.sql.Clob </code>  interface.
An InputStream, which can stream data from a file, starting from any offset in the file. This stream operates on top of a RandomAccessFile object. This class overrides InputStream methods to read from the given RandomAccessFile and provides an addtional method <code>seek(..)</code> to position the stream at offset in the file. Helper class to read large object data at random locations from a file that contains large object data.


An InListOperatorNode represents an IN list.
Perform Index maintenance associated with DML operations for a single index.
Basic implementation of ColumnOrdering. Not sure what to tell callers about 0-based versus 1-based numbering. Assume 0-based for now.
This class is the superclass for the classes that describe actions that are ALWAYS performed for a CREATE/DROP INDEX Statement at Execution time.
This interface describes an index. It is used in the column SYS.SYSCONGLOMERATES.DESCRIPTOR and describes everything about an index except the index name and the table on which the index is defined. That information is available in the columns NAME and TABLEID of the table SYS.SYSCONGLOMERATES. <p> Whereas non-deferrable constraints are backed by UNIQUE indexes, deferrable constraints are backed by non-unique indexes. The duplicate checking on inserts and updates for deferrable constraints are handled at the language level, not by the store level. The following table shows the correspondence between the constraint types and the index attributes used: <ul> <li>Non-deferrable PRIMARY KEY and UNIQUE NOT NULL on all constraint columns <pre> \  Value  | Number of index columns | Check Attribute                 \        | in physical BTree key   | in -------------------------------------------------------------------- unique                     | true  | N - 1 (row location     | isUniqueWithDuplicateNulls | false |        not part of key) | Store uniqueDeferrable           | false |                         | Btree hasDeferrableChecking      | false |                         | code </pre> <li>Non-deferrable UNIQUE, where at least one constraint column is nullable. <pre> \  Value  | Number of index columns | Check Attribute                 \        | in physical BTree key   | in ------------------------------------------------------------ ------- unique                     | false | N                       | isUniqueWithDuplicateNulls | true  |                         | Store uniqueDeferrable           | false |                         | Btree hasDeferrableChecking      | false |                         | code </pre> <li>Deferrable PRIMARY KEY and UNIQUE NOT NULL on all constraint columns <pre> \  Value  | Number of index columns | Check Attribute                 \        | in physical BTree key   | in ------------------------------------------------------------ ------- unique                     | false | N                       | isUniqueWithDuplicateNulls | false |                         | Lang. uniqueDeferrable           | true  |                         | code hasDeferrableChecking      | true  |                         | </pre> <li>Deferrable UNIQUE, where at least one constraint column is nullable. <pre> \  Value  | Number of index columns | Check Attribute                 \        | in physical BTree key   | in ------------------------------------------------------------ ------- unique                     | false | N                       | isUniqueWithDuplicateNulls | true  |                         | Lang. uniqueDeferrable           | false |                         | code hasDeferrableChecking      | true  |                         | </pre> </ul>
See also {@link org.apache.derby.iapi.sql.dictionary.IndexRowGenerator}. <p> For a description of how deferrable and non-deferrable constraints are backed differently, including the meaning of the boolean attributes used here, see {@link org.apache.derby.catalog.IndexDescriptor}.
A poor mans structure used in DataDictionaryImpl.java. Used to save information about system indexes.
Test client which performs an index join between the {@code TENKTUP1} and {@code ONEKTUP} tables generated by {@code WisconsinFiller}.
This interface gathers up some tasty information about the indices on a table from the DataDictionary.
Basic implementation of ExecIndexRow.
This class extends IndexDescriptor for internal use by the DataDictionary. <p> For a description of how deferrable and non-deferrable constraints are backed differently, including the meaning of the boolean attributes used here, see {@link org.apache.derby.catalog.IndexDescriptor}.
Takes a result set with a RowLocation as the last column, and uses the RowLocation to get and return a row from the given base conglomerate. Normally, the input result set will be a TableScanResultSet scanning an index conglomerate.
Perform Index maintenace associated with DML operations for a table's indexes.
Daemon acting as a coordinator for creating and updating index statistics. <p> There are two modes of operation: <ul> <li>explicit - generates index statistics due to an explict request from the user. The entrypoint is <tt>runExplicitly</tt>.</li> <li>background - generates index statistics as a background task due to an event that has triggered a statistics update. The entrypoint is <tt>schedule</tt>.</li> <ul> <p> The modes differ in how the operation affects other operations in the running system, and also how errors are dealt with. The background mode will try to affect other operations as little as possible, and errors won't be reported unless they are severe. The explicit mode will do more to make sure the operation succeeds (for instance by using locks), and will report all errors.
Daemon acting as a coordinator for creating and updating index cardinality statistics. <p> The need for updated statistics is currently determined when compiling a SELECT query. The unit of work is then scheduled with this daemon, and the work itself will be carried out in a separate thread. If the worker thread doesn't exist it is created, if it is idle the unit of work will be processed immediately, and if it is busy the unit of work has to wait in the queue. <p> The daemon code has a notion of a background task. If the update is run as a background task, it will try to affect other activity in the Derby database as little as possible. As far as possible, it will not set locks on the conglomerates it scans, and if it needs to take locks it will give up immediately if the locks cannot be obtained. In some cases it will also roll back to release locks already taken, ad then retry. Since we are accessing shared structures the background work may still interfere with the user activity in the database due to locking, but all such operations carried out by the daemon are of short duration. <p> The high level flow of an update to index statistics is: <ol> <li>schedule update (the only action carried out by the user thread)<li> <li>for each index:</li> <ol> <li>scan index</li> <li>invalidate statements dependent on current statistics</li> <li>drop existing statistics</li> <li>add new statistics</li> </ol> </ol> <p> List of possible improvements: <ol> <li>Reduce potential impact of multiple invalidations (per table), probably by finding a way to invalidate only once after all indexes for a table have had their statistics updated. So far invalidation has proven to be the most difficult piece of the puzzle due to the interaction with the data dictionary and sensitivity to concurrent activity for the table.</li> </ol> <p> <em>Implementation notes:</em> List of potential cleanups before going into a release: <ol> <li>Consider removing all tracing code. May involve improving logging if parts of the trace output is valuable enough.</li> </ol>
Helper class for obtaining index statistics and doing asserts on them. <p> This implementation assumes all tables/indexes belong to the current schema. <p> The <em>timeout</em> value is used to make the utility more resilient to differences in timing due to varying scheduling decisions, processor speeds, etc. If the system table contains the wrong number of statistics objects for the query, it will be queried repeatedly until the right number of statistics objects is obtained or the query times out.
This node type translates an index row to a base row.  It takes a FromBaseTable as its source ResultSetNode, and generates an IndexRowToBaseRowResultSet that takes a TableScanResultSet on an index conglomerate as its source.
Mapper of ValueRow into ExecIndexRow.
The Basic Services provide InfoStreams for reporting information. <p> When creating a message for a stream, you can create an initial entry with header information and then append to it as many times as desired. <p>
This operation initializes the page that is being allocated, this operation does not change the alloc page information. <PRE>
Initializer: Main Class that populates the tables needed for the test
A class that uses a ZipEntry to be a single container file, but read-only.
This class provides the base for read-only stream implementations of the StorageFile interface. It is used with the classpath, jar, http, and https subsubprotocols
Utility methods for InputStream that are stand-ins for a small subset of DataInput methods. This avoids pushing a DataInputStream just to get this functionality.
This class  describes compiled constants that are passed into InsertResultSets. CLASS METHODS
Test Insert statement called from within static initializer holds onto locks it should hold onto and doesn't hold onto locks it shouldn't hold onto.
An InsertNode is the top node in a query tree for an insert statement. <p> After parsing, the node contains targetTableName: the target table for the insert collist: a list of column names, if specified queryexpr: the expression being inserted, either a values clause or a select form; both of these are represented via the SelectNode, potentially with a TableOperatorNode such as UnionNode above it. <p> After binding, the node has had the target table's descriptor located and inserted, and the queryexpr and collist have been massaged so that they are identical to the table layout.  This involves adding any default values for missing columns, and reordering the columns to match the table's ordering of them. <p> After optimizing, ... end of class InsertNode
Represents an insert of a record onto a page. <PRE>
Insert the rows from the source into the specified base table. This will cause constraints to be checked and triggers to be executed based on the c's and t's compiled into the insert plan.
Insert the rows from the source into the specified base table. This will cause constraints to be checked and triggers to be executed based on the c's and t's compiled into the insert plan.


A VTI which returns a row of ints.
A set of operations available on internal Clob content representations. <p> The methods defined by {@link java.sql.Clob} must be implemented on top of this interface. In addition, there are some methods to aid internal tasks and organization, like transferring one internal Clob representation to another one. End interface InternalClob
Factory class and API for JDBC objects.
There is one of these beasts per INSERT/DELETE/UPDATE statement.  It fulfills the contract for the externally visible trigger execution context and it validates that a statement that is about to be executed doesn't violate the restrictions placed upon what can be executed from a trigger. <p> Note that it is crucial that cleanup() is called once the DML has completed, cleanup() makes sure that users can't do something invalid on a tec reference that they were holding from when the trigger fired.

An exception used to pass a specific "error code" through various layers of software.
Static methods to save and retrieve information about a (session) thread's interrupt status flag. If during operation we notice an interrupt, Derby will either: <ul> <li>immediately throw an exception to cut execution short, also resurrecting the thread's interrupted status flag. This does not require use of this class. <li>just note the fact using this class ({@code noteAndClearInterrupt}, or ({@code setInterrupted})), and retry whatever got interrupted, continuing execution. To achieve this, Derby will always temporarily clear the interrupted status flag. Later, depending on the type of SQL statement, we may wish to interrupt execution by throwing an SQLException at a safe place, say, after a statement in a batch is complete ({@code throwIf}), or just let the execution run to completion, and then just prior to returning to the appliction, the thread's interrupted status flag will resurrected ({@code restoreIntrFlagIfSeen}) </ul> Normally, the information is saved away in the session's LanguageConnectionContext, if available. If not, we save it in a thread local variable.
A IntersectOrExceptNode represents an INTERSECT or EXCEPT DML statement.
A Framework exception used to indicate that a filter string has an invalid syntax. <p> An <code>InvalidSyntaxException</code> object indicates that a filter string parameter has an invalid syntax and cannot be parsed. See {@link Filter} for a description of the filter string syntax. <p> This exception is updated to conform to the general purpose exception chaining mechanism.
Represents invalidating a page due to deallocation. This operation invalidates the page that is being deallocated, as opposed to deallocatePage that happens on the alloc page. <PRE>

This node represents either a unary IS NULL or IS NOT NULL comparison operator
Utility methods related to J2EE JDBC DataSource objects. Separated out from JDBCDataSource to ensure that no ClassNotFoundExceptions are thrown with JSR169.
Simple class used for determining the location of the jar file (based on the user's classpath) that contains the JAXP implementation.
JBitSet is a wrapper class for BitSet.  It is a fixed length implementation which can be extended via the grow() method.  It provides additional methods to manipulate BitSets. NOTE: JBitSet was driven by the (current and perceived) needs of the optimizer, but placed in the util package since it is not specific to query trees.. NOTE: java.util.BitSet is final, so we must provide a wrapper class which includes a BitSet member in order to extend the functionality. We want to make it look like JBitSet extends BitSet, so we need to provide wrapper methods for all of BitSet's methods.
This CipherFactory creates new JCECipherProvider.
Cipher Factory instance builder. New instances of the cipher factory are created based on the on the user specified encryption properties.
This is a wrapper for a Cipher
Simple JBDC mbean that pulls information about the JDBC driver.
This class is a refactoring wrapper around the shared JDBC40Translation class.
A class to boot a Derby system that includes a JDBC driver. Should be used indirectly through JDBCDriver or JDBCServletBoot or any other useful booting mechanism that comes along.
Type-safe enumerator of valid JDBC clients. Each JDBC client definition consists of the client name, the name of the JDBC driver class, the name of a DataSource class and the base JDBC url.
Change to a specified JDBCClient configuration based upon the current configuration at setup time. Previous configuration is restored at tearDown time.
Utility methods related to JDBC DataSource objects. J2EEDataSource exists to return XA and connection pooling data sources.
This class contains utility methods for displaying JDBC objects and results. <p> All of the methods are static. The output stream to write to is always passed in, along with the JDBC objects to display.
Management and information for the embedded JDBC driver. <P> Key properties for registered MBean: <UL> <LI> <code>type=JDBC</code> <LI> <code>system=</code><em>runtime system identifier</em> (see overview) </UL>
A cache for JDBC statement objects. <p> The entries in the cache contains objects implementing the <code>java.sql.PreparedStatement</code> interface, and they are inserted with a key object implementing the interface <code>StatementKey</code>. The cached objects can be either <code>java.sql.PreparedStatement</code> or <code>java.sql.CallableStatement</code>. These two should be separated by using different types of keys. <p> The cache only contains free statement objects, and on a successful request for a cached statement the statement is removed from the cache. The cache is not intended to hold duplicate statements. The physical prepared statement should be (re-)inserted into the cache when <code>close</code> is called on the logical prepared statement using it. <p> There is a maximum number of cached statements associated with the cache. If this number is exceeded, the oldest entry will be thrown out. One can always throw out an entry, because the fact that it is in the cache means it is free and not in use. @ThreadSafe End JDBCStatementCache
Interface for MBeanTest to get a MBeanServerConnection connection from. A decorator will setup mbeanServerConnector to point to an implementation of this class to obtain JMX connections.
Interface for MBeanTest to get a MBeanServerConnection connection from. A decorator will setup mbeanServerConnector to point to an implementation of this class to obtain JMX connections.
This class implements the ManagementService interface and provides a simple management and monitoring service. An mbean registered with this service remains until it is unregistered. While registered with this service it may be registered and unregistered with the jmx service a number of times.
This is the base JNDI authentication scheme class. The generic environment JNDI properties for the selected JNDI scheme are retrieved here so that the user can set JNDI properties at the database or system level.
This is the JNDI Authentication Service base class. <p> It instantiates the JNDI authentication scheme defined by the user/ administrator. Derby supports LDAP JNDI providers. <p> The user can configure its own JNDI provider by setting the system or database property derby.authentication.provider .
Type descriptor which wraps all 3 kinds of types supported in Derby's JSQL language: SQL types, Java primitives, Java classes. This interface was originally added to support the serializing of WorkUnit signatures.
This class is used to determine which Java specification Derby will run at. For a useful discussion of how this class is used, please see DERBY-3176.
This class provides a jar file based implementation of the StorageFile interface. It is used by the database engine to access persistent data and transaction logs under the jar subsubprotocol.

Abstract out the loading of JarFiles.
This class provides a Jar file based implementation of the StorageFactory interface. It is used by the database engine to access persistent data and transaction logs under the jar subsubprotocol.

<p> System procedures which run only on Java 5 or higher. </p>
JavaFactory provides generators for Java constructs. Once Java constructs have been connected into a complete class definition, the class can be generated from them. The generated class is created as a byte-code array that can then be loaded by a class loader or, in our case, the class utilities wrapper around our special class loader. <p> Each method shows the equivalent Java in the line starting "Java:" in the header comment.  Items in the java code that begin with # refer to parameters used in constructing the object.  So, for example, newReturnStatement takes a parameter named value; its Java code is: <verbatim> Java: return #value; </verbatim> <p> This represents the fact that newReturnStatement returns a object that represents a return statement that returns the value represented by the parameter named value. <p> REVISIT: when StandardException is moved to BasicServices, all of these want to support it so they can throw real NotImplementedYet exceptions. It is expected that alot of this interface can be not-implemented for engines that do not need this complete treatment of the language. <p> Known Java constructs missing from this interface include: <ul> <li> array initializers <li> ,-lists of statements in for segments <li> accessing a field of the current object or class without including this or the class name <li> declaring a list of variables against one type <li> conversions/coercions/promotions of types <li> empty statement <li> labeled statement <li> switch statement <li> break, continue statements <li> "super" expression (akin to the "this" expression). <li> operations on multi-dimensional arrays </ul> <p> This interface also does not do real compilation -- there are no checks for things like initialization before use of variables, inclusion of catchs on throws, dead code, etc. Its purpose is to let other parts of the system piece together what they know is valid code and get bytecode out of doing that. <p> Also, implementations will require that the constructs be built appropriately or they may fail to produce a valid class.  For example, newStaticMethodCall must be used to call static methods only, not non-static local instance methods. <p> Implementations may be more, or less strict.  You are best off assuming you have to piece together each java construct and be as explicit as possible.  So, constructors must be created with newConstructor, not newMethodBuilder; constructors must include the explicit call to super(...) or this(...), as their first statement; all methods and constructors must contain a final return statement at the end of their code path(s). Method calls will derive the method to call based on the type of the argument, so you must cast arguments as the system will not search for a close method and coerce arguments appropriately.  This includes coercing them to be some superclass or interface that they already are.
This node type converts a value from the Java domain to the SQL domain.
This abstract node class represents a data value in the Java domain.
To break down the java version into major and minor Used by the test harness for special cases

An issue from JIRA.
A JoinNode represents a join result set for either of the basic DML operations: SELECT and INSERT.  For INSERT - SELECT, any of the fields in a JoinNode can be used (the JoinNode represents the (join) SELECT statement in the INSERT - SELECT).  For INSERT, the resultColumns in the selectList will contain the names of the columns being inserted into or updated.
Takes 2 NoPutResultSets and a join filter and returns the join's rows satisfying the filter as a result set.
A JoinStrategy represents a strategy like nested loop, hash join, merge join, etc.  It tells the optimizer whether the strategy is feasible in a given situation, how much the strategy costs, whether the strategy requires the data from the source result sets to be ordered, etc.
Suite holding all of the tests for the optional simple json support.
This interface is used to get information from a KeyConstraintDescriptor. A KeyConstraintDescriptor can represent a primary/unique/foreign key constraint.
Provides the ability to hash on multiple objects.


This is the Derby LDAP authentication scheme implementation. JNDI system/environment properties can be set at the database level as database properties. They will be picked-up and set in the JNDI initial context if any are found. We do connect first to the LDAP server in order to retrieve the user's distinguished name (DN) and then we reconnect and try to authenticate with the user's DN and passed-in password. In 2.0 release, we first connect to do a search (user full DN lookup). This initial lookup can be done through anonymous bind or using special LDAP search credentials that the user may have configured on the LDAP settings for the database or the system. It is a typical operation with LDAP servers where sometimes it is hard to tell/guess in advance a users' full DN's. NOTE: In a future release, we will cache/maintain the user DN within the the Derby database or system to avoid the initial lookup. Also note that LDAP search/retrieval operations are usually very fast. The default LDAP url is ldap:/// (ldap://localhost:389/)
LOBFile is a wrapper over StorageRandomAccessFile. The purpose of this class is to let the user of this class access StorageRandomAccessFile in plain and in encrypted for without having to change code.
This input stream is built on top of {@link LOBStreamControl}. <p> All the read methods are routed to {@link LOBStreamControl}.
This is an output stream built on top of LOBStreamControl. All the write methods are routed to LOBStreamControl.
An object that tracks the state of large objects (LOBs) for the current row in a result set. <p> A LOB's state is either unpublished or published. When a LOB is published, it means that the end-user has been given a reference to the LOB object. This implies that the LOB cannot be automatically freed/released when the result set position changes (i.e. typically {@code rs.next()}), because the LOB object must be kept valid/alive until the transaction is ended or the LOB object is explicitly freed. <p> This class covers two types of functionality regarding LOBs; <ul> <li>Keep track of whether a LOB column has been published or not.</li> <li>Release LOB locators on the server.</li> </ul> Both functionalities will be disabled if the server doesn't support locators. If locators are enabled, they will be freed when {@link #checkCurrentRow} is called. <p> The tracker has a notion of current row. The current row is changed by calling {@link #checkCurrentRow checkCurrentRow}. The owner of the tracker is repsonsible for invoking the method at the correct time, and only when the cursor is positioned on a valid data row. The method must be invoked before the cursor changes the position. Note that calling the method {@link #discardState discardState} makes {@code checkCurrentRow} ignore all LOBs on the subsequent call.
Contains the stored procedures that will be used in the LOB client side methods.
This class acts as a layer of blob/clob repository (in memory or file). The max bytes of data stored in memory depends on the way this class is created. If the class is created with initial data, the buffer size is set to the size of the byte array supplied, but no larger than MAX_BUF_SIZE. If no initial data is supplied, or if the initial data size is less than DEFAULT_BUF_SIZE, the buffer size is set to DEFAULT_BUF_SIZE. When write increases the data beyond this value a temporary file is created and data is moved into that. If truncate reduces the size of the file below initial buffer size, the data is moved into memory. This class also creates InputStream and OutputStream which can be used to access blob data irrespective of if its in memory or in file.
This class implements TypeCompiler for the SQL LOB types.
LangScripts runs SQL scripts (.sql files) in the lang package and compares the output to a canon file in the standard master package. <BR> Its suite() method returns a set of tests where each test is an instance of this class for an individual script wrapped in a clean database decorator. <BR> It can also be used as a command line program to run one or more language based SQL scripts as tests.
LanguageConnectionContext keeps the result sets, and activations in use by the current connection. <p> More stable items, like other factories, are accessible through the LanguageConnectionFactory or the LanguageFactory.
Factory interface for items specific to a connection in the language system. This is expected to be used internally, and so is not in Language.Interface. <p> This Factory provides pointers to other language factories; the LanguageConnectionContext holds more dynamic information, such as prepared statements and whether a commit has occurred or not. <p> This Factory is for internal items used throughout language during a connection. Things that users need for the Database API are in LanguageFactory in Language.Interface. <p> This factory returns (and thus starts) all the other per-database language factories. So there might someday be properties as to which ones to start (attributes, say, like level of optimization). If the request is relative to a specific connection, the connection is passed in. Otherwise, they are assumed to be database-wide services.
A class to handle setting language database properties
Factory interface for the Language.Interface protocol. This is used via the Database API by users, and is presented as a System Module (not a service module).  That could change, but for now this is valid for any database.
This is a holder of language properties that are exposed users.  Consolodate all properties here.
Return the last key in an index.  Used to perform max(). Print the parameters that constructed this result set to the trace stream. private final void traceScanParameters() { if (SanityManager.DEBUG) { HeaderPrintWriter traceStream = SanityManager.GET_DEBUG_STREAM(); traceStream.println(""); traceStream.println("LastIndexKeyResultSet number " + resultSetNumber + " parameters:"); traceStream.println(""); traceStream.println("\tTable name: " + tableName); if (indexName != null) { traceStream.println("\tIndex name: " + indexName); } traceStream.println(""); } }
A Latch represents a latch held in the lock manager.
Implementation of InputStream which get EXTDTA from the DDMReader. This class can be used to stream LOBs from Network client to the Network server. Furthermore, this class is used when layer B streaming is carried out and expects corresponding DDMReader start layer B streaming when the object of this class is instantiated.

This node represents a unary XXX_length operator
Like matching algorithm. Not too speedy for %s. SQL92 says the escape character can only and must be followed by itself, %, or _.  So if you choose % or _ as the escape character, you can no longer do that sort of matching. Not the most recent Like -- missing the unit tests
This node represents a like comparison operator (no escape) If the like pattern is a constant or a parameter then if possible the like is modified to include a &gt;= and &lt; operator. In some cases the like can be eliminated.  By adding =, &gt;= or &lt; operators it may allow indexes to be used to greatly narrow the search range of the query, and allow optimizer to estimate number of rows to affected. constant or parameter LIKE pattern with prefix followed by optional wild card e.g. Derby% CHAR(n), VARCHAR(n) where n &lt; 255 &gt;=   prefix padded with '\u0000' to length n -- e.g. Derby\u0000\u0000 &lt;=   prefix appended with '\uffff' -- e.g. Derby\uffff [ can eliminate LIKE if constant. ] CHAR(n), VARCHAR(n), LONG VARCHAR where n &gt;= 255 &gt;= prefix backed up one characer &lt;= prefix appended with '\uffff' no elimination of like parameter like pattern starts with wild card e.g. %Derby CHAR(n), VARCHAR(n) where n &lt;= 256 &gt;= '\u0000' padded with '\u0000' to length n &lt;= '\uffff' no elimination of like CHAR(n), VARCHAR(n), LONG VARCHAR where n &gt; 256 &gt;= NULL &lt;= '\uffff' Note that the Unicode value '\uffff' is defined as not a character value and can be used by a program for any purpose. We use it to set an upper bound on a character range with a less than predicate. We only need a single '\uffff' appended because the string 'Derby\uffff\uffff' is not a valid String because '\uffff' is not a valid character.
Methods that allow limits to be placed on an input or output stream to avoid clients reading or writing too much information.
An abstract InputStream that provides abstract methods to limit the range that can be read from the stream.
Limit and ErrorObjectInput capabilities. Combin
A  Reader that provides methods to limit the range that can be read from the reader.

<p> This VTI makes a table out of a text file. The table has one column, containing the contents of a line in the file. Leading and trailing white space are trimmed. </p> /////////////////////////////////////////////////////////////////////////////////  ResultSet METHODS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////////////////
Interface for a client to populate the database. Various implementations can be provided, e.g. via SQL, via VTI, via import etc An implementation for Load must be able to do the following <OL> <LI> Use the setupLoad to perform any necessary initialization for the load phase <LI> Load data into all the tables </OL> <P> DECIMAL values are represented as String objects to allow Order Entry to be run on J2ME/CDC/Foundation which does not support BigDecimal.
These exceptions are thrown by the import and export modules.
Interface implemented by load generators. A load generator generates the test load on the DBMS by invoking the clients' {@code doWork()} methods. Different load generators may generate load with different characteristics. The actual database operations performed are decided by the clients passed in to the load generator's {@code init()} method.


<p> This is a version of GenericMode for use with types which are not Comparable. The class started out for use with Blob and Clob but was pressed into service for other types also. </p> <p> In particular, this is a mode aggregator for use with the JDBC date/time classes too. You can't use GenericMode with those types because they do not satisfy its type bounds. That is because they inherit the Comparable implementation of java.util.Date rather than implementing their own more specific version of Comparable. That is, java.sql.Date implements Comparable&lt;java.util.Date&gt; rather than Comparable&lt;java.sql.Date&gt;. </p>
Program that attempts to flag the derby i18n properties files for a variety of possible and actual problems. For syntax, see USAGE string ( obtained with -h) For further info, see readme file
A field within the generated class. Return an expression that's the value of this field Expression getField(); Return an expression that assigns the passed in value to the field and returns the value set. Expression putField(Expression value);
A LocaleFinder gets a Locale and things associated with Locales.



<p> Mutable holder for the column values and RowLocation of a conglomerate row. Use with caution because values and arrays are not copied when they are passed in and out. </p>
A Lock represents a granted or waiting lock request. <BR> MT - Mutable - Immutable identity : Thread Aware
A LockControl contains a reference to the item being locked and doubly linked lists for the granted locks and the waiting locks. <P> MT - Mutable - Container object : single thread required
Generic locking of objects. Enables deadlock detection. <BR> MT - Mutable - Container Object - Thread Safe
Interface for classes that represent an owner of the locks within a compatibility space.
An Enumeration that returns the the Lockables in a group. A LockSpace represents the complete set of locks held within a single compatibility space, broken into groups of locks. A LockSpace contains a HashMap keyed by the group reference, the data for each key is a HashMap of Lock's. <p> A <code>LockSpace</code> can have an owner (for instance a transaction). Currently, the owner is used by the virtual lock table to find out which transaction a lock belongs to. Some parts of the code also use the owner as a group object which guarantees that the lock is released on a commit or an abort. The owner has no special meaning to the lock manager and can be any object, including <code>null</code>. </p>
LockTable is a virtual table that shows all locks currently held in the database. This virtual table can be invoked by calling it directly <PRE> select * from SYSCS_DIAG.LOCK_TABLE </PRE> <P>The LockTable virtual table takes a snap shot of the lock table while the system is in flux, so it is possible that some locks may be in transition state while the snap shot is taken. We choose to do this rather then impose extranous timing restrictions so that the use of this tool will not alter the normal timing and flow of execution in the application. <P>The LockTable virtual table has the following columns: <UL><LI>XID varchar(15) - not nullable.  The transaction id, this can be joined with the TransactionTable virtual table's XID.</LI> <LI>TYPE varchar(5) - nullable.  The type of lock, ROW, TABLE, or LATCH</LI> <LI>MODE varchar(4) - not nullable.  The mode of the lock, "S", "U", "X", "IS", "IX".</LI> <UL><LI>S is shared lock (N/A to Latch) </LI> <LI>U is update lock (N/A to Latch) </LI> <LI>X is exclusive lock </LI> <LI>IS is intent shared lock (N/A to Latch or Row lock) </LI> <LI>IX is intent exclusive lock (N/A to Latch or Row lock) </LI> </UL> <LI>TABLENAME varchar(128) - not nullable. The name of the base table the lock is for </LI> <LI>LOCKNAME varchar(20) - not nullable.  The name of the lock </LI> <LI>STATE varchar(5) - nullable.  GRANT or WAIT </LI> <LI>TABLETYPE varchar(9) - not nullable.  'T' for user table, 'S' for system table </LI> <LI>LOCKCOUNT varchar(5) - not nullable.  Internal lock count.</LI> <LI>INDEXNAME varchar(128) - normally null.  If non-null, a lock is held on the index.</LI> </UL>
This class describes actions that are ALWAYS performed for a LOCK TABLE Statement at Execution time.
A LockTableNode is the root of a QueryTree that represents a LOCK TABLE command: LOCK TABLE <TableName> IN SHARE/EXCLUSIVE MODE
This provides an Enumeration of Latch's from a clone of the lock table. A Latch is badly named, it represents lock information.
Any object that needs to be locked must implement Lockable. This allows a generic lock manager that can have locking policies defined on a per-object basis. A request to lock the object takes a qualifier, this qualifier may be used the object to implement a complex locking policy, e.g. traditional database shared, update and exclusive locks. <P> The lock manager uses this ordered protocol to determine if a lock request on a Lockable <TT> L </TT> with qualifier <TT> Q1 </TT> in compatibility space <TT> CS1 </TT> can be granted: <OL> <LI>If no locks are held on <TT> L </TT> in any compatability space then the request is granted. <LI>If <TT>L.requestCompatible(Q1)</TT> returns true then the lock is granted. <LI>Otherwise the request is granted if the following expression evaluates to true for every other lock <TT>{ CSn, Qn}</TT> held on <TT> L </TT> <UL> <LI> <PRE>    ( ( CSn == CS1 ) &amp;&amp; L.lockerAlwaysCompatible() ) </PRE> <LI> <PRE> || (L.reqestCompatible(Q1, Qn)) </PRE> </UL> </OL> <BR> If the request is granted then a call is made to <TT> L.lockEvent(CS1, Q1) </TT>. <BR> When the lock is released a call is made to <TT> L.unlockEvent(CS1, Q1) </TT>. <P> The lock manager uses equals() and hashCode() to identify unique Lockables. <BR> If the class implementing Lockable requires that each instance of class correspond to a different locked object then the equals() method must test equality via the reference equality test (==), this is the default behaviour for equality. <BR> If the class implementing Lockable require that each instance of the class that has the same value (as defined by the class) corresponds to a locked object then its equals() method must reflect that, e.g. by testing equality of its fields. In this case the first Lockable to be locked will be kept by lock manager as the key for the lock. Thus even after the first caller unlocks the object, its reference will still be kept by the lock manager. Thus Lockable's that per value equality must be designed so that they are never re-used for different lockable concepts. <BR> In either case the equals() method must accept a reference to an object of a different type. <BR> As per standard hashtable rules the value returned by hashCode() must be in sync with the equals() method. <BR> MT - Mutable - : single thread required, synchronization is provided by the lock manager. If the class implementing Lockable uses value equality then it must have an immutable identity.
Any object that implements this interface can be used as a locking policy for accessing a container. <P> The locking policy must use the defined lock qualifiers (ContainerLock.CIS, RowLock.RS, etc.) and the standard lock manager. (A locking policy that just performs no locking wouldn't need to use these :-) <P> A locking policy must use the object that is an instance of Transaction (originally obtained via startTransaction() in RawStoreFactory) as the compatibilitySpace for the LockFactory calls. <BR> A locking policy must use the passed in transaction as the compatability space and the lock group. This chain (group) of locks has the following defined behaviour <UL> <LI>Locks are released at transaction.commit() <LI>Locks are released at transaction.abort() </UL> <BR> MT - Thread Safe
Wraps a RandomAccessFile file to provide buffering on log writes. Only supports the write calls required for the log! MT - unsafe.  Caller of this class must provide synchronization.  The one exception is with the log file access, LogAccessFile will touch the log only inside synchronized block protected by the semaphore, which is defined by the creator of this object. Write to the log buffers are allowed when there are free buffers even when dirty buffers are being written(flushed) to the disk by a different thread. Only one flush writes to log file at a time, other wait for it to finish. Except for flushLogAccessFile , SyncAccessLogFile other function callers must provide syncronization that will allow only one of them to write to the buffers. Log Buffers are used in circular fashion, each buffer moves through following stages: freeBuffers --&gt; dirtyBuffers --&gt; freeBuffers. Movement of buffers from one stage to 	another stage is synchronized using	the object(this) of this class. A Checksum log record that has the checksum value for the data that is being written to the disk is generated and written 	before the actual data. Except for the large log records that does not fit into a single buffer, checksum is calcualted for a group of log records that are in the buffer when buffers is switched. Checksum log record is written into the reserved space in the beginning buffer. In case of a large log record that does not fit into a buffer, the checksum is written to the byte[] allocated for the big log record. Checksum log records helps in identifying the incomplete log disk writes during recovery. This is done by recalculating the checksum value for the data on the disk and comparing it to the the value stored in the checksum log record.
A single buffer of data. ************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Public Methods of XXXX class: *************************************************************************
ReplicationLogBuffer consists of n LogBufferElements, each of which can store a number of log records in a single byte[]. <p> The format of each log record in the LogBufferElement is the same as is written to log file in LogAccessFile:<br> (int)    total_length (data[].length + optionaldata[].length)<br> (long)   instant<br> (byte[]) data+optionaldata<br> (int)    total_length<br> </p> In addition to adding a chunk of log records to the byte[], the greatestInstant variable is updated for every append so that getLastInstant can be used to get the highest log instant in this LogBufferElement.

Purpose of this class is to test the database recovery of the inserts executed with simulated log corruption in LogChecksumSetup.java and perform some updates after a successfully boot. This test should be run after the store/LogChecksumSetup.java. @version 1.0 @see LogChecksumSetup
Purpose of this class is to test the database recovery of the updates statements executed in LogChecksumRecovery.java. This test should be run after the store/LogChecksumRecovery.java. @version 1.0 @see LogChecksumSetup @see LogChecksumRecovery
Purpose of this class is to simulate out of order incomplete log write corruption (see derby-96 for details) using the proxy storage factory (org.apache.derbyTesting.functionTests.util.corruptio. CorruptDiskStorageFactory) instead of the default storage factory. By defailt all io is delegated to the default database storage factory, except when corruption is enabled through CorruptibleIo class. Proxy storage factory is loaded using the following properties in the test properties file: derby.subSubProtocol.csf=org.apache.derbyTesting.functionTests. util.corruptio.CorruptDiskStorageFactory database=jdbc:derby:csf:wombat @version 1.0 @see CorruptibleIo
A very simple log instant implementation. Within the stored log record a log counter is represented as a long, hence the getValueAsLong() method. Outside the LogFactory the instant is passed around as a LogCounter (through its LogInstant interface). The way the long is encoded is such that &lt; == &gt; correctly tells if one log instant is lessThan, equals or greater than another.

utility class that logs messages to the given log file
Describes a position in the log.
The log record written out to disk. This log record includes: <P> The is a holder object that may be setup using the setValue() and re-used rather than creating a new object for each actual log record. <P>	<PRE> The format of a log record is
LogScan provides methods to read a log record from the log. The how and what a logScan returns is left to the specific implementation. This interface is here so that a LogFactory can return it.
This is the interface for the replication log shipper. The log shipper is started by the master controller service. The log shipper is responsible for shipping of the log chunks from the log buffer (on the master) to the slave. The log shipper handles both periodic shipping of log records as well as request based shipping. The request based shipping would be useful when the log buffer becomes full and needs to be freed before it can store subsequent log chunks.
This is an implementation of the log using a non-circular file system file. No support for incremental log backup or media recovery. Only crash recovery is supported. <P> The 'log' is a stream of log records.  The 'log' is implemented as a series of numbered log files.  These numbered log files are logically continuous so a transaction can have log records that span multiple log files. A single log record cannot span more then one log file.  The log file number is monotonically increasing. <P> The log belongs to a log factory of a RawStore.  In the current implementation, each RawStore only has one log factory, so each RawStore only has one log (which composed of multiple log files). At any given time, a log factory only writes new log records to one log file, this log file is called the 'current log file'. <P> A log file is named log<em>logNumber</em>.dat <P> Everytime a checkpoint is taken, a new log file is created and all subsequent log records will go to the new log file.  After a checkpoint is taken, old and useless log files will be deleted. <P> RawStore exposes a checkpoint method which clients can call, or a checkpoint is taken automatically by the RawStore when <OL> <LI> the log file grows beyond a certain size (configurable, default 100K bytes) <LI> RawStore is shutdown and a checkpoint hasn't been done "for a while" <LI> RawStore is recovered and a checkpoint hasn't been done "for a while" </OL> <P> This LogFactory is responsible for the formats of 2 kinds of file: the log file and the log control file.  And it is responsible for the format of the log record wrapper. <P> <PRE> Format of log control file

A Loggable is a record of a change of state or an event that happened in the RawStore in the context of a transaction. All changes in the RawStore must be logged. This is the root class for all log operations.



A wrapper class for a physical Derby callable statement. <p> The idea behind the logical prepared statement is to allow reuse of the physical callable statement. In general the logical entity will forward all calls to the physical entity. A few methods have special implementations, the most important one being {@link #close}. Each method will check that the logical statement is still open before the call is forwarded to the underlying physical statement.
JDBC 4 specific wrapper class for a Derby physical callable statement.
A simple delegation wrapper handle for a physical connection. <p> All methods of the {@code Connection} interface are forwarded to the underlying physical connection, except for {@link #close()} and {@link #isClosed()}. When a physical connection is wrapped, it is non-null, when the logical connection is closed, the wrapped physical connection is always set to {@code null}. Both the finalizer and the {@code close}-methods will always set the physical connection to {@code null}. After the physical connection has been nulled out, only the {@code PooledConnection} instance will maintain a handle to the physical connection.
A metadata object to be used with logical connections when connection pooling is being used. <p> The purpose of this object is to make sure references to the underlying physical connection don't leak to the client / user, and to make the lifetime of the metadata object equal to the logical connection instead of the underlying physical connection.
An abstract class that is used for logical log operation.  A logical log operation is one where the undo of the operation may be applied to a different page than the original operation. <PRE>
A wrapper class for a physical Derby prepared statement. <p> The idea behind the logical prepared statement is to allow reuse of the physical prepared statement. In general the logical entity will forward all calls to the physical entity. A few methods have special implementations, the most important one being {@link #close}. Each method will check that the logical statement is still open before the call is forwarded to the underlying physical statement.
JDBC 4.2 specific wrapper class for a Derby physical prepared statement.
Common class interacting with the JDBC statement cache for logical prepared statements and logical callable statements. <p> Note that {@link #getPhysPs} and {@link #getPhysCs} takes care of checking if the logical statement has been closed. The physical statement will take care of validating itself. <p> Beside from the above, special treatment of logical entities happens on close. This is the point where cache interaction takes place, and also where the appropriate methods are called on the physical statement to perform the necessary clean up for later reuse. <p> A note regarding the thread safety of this class, is that access to {@code physicalPs} and {@code physicalCs} is guarded by the instance of this class, but it is assumed that operation on/within the physical statement is synchronized in the physical statement itself . @ThreadSafe
A Logical undo is an undo operation that operates on a different page from the page that has the original change.  The reason one would need logical undo is when an uncommitted row move from one page to another in a nested internal transaction which is committed.  For example, an uncommitted insert on a btree may be moved by a later split operation to another page, the split operation will have committed.  If the insert needs to be rolled back, it can only be found at the new page where the split puts it and not at the original page where it is inserted. <P> The logging and recovery system does not know how to do logical undo. Client of the logging system must provide it with a call back function so that during undo time (both runtime undo and recovery undo), the appropriate page and row can be found so that the logging system can apply the log's undo operation. <P> Any log operation that needs logical undo must implement this LogicalUndo interface, which serves the purpose of a callback function pointer.  This callback function findUndoInfo is called by log operation generateUndo and will be given all the information in the log operation. <P> FindUndo uses the information in the pageOp to find the correct page and record that needs to be rolled back, i.e., a latched page (undoPage) and the recordId (undoRID).  It returns the latched undoPage, and modifies the pageOp to contain the correct segmentId, containerId, pageNumber and recordId etc.  It also need to supply a releaseResource() method that the logging system can call to unlatch the page and release the container, etc, after the undo has been applied. <P> The logging system will use the information in the undoPackage to put together a Compensation operation which has the undoPage number and undoRID.  Logical Undo is only called during the generation of a CLR, never during recovery redo. <P> <B>Note: LogicalUndo is a call back function pointer that will be written out as part of the log operation, it should not contain any non-transient member fields </B> <P> Details. <P> LogicalUndo, and LogicalUndoable is the interface used by logical undo between the logging system in RawStore and Access.  A log operation that needs logical undo should implment LogicalUndoable intead of Undoable.  A LogicalUndoable log operation contains a LogicalUndo member field, which is a function pointer to an Access function that provides the logical undo logic of, say, traversing a btree. <P> When called to generateUndo, that LogicalUndoable log operation will call LogicalUndo.findUndo instead of relying on the page number and recordId that is stored in it during the runtime roll forward operation.  <B>The logging system opens the container before it calls findUndo, therefore the container where the log operation is applied cannot between rollforward and rollback.</B> <P> In LogicalUndo.findUndo, it can use information stored in the LogicalUndoable, such as pageNumber, containerId, to come up with a template row.  It can then ask the LogicalUndoable log record to restore a row from the log record that fits the template.  Using this restored row, LogicalUndo can, e.g., restore the key to the btree and traverses the btree.  Once it finds the correct RecordHandle where the rollback should go, findUndo should call pageOp.resetRecord and return a latched page where the undo should go. <P> Upon the return of findUndo, the LogicalUndoable log operation should have information about the new RecordHandle and the page should be return latched.  A compensation operation is then generated with the new record location and undoMe is applied on the correct location. <P> The logging system will unlatch the undoPage when it is done with rollback and will close the container.
LogicalUndoOperation is a compensation operation that rolls back the change of an LogicalUndoable operation.  A LogicalUndoOperation itself is not undo-able, i.e, it is loggable but not undoable. <PRE>
A LogicalUndoable is a log operation that operates on the content of a page and the log operation needs logical undo.  This interface is used by LogicalUndo to extract information out of the log record, and to pass back to the logging system the real location where the roll back should happen. <P> It has specific page information such as its segment Id, container Id, page number, and it knows how to restore a storable row from the information stored in the log record.

An exception used to pass a specfic "error code" through various layers of software.
An aggregate which computes max(abs()), always returning a Long.
A stream returning characters by looping over an alphabet. End class LoopingAlphabetReader
A stream returning a cycle of the 26 lowercase letters of the modern Latin alphabet. End class LoopingAlphabetStream
Methods to aid classes recover from OutOfMemoryErrors by denying or reducing service rather than a complete shutdown of the JVM. It's intended that classes use to functionality to allow then to deny service when memory is low to allow the JVM to recover, rather than start new operations that are probably doomed to failure due to the low memory. <P> Expected usage is one instance of this class per major logical operation, e.g. creating a connection, preparing a statement, adding an entry to a specific cache etc. <BR> The logical operation would call isLowMemory() before starting the operation, and thrown a static exception if it returns true. <BR> If during the operation an OutOfMemoryException is thrown the operation would call setLowMemory() and throw its static exception representing low memory. <P> Future enhancments could be a callback mechanism for modules where they register they can reduce memory usage on a low memory situation. These callbacks would be triggered by a call to setLowMemory. For example the page cache could reduce its current size by 10% in a low memory situation.
<p> A descriptor for how a Lucene index is created and queried. </p>
Provides a table interface to the Lucene indexes in this database. See org.apache.derby.optional.lucene.LuceneSupport.listIndexes.
A VTI that provides the results of Lucene queries and associated Lucene assigned document ids. This is intended for use through the provided query function LuceneSupport.luceneQuery.
Suite holding all of the tests for the Lucene plugin.
Support for creating, updating, and querying Lucene indexes in Derby, and associated utility functions.
<p> Utility methods for the Lucene optional tool. </p>
This has the main method with arguements for embedded and NWserver.
This is the controller for ij. It uses two parsers: one to grab the next statement, and another to see if it is an ij command, and if so execute it. If it is not an ij command, it is treated as a JSQL statement and executed against the current connection. ijParser controls the current connection, and so contains all of the state information for executing JSQL statements. <p> This was written to facilitate a test harness for language functionality tests.
* *	Keeps a copy of the system properties saved at a critical early *	point during the running of the test harness.  Uses this copy *	to create new copies which can then be mussed up and thrown *	away, as needed.
Management MBean to allow applications to dynamically control visibility of Derby's MBeans. If Derby does not register its ManagementMBean then an application may register this implementation of ManagementMBean itself and use it to start Derby's JMX management. <P> If Derby is not booted then invoking startManagement will do nothing.
JMX MBean inteface to control visibility of Derby's MBeans. When Derby boots it attempts to register its MBeans. It may fail due to lack of valid permissions. If Derby does not register its MBeans then an application may register the Management implementation of ManagementMBean itself and use it to start Derby's JMX management. <P> Key properties for registered MBean when registered by Derby: <UL> <LI> <code>type=Management</code> <LI> <code>system=</code><em>runtime system identifier</em> (see overview) </UL>
This interface represents a Management Service. An implementation of this service is started by the Derby monitor if the system property derby.system.jmx has been set. The following services are provided: <li> Create and start an instance of MBean server to register MBeans. <li> Create managed beans (MBeans) to instrument derby resources for management and monitoring. The following code can be used to locate an instance of this service if running. ManagementService ms = (ManagementService) Monitor.getSystemModule(Module.JMX);
This class is for testing method calls on user-defined types.  It has many different methods for testing different cases.
An input stream whose internal data is in blocks, the format of each block is (boolean isLastBlock, int blockLength, sequence of blockLength bytes) All blocks except for the last block must have isLastBlock set to false. The last block must have isLastBlock set to true. This class implements an input stream whose length is limited, yet the creator (writer) of the stream does not need to know the entire length before creating it.
<p> This is an implementation of the replication master controller service. The service is booted when this instance of Derby will have the replication master role for this database. </p> <p> Note: The current version of the class is far from complete. Code to control the replication master behavior will be added as more parts of the replication functionality is added to Derby. </p>
<p> This is the interface for the replication master controller service. The master controller service is booted when this instance of Derby will have the replication master role for this database. </p> <p> The replication master service is responsible for managing all replication related functionality on the master side of replication. This includes connecting to the slave, setting up a log buffer to temporarily store log records from the LogFactory, and to ship these log records to the slave. </p> <p> The master controller currently only supports asynchronous replication. This means that there are no guarantees that transactions that have committed here (the master side) are also reflected on the slave side. However, the slave version of the database IS guaranteed to be transaction consistent. This implies that: <br> <ul> <li>A transaction t that is committed on the master will either be fully reflected or not be reflected at all on the slave when the slave database is turned into a non-replicated database (that is, at failover time)</li> <li>Slave execution of operations is in the same serial order as on the master because replication is based on redoing log records to the slave. By definition, log records are in serial order. This implies that if transaction t1 commits before t2 on the master, and t2 has been committed on the slave, t1 is also guaranteed to have committed on the slave.</li> </ul> </p>
Provides the ability for an object to match a subset of a group of other objects. E.g in a cache.
Describes the execution machinery needed to evaluate a WHEN [ NOT ] MATCHING clause of a MERGE statement.
Node representing a WHEN MATCHED or WHEN NOT MATCHED clause in a MERGE statement.


A MaterializeResultSetNode represents a materialization result set for any child result set that needs one.
A MaterializeSubqueryNode is used to replace the nodes for a subquery, to facilitate code generation for materialization if possible.  See beetle 4373 for details.
Materialize the underlying ResultSet tree into a temp table on the 1st open. Return rows from temp table on subsequent scans.  class implementation
This class tests log writes to the transaction log files with large log file id's and does a setup to test recovery with large log file id's in MaxLogNumberRecovery.java test. Large log file id's are simulated using a debug flag 'testMaxLogFileNumber' in the log factory, this is enabled by setting derby.debug.true=testMaxLogFileNumber in the properties file. In Non debug mode, this tests just acts as a plain log recovery test. @version 1.0
This class  tests recovery logic with large log file id's and  the error handling logic when Max possible log file limit is reached. MaxLogNumber.java test does the setup, so it should be run before this test. In Non debug mode, this tests just acts as a plain log recovery test. @version 1.0 @see MaxLogNumber
Defintion for the MAX()/MIN() aggregates.
Aggregator for MAX()/MIN().  Defers most of its work to OrderableAggregator.
A ByteHolder that stores all its bytes in memory.


Collection of convenience methods for dealing with in-memory databases. The class will keep track of databases, connections and statements created through its methods, and will delete / close these when the clean up method is invoked. This is very much the same as what {@code BaseJDBCTestCase} does, with the exception of deleting the databases. <p> Note: It may be possible to integrate this functionality into the existing JUnit framework, for instance if you want to run the entire test suite with the in-memory back end.
Describes the execution machinery needed to evaluate a MERGE statement.

/////////////////////////////////////////// WARNING: THIS HAS NOT BEEN TESTED (OR USED) YET, SO USE AT YOUR OWN RISK /////////////////////////////////////////// Merge two result sets.  The left result set (the outer result set) MUST be unique for this to work correctly.
<p> A MergeNode represents a MERGE statement. The statement looks like this... </p> <pre> MERGE INTO targetTable USING sourceTable ON searchCondition matchingClause1 ... matchingClauseN </pre> <p> ...where each matching clause looks like this... </p> <pre> WHEN MATCHED [ AND matchingRefinement ] THEN DELETE </pre> <p> ...or </p> <pre> WHEN MATCHED [ AND matchingRefinement ] THEN UPDATE SET col1 = expr1, ... colM = exprM </pre> <p> ...or </p> <pre> WHEN NOT MATCHED [ AND matchingRefinement ] THEN INSERT columnList VALUES valueList </pre> <p> The Derby compiler essentially rewrites this statement into a driving left join followed by a series of DELETE/UPDATE/INSERT actions. The left join looks like this: </p> <pre> SELECT selectList FROM sourceTable LEFT OUTER JOIN targetTable ON searchCondition </pre> <p> The selectList of the driving left join consists of the following: </p> <ul> <li>All of the columns mentioned in the searchCondition.</li> <li>All of the columns mentioned in the matchingRefinement clauses.</li> <li>All of the columns mentioned in the SET clauses and the INSERT columnLists and valueLists.</li> <li>All additional columns needed for the triggers and foreign keys fired by the DeleteResultSets and UpdateResultSets constructed for the WHEN MATCHED clauses.</li> <li>All additional columns needed to build index rows and evaluate generated columns needed by the UpdateResultSets constructed for the WHEN MATCHED...THEN UPDATE clauses.</li> <li>A trailing targetTable.RowLocation column.</li> </ul> <p> The driving left join's selectList then looks like this... </p> <pre> sc1, ..., scN, tc1, ..., tcM, targetTable.RowLocation </pre> <p> Where sc1...scN are the columns we need from the source table (in alphabetical order) and tc1...tcM are the columns we need from the target table (in alphabetical order). </p> <p> The matchingRefinement expressions are bound and generated against the FromList of the driving left join. Dummy DeleteNode, UpdateNode, and InsertNode statements are independently constructed in order to bind and generate the DELETE/UPDATE/INSERT actions. </p> <p> At execution time, the targetTable.RowLocation column is used to determine whether a given driving row matches. The row matches iff targetTable.RowLocation is not null. The driving row is then assigned to the first DELETE/UPDATE/INSERT action to which it applies. The relevant columns from the driving row are extracted and buffered in a temporary table (the "then" rows) specific to that DELETE/UPDATE/INSERT action. After the driving left join has been processed, the DELETE/UPDATE/INSERT actions are run in order, each taking its corresponding temporary table as its source ResultSet. </p> <p> Name resolution was a particularly thorny problem. This is because name resolution behaves differently for SELECTs and UPDATEs. In particular, while processing UPDATEs, the compiler throws away name resolution information; this happens as a consequence of work done on DERBY-1043. In the end, I had to invent more name resolution machinery in order to compensate for the differences in the handling of SELECTs and UPDATEs. If we are to allow subqueries in matching refinement clauses and in the values expressions of INSERT and UPDATE actions, then we probably need to remove this special name resolution machinery. And that, in turn, probably means revisiting DERBY-1043. </p> <p> The special name resolution machinery involves marking source and target column references in order to make it clear which table they belong to. This is done in associateColumn(). The markers are consulted at code-generation time in order to resolve column references when we generate the expressions needed to populate the rows which go into the temporary tables. That resolution happens in MatchingClauseNode.getSelectListOffset(). </p>
INSERT/UPDATE/DELETE a target table based on how it outer joins with a driving table. For a description of how Derby processes the MERGE statement, see the header comment on MergeNode.
A sort scan that is capable of merging as many merge runs as will fit in the passed-in sort buffer.
Wrapping the output of a MergeScan in a RowSource for the benefit of the createAndLoadConglomerate and loadConglomerate interface.  The output of a MergeScan is written to a file when we need more than one level of merge runs. MergeScan implements ScanController, this class just implements the RowSource interface.
A sort implementation which does the sort in-memory if it can, but which can do an external merge sort so that it can sort an arbitrary number of rows.
This object provides performance information related to a sort. The information is accumulated during operations on a SortController() and then copied into this object and returned by a call to SortController.getSortInfo().
<p> This tool generates the engine's message strings (message_en.properties) as well the dita source for the SQLState documentation in the Derby Reference Guide. </p>
This class is a refactoring wrapper around the new location in shared/common/reference
Message Service implementation provides a mechanism for locating messages and substituting arguments for message parameters. It also provides a service for locating property values. <p> It uses the resource bundle mechanism for locating messages based on keys; the preferred form of resource bundle is a property file mapping keys to messages.
Class comments here

Class that checks the message files for common problems.

Describe a method alias.
MethodBuilder is used to generate the code for a method. <P> The code for a method is built in a way that corresponds to the layout of the stack machine that is the Java Virtual Machine. Values are pushed on the stack, moved about on the stack and then popped off the stack by operations such as method calls. An understanding of hoe the JVM operates is useful before using this class. <P> All the method descriptions below are generating bytecode to achieved the desired behaviour when the generated class is loaded. None of this class's methods calls actually invoke methods or create objects described by the callers.
A MethodCallNode represents a Java method call.  Method calls can be done through DML (as expressions) or through the CALL statement.
The interface of all access method factories.  Specific method factories (sorts, conglomerates), extend this interface.
This is a wrapper class which invokes the Execution-time logic for Misc statements. The real Execution-time logic lives inside the executeConstantAction() method. Note that when re-using the language result set tree across executions (DERBY-827) it is not possible to store the ConstantAction as a member variable, because a re-prepare of the statement will invalidate the stored ConstantAction. Re-preparing a statement does not create a new Activation unless the GeneratedClass has changed, so the existing result set tree may survive a re-prepare.
A MiscellaneousStatement represents any type of statement that doesn't fit into the well defined categories: SET (non-transaction).
<p> This is a mode aggregator for ints. </p>
A ModifyColumnNode represents a modify column in an ALTER TABLE statement.

ModuleControl is <B>optionally</B> implemented by a module's factory class.
The monitor provides a central registry for all modules in the system, and manages loading, starting, and finding them.
A description of an instance of a module.
Allows a module to check its environment before it is selected as an implementation.
<P><B>Services</B><BR> A service is a collection of modules that combine to provide the full functionality defined by the service. A service is defined by three pieces of information: <OL> <LI>A fully qualified java class name that identifies the functionality or API that the service must provide. Typically this class represents a java interface. This class name is termed the <EM>factory interface</EM>. <LI>The <EM>identifier</EM> of the service. Services are identified by a String, this may be hard-coded, come from a UUID or any other source. <LI>An optional java.util.Properties set. </OL> <BR> The running functionality of the service is provided by a module that implements the factory interface. The identifier of the this module is not (need not be) the same as the identifier of the service. The identifier of the service is held by the monitor in its service tables. <BR> Each module in a service is keyed by at least one factory interface, identifier} pair. This pair is guaranteed to be unique within the service. <BR> The lifetime of a module in a service is no longer than the lifetime of the service. Thus shutting down a service shuts down all the modules within a service. <B>Optionally - </B> an individual module within a service may be shutdown, this will in turn shutdown any modules it started if those module are not in use by other modules within the service. This would be handled by the monitor, not the module itself. <BR> A service may be persistent, it goes through a boot in create mode, and subsequently boot in non-create mode, or a non-peristent service, it always boots in non-create mode. Persistent services can store their re-start parameters in their properties set, the monitor provides the persistent storage of the properties set. Non-persistent services do not have a properties set. <P><B>Booting Services</B><BR> Services can be booted a number of ways <UL> <LI>A non-persistent service can be booted by having a property in the application properties or the system (JVM) set. <PRE> derby.service.<EM>service name</EM>=<EM>class name</EM> e.g. # Added to the properties automatically by the class org.apache.derby.jdbc.EmbeddedDriver derby.service.jdbc=java.sql.Driver </PRE> <LI>A persistent service can be booted by having a property in the application properties or the system (JVM) set. <PRE> derby.service.<EM>service name</EM>=<EM>persistent storage type</EM> e.g. derby.service.mydatabase=serviceDirectory </PRE> serviceDirectory is a type understood by the monitor which means that there is a directory named mydatabase within the system directory and within it is a properties file service.properties. This properties set is the set for the service and must contain a property <PRE> derby.protocol=<EM>class name</EM> </PRE> This is then the factory interface for the service. Other storage types could be added in the future. <LI> The monitor at start time looks for all persistent services that it can find and starts them. E.g. all directories in the system directory that have a file service.properties are started as services. <LI>Services are started on demand, e.g. a findService attempts to boot a service if it cannot be found. </UL> <B>Any or all of these three latter methods can be implemented. A first release may just implement the look for all services and boot them.</B> . <P><B>System Service</B><BR> A special service exists, the System Service. This service has no factory interface, no identifier and no Properties set. It allows modules to be started that are required by another service (or the monitor itself) but are not fundamentally part of the service. Modules within this service are unidentified. Typically these modules are system wide types of functionality like streams, uuid creation etc. <BR> The lifetime of a system module is the lifetime of the monitor. <B>Optionally - </B> this could be changed to reference count on individual modules, requires some minor api changes. <P><B>Modules</B><BR> A module is found or booted using four pieces of information: <OL> <LI>The service the module lives in or will live in. <LI>A fully qualified java class name that identifies the functionality or API that the module must provide. Typically this class represents a java interface. This class name is termed the <EM>factory interface</EM>. <LI>The <EM>identifier</EM> of the module. Modules are identified by a String, this may be null, be hard-coded, come from a UUID or any other source. If the identifier is null then the module is described as <EM>unidentified</EM>. <LI>Boot time only - A java.util.Properties set. This Properties set is service wide and typically contains parameters used to determine module implementation or runtime behaviour. </OL> <BR> The service is identified by explicitly identifiying the System Service or by providing a reference to a module that already exists with the required service. <BR> The factory interface is provided by a String constant of the form class.MODULE from the required interface. <BR> The module identifier is provided in a fashion determined by the code, in most cases a unidentified module will suffice. <BR> The Properties set is also determined in a fashion determined by the code at create or add service time. <P><B>Module Implementations</B><BR> When creating an instance of a module, an implementation is found through lists of potential implementations. <BR> A list of potential implementations is obtained from a Properties set. Any property within this set that is of the form <PRE> derby.module.<EM>tag</EM>=<EM>java class name</EM> </PRE> is seen by the monitor as a possible implementation. <EM>tag</EM> has no meaning within the monitor, it is only there to provide uniqueness within the properties file. Typically the tag is to provide some description for human readers of the properties file, e.g. derby.module.lockManager for an implementation of a lock manager. <BR> The monitor looks through four properties sets for lists of potential implementations in this order. <OL> <LI>The properties set of the service (i.e. that passed into Monitor.createPersistentService() or Monitor.startService()). <LI>The System (JVM) properties set (i.e. java.lang.System.getProperties()). <LI>The application properties set (i.e. obtained from the derby.properties file). <LI>The default implementation properties set (i.e. obtained from the /org/apache/derby/modules.properties resource). </OL> Any one of the properties can be missing or not have any implementations listed within it. <BR> Every request to create an instance of a module searches the four implementation lists in the order above. Which list the current running code or the passed in service module came from is not relevant. <BR> Within each list of potential implementations the search is conducted as follows: <OL> <LI>Attempt to load the class, if the class cannot be loaded skip to the next potential implementation. <LI>See if the factory interface is assignable from the class (isAssignableFrom() method of java.lang.Class), if not skip to the next potential implementation. <LI>See if an instance of the class can be created without any exceptions (newInstance() method of java.lang.Class), if not skip to the next potential implementation. <LI>[boot time only] See if the canSupport() method of ModuleControl returns true when called with the Properties set of the service, if not skip to the next potential implementation. </OL> If all these checks pass then the instance is a valid implementation and its boot() method of ModuleControl is called to activate it. Note that the search order within the list obtained from a Properties set is not guaranteed. <P><B>Module Searching</B><BR> When searching for a module the search space is always restricted to a single service. This service is usually the system service or the service of the module making the search request. It would be very rare (wrong?) to search for a module in a service that was not the current service and not the system service. <BR> Within the list of modules in the service the search is conducted as follows: <OL> <LI>See if the instance of the module an instance of the factory interface (isInstance() method of java.lang.Class), if not skip to the next module. <LI>See if the identifier of the module matches the required identifier, if not skip to the next module. <LI>See if the canSupport() method of ModuleControl returns true when called with the Properties set of the service, if not skip to the next module. </OL> Note that no search order of the modules is guaranteed. <BR> Also note that a module may be found by a different factory interface to the one it was created under. Thus a class may implement multiple factory interfaces, its boot method has no knowledge of which factory interface it was requested by. <P><B>Service Properties</B><BR> Within the service's Properties a module may search for its parameters. It identifies its parameters using a unqiue parameter name and its identifier. <BR> Unique parameter names are made unique through the 'dot' convention of Properties files. A module protocol picks some unique key portion to start, e.g. RawStore for the RawStoreFactory and then extends that for specific parameters, e.g. RawStore.PageSize. Thus parameters that are typically understood by all implementations of that protocol would start with that key portion. Parameters for specific implementations add another key portion onto the protocol key portion, e.g. RawStore.FileSystem for an file system implementation of the raw store, with a specific parameter being RawStore.FileSystem.SectorSize. <BR>These are general guidelines, UUID's could be used as the properties keys but would make the parameters hard to read. <BR> When a module is unidentified it should look for a parameter using just the property key for that parameter, e.g. getProperty("RawStore.PageSize"). <BR> When a module has an identifier is should look for a property using the key with a dot and the identifier appended, e.g. getProperty("RawStore.PageSize" + "." + identifier). <BR> In addition to searching for parameters in the service properties set, the system and application set may be searched using the getProperty() method of ModuleFactory. <BR><B>Should any order be defined for this, should it be automatic?</B>
static methods set up automatically first time it's used default trigger is time-bomb, but refer to config for other possibilities add timestamps, thread ID's, (stack location info?)
Result set that fetches rows from a scan by "probing" the underlying table with a given list of values.  Repeated calls to getNextRowCore() will first return all rows matching probeValues[0], then all rows matching probeValues[1], and so on (duplicate probe values are ignored).  Once all matching rows for all values in probeValues have been returned, the call to getNextRowCore() will return null, thereby ending the scan. The expectation is that this kind of result set only ever appears beneath some other top-level result set (esp. IndexRowToBaseRowResultSet), in which case all result sets higher up in the result set tree will just see a stream of rows satisfying the list of probe values. Currently this type of result is used for evaluation of IN lists, where the user wants to retrieve all rows for which some target column has a value that equals one of values in the IN list.  In that case the IN list values are represented by the probeValues array. Most of the work for this class is inherited from TableScanResultSet. This class overrides four public methods and two protected methods from TableScanResultSet.  In all cases the methods here set probing state and then call the corresponding methods on "super".
Execute transactions using multiple threads. A single thread uses a single submitter, submitters are created outside of this class.
A NOPStatement node is for statements that don't do anything.  At the time of this writing, the only statements that use it are SET DB2J_DEBUG ON and SET DB2J_DEBUG OFF.  Both of these are executed in the parser, so the statements don't do anything at execution
NWServerThread: Start a Network Server in a new Thread, based on the NsTest.START_SERVER_IN_SAME_VM setting
This is a naive trust manager we use when we don't want server authentication. Any certificate will be accepted.
This class implements a Cacheable for a DataDictionary cache of table descriptors, with the lookup key being the name of the table. Assumes client passes in a string that includes the schema name.  If this code is required it should be moved into a D_ class. - djd public boolean isConsistent(HeaderPrintWriter reportInconsistent) throws StandardException { boolean retval = true; if (SanityManager.DEBUG) { TableDescriptor uncachedTD; try { uncachedTD = dd.getUncachedTableDescriptor(identity); } catch (StandardException se) { reportInconsistent.println("Unexpected exception " + se + " while getting cached table descriptor in NameTDCacheable."); uncachedTD = null; } retval = checkConsistency(uncachedTD, identity, reportInconsistent); } return retval; }
<p> This authentication service supports Derby NATIVE authentication. </p> <p> To activate this service, set the derby.authentication.provider database or system property to a value beginning with the token "NATIVE:". </p> <p> This service instantiates and calls the basic User authentication scheme at runtime. </p> <p> User credentials are defined in the SYSUSERS table. </p>
Takes 2 NoPutResultSets and a join filter and returns the join's rows satisfying the filter as a result set.

Takes 2 NoPutResultSets and a join filter and returns the join's rows satisfying the filter as a result set plus the rows from the left joined with a null row from the right when there is no matching row in the right result set.








This class traces communication buffers for both sends and receives. The value of the hex bytes are traced along with the ascii and ebcdic translations.
--------------------- parse DDM Reply Data-------------------------------------- ------------------------parse DDM Scalars-----------------------------







This servlet can be used to start Derby Network Server from a remote location. <P> These servlet configuration parameters are understood by this servlet. <UL> <LI><PRE>portNumber</PRE> - Port number to use. The default is 1527.</LI> <LI><PRE>startNetworkServerOnInit</PRE> - Starts the Derby Network Server at servlet initialization if 'true'.</LI> <LI><PRE>tracingDirectory</PRE> - Directory for trace files</LI> </UL>




For performance, should we worry about the ordering of our DDM command parameters





NetworkServerControl provides the ability to start a Network Server or connect to a running Network Server to shutdown, configure or retrieve diagnostic information.  With the exception of ping, these commands can  only be performed from the  machine on which the server is running. Commands can be performed from  the command line with the following arguments: <P> <UL> <LI>start [-h &lt;host&gt;] [-p &lt;portnumber&gt;] [-ssl &lt;sslmode&gt;]:  This starts the Network Server on the port/host specified or on localhost, port 1527 if no host/port is specified and no properties are set to override the defaults. By default a security manager with a default security policy will be installed. The default security policy file is called server.policy. By default the Network Server will only listen for connections from the machine on which it is running. Use -h 0.0.0.0 to listen on all interfaces or -h &lt;hostname&gt; to listen on a specific interface on a  multiple IP machine. For documentation on &lt;sslmode&gt;, consult the Server and Administration Guide.</LI> <LI>shutdown [-h &lt;host&gt;][-p &lt;portnumber&gt;] [-ssl &lt;sslmode&gt;] [-user &lt;username&gt;] [-password &lt;password&gt;]: This shutdowns the Network Server with given user credentials on the host and port specified or on the local host and port 1527(default) if no host or port is specified.  </LI> <LI>ping [-h &lt;host&gt;] [-p &lt;portnumber&gt;] [-ssl &lt;sslmode&gt;] This will test whether the Network Server is up. </LI> <LI>sysinfo [-h &lt;host&gt;] [-p &lt;portnumber&gt;] [-ssl &lt;sslmode&gt;]:  This prints classpath and version information about the Network Server, the JVM and the Derby engine. </LI> <LI>runtimeinfo [-h &lt;host] [-p &lt;portnumber] [-ssl &lt;sslmode&gt;]: This prints extensive debbugging information about sessions, threads, prepared statements, and memory usage for the running Network Server. </LI> <LI>logconnections {on | off} [-h &lt;host&gt;] [-p &lt;portnumber&gt;] [-ssl &lt;sslmode&gt;]: This turns logging of connections on or off. Connections are logged to derby.log. Default is off.</LI> <LI>maxthreads &lt;max&gt; [-h &lt;host&gt;][-p &lt;portnumber&gt;] [-ssl &lt;sslmode&gt;]: This sets the maximum number of threads that can be used for connections. Default 0 (unlimitted). </LI> <LI>timeslice &lt;milliseconds&gt; [-h &lt;host&gt;][-p &lt;portnumber&gt;] [-ssl &lt;sslmode&gt;]: This sets the time each session can have using a connection thread before yielding to a waiting session. Default is 0 (no yeild). </LI> <LI>trace {on | off} [-s &lt;session id&gt;] [-h &lt;host&gt;] [-p &lt;portnumber&gt;]  [-ssl &lt;sslmode&gt;]: This turns drda tracing on or off for the specified session or if no session is  specified for all sessions. Default is off</LI> <LI>tracedirectory &lt;tracedirectory&gt; [-h &lt;host&gt;] [-p &lt;portnumber&gt;]  [-ssl &lt;sslmode&gt;]: This changes where new trace files will be placed. For sessions with tracing already turned on, trace files remain in the previous location. Default is derby.system.home, if it is set. Otherwise the default is the current directory.</LI> </UL> </P> <P>Properties can be set in the derby.properties file or on the command line. Properties on the command line take precedence over properties in the derby.properties file.  Arguments on the command line take precedence over properties. The following is a list of properties that can be set for NetworkServerControl: <UL><LI>derby.drda.portNumber=&lt;port number&gt;: This property indicates which port should be used for the Network Server. </LI> <LI>derby.drda.host=&lt;host name  or ip address &gt;: This property indicates the ip address to which NetworkServerControl should connect. </LI> <LI>derby.drda.traceDirectory=&lt;trace directory&gt;: This property indicates where to put trace files. </LI> <LI>derby.drda.traceAll=true:  This property turns on tracing for all sessions. Default is tracing is off.</LI> <LI>derby.drda.logConnections=true:  This property turns on logging of connections. Default is connections are not logged.</LI> <LI>derby.drda.minThreads=&lt;value&gt;: If this property is set, the &lt;value&gt; number of threads will be created when the Network Server is booted. </LI> <LI>derby.drda.maxThreads=&lt;value&gt;: If this property is set, the &lt;value&gt; is the maximum number of connection threads that will be created.  If a session starts when there are no connection threads available and the maximum number of threads has been reached, it will wait until a conection thread becomes available. </LI> <LI>derby.drda.timeSlice=&lt;milliseconds&gt;: If this property is set, the connection threads will not check for waiting sessions until the current session has been working for &lt;milliseconds&gt;. A value of 0 causes the thread to work on the current session until the session exits. If this property is not set, the default value is 0. </LI> <LI>derby.drda.sslMode=&lt;sslmode&gt; This property sets the SSL mode of the server. </LI> </UL> </P> <P><B>Examples.</B></P> <P>This is an example of shutting down the server on port 1621. <PRE> java org.apache.derby.drda.NetworkServerControl shutdown -p 1621 </PRE> </P> <P>This is an example of turning tracing on for session 3 <PRE> java org.apache.derby.drda.NetworkServerControl  trace on -s 3 </PRE> </P> <P>This is an example of starting and then shutting down the Network Server on port 1621 on machine myhost <PRE> java org.apache.derby.drda.NetworkServerControl  start -h myhost -p 1621 java org.apache.derby.drda.NetworkServerControl  shutdown -h myhost -p 1621 </PRE> </P> <P> This is an example of starting and shutting down the Network Server in the example above with the API. <PRE> NetworkServerControl serverControl = new NetworkServerControl(InetAddress.getByName("myhost"),1621) serverControl.shutdown(); </PRE> </P>
NetworkServerControlImpl does all the work for NetworkServerControl
A class wrapping a {@code NetworkServerControl} instance, using reflection to allow {@code TestConfiguration} to be used without having <tt>derbynet.jar</tt> on the classpath. <p> Only methods actually required by {@code TestConfiguration} are wrapped, and this class depends on the functionality implemented by {@link org.apache.derbyTesting.junit.NetworkServerTestSetup}. <p> The problem before was that an exception was thrown during class loading time, even if the network server functionality was never required by the tests being run. With this wrapper, an exception will be thrown only if the functionality is actually required and the necessary classes are not on the classpath.
<p> This is an MBean defining a JMX management and monitoring interface of Derby's Network Server.</p> <p> This MBean is created and registered automatically at Network Server startup if all requirements are met (J2SE 5.0 or better).</p> <p> Key properties for the registered MBean:</p> <ul> <li><code>type=NetworkServer</code></li> <li><code>system=</code><em>runtime system identifier</em> (see <a href="../package-summary.html#package_description">description of package org.apache.derby.mbeans</a>)</li> </ul> <p> If a security manager is installed, accessing attributes and operations of this MBean may require a <code>SystemPermission</code>; see individual method documentation for details.</p> <p> For more information on Managed Beans, refer to the JMX specification.</p> No other management operations yet due to security concerns, see DERBY-1387 for details.
<p> This is an implementation of the <code>org.apache.derby.mbeans.drda.NetworkServerMBean</code>, providing management and monitoring capabilities related to the Network Server through JMX.</p> <p> This bean uses callbacks to the NetworkServerControlImpl class instead of invoking NetworkServerControl, as it is the impl class that contains most of the information we want to expose via JMX.</p>
Class for starting the Derby NetworkServer on a separate Thread. This class provides methods to start, and shutdown the server
A NewInvocationNode represents a new object() invocation.
A class that represents a value obtained from a Sequence using 'NEXT VALUE'
Run the NIST scripts as a single suite.
A locking policy that implements no locking.
Dummy management service for environments that do not support JMX, such as Java SE compact profile 2.

A TransactionController that does nothing. This allows the existing transaction aware language code to remain unchanged while not supporting transactions for the storeless engine.
The NoPutResultSet interface is used to provide additional operations on result sets that can be used in returning rows up a ResultSet tree. <p> Since the ResulSet operations must also be supported by NoPutResultSets, we extend that interface here as well.
Abstract ResultSet with built in Activation support for operations that return rows but do not allow the caller to put data on output pipes. This implementation of ResultSet is meant to be overridden by subtypes in the execution engine. Its primary users will be DML operations that do not put data on output pipes, but simply return it due to being result sets themselves. <p> This abstract class does not define the entire ResultSet interface, but leaves the 'get' half of the interface for subtypes to implement. It is package-visible only, with its methods being public for exposure by its subtypes. <p>
Abstract ResultSet for implementations that do not return rows. Examples are DDL statements, CALL statements and DML. <P> An implementation must provide a ResultSet.open() method that performs the required action. <P> ResultSet.returnsRows() returns false and any method that fetches a row will throw an exception.
An exception used to pass a specfic "error code" through various layers of software.
A node in a balanced binary tree.  This class is effectively a struct to the balanced tree class.
NodeAllocator manages an array of nodes which can be reused.
A NonStaticMethodCallNode is really a node to represent a (static or non-static) method call from an object (as opposed to a static method call from a class.
This authentication service does not care much about authentication. <p> It is a quiescient authentication service that will basically satisfy any authentication request, as JBMS system was not instructed to have any particular authentication scheme to be loaded at boot-up time.
Cast the rows from the source result set to match the format of the result set for the entire statement.
A NormalizeResultSetNode represents a normalization result set for any child result set that needs one.  See non-javadoc comments for a walk-through of a couple sample code paths. Below are a couple of sample code paths for NormlizeResultSetNodes. These samples were derived from Army Brown's write-ups attached to DERBY-3310 and DERBY-3494.  The text was changed to include the new code path now that all of the NormalizeResultSetNode  code has been moved into the init() method. There are two sections of code in NormalizeResultSetNode.init() that are relevant: First the code to generate the new node based on the child result set. We will call this "normalize node creation". ResultSetNode rsn  = (ResultSetNode) childResult; ResultColumnList rcl = rsn.getResultColumns(); ResultColumnList targetRCL = (ResultColumnList) targetResultColumnList; ... ResultColumnList prRCList = rcl; rsn.setResultColumns(rcl.copyListAndObjects()); ... this.resultColumns = prRCList; Next the code to adjust the types for the NormalizeResultSetNode. We will call this "type adjustment" if (targetResultColumnList != null) { int size = Math.min(targetRCL.size(), resultColumns.size()); for (int index = 0; index < size; index++) { ResultColumn sourceRC = (ResultColumn) resultColumns.elementAt(index); ResultColumn resultColumn = (ResultColumn) targetRCL.elementAt(index); sourceRC.setType(resultColumn.getTypeServices()); } --- Sample 1 : Type conversion from Decimal to BigInt on insert --- (DERBY-3310 write-up variation) The SQL statement on which this sample focuses is: create table d3310 (x bigint); insert into d3310 select distinct * from (values 2.0, 2.1, 2.2) v; There are three compilation points of interest for this discussion: 1. Before the "normalize node creation" 2. Before the "type adjustment" 3. After the  "type adjustment" Upon completion of the "type adjustment", the compilation query tree is then manipulated during optimization and code generation, the latter of which ultimately determines how the execution-time ResultSet tree is going to look.\u00a0 So for this discussion we walk through the query tree as it exists at the various points of interest just described. 1) To start, the (simplified) query tree that we have looks something like the following: InsertNode (RCL_0:ResultColumn_0<BigInt>) | SelectNode (RCL_1:ResultColumn_1<Decimal>) | FromSubquery (RCL_2:ResultColumn_2<Decimal>) | UnionNode (RCL_3:ResultColumn_3<Decimal>) Notation: In the above tree, node names with "_x" trailing them are used to distinguish Java Objects from each other. So if ResultColumn_0 shows up more than once, then it is the *same* Java object showing up in different parts of the query tree.  Type names in angle brackets, such as "<BigInt>", describe the type of the entity immediately preceding the brackets. So a line of the form: RCL_0:ResultColumn_0<BigInt> describes a ResultColumnList object containing one ResultColumn object whose type is BIGINT. We can see from the above tree that, before normalize node creation, the top of the compile tree contains an InsertNode, a SelectNode, a FromSubquery, and a UnionNode, all of which have different ResultColumnList objects and different ResultColumn objects within those lists. 2) After the normalize node creation The childresult passed to the init method of NormalizeResultSetNode is the InsertNode's child, so it ends up creating a new NormalizeResultSetNode and putting that node on top of the InsertNode's child--that is, on top of the SelectNode. At this point it's worth noting that a NormalizeResultSetNode operates based on two ResultColumnLists: a) its own (call it NRSN_RCL), and b) the ResultColumnList of its child (call it NRSN_CHILD_RCL). More specifically, during execution a NormalizeResultSet will take a row whose column types match the types of NRSN_CHILD_RCL, and it will "normalize" the values from that row so that they agree with the types of NRSN_RCL. Thus is it possible--and in fact, it should generally be the case--that the types of the columns in the NormalizeResultSetNode's own ResultColumnList are *different* from the types of the columns in its child's ResultColumnList. That should not be the case for most (any?) other Derby result set. So we now have: InsertNode (RCL_0:ResultColumn_0<BigInt>) | NormalizeResultSetNode (RCL_1:ResultColumn_1<Decimal> -> VirtualColumnNode<no_type> -> ResultColumn_4<Decimal>) | SelectNode (RCL_4:ResultColumn_4<Decimal>) | FromSubquery (RCL_2:ResultColumn_2<Decimal>) | UnionNode (RCL_3:ResultColumn_3<Decimal>) Notice how, when we generate the NormalizeResultSetNode, three things happen: a) The ResultColumList object for the SelectNode is "pulled up" into the NormalizeResultSetNode. b) SelectNode is given a new ResultColumnList--namely, a clone of its old ResultColumnList, including clones of the ResultColumn objects. c) VirtualColumnNodes are generated beneath NormalizeResultSetNode's ResultColumns, and those VCNs point to the *SAME RESULT COLUMN OBJECTS* that now sit in the SelectNode's new ResultColumnList. Also note how the generated VirtualColumnNode has no type of its own; since it is an instance of ValueNode it does have a dataTypeServices field, but that field was not set when the NormalizeResultSetNode was created.  Hence "<no_type>" in the above tree. And finally, note that at this point, NormalizeResultSetNode's ResultColumnList has the same types as its child's ResultColumnList --so the NormalizeResultSetNode doesn't actually do anything in its current form. 3) Within the "type adjustment" The purpose of the "type adjustment" is to take the types from the InsertNode's ResultColumnList and "push" them down to the NormalizeResultSetNode. It is this method which sets NRSN_RCL's types to match the target (INSERT) table's types--and in doing so, makes them different from NRSN_CHILD_RCL's types.  Thus this is important because without it, NormalizeResultSetNode would never change the types of the values it receives. That said, after the call to sourceRC.setType(...) we have: InsertNode (RCL0:ResultColumn_0<BigInt>) | NormalizeResultSetNode (RCL1:ResultColumn_1<BigInt> -> VirtualColumnNode_0<no_type> -> ResultColumn_4<Decimal>) | SelectNode (RCL4:ResultColumn_4<Decimal>) | FromSubquery (RCL2:ResultColumn_2<Decimal>) | UnionNode (RCL3:ResultColumn_3<Decimal>) The key change here is that ResultColumn_1 now has a type of BigInt intead of Decimal.  Since the SelectNode's ResultColumn, ResultColumn_4, still has a type of Decimal, the NormalizeResulSetNode will take as input a Decimal value (from SelectNode) and will output that value as a BigInt, where output means pass the value further up the tree during execution (see below). Note before the fix for DERBY-3310, there was an additional type change that caused problems with this case. See the writeup attached to DERBY-3310 for details on why this was a problem. 4) After preprocessing and optimization: After step 3 above, Derby will move on to the optimization phase, which begins with preprocessing.  During preprocessing the nodes in the tree may change shape/content to reflect the needs of the optimizer and/or to perform static optimizations/rewrites. In the case of our INSERT statement the preprocessing does not change much: InsertNode (RCL0:ResultColumn_0<BigInt>) | NormalizeResultSetNode (RCL1:ResultColumn_1<BigInt> -> VirtualColumnNode<no_type> -> ResultColumn_4<Decimal>) | SelectNode (RCL4:ResultColumn_4<Decimal>) | ProjectRestrictNode_0 (RCL2:ResultColumn_2<Decimal>) | UnionNode (RCL3:ResultColumn_3<Decimal>) The only thing that has changed between this tree and the one shown in step 3 is that the FromSubquery has been replaced with a ProjectRestrictNode. Note that the ProjectRestrictNode has the same ResultColumnList object as the FromSubquery, and the same ResultColumn object as well.  That's worth noting because it's another example of how Java objects can be "moved" from one node to another during Derby compilation. 5) After modification of access paths: As the final stage of optimization Derby will go through the modification of access paths phase, in which the query tree is modified to prepare for code generation.  When we are done modifying access paths, our tree looks something like this: InsertNode (RCL0:ResultColumn_0<BigInt>) | NormalizeResultSetNode (RCL1:ResultColumn_1<BigInt> -> VirtualColumnNode<no_type> -> ResultColumn_4<Decimal>) | DistinctNode (RCL4:ResultColumn_4<Decimal> -> VirtualColumnNode<no_type> -> ResultColumn_5<Decimal>) | ProjectRestrictNode_1 (RCL5:ResultColumn_5<Decimal>) | ProjectRestrictNode_0 (RCL2:ResultColumn_2<Decimal>) | UnionNode (RCL3:ResultColumn_3<Decimal>) The key thing to note here is that the SelectNode has been replaced with two new nodes: a ProjectRestrictNode whose ResultColumnList is a clone of the SelectNode's ResultColumnList, and a DistinctNode, whose ResultColumnList is the same object as the SelectNode's old ResultColumnList.  More specifically, all of the following occurred as part of modification of access paths: a)    The SelectNode was replaced with ProjectRestrictNode_1, whose ResultColumnList was the same object as the SelectNode's ResultColumnList. b)    the ResultColumList object for ProjectRestrictNode_1 was pulled up into a new DistinctNode. c)    ProjectRestrictNode_1 was given a new ResultColumnList--namely, a clone of its old ResultColumnList, including clones of the ResultColumn objects. d)    VirtualColumnNodes were generated beneath the DistinctNode's ResultColumns, and those VCNs point to the same result column objects that now sit in ProjectRestrictNode_1's new ResultColumnList. 6) After code generation: During code generation we will walk the compile-time query tree one final time and, in doing so, we will generate code to build the execution-time ResultSet tree. As part of that process the two ProjectRestrictNodes will be skipped because they are both considered no-ops--i.e. they perform neither projections nor restrictions, and hence are not needed. (Note that, when checking to see if a ProjectRestrictNode is a no-op, column types do *NOT* come into play.) Thus the execution tree that we generate ends up looking something like: InsertNode (RCL0:ResultColumn_0<BigInt>) | NormalizeResultSetNode (RCL1:ResultColumn_1<BigInt> -> VirtualColumnNode<no_type> -> ResultColumn_4<Decimal>) | DistinctNode (RCL4:ResultColumn_4<Decimal> -> VirtualColumnNode<no_type> -> ResultColumn_5<Decimal>) | ProjectRestrictNode_1 (RCL5:ResultColumn_5<Decimal>) | ProjectRestrictNode_0 (RCL2:ResultColumn_2<Decimal>) | UnionNode (RCL3:ResultColumn_3<Decimal>) At code generation the ProjectRestrictNodes will again be removed and the execution tree will end up looking like this: InsertResultSet (BigInt) | NormalizeResultSet (BigInt) | SortResultSet (Decimal) | UnionResultSet (Decimal) where SortResultSet is generated to enforce the DistinctNode, and thus expects the DistinctNode's column type--i.e. Decimal. When it comes time to execute the INSERT statement, then, the UnionResultSet will create a row having a column whose type is DECIMAL, i.e. an SQLDecimal value.  The UnionResultSet will then pass that up to the SortResultSet, who is *also* expecting an SQLDecimal value.  So the SortResultSet is satisfied and can sort all of the rows from the UnionResultSet. Then those rows are passed up the tree to the NormalizeResultSet, which takes the DECIMAL value from its child (SortResultSet) and normalizes it to a value having its own type--i.e. to a BIGINT.  The BIGINT is then passed up to InsertResultSet, which inserts it into the BIGINT column of the target table.  And so the INSERT statement succeeds. ---- Sample 2 -  NormalizeResultSetNode and  Union (DERBY-3494 write-up variation) Query for discussion create table t1 (bi bigint, i int); insert into t1 values (100, 10), (288, 28), (4820, 2); select * from (select bi, i from t1 union select i, bi from t1) u(a,b) where a > 28; Some things to notice about this query: a) The UNION is part of a subquery. b) This is *not* a UNION ALL; i.e. we need to eliminate duplicate rows. c) The left side of the UNION and the right side of the UNION have different (but compatible) types: the left has (BIGINT, INT), while the right has (INT, BIGINT). d) There is a predicate in the WHERE clause which references a column from the UNION subquery. e) The table T1 has at least one row. All of these factors plays a role in the handling of the query and are relevant to this discussion. Building the NormalizeResultSetNode. When compiling a query, the final stage of optimization in Derby is the "modification of access paths" phase, in which each node in the query tree is given a chance to modify or otherwise perform maintenance in preparation for code generation.  In the case of a UnionNode, a call to modifyAccessPaths() will bring us to the addNewNodes() method, which is where the call is made to generate the NormalizeResultSetNode. if (! columnTypesAndLengthsMatch()) { treeTop = (NormalizeResultSetNode) getNodeFactory().getNode( C_NodeTypes.NORMALIZE_RESULT_SET_NODE, treeTop, null, null, Boolean.FALSE, getContextManager()); } The fact that the left and right children of the UnionNode have different types (observation c above) means that the if condition will return true and thus we will generate a NormalizeResultSetNode above the UnionNode. At this point (before the NormalizeResultSetNode has been generated) our (simplified) query tree looks something like the following. PRN stands for ProjectRestrictNode, RCL stands for ResultColumnList: PRN0 (RCL0) (restriction: a > 28 {RCL1}) | UnionNode          // <-- Modifying access paths... (RCL1) /      \ PRN2     PRN3 |        | PRN4     PRN5 |        | T1       T1 where 'a > 28 {RCL1}' means that the column reference A in the predicate a > 28 points to a ResultColumn object in the ResultColumnList that corresponds to "RCL1".  I.e. at this point, the predicate's column reference is pointing to an object in the UnionNode's RCL. "normalize node creation"  will execute: ResultColumnList prRCList = rcl; rsn.setResultColumns(rcl.copyListAndObjects()); // Remove any columns that were generated. prRCList.removeGeneratedGroupingColumns(); ... prRCList.genVirtualColumnNodes(rsn, rsn.getResultColumns()); this.resultColumns = prRCList; to create a NormalizeResultSetNode whose result column list is prRCList. This gives us: PRN0 (RCL0) (restriction: a > 28 {RCL1}) | NormalizeResultSetNode (RCL1)              // RCL1 "pulled up" to NRSN | UnionNode (RCL2)              // RCL2 is a (modified) *copy* of RCL1 /      \ PRN2     PRN3 |        | PRN4     PRN5 |        | T1       T1 Note how RCL1, the ResultColumnList object for the UnionNode, has now been *MOVED* so that it belongs to the NormalizeResultSetNode.  So the predicate a > 28, which (still) points to RCL1, is now pointing to the NormalizeResultSetNode instead of to the UnionNode. After this, we go back to UnionNode.addNewNodes() where we see the following: treeTop = (ResultSetNode) getNodeFactory().getNode( C_NodeTypes.DISTINCT_NODE, treeTop.genProjectRestrict(), Boolean.FALSE, tableProperties, getContextManager()); I.e. we have to generate a DistinctNode to eliminate duplicates because the query specified UNION, not UNION ALL. Note the call to treeTop.genProjectRestrict().  Since NormalizeResultSetNode now sits on top of the UnionNode, treeTop is a reference to the NormalizeResultSetNode.  That means we end up at the genProjectRestrict() method of NormalizeResultSetNode.  And guess what?  The method does something very similar to what we did in NormalizeResultSetNode.init(), namely: ResultColumnList prRCList = resultColumns; resultColumns = resultColumns.copyListAndObjects(); and then creates a ProjectRestrictNode whose result column list is prRCList.  This gives us: PRN0 (RCL0) (restriction: a > 28 {RCL1}) | PRN6 (RCL1)              // RCL1 "pulled up" to new PRN. | NormalizeResultSetNode (RCL3)              // RCL3 is a (modified) copy of RCL1 | UnionNode (RCL2)             // RCL2 is a (modified) copy of RCL1 /      \ PRN2     PRN3 |        | PRN4     PRN5 |        | T1       T1 On top of that we then put a DistinctNode.  And since the init() method of DistinctNode does the same kind of thing as the previously-discussed methods, we ultimatley end up with: PRN0 (RCL0) (restriction: a > 28 {RCL1}) | DistinctNode (RCL1)              // RCL1 pulled up to DistinctNode | PRN6 (RCL4)              // RCL4 is a (modified) copy of RCL1 | NormalizeResultSetNode (RCL3)              // RCL3 is a (modified) copy of RCL1 | UnionNode (RCL2)             // RCL2 is a (modified) copy of RCL1 /      \ PRN2     PRN3 |        | PRN4     PRN5 |        | T1       T1 And thus the predicate a > 28, which (still) points to RCL1, is now pointing to the DistinctNode instead of to the UnionNode. And this is what we want: i.e. we want the predicate a > 28 to be applied to the rows that we retrieve from the node at the *top* of the subtree generated for the UnionNode. It is the non-intuitive code in the normalize node creation that allows this to happen.
A NotNode represents a NOT operator. Preprocessing will eliminate the NotNodes which exist above comparison operators so that the optimizer will see a query tree in CNF.
The Network Server sample demo program is a simple JDBC application that interacts with the Derby Network Server. The program: 1.	starts the Derby Network Server 2. creates the database if not already created 3. checks to see if the schema is already created, and if not, 4. creates the schema which includes the table SAMPLETBL and corresponding indexes. 5. connects to the database 6. loads the schema by inserting data 7. starts client threads to perform database related operations 8. has each of the clients perform DML operations (select, insert, delete, update) using JDBC calls, i)	 one client opens an embedded connection to perform database operations You can open an embedded connection in the same JVM that starts the Derby Network Server. ii)  one client opens a client connection to the Derby Network Server to perform database operations. 9. waits for the client threads to finish the tasks 10.shuts down the Derby Network Server at the end of the demo <P> Usage: java nserverdemo.NsSample <P> Please note, a file derby.log is created in the directory you run this program. This file contains the logging of connections made with the derby network server
NsSampleClientThread thread to perform the NsSampleWork NsSampleWork class represents all the work done in the sample demo program. It includes getting a connection to the database, creating and loading of schema, preparing and execution of SQL statements (insert, select, update, delete ) end of class NsSampleWork
An OutputStream that simply discards all data written to it.
NumberDataType is the superclass for all exact and approximate numeric data types. It exists for the purpose of allowing classification of types for supported implicit conversions among them.


This class implements TypeId for the SQL numeric datatype.
This class is used at COMPILE TIME ONLY.  It is responsible for generating ODBC metadata queries based on existing JDBC queries.  In a word, this class reads from the org/apache/derby/impl/jdbc/metadata.properties file (which is where the JDBC queries are stored), and for each query, performs the changes/additions required to make it comply with ODBC standards.  The generated ODBC queries are written to an output file that is then used, at build time, to create a full set of both JDBC and ODBC queries, all of which are then loaded into the database system tables at creation time. For more on the ODBC specification of the metadata methods in question, see: "http://msdn.microsoft.com/library/default.asp?url=/library/en-us/odbc/ htm/odbcsqlprocedures.asp" For more on how the generated queries are used at execution time, see EmbedDatabaseMetadata.java and SystemProcedures.java in the codeline.
Do some OEChecks on the Order Entry database.
Methods to implement the random database population types for the Order Entry Benchmark. The rules for generating the random data is per the TPC-C specification.
This class implements a Cacheable for a DataDictionary cache of table descriptors, with the lookup key being the UUID of the table.  If this code is required it should be moved into a D_ class. - djd public boolean isConsistent(HeaderPrintWriter reportInconsistent) throws StandardException { boolean retval = true; if (SanityManager.DEBUG) { TableDescriptor uncachedTD; try { uncachedTD = dd.getUncachedTableDescriptor(identity); } catch (StandardException se) { reportInconsistent.println("Unexpected exception " + se + " while getting cached table descriptor in OIDTDCacheable."); uncachedTD = null; } retval = checkConsistency(uncachedTD, identity, reportInconsistent); } return retval; }
Get all nodes of a certain type in a query tree, and return them in the order in which they appear in the original SQL text. This visitor is useful when rewriting SQL queries by replacing certain tokens in the original query.
Class to simply read the old format written by DataTypeDescriptor prior to DERBY-2775 being addressed. The format was incorrect used in system catalogs for routine parameter and return types. The format contained repeated information. DERBY-2775 changed the code so that these catalog types were written as TypeDescriptor (which is what always had occurred for the types in SYSCOLUMNS).
<p> Old versions visible to the upgrade machinery. </p>
Takes an expression subquery's result set and verifies that only a single scalar value is being returned. NOTE: A row with a single column containing null will be returned from getNextRow() if the underlying subquery ResultSet is empty.
This class provides  functionalty for tests to perform online backup  in a separate thread. And functions to create/restore/rollforard recovery from the backup.
This contains mnemonics for all of the opcodes of the JVM. It is a separate class so that it will not get loaded if the system does not need it (i.e. when compiled without debugging).  We even leave out the initialization in that case.
An open b-tree contains fields and methods common to scans and controllers. <P> <B>Concurrency Notes</B> <P> An instance of an open b-tree is owned by a single context.  The b-tree code assumes that the context ensures that only one thread at a time is using the open b-tree.  The open b-tree itself does not enforce or check this.
A Generic class which implements the basic functionality needed to operate on an "open" conglomerate.  This class assumes the following general things about the access method. <p> The access method is page based and contained in a single container maintained by raw store.
A utility class to store and use temporary scratch space associated with a conglomerate.


Interface for a client to execute the logical operations. Various implementations can be provided, e.g. client side SQL, procedure, etc. <P> Typical model is that each client has its own instance of an object that implements Operations. For example the implementation in a client side SQL implementation would have a reference to its own JDBC connection and prepared statements. <P> Implementations of the execution methods must perform the following: <OL> <LI>Execute business transaction <LI>Populate POJO objects required by display method <LI>Commit the database transaction(s) <LI>Call the appropriate display method from Display </UL> <P> DECIMAL values are represented as String objects to allow Order Entry to be run on J2ME/CDC/Foundation which does not support BigDecimal.
Provides an interface for an operator that operates on a range of objects E.g in a cache.
Abstract base-class for the various operator nodes: UnaryOperatorNode, BinaryOperatorNode and TernarnyOperatorNode.
Interface for optimizer tracing.
<p> OptionalTool for viewing the output created when you xml-trace the optimizer. </p>
Optimizable provides services for optimizing a table in a query.
OptimizableList provides services for optimizing a list of Optimizables (tables) in a query.
OptimizablePredicate provides services for optimizing predicates in a query.
OptimizablePredicateList provides services for optimizing a table in a query. RESOLVE - the methods for this interface need to get defined.
Optimizer provides services for optimizing a query. RESOLVE: o  Need to figure out what to do about subqueries, figuring out their attachment points and how to communicate them back to the caller.
This is simply the factory for creating an optimizer. <p> There is expected to be only one of these configured per database.
This is simply the factory for creating an optimizer.
Optimizer uses OptimizableList to keep track of the best join order as it builds it.  For each available slot in the join order, we cost all of the Optimizables from that slot til the end of the OptimizableList.  Later, we will choose the best Optimizable for that slot and reorder the list accordingly. In order to do this, we probably need to move the temporary pushing and pulling of join clauses into Optimizer, since the logic will be different for other implementations.  (Of course, we're not pushing and pulling join clauses between permutations yet.)
<p> High level description of a plan for consideration by the Optimizer. This is used to specify a complete plan via optimizer overrides. A plan is a tree whose interior nodes are join operators and whose leaves are row sources (conglomerates or tableFunctions). </p>
<P> This  class provides static methods for controlling the optimizer tracing in a Derby database.
<p> OptionalTool for tracing the Optimizer. </p>
<p> Interface implemented by optional tools which can be loaded and unloaded. In addition to the methods listed here, an OptionalTool must have a public no-arg constructor so that it can be instantiated by the DataDictionary. </p>
<p> Simple OptionalTool. </p>

An Order Entry order. <P> Fields map to definition in TPC-C for the ORDER table. The Java names of fields do not include the O_ prefix and are in lower case. For clarity these fields are renamed in Java <UL> <LI>w_id =&gt; warehouse (SQL column O_W_ID) <LI>d_id =&gt; district (SQL column O_D_ID) <LI>c_id =&gt; customer (SQL column O_C_ID) </UL> <BR> The columns that map to an address are extracted out as a Address object with the corresponding Java field address. <BR> All fields have Java bean setters and getters. <P> Primary key maps to {warehouse,district,id}. <P> An Order object may sparsely populated, when returned from a business transaction it is only guaranteed to contain  the information required to display the result of that transaction.
Tests for DERBY-4397 Allow {@code ORDER BY} in subqueries and       DERBY-4398 Allow {@code OFFSET/FETCH} in subqueries.
Tests for DERBY-3926. Optimizer is choosing to avoid sort which is causing the results to be returned in wrong order.
An OrderByColumn is a column in the ORDER BY clause.  An OrderByColumn can be ordered ascending or descending. We need to make sure that the named columns are columns in that query, and that positions are within range.
An OrderByList is an ordered list of columns in the ORDER BY clause. That is, the order of columns in this list is significant - the first column in the list is the most significant in the ordering, and the last column in the list is the least significant.
An OrderByNode represents a result set for a sort operation for an order by list.  It is expected to only be generated at the end of optimization, once we have determined that a sort is required.
An Order Entry order line item. <P> Fields map to definition in TPC-C for the ORDERLINE table. The Java names of fields do not include the OL_ prefix and are in lower case. <BR> All fields have Java bean setters and getters. <BR> Fields that are DECIMAL in the database map to String in Java (rather than BigDecimal) to allow running on J2ME/CDC/Foundation. <P> Primary key maps to {Order,id}, it is assumed that an OrderLine object exists in the context of an Order object, thus the columns {OL_O_ID, OL_D_ID, OL_W_ID}  are not represented in this class. <P> An OrderLine object may sparsely populated, when returned from a business transaction it is only guaranteed to contain  the information required to display the result of that transaction.
The Orderable interface represents a value that can be linearly ordered. <P> Currently only supports linear (&lt;, =, &lt;=) operations. Eventually we may want to do other types of orderings, in which case there would probably be a number of interfaces for each "class" of ordering. <P> The implementation must handle the comparison of null values.  This may require some changes to the interface, since (at least in some contexts) comparing a value with null should return unknown instead of true or false.
Abstract aggregator for Orderable aggregates (max/min).
An ordered column has position.   It is an abstract class for group by and order by columns.
List of OrderedColumns
OsName is used to store constants for the System Property os.name that can be passed to the BaseTestCase.isPlatform(String) method. Started this class with a few known values. TODO: Expand for all known os.names for platforms running Derby tests
A OverflowInputStream is used by store to turn a long column into an InputStream. <p> Any time store fetches a long column, the value is returned as a stream. A long column is any column that at some point was longer than a page, so a long column in one table may not be long in another depending on page size. <p> When the column is fetched a new OverflowInputStream is created and then the datatype's stream is set using: ((StreamStorable)sColumn).setStream(OverflowInputStream);

A bundle's authority to import or export a package. <p> A package is a dot-separated string that defines a fully qualified Java package. <p> For example: <pre> <code> org.osgi.service.http </code> </pre> <p> <code>PackagePermission</code> has two actions: <code>EXPORT</code> and <code>IMPORT</code>. The <code>EXPORT</code> action implies the <code>IMPORT</code> action. Stores a set of <code>PackagePermission</code> permissions.
A Page contains an ordered set of records which are the stored form of rows. A record is a stream of bytes created from a row array. The record contains one or more fields, fields have a one to one correlation with the DataValueDescriptor's contained within a row array. <P> A Page represents <B>exclusive</B> access to a data page within a container. Exclusive access is released by calling the unlatch() method, once that occurs the caller must no longer use the Page reference. <P> Several of the methods in Page take a RecordHandle as an argument. RecordHandles are obtained from a Page, while holding exclusive access of Page or a from a previous exclusive access of a Page representing the same data page. All RecordHandle's used as arguments to methods (with the exception of recordExists()) must be valid for the current state of the page. If they are not valid then the method will throw an exception. A caller can ensure that a record handle is valid by: <UL> <LI> Obtaining the handle during this exclusive access of this page <LI> Checking the record still exists with the method recordExists() <LI> Not using a handle after a deleteAtSlot(). </UL> <P> Several of the methods in Page take a slot number as an argument.  A slot always correspond to a record, which may be deleted or undeleted. <BR> MT - Latched - In general every method requires the page to be latched. <P> <B>Latching</B> <P> All page methods which are not valid for a latched page throw an exception if the page is not latched.  [@exception clauses on all the methods should be updated to reflect this]. <P> <B>Aux Objects</B> <BR> The page cache will manage a client object along with the page as long as it remains in cache.  This object is called the "aux object".  The aux object is associated with the page with setAuxObject(), and can be retreived later with getAuxObject().  The aux object will remain valid as long as the page is latched, but callers cannot assume that an aux object will ever stick around once the page is unlatched.  However, the page manager promises to call pageBeingEvicted() once before clearing the aux reference from the page.

A PageBasicOperation changed the content of a page, this is the root class of all page oriented operation. Each PageBasicOperation record change(s) that apply to <B>one and only one page</B>.  The pageID that is changed must be recorded in the log operation - in other words, redo must be physical (or more correctly, in Gray's term, physiological, since changes are logical <B>within</B> a page). <BR>Undo can be logical, but the undo logic must be hidden in generateUndo. By the time a compensation operation is logged as a LogOperation, the page that needs roll back must be determined. <PRE>
This class holds information that is passed to {@code CachedPage.createPage()} and used when a page object (either a {@code StoredPage} or an {@code AllocPage}) is created.
A key that identifies a BasePage. Used as the key for the caching mechanism. <BR> MT - Immutable :
The type definition of a time stamp that can be associated with pages that supports 'time stamp'. What a time stamp contains is up to the page.  It is expected that a time stamp implementation will collaborate with the page to implement a value equality. No method definition.  This is a type definition
A per page version number is one way to implement a page time stamp
This node type represents a ? parameter.
A ParameterValueSet is a set of parameter values that can be assembled by a JDBC driver and passed to a PreparedStatement all at once. The fact that they are all passed at once can reduce the communication overhead between client and server.
This exception is thrown when parse errors are encountered. You can explicitly create objects of this exception type by calling the method generateParseException in the generated parser. You can modify this class to customize your error reporting mechanisms so long as you retain the public fields.
The Parser interface is intended to work with Jack-generated parsers (now JavaCC). We will specify "STATIC=false" when building Jack parsers - this specifies that the generated classes will not be static, which will allow there to be more than one parser (this is necessary in a multi-threaded server). Non-static parsers do not have to be re-initialized every time they are used (unlike static parsers, for which one must call ReInit() between calls to the parser).

Unchecked exception class that can be used to pass checked exceptions out of methods that are not declared to throw any checked exception.
<p> This machine performs the hashing of Derby passwords. </p>
Helper methods to deal with paths in the in-memory "file system". <p> These methods are similar to those in {@code java.io.File}. <p> <em>Note</em>: The system has been hardcoded to use the separator specified by {@code java.io.File}.
utility class that prints out the time in a formatted way

This class describes rows in the SYS.SYSPERMS system table, which keeps track of the permissions that have been granted but not revoked.
<p> Helper class for testing sql authorization. </p>
This class implements a Cacheable for a DataDictionary cache of permissions.

This class is used by rows in the SYS.SYSTABLEPERMS, SYS.SYSCOLPERMS, and SYS.SYSROUTINEPERMS system tables.
A PersistentService modularises the access to persistent services, abstracting out details such as finding the list of services to be started at boot time, finding the service.properties file and creating and deleting the persistent storage for a service. <P> These modules must only be used by the monitor. <P> Possible examples of implementations are: <UL> <LI> Store services in a directory in the file system. <LI> Store services in a zip file <LI> Service data is provided by a web server <LI> Service data is stored on the class path. </UL> <P> This class also serves as the registry the defined name for all the implementations of PersistentService. These need to be kept track of as they can be used in JDBC URLS. <P> An implementation of PersistentService can implement ModuleSupportable but must not implement ModuleControl. This is because the monitor will not execute ModuleControl methods for a PersistentService.

Decorator that sets the phase of the upgrade process for a suite of upgrade tests.
An abstract class that is used for physical log operation.  A physical log operation is one where the undo of the operation must be applied to the same page as the original operation, and the undo operation must store the byte image of the row(s) changed to its before image.  (If a logical page operation happened to the page or if another transaction altered other rows on the page, the undo of this operation will only restore the before image of the row(s) affected). <PRE>
PhysicalUndoOperation is a compensation operation that rolls back the change of an Undo-able operation.  A PhysicalUndoOperation itself is not undo-able, i.e, it is loggable but not undoable. <PRE>
Tracks the most recently piggy-backed session attributes, and provides methods to determine if they have been modified and need to be re-sent to the client.
Class representing a PKGNAMCSN object (RDB Package Name, Consistency Token, and Section Number).
This class is the main entry point to the tool Graphical Query Explainer.
JMXConnectionGetter using the platform MBean server.
Load generator which generates Poisson distributed requests.
Driver to do the load phase for the Order Entry benchmark. This class takes in following arguments currently: Usage: java org.apache.derbyTesting.system.oe.run.Populate options Options: <OL> <LI>-scale warehouse scaling factor. Takes a short value. If not specified defaults to 1 <LI>-createConstraintsBeforeLoad create constraints before initial load of data, takes a boolean value. If not specified, defaults to true <LI>-doChecks check consistency of data, takes a boolean value. If not specified, defaults to true <LI>-loaderThreads Number of threads to populate tables, defaults to number of cores <LI>-help prints usage </OL> To load database with scale of 2, to load constraints after the population, and to not do any checks, the command to run the test is as follows: <BR> java org.apache.derbyTesting.system.oe.run.Populate -scale 2 -doChecks false -createConstraintsBeforeLoad false <BR> This class uses the junit performance framework in Derby and the tests the performance of the following operations. <OL> <LI> create schema with or without constraints (configurable) <LI> populate the schema <LI> Check the cardinality of the tables. </OL>
A wrapper-stream able to reposition the underlying store stream. <p> Where a user expects the underlying stream to be at a given position, {@link #reposition} must be called with the expected position first. A use case for this scenario is the LOB objects, where you can request a stream and at the same time (this does not mean concurrently) query the LOB about its length or ask to get a part of the LOB returned. Such multiplexed operations must result in consistent and valid data, and to achieve this the underlying store stream must be able to reposition itself. <em>Synchronization</em>: Access to instances of this class must be externally synchronized on the connection synchronization object. There are two reasons for this: <ul> <li>Access to store must be single threaded. <li>This class is not thread safe, and calling the various methods from different threads concurrently can result in inconsistent position values. To avoid redundant internal synchronization, this class assumes and <b>requires</b> external synchronization (also called client-side locking). </ul> @NotThreadSafe End class PositionedStoreStream
This interface describes a stream that is aware of its own position and can reposition itself on request. <p> This interface doesn't convey any information about how expensive it is for the stream to reposition itself.
A Predicate represents a top level predicate.
A PredicateList represents the list of top level predicates. Each top level predicate consists of an AndNode whose leftOperand is the top level predicate and whose rightOperand is true.  It extends QueryTreeNodeVector.
Test the dependency system for active statements when a DDL is executed in a separate connection after the prepare but before the execute.
The PreparedStatement interface provides methods to execute prepared statements, store them, and get metadata about them.
Methods implemented by the common Prepared Statement class to handle certain events that may originate from the material or common layers.  Reply implementations may update prepared statement state via this interface.  The event interfaces are undergoing change, new events will be added soon We want coarse grained event methods, rather than fine-grained events
Sample UDT for tests.
Get a header to prepend to a line of output. A HeaderPrintWriter requires an object which implements this interface to construct headers for output lines.

This node represents a set of privileges that are granted or revoked on one object.
This is a descriptor for schema object which can have privileges granted on it.
<p> Procedures used by the Scores application. </p> //////////////////////////////////////////////////////  MINIONS  //////////////////////////////////////////////////////


Holder class for Derby genus names. <P> A product genus defines a product's category (tools, DBMS etc). Currently, Derby only ships one jar file per genus. The info file defined in this file is used by sysinfo to determine version information. <P> A correct run time environment should include at most one Derby jar file of a given genus. This helps avoid situations in which the environment loads classes from more than one version. <P> Please note that the list provided here serves to document product genus names and to facilitate consistent naming in code. Because the list of supported Derby genus names may change with time, the code in this package does *NOT* restrict users to the product genus names listed here.

Class to hold a Derby Product version. This class includes the following product version features. <OL> <LI>Save the product version information this holds as a String. We call the string a 'product version string'. <LI>Construct a ProductVersionHolder from a valid 'product version string'. <LI>Determine if two product versions are feature compatible. This means products of these versions may interoperate with ***NO*** compatibility problems. <LI>Determine if two product versions are the same. This is a stronger test than the test for feature compatibility. </OL> Cloudscape 5.1 and older versions used the majorVersion, minorVersion, maintVersion versions directly. That is a three part version number, majorVersion.minorVersion.maintVersion, e.g. 5.1.21. For Cloudscape 5.2 onwards a four part name is required. majorVersion.minorVersion.fixPack.bugVersion e.g. 5.2.1.2 This follows the IBM standard and allows us to state that a fix pack will be 5.2.3 without worrying about how many maintence fixes there are between fix packs. We implement this using the existing format of ProductVersionHolder to reduce disruption to the code, however we make the maintVersion encode the {fixPack.bugVersion}. Since the maintVersion is represented by a int (2G values) we have plenty of room for encoding. If we assign a given majorVersion.minorVersion.fixPack a 10 year life, then we about the maximum number of individual releases it can have is 10 years * 365 days/year = 3650. Thus with the pre 5.2 scheme we would not expect a 5.1.x to have an x &gt; 3650 (approximately). Usually the rate of point releases has been much less than one per day, 5.1.31 is released about 225 days after GA which makes around a point release every 7 days. But in the encoding we need to be conservative. With fix packs the maximum is about 2 per year and fix packs are only made to the current release, thus with a yearly minor release cycle we would imagine only 2 fixpacks per major.minor. However like other IBM products or release cycle may be extended thus we can expect up to a handful of fix packs. Thus we might imagine releases like 5.2.0.12 5.2.0.234 5.2.1.34 5.2.4.2445 but highly unlikey to have 5.2.2.59321 5.2.23.1 The encoding number must continue to increase so that the encodedMaintB &gt; encodedMaintA if (fixPackB &gt; fixPackA) || ((fixPackB == fixPackA) &amp;&amp; (bugB &gt; bugA)) Selected encoding encodedMaint = (fixPack * 1,000,000) + (bugVersion); Handles many many fixpacks and upto one million bug fixes per fix pack and remains somewhat human readable. Special fix packs fixpack == 0 = alpha (version off main codeline) fixpack == 1 = first release of major.minor (may be marked with beta) fixpack == 2 = first fix pack (displayed as 1) The drdaMaintVersion is sent in the Network Server PRDID. It never displays but may be used by the client for version specific behaviour. It should be reset to 0 with each minor release. The product version string has the form: <PRE> productVendorName - ProductName - majorVersion.minorVersion.maintVersion [beta] - (buildNumber) </PRE>
A ProjectRestrictNode represents a result set for any of the basic DML operations: SELECT, INSERT, UPDATE, and DELETE.  For INSERT with a VALUES clause, restriction will be null. For both INSERT and UPDATE, the resultColumns in the selectList will contain the names of the columns being inserted into or updated. NOTE: A ProjectRestrictNode extends FromTable since it can exist in a FromList.
Takes a table and a table filter and returns the table's rows satisfying the filter as a result set.
List of all properties understood by the system. It also has some other static fields. <P> This class exists for two reasons <Ol> <LI> To act as the internal documentation for the properties. <LI> To remove the need to declare a java static field for the property name in the protocol/implementation class. This reduces the footprint as the string is final and thus can be included simply as a String constant pool entry. </OL> <P> This class should not be shipped with the product. <P> This class has no methods, all it contains are String's which by are public, static and final since they are declared in an interface.
Only used for exclusive lock purposes. Stores properties in a congolmerate with complete transactional support. <p> The PropertyConglomerate contains one row with 2 columns per property. Column 0 is the UTF key, and column 1 is the data. <p> <p> The property conglomerate manages the storage of database properties and thier defaults. Each property is stored as a row in the PropertyConglomerate <OL> <LI>Column 0 is the UTF key, <LI>Column 1 is the data. </OL> All the property defaults are stored in a single row of the Property Conglomerate: <OL> <LI>Column 0 is the UTF key "derby.defaultPropertyName". <LI>Column 1 is a FormatableProperties object with one row per default property. </OL> <p> In general a propery default defines it value if the property itself is not defined. <p> Because the properties conglomerate is stored in a conglomerate the information it contains is not available before the raw store runs recovery. To make a small number of properties (listed in servicePropertyList) available during early boot, this copies them to services.properties.
Module interface for an Property validation. <p> An PropertyFactory is typically obtained from the Monitor: <p> <blockquote><pre> // Get the current validation factory. PropertyFactory af; af = (PropertyFactory) Monitor.findServiceModule(this, org.apache.derby.iapi.reference.Module.PropertyFactory); </pre></blockquote>
<p> This VTI makes a table out of a property file. </p> /////////////////////////////////////////////////////////////////////////////////  ResultSet METHODS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////////////////
PropertyInfo is a class with static methods that  set  properties associated with a database. <P> This class can only be used within an SQL-J statement, a Java procedure or a server side Java method. <p>This class can be accessed using the class alias <code> PROPERTYINFO </code> in SQL-J statements.
This class defines the names of the properties to use when you extract the parts of a product version from a properties file.
<p> This is an ant Task which prompts the user for a property's value and sets it if the property hasn't been set already. </p>

There are 5 property objects within a JBMS system. 1) JVM - JVM set - those in System.getProperties 2) APP - Application set - derby.properties file 3) SRV - Persistent Service set - Those stored in service.properties 4) TRAN - Persistent Transactional set - Those stored via the AccessManager interface 5) BOOT - Set by a boot method (rare) This class has a set of static methods to find a property using a consistent search order from the above set. <BR> getSystem*() methods use the search order. <OL> <LI> JVM <LI> APP </OL> <BR> getService* methods use the search order <OL> <LI> JVM <LI> TRAN <LI> SRV <LI> APP </OL>

A class that represents a key for a module search.
A provider is an object that others can build dependencies on.  Providers can themselves also be dependents and thus be invalid/revalidated in turn. Revalidating a provider may, as a side-effect, re-validate its dependents -- it is up to the implementation to determine the appropriate action.
A ProviderInfo associates a DependableFinder with a UUID that stands for a database object.  For example, the tables used by a view have DependableFinders associated with them, and a ProviderInfo associates the tables' UUIDs with their DependableFinders.
ProviderList is a list of Providers that is being tracked for some object other than the current dependent.
Class that wraps StandardExceptions in a SQLException. This is used to make any public API methods always throw SQLException rather than a random collection.
A ByteArrayOutputStream which gives a direct reference of the buffer array

USE WITH EXTREME Caution: Purge records from a Page. Represents purging of a range of rows from the page. <PRE>

Support for pushing SQL statement information down into a virtual table. A read-write virtual tables (one that implements java.sql.PreparedStatement) implements this interface to support pushing information into the VTI. <BR> Read-only VTIs (those that implement java.sql.ResultSet) do not support the Pushable interface.
A qualified identifier made of a session name and a local name
<p> A structure which is used to "qualify" a column.  Specifies that the column value in a given column identified by column id is to be compared via a specific operator to a particular DataValueDescriptor value. <p> The implementation of this interface is provided by the client; the methods of Qualifier are the methods the access code uses to use it. <p> Arrays of qualifiers are provided to restrict the rows returned by scans.  A row is returned from a scan if all qualifications in the array return true. <p> A qualification returns true if in the following pseudo-code compare_result is true. <p> <blockquote><pre> if (qualifier.negateCompareResult()) { compare_result = row[(qualifier.getColumnId())].compare( qualifier.getOperator(), qualifier.getOrderable(), qualifier.getOrderedNulls(), qualifier.getUnknownRV()) if (qualifier.negateCompareResult()) { compare_result = !(compare_result); } } </blockquote></pre> <p> Qualifiers are often passed through interfaces as a set of Qualifiers, rather than one at a time, for example see the qualifier argument in TransactionController.openScan(). <p> To make this consistent the following protocols are to be used when passing around sets of Qualifiers. <p> A single dimensional array is to be used to pass around a set of AND'd qualifiers.  Thus qualifier[] argument is to be treated as: <blockquote><pre> qualifier[0] AND qualifer[1] ... AND qualifier[qualifer.length - 1] </blockquote></pre> <p> A two dimensional array is to be used to pass around a AND's and OR's in conjunctive normal form.  The top slot of the 2 dimensional array is optimized for the more frequent where no OR's are present.  The first array slot is always a list of AND's to be treated as described above for single dimensional AND qualifier arrays.  The subsequent slots are to be treated as AND'd arrays of OR's.  Thus the 2 dimensional array qual[][] argument is to be treated as the following, note if qual.length = 1 then only the first array is valid and it is and an array of AND clauses: <blockquote><pre> (qual[0][0] AND qual[0][0] ... AND qual[0][qual[0].length - 1]) AND (qual[1][0] OR  qual[1][1] ... OR  qual[1][qual[1].length - 1]) AND (qual[2][0] OR  qual[2][1] ... OR  qual[2][qual[2].length - 1]) ... AND (qual[qual.length - 1][0] OR  qual[1][1] ... OR  qual[1][2]) </blockquote></pre> <p> If any of the array's qual[0].length ... qual[qual.length -1] are 0 length they will be evaluated as TRUE; but they must be not NULL.  See Example 4 for encoding of (a or b) that takes advantage of this. <p> Note that any of the arrays qual[0].length ... qual[qual.length -1] may also be of length 1, thus no guarantee is made the presence of OR predicates if qual.length &lt; 1. See example 1a. <p> The following give pseudo-code examples of building Qualifier arrays: <p> Example 1: "a AND b AND c" <blockquote><pre> qualifier = new Qualifier[1][3]; // 3 AND clauses qualifier[0][0] = a qualifier[0][1] = b qualifier[0][2] = c </blockquote></pre> <p> Example 1a "a AND b AND c" - less efficient than example 1 but legal <blockquote><pre> qualifier = new Qualifier[3]; // 3 AND clauses qualifier[0] = new Qualifier[1]; qualifier[1] = new Qualifier[1]; qualifier[2] = new Qualifier[1]; qualifier[0][0] = a qualifier[1][0] = b qualifier[2][0] = c </blockquote></pre> <p> Example 2: "(f) AND (a OR b) AND (c OR d OR e)" Would be represented by an array that looks like the following: <blockquote><pre> qualifier = new Qualifier[3]; // 3 and clauses qualifier[0] = new Qualifier[1]; // to be intitialized to f qualifier[1] = new Qualifier[2]; // to be initialized to (a OR b) qualifier[2] = new Qualifier[3]; // to be initialized to (c OR d OR e) qualifier[0][0] = f qualifier[1][0] = a qualifier[1][1] = b qualifier[2][0] = c qualifier[2][1] = d qualifier[2][2] = e </blockquote></pre> <p> Example 3: "(a OR b) AND (c OR d) AND (e OR f)" <blockquote><pre> qualifier = new Qualifier[3]; // 3 and clauses qualifier = new Qualifier[4]; // 4 and clauses qualifier[0] = new Qualifier[1]; // to be intitialized to TRUE qualifier[1] = new Qualifier[2]; // to be initialized to (a OR b) qualifier[2] = new Qualifier[2]; // to be initialized to (c OR d) qualifier[3] = new Qualifier[2]; // to be initialized to (e OR f) qualifier[0][0] = TRUE qualifier[1][0] = a qualifier[1][1] = b qualifier[2][0] = c qualifier[2][1] = d qualifier[3][0] = e qualifier[3][1] = f </blockquote></pre> <p> Example 4: "(a OR b)" <blockquote><pre> qualifier = new Qualifier[2]; // 2 and clauses qualifier[0] = new Qualifier[0]; // 0 length array is TRUE qualifier[1] = new Qualifier[2]; // to be initialized to (a OR b) qualifier[1][0] = a qualifier[1][1] = b </blockquote></pre>

Class Query1: Returns a list of queries that Selects from a single view
Class Query2: Returns a list of queries that Selects from multiple views using joins
Class Query3: Returns a list of queries that Selects from multiple views with joins on columns having indexes
Class Query4: Returns a list of queries that Selects from multiple views with joins on columns having no indexes
Class Query5: Returns a list of queries that Selects from multiple views with joins on columns, one with index and one without index
Class Query6: Returns a list of queries that Selects from multiple views with combination of nested views and aggregate views

QueryTreeNode is the root class for all query tree nodes. All query tree nodes inherit from QueryTreeNode except for those that extend QueryTreeNodeVector.
QueryTreeNodeVector is the root class for all lists of query tree nodes. It provides a wrapper for java.util.ArrayList. All lists of query tree nodes inherit from QueryTreeNodeVector.
RAFContainer (short for RandomAccessFileContainer) is a concrete subclass of FileContainer for FileContainers which are implemented on java.io.RandomAccessFile.
RAFContainer4 overrides a few methods in FileContainer/RAFContainer in order to use FileChannel from Java 1.4's New IO framework to issue multiple IO operations to the same file concurrently instead of strictly serializing IO operations using a mutex on the container object. Since we compile with Java 1.4, the override "annotations" are inside the method javadoc headers. <p> Note that our requests for multiple concurrent IOs may be serialized further down in the IO stack - this is entirely up to the JVM and OS. However, at least in Linux on Sun's 1.4.2_09 JVM we see the desired behavior: The FileChannel.read/write(ByteBuffer buf, long position) calls map to pread/pwrite system calls, which enable efficient IO to the same file descriptor by multiple threads. <p> This whole class should be merged back into RAFContainer when Derby officially stops supporting Java 1.3. <p> Significant behavior changes from RAFContainer: <ol> <li> Multiple concurrent IOs permitted. <li> State changes to the container (create, open, close) can now happen while IO is in progress due to the lack of locking. Closing a container while IO is in progress will cause IOExceptions in the thread calling readPage or writePage. If this happens something is probably amiss anyway. The iosInProgress variable is used in an attempt to detect this should it happen while running a debug build. </ol>

A class that provides interface to be called when undo of an Insert happens in raw store. ************************************************************************ Public Methods of XXXX class: *************************************************************************
debugging

end of class RFResource
Do a merge run comparing all the foreign keys from the foreign key conglomerate against the referenced keys from the primary key conglomerate.  The scanControllers are passed in by the caller (caller controls locking on said conglomerates). <p> The comparision is done via a merge.  Consequently, it is imperative that the scans are on keyed conglomerates (indexes) and that the referencedKeyScan is a unique scan. <p> Performance is no worse than N + M where N is foreign key rows and M is primary key rows. <p> Bulk fetch is used to further speed performance.  The fetch size is LanguageProperties.BULK_FETCH_DEFAULT
Checks a set or referential integrity constraints.  Used to shield the caller from ReferencedKeyRIChecker and ForeignKeyRICheckers.
<p> Table function wrapping the result set meta data for a query. </p>
RawContainerHandle is the form of ContainerHandle that is used within the raw store.  This allows the raw store to have a handle on dropped container without exposing this to the external interface, which is not allowed to get back a handle on a dropped container
<p> OptionalTool to create wrapper functions and views for all of the user heap conglomerates in the seg0 subdirectory of a corrupt database. </p>

A Raw store that implements the RawStoreFactory module by delegating all the work to the lower modules TransactionFactory, LogFactory and DataFactory. <PRE> String TransactionFactoryId=<moduleIdentifier> </PRE> <P> Class is final as it has methods with privilege blocks and implements PrivilegedExceptionAction.
RawStoreFactory implements a single unit of transactional storage. A RawStoreFactory contains Segments and Segments contain Containers. <P> Segments are identified by integer identifiers that are unique within a RawStoreFactory. <P> Containers are also identified by unique integer identifiers within a RawStoreFactory, but will overlap with segment identifiers. <P><B>LIMITS</B><BR> This is a list of (hopefully) all limits within the raw store. Where a size has more than one limit all are documented (rather than just the most restrictive) so that the correct limit can be found if the most restictive is every removed. <UL> <LI>Field - <UL> <LI>Max length 2^31 - 1  (2147483647) - </UL> <LI>Record - <UL> <LI>Max number of fields 2^31 - 1  (2147483647) - from use of Object[] array to represent row, which can "only" have int sized number of array members. </UL> <LI>Page - <LI>Container - <LI>Segment - <LI>Raw Store - </UL> <P> Access and RawStore work together to provide the ACID properties of transactions. On a high level, RawStore deals with anything that directly impacts persistency. On a more detailed level, RawStore provides logging, rollback and recovery, data management on page, page allocation and deallocation, container allocation and deallocation. <P> RawStore is organized as 3 branches, transaction, data, and logging.  These branches each have its own "factory", the transaction factory hands out transactions, the data factory hands out containers, and the log factory hands out logger (or log buffers) for transactions to write on.  For a more detailed description on these factories, please see their corresponding javadocs. MT - Thread Safe
Stream that takes a raw input stream and converts it to the on-disk format of the binary types by prepending the length of the value. <P> If the length of the stream is known then it is encoded as the first bytes in the stream in the defined format. <BR> If the length is unknown then the first four bytes will be zero, indicating unknown length. <BR> Note: This stream cannot be re-used. Once end of file is reached, the next read call will throw an EOFException
RawTransaction is the form of Transaction used within the raw store. This allows the break down of RawStore functionality into (at least) three modules (Transactions, Data, Log) without exposing internal information on the external interface. <P> The transaction will notify any Observer's just before the transaction is committed, aborted or a rollback to savepoint occurs. The argument passed to the update() method of the Observer's will be one of <UL> <LI> RawTransaction.COMMIT - transaction is committing <LI> RawTransaction.ABORT - transaction is aborting <LI> RawTransaction.SAVEPOINTROLLBACK - transaction is being rolled back to a savepoint </UL> The observer's must perform a value equality check (equals()) on the update arg to see why it is being notified.
ReEncodedInputStream passes stream from Reader, which is stream of decoded style, to user of this subclass of InputStream, which is stream of encoded style. The encoding of stream passed to user is limited to UTF8. This class will be used to pass stream, which is served as a Reader, as a InputStream of a arbitrary encoding.
An RePreparable operation is an operation that changed the state of the RawStore in the context of a transaction and the locks for this change can be reclaimed during recovery, following redo.

A read-only version of the log factory. It doesn't do anything, it doesn't check that the database needs recovery or not. <P> It doesn't handle undo.  No recovery. <P>Multithreading considerations:<BR> This class must be MT-safe.
ReaderToAscii converts Reader (with characters) to a stream of ASCII characters.
Converts the characters served by a {@code java.io.Reader} to a stream returning the data in the on-disk modified UTF-8 encoded representation used by Derby. <p> Length validation is performed. If required and allowed by the target column type, truncation of blanks will also be performed. @NotThreadSafe
ResultSetStatistics implemenation for AnyResultSet.
ResultSetStatistics implemenation for BasicNoPutResultSetImpl.
ResultSetStatistics implemenation for CurrentOfResultSet.
ResultSetStatistics implemenation for DeleteCascadeResultSet.
ResultSetStatistics implemenation for DeleteResultSet.
ResultSetStatistics implemenation for DeleteVTIResultSet.
ResultSetStatistics implemenation for DistinctScalarAggregateResultSet.
ResultSetStatistics implemenation for DistinctScanResultSet.
ResultSetStatistics implemenation for GroupedAggregateResultSet.
ResultSetStatistics implemenation for HashJoinResultSet.
ResultSetStatistics implemenation for HashLeftOuterJoinResultSet.
ResultSetStatistics implemenation for HashScanResultSet.
ResultSetStatistics implemenation for HashTableResultSet.
ResultSetStatistics implemenation for IndexRowToBaseRowResultSet.
ResultSetStatistics implemenation for InsertResultSet.
ResultSetStatistics implemenation for InsertVTIResultSet.
ResultSetStatistics implemenation for JoinResultSet.
ResultSetStatistics implemenation for RealLastIndexKeyScanResultSet.
ResultSetStatistics implemenation for MaterializedResultSet.
ResultSetStatistics implemenation for NestedLoopJoinResultSet.
ResultSetStatistics implemenation for NestedLoopLeftOuterJoinResultSet.
ResultSetStatistics implemenation for NoPutResultSetImpl.
ResultSetStatistics implemenation for NoPutResultSetImpl.
ResultSetStatistics implemenation for NormalizeResultSet.
ResultSetStatistics implemenation for OnceResultSet.
ResultSetStatistics implemenation for ProjectRestrictResultSet.
ResultSetStatisticsFactory provides a wrapper around all of objects associated with run time statistics. <p> This implementation of the protocol is for returning the "real" run time statistics.  We have modularized this so that we could have an implementation that just returns null for each of the objects should we decided to provide a configuration without the run time statistics feature.

ResultSetStatistics implemenation for RowResultSet.
ResultSetStatistics implemenation for ScalarAggregateResultSet.
ResultSetStatistics implemenation for ScrollInsensitiveResultSet.
ResultSetStatistics implementation for SetOpResultSet.
ResultSetStatistics implemenation for SortResultSet.
ResultSetStatistics implemenation for TableScanResultSet.
ResultSetStatistics implemenation for UnionResultSet.
ResultSetStatistics implemenation for UpdateResultSet.
ResultSetStatistics implemenation for VTIResultSet.
ResultSetStatistics implementation for WindowResultSet.
Post commit work to reclaim some space from the raw store.  This is a wrapper class for the real serviceable class who wraps this on top of itself so different things can be identified.
This class helps a BaseDataFactory reclaims unused space. Space needs to be reclaimed in the following cases: <BR><NL> <LI> Row with long columns or overflow row pieces is deleted <LI> Insertion of a row that has long columns or overflows to other row pieces is rolled back <LI> Row is updated and the head row or some row pieces shrunk <LI> Row is updated and some long columns are orphaned because they are updated <LI> Row is updated and some long columns are created but the update rolled back <LI> Row is updated and some new row pieces are created but the update rolled back </NL> <P> We can implement a lot of optimization if we know that btree does not overflow. However, since that is not the case and Raw Store cannot tell if it is dealing with a btree page or a heap page, they all have to be treated gingerly.  E.g., in heap page, once a head row is deleted (via a delete operation or via a rollback of insert), all the long rows and long columns can be reclaimed - in fact, most of the head row can be removed and reclaimed, only a row stub needs to remain for locking purposes.  But in the btree, a deleted row still needs to contain the key values so it cannot be cleaned up until the row is purged. <P><B> Row with long columns or long row is deleted </B><BR> When Access purge a committed deleted row, the purge operation will see if the row has overflowed row pieces or if it has long columns.  If it has, then all the long columns and row pieces are purged before the head row piece can be purged.  When a row is purged from an overflow page and it is the only row on the page, then the page is deallocated in the same transaction. Note that non-overflow pages are removed by Access but overflow pages are removed by Raw Store.  Note that page removal is done in the same transaction and not post commit.  This is, in general, dangerous because if the transaction does not commit for a long time, uncommit deallocated page slows down page allocation for this container.  However, we know that access only purges committed delete row in access post commit processing so we know the transaction will tend to commit relatively fast.  The alternative is to queue up a post commit ReclaimSpace.PAGE to reclaim the page after the purge commits.  In order to do that, the time stamp of the page must also be remembered because post commit work may be queued more than once, but in this case, it can only be done once. Also, doing the page deallocation post commit adds to the overall cost and tends to fill up the post commit queue. <BR> This approach is simple but has the drawback that the entire long row and all the long columns are logged in the purge operation.  The alternative is more complicated, we can remember all the long columns on the head row piece and where the row chain starts and clean them up during post commit.  During post commit, because the head row piece is already purged, there is no need to log the long column or the long rows, just wipe the page or just reuse the page if that is the only thing on the page.  The problem with this approach is that we need to make sure the purging of the head row does indeed commit (the transaction may commit but the purging may be rolled back due to savepoint). So, we need to find the head row in the post commit and only when we cannot find it can we be sure that the purge is committed.  However, in cases where the page can reuse its record Id (namely in btree), a new row may reuse the same recordId.  In that case, the post commit can purge the long columns or the rest of the row piece only if the head piece no longer points to it.  Because of the complexity of this latter approach, the first simple approach is used. However, if the performance due to extra logging becomes unbearble, we can consider implementing the second approach. <P><B> Insertion of a row with long column or long row is rolled back. </B><BR> Insertion can be rolled back with either delete or purge.  If the row is rolled back with purge, then all the overflow columns pieces and row pieces are also rolled back with purge.  When a row is purged from an overflow page and it is the only row on the page, then a post commit ReclaimSpace.PAGE work is queued by Raw Store to reclaim that page.<BR> If the row is rolled back with delete, then all the overflow columns pieces and row pieces are also rolled back with delete.  Access will purge the deleted row in due time, see above. <P><B> Row is updated and the head row or some row pieces shrunk </B><BR> Every page that an update operation touches will see if the record on that page has any reserve space.  It it does, and if the reserve space plus the record size exceed the mininum record size, then a post commit ROW_RESERVE work will be queued to reclaim all unnecessary row reserved space for the entire row. <P><B> Row is updated and old long columns are orphaned </B><BR> The ground rule is, whether a column is a long column or not before an update has nothing to do with whether a column will be a long column or not after the update.  In other words, update can turn a non-long column into a long column, or it can turn a long column into a non-long column, or a long column can be updated to another long column and a non-long column can be updated to a non-long column.  The last case - update of a non-long column to another non-long column - is only of concern if it shrinks the row piece it is on (see above).<BR> So update can be looked at as 2 separate problems: A) a column is a long column before the update and the update will "orphaned" it.  B) a column is a long column after the update and the rollback of the update will "orphaned" it if it is rolled back with a delete.  This section deals with problem A, next section deals with problem B.<BR> Update specifies a set of columns to be updated.  If a row piece contains one or more columns to be updated, those columns are examined to see if they are actually long column chains.  If they are, then after the update, those long column chains will be orphaned.  So before the update happens, a post commit ReclaimSpace.COLUMN_CHAIN work is queued which contains the head rows id, the column number, the location of the first piece of the column chain, and the time stamp of the first page of the column chain. <BR> If the update transaction commits, the post commit work will walk the row until it finds the column number (note that it may not be on the page where the update happened because of subsequent row splitting), and if it doesn't point to the head of the column chain, we know the update operation has indeed committed (versus rolled back by a savepoint).  If a piece of the the column chain takes up an entire page, then the entire page can be reclaimed without first purging the row because the column chain is already orphaned.<BR> We need to page time stamp of the first page of the column chain because if the post commit ReclaimSpace.COLUMN_CHAIN is queued more than once, as can happen in repeated rollback to savepoint, then after the first time the column is reclaimed, the pages in the column chain can be reused.  Therefore, we cannot reclaim the column chain again.  Since there is no back pointer from the column chain to the head row, we need the timestamp to tell us if that column chain has already been touched (reclaimed) or not. <P><B> Row is updated with new long columns and update is rolled back. </B><BR> When the update is rolled back, the new long columns, which got there by insertion, got rolled back either by delete or by purge.  If they were rolled back with delete, then they will be orphaned and need to be cleaned up with post abort work.  Therefore, insertion of long columns due to update must be rolled back with purge.<BR> This is safe because the moment the rollback of the head row piece happens, the new long column is orphaned anyway and nobody will be able to get to it.  Since we don't attempt to share long column pages, we know that nobody else could be on the page and it is safe to deallocate the page. <P><B> Row is updated with new long row piece and update is rolled back. </B><BR> When the update is rolled back, the new long row piece, which got there by insertion, got rolled back either by delete or by purge.  Like update with new long row, they should be rolled back with purge.  However, there is a problem in that the insert log record does not contain the head row handle.  It is possible that another long row emanating from the same head page overflows to this page.  That row may since have been deleted and is now in the middle of a purge, but the purge has not commit.  To the code that is rolling back the insert (caused by the update that split off a new row piece) the overflow page looks empty.  If it went ahead and deallocate the page, then the transaction which purged the row piece on this page won't be able to roll back.  For this reason, the rollback to insert of a long row piece due to update must be rolled back with delete.  Furthermore, there is no easy way to lodge a post termination work to reclaim this deleted row piece so it will be lost forever. <BR> RESOLVE: need to log the head row's handle in the insert log record, i.e., any insert due to update of long row or column piece should have the head row's handle on it so that when the insert is rolled back with purge, and there is no more row on the page, it can file a post commit to reclaim the page safely. The post commit reclaim page needs to lock the head row and latch the head page to make sure the entire row chain is stable. <P><B>
A handle to a record within a container. This interface does not provide an information about the data of the record, it is only used to perform updates, deletes and allow ordered record manipulation. MT - immutable
Implementation of RecordHandle. <BR> MT - Mutable - Immutable identity : Thread Aware - <BR>The methods of RecordHandle only access the identity of the object and so the object appears immutable to them, as required. <BR>The methods of Lockable  are single thread required.
This test contains a recovery for a database that did recovery just before it went down. After recovery more records are inserted into the database before the database is shutdown.  Then, roll-forward recovery of the database from the backup is performed.  It is then checked that the records inserted after the first recovery is still present.  This test was made to recreate the problem in DERBY-298. The test should be run after store/RecoveryAfterBackupSetup.java.
This class will do the setup for testing recovery after backup. This test will insert some records into a table, do a backup and end without shutting down the database.  The succeeding test, RecoveryAfterBackup, will then do recovery of the database.

This class implements TypeCompiler for the SQL REF datatype.
The data source factory for Derby embedded driver data sources. <p> This factory reconstructs a Derby data source object when it is retrieved from JNDI. References are needed since many naming services don't have the ability to store Java objects in their serialized form. When a data source object is bound in this type of naming service the {@link javax.naming.Reference} for that object is actually stored by the JNDI implementation, not the data source object itself. <p> A JNDI administrator is responsible for making sure that both the object factory and data source implementation classes provided by a JDBC driver vendor are accessible to the JNDI service provider at runtime. <p> An object factory implements the {@link javax.naming.spi.ObjectFactory} interface. This interface contains a single method, {@code getObjectInstance} which is called by a JNDI service provider to reconstruct an object when that object is retrieved from JNDI. A JDBC driver vendor should provide an object factory as part of their JDBC product.
Provides information about the columns that are referenced by a CHECK CONSTRAINT definition. It is used in the column SYS.SYSCHECKS.REFERENCEDCOLUMNSDESCRIPTOR.
For triggers, ReferencedColumnsDescriptorImpl object has 3 possibilites 1)referencedColumns is not null but referencedColumnsInTriggerAction is null - meaning the trigger is defined on specific columns but trigger action does not reference any column through old/new transient variables. Another reason for referencedColumnsInTriggerAction to be null(even though trigger action does reference columns though old/new transient variables would be that we are in soft-upgrade mode for pre-10.7 databases and hence we do not want to write anything about referencedColumnsInTriggerAction for backward compatibility (DERBY-1482). eg create trigger tr1 after update of c1 on t1 for each row values(1); 2)referencedColumns is null but referencedColumnsInTriggerAction is not null - meaning the trigger is not defined on specific columns but trigger action references column through old/new transient variables eg create trigger tr1 after update on t1 referencing old as oldt for each row values(oldt.id); 3)referencedColumns and referencedColumnsInTriggerAction are not null - meaning the trigger is defined on specific columns and trigger action references column through old/new transient variables eg create trigger tr1 after update of c1 on t1 referencing old as oldt for each row values(oldt.id);
A ReferencedConstraintDeescriptor is a primary key or a unique key that is referenced by a foreign key.
A Referential Integrity checker for a change to a referenced key (primary or unique).   Makes sure that all the referenced key row is not referenced by any of its foreign keys.  see ForeignKeyRIChecker for the code that validates changes to foreign keys.
Build a JBitSet of all of the referenced tables in the tree.
Reflect loader with Privileged block for Java 2 security.




Registration of TypedFormat classes. <P> A TypedFormat is registered by placing a class name at the correct place in the correct array, driven by the base format number: <UL> <LI>2 byte - MIN_TWO_BYTE_FORMAT_ID - TwoByte </UL> The offset from the base format number (0 based) gives the offset in the array. <P> The class name is either: <UL> <LI> The actual class name of the TypeFormat. <LI> The name of a class that extends org.apache.derby.iapi.services.io.FormatableInstanceGetter. In this case the monitor will register an instance of the class after calling its setFormatId() method with format id it is registered as. </UL>
This interface is an abstraction of a relational operator.  It was created for optimization, to allow binary comparison operators and IS NULL to be treated the same.
<p> This tool reads a release note from a stream. When run standalone, this is a file stream. When run from the ReleaseNoteGenerator, the stream is opened on the URL of a release note stored in JIRA. The purpose of this class it to help people verify that their release notes can be digested by the ReleaseNoteGenerator. </p>
<p> This tool generates Release Notes for a Derby release. See the USAGE constant for details on how to run this tool standalone. It is recommended that you freshly regenerate your BUG_LIST just before you run this tool. </p> <p> The tool is designed to be run from Derby's ant build scripts. The build script will integrate the various steps of generating the release notes into a single ant target. This includes generating the issue list by querying the Apache JIRA instance. For this reason, the properties below must be specified when invoking the ant target. You can specify them in <tt>ant.properties</tt>, or on the command line.<br/> To run under ant, do the following: </p> <ul> <li>Make sure the Maven 2 executable is in your path.</li> <li>Fill in information in <tt>releaseSummary.xml</tt>.<br/> See <tt>tools/release/templates/releaseSummaryTemplate.xml</tt> for details.</li> <li>Define <tt>jira.user</tt>.<br/> This variable is your JIRA user name.</li> <li>Define <tt>jira.password</tt>.<br/> This variable is your JIRA password.</li> <li>Define <tt>jira.filter.id</tt>.<br/> This variable holds the id for the manually created JIRA filter that will select the issues addressed by the release. The id consists of digits only.</li> <li>Define <tt>release.version</tt>.<br/> The version of the release, i.e. "10.7.1.0".</li> <li>Define <tt>relnotes.src.reports</tt>.<br/> This variable points at the directory which holds the list of JIRA issues addressed by the release. The file, called <tt>fixedBugsList.txt</tt>, will be generated when you invoke the ant target.</li> <li>cd into <tt>tools/release</tt> and run ant thusly: <tt>ant [properties] genrelnotes</tt></li> </ul> Running the ant target successfully requires a working Internet connection to the Apache JIRA instance, as well as a valid JIRA username/password and the id of an existing JIRA filter. </p> <p>For more information on this tool, please see the JIRAs which introduced it: <ul> <li><a href="http://issues.apache.org/jira/browse/DERBY-4857">DERBY-4857</a></li> <li><a href="http://issues.apache.org/jira/browse/DERBY-2570">DERBY-2570</a></li> </ul> </p>
<p> This is an ant task which transforms the Derby release notes into a form which can be digested by the Forrest tool and published on the Derby download site. This involves the following transformations: </p> <ul> <li><b>Remove blockquotes</b> - Forrest silently swallows blockquoted text.</li> <li><b>Remove TOC</b> - Forrest adds its own table of contents and transforms the original TOC into a block of dead links.</li> <li><b>Remove mini TOC</b> - Forrest also transforms the mini TOC in the Issues section into a block of dead links.</li> </ul> <p> In addition, this task adds a pointer to the download page to src/documentation/conf/cli.xconf. This causes the site-building scripts to pull the download page into the build. </p>
<p> This ant task creates the release properties needed to define the release id when building the Derby distributions. For a description of the Derby release id, see http://db.apache.org/derby/papers/versionupgrade.html </p> <p> This task also sets a property for use by downstream targets during the release-build: </p> <ul> <li><b>derby.release.id.new</b> - The new id for the branch, in case we were asked to bump the release id.</li> </ul>
A repository for Derby releases. <p> The releases are used by tests, for instance by the upgrade and compatibility tests, to verify characteristics and behavior across Derby releases. <p> This particular repository is rather dumb - it is up to the user to keep the repository content updated. The repository layout is based on the layout of the SVN repository for releases at {@code https://svn.apache.org/repos/asf/db/derby/jars}. This means there will be a directory for each release, where the directory name is the release version. Inside this directory, all the distribution JARs can be found. <p> The repository location defaults to {@code $HOME/.derbyTestingReleases} on UNIX-like systems, and to {@code %UserProfile%\.derbyTestingReleases} on Windows (in Java, both of these maps to the system property 'user.home'). The location can be overridden by specifying the system property {@code derbyTesting.oldReleasePath}. <p> If the default location doesn't exist, and the system property {@code derbyTesting.oldReleasePath} is unspecified, it is up to the tests using the release repository to decide if this condition fails the test or not. If the system property is set to a non-existing directory an exception will be thrown when instantiating the repository. <p> The repository is lazily initialized, as there's no reason to incur the initialization cost when running tests that don't require the repository. The disk is inspected only when the repository is instantiated, any updates to the on-disk repository after the repository has been instantiated will not take effect. <p> <em>Implementation note</em>: This code should be runnable with J2ME, which means that it needs to be compatible with J2SE 1.4 for the time being.
Remap/unremap the CRs to the underlying expression. //////////////////////////////////////////////  CLASS INTERFACE  //////////////////////////////////////////////
A FilterInputStream that remembers read or skipped bytes. <P>In record mode this stream remembers all the bytes a caller reads or skips. After reading some bytes this returns a 'replay' stream to re-read them. <P>A caller may call getReplaySteam to get a stream to re-read the the remembered bytes. Any number of calls to getReplayStream are supported. <P>The clear function causes this stream to forget the remembered bytes and re-enter record mode.
JMXConnectionGetter using a JMXServiceURL, currently with no authentication and not using SSL.


This class  describes actions that are ALWAYS performed for a RENAME TABLE/COLUMN/INDEX Statement at Execution time.
A RenameNode is the root of a QueryTree that represents a RENAME TABLE/COLUMN/INDEX statement.
Replace all aggregates with result columns.
Replace all occurrences of a specific node with another node.
Replace all window function calls with result columns.
Interface that must be implemented by classes that provide a replacement algorithm for <code>ConcurrentCache</code>.
Used for the replication master role only. When a Derby instance has the replication master role for a database 'x', all log records that are written to the local log file are also appended to this log buffer. The replication master service will consume chunks of log from this buffer and send it to the Derby instance with the slave role for 'x'. ReplicationLogBuffer consists of a number of LogBufferElements. Elements that are not in use are in the freeBuffers list, while elements that contains dirty log are in dirtyBuffers. Chunks of log records are appended to the buffer element in currentDirtyBuffer. Hence, the life cycle of buffer elements is: freeBuffers -&gt; currentDirtyBuffer -&gt; dirtyBuffers -&gt; freeBuffers To append chunks of log records to the buffer, use appendLog(...) To consume chunks of log records, use next() followed by getData(), getLastInstant() and getSize(). These get-methods throw NoSuchElementException if next() returned false, meaning that there were no dirty log at the time next() was called. Threads: ReplicationLogBuffer is threadsafe. It can be used by a logger (LogToFile) and a log consumer (LogShipping service) concurrently without further synchronization. Important: If methods in this class calls methods outside this package (e.g. MasterFactory#workToDo), make sure that deadlocks are not introduced. If possible, a call to any method in another package should be done without holding latches in this class.
<p> Scan a chunk of log received from the master. The log chunk (byte[] logToScan) is assumed to always contain (an unknown) number of complete log records. If the last log record is incomplete, something is wrong. This will raise a StandardException, and will probably mean that replication has to be aborted. That decision is not made here, though. </p> <p> When a new chunk of log records is to be scanned, ReplicationLogScan is initialized by calling init(...). Every time next() is called after that, ReplicationLogScan reads a new log record from logToScan. If next() returns true, the next log record was successfully read. The information in this last read log record either indicates that a log file switch has taken place on the master (isLogSwitch() = true) or it is a normal log record which information can be retrieved by using the get-methods (if isLogRecord() = true). </p> <p> Threads: The class does not provide thread synchronization since it is assumed that only one thread will be receiving and applying log per replicated database when in slave mode. Since a ReplicationLogScan object belongs to only one slave database, this means that only one thread will access the object. </p> <p> The format of the log chunk byte[] is defined in org.apache.derby.impl.store.raw.log.LogAccessFile </p>

This message is used for the communication between the master and the slave during Replication. The message is composed of a type flag and corresponding object. Each type flag indicating the type of the content bundled inside the message.
This class is the Receiver (viz. Socket server or listener) part of the network communication. It receives the message from the master and performs appropriate action depending on the type of the message.
Used to send replication messages to the slave. Called by the Master controller to transmit replication messages wrapped in a <code>ReplicationMessage</code> object to a receiver. The receiver is implemented by the <code>ReplicationMessageReceive</code> class.
Framework to run replication tests. Subclass to create specific tests as in ReplicationRun_Local and ReplicationRun_Distributed.
This is NOT a test but an attempt to make sure replication master and slave servers are really gone when ReplicationSuite is done.
Run a replication test in a distributed environment, where master and slave hosts, and master and slave ports are specified in a property file. Which test to run is also specified in the property file.

Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports.
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports.
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports. Verify that indexes are replicated.
Defining startSlave, stopmaster, stopSlave and failOver methods returning SQLException for negative testing (ReplicationRun_Local_3 set of tests).
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports. Test that - stopSlave is not accepted on replicating slave, - stopSlave is not accepted on replicating master, - stopMaster is accepted on replicating master, - stopSlave is not accepted on non-replicating slave host, - failOver is not accepted on non-replicating master host.
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports.
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports. Verify that a second failover is not accepted.
Verify that "internal_stopslave=true" is NOT accepted as user supplied connection attr.
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports. Test DERBY-3924 verifying the fix for DERBY-3878. private void _killServer(String masterServerHost, int masterServerPort) throws InterruptedException, IOException { // This will work for "Unix" only!! util.DEBUG("_killServer: " + masterServerHost +":" + masterServerPort); String PID = runUserCommand(getDerbyServerPID + " " + masterServerPort,testUser); runUserCommand("kill " + PID,testUser); } private void mk_getDerbyServerPID_Cmd(String cmdName) throws IOException { String cmd = "#!/bin/bash" + LF + "PORTNO=$1" + LF + "if [ \"${PORTNO}\" == \"\" ]" + LF + "then" + LF + "  echo UNDEFINED_PORT_NUMBER" + LF + "  PORTNO=UNDEFINED_PORT_NUMBER" + LF + "fi" + LF + "ps auxwww" + "| grep \"org.apache.derby.drda.NetworkServerControl " + "start -h 0.0.0.0 -p ${PORTNO}\" | grep -v grep " + "| gawk '{ print $2 }'"; util.writeToFile(cmd, cmdName); runUserCommand("chmod +x " + cmdName,testUser); }
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports. Test for DERBY-3896.
Test that the startSlave command doesn't fail if it takes more than a second before the master attempts to connect to the slave. Regression test case for DERBY-4910.
Testing replication of encrypted databases. Required DERBY-3890.
Run a replication test on localhost by using default values for master and slave hosts, and master and slave ports. This test is intended to be run separatly showing state change during a "normal" replication session.
Suite to run all JUnit tests in this package: org.apache.derbyTesting.functionTests.tests.replicationTests


This interface provides a representation of the required ordering of rows from a ResultSet.  Different operations can require ordering: ORDER BY, DISTINCT, GROUP BY.  Some operations, like ORDER BY, require that the columns be ordered a particular way, while others, like DISTINCT and GROUP BY, reuire only that there be no duplicates in the result.
This is a simple interface that is used by streams that can initialize and reset themselves. The purpose is for the implementation of BLOB/CLOB. It defines a methods that can be used to initialize and reset a stream.
The resource adapter is the clearing house for managing connections, transactions, and XAResources in a JDBC based resource manager living in the distributed transaction processing environment. <P> There is one instance of ResourceAdapter per Resource Manager (database). The ResourceAdapter is responsible for keeping track of all run time global transactions and their state.   The resource adapter only knows of run time global transactions, i.e., it does not know of in-doubt global transactions re-created by recovery. <P>	The following is an overall design of the JTA implementation in Derby, most of it has little to do with the ResourceAdapter interface itself. <P><B>Design Overview </B> <P>The overriding design principle is that existing code should be disturbed as little as possible.  This is so that DTP code will not add to the bloat and drag of a normal, local, embbeded system.  The second design principle is that as much of the JDBC 2.0 extension functionality is to be implemented in the Connectivity layer and not in the underlying storage system as possible.  Ideally, the additional storage interface will implement no more than what is necessary to support the XAResource interface. <P>Language and replication code should not be touched, or have very minimal API changes.  The API changes are confined to passing XA calls down to the store. <P>Some change will be made to existing Connectivity code and new XA modules will be added.  This collection of code is hereby referred to as the "blob of mysterious connectivity code", or the "resource adapter", or "RA" for short.  In the JTA doc, the resource adapter is considered to be part of the JDBC driver.  This RA means "some connectivity code", it doesn't mean the object that implements the ResourceAdapter interface. <P>The most important difference, in terms of implementation, between a Connection that deals with a local transaction and a Connection that deals with a global transaction is that in a global transaction, 2 or more objects and threads can influence it - maybe concurrently.  The normal JDBC interaction goes thru the Connection, but transaction demarcation comes from an XAResource object(s).  The RA will channel all XAResource calls that deal with a run time XA transaction (i.e., commit, end, forget, prepare, start) thru the TransactionController that represents the real transaction underneath.   Furthermore, the RA will make sure that all calls thru a Connection or thru any XAResource objects must pass thru some sort of synchronized object before it can get to the underlying transaction object.  This is so that there is only one path to change the state of a run time transaction and the transaction object and the context manager can remain single thread access. <P>In-doubt transaction (i.e., transactions re-created by recovery) management and concurrency control is the responsibiliy of store. Moreover, since the RA does not know the identities of the list of in-doubt transactions, store must deal with (throw exception) when someone wants to start a transaction with the same Xid as an existing in-doubt transaction. <P>In terms of what this means to the app server that is calling us: if the Connection and the XAResource that represents a global transaction is being accessed by 2 different threads, they will access the database serially and not concurrently. An in-doubt transaction gotten thru recovery has no transaction object that is ever visible to the RA - because there is no connection that was ever made to it.  Therefore it is safe to influence the state of an in-doubt transaction directly thru some store factory interface - and have that go thru the transaction table underneath to find the actual transaction object and context manager etc. <P>One new functionality of a Connection is the ability to switch around with different transactions.  Before JTA, the lifetime of a transaction is bounded by a connection, and a transaction cannot migrate from one connection to another.  In JTA, a global transaction can be detached from a Connection.  A transaction can move around and be attached to different connections and its lifetime is not confine to the connection that started it.  From the Connection's point of view, before JTA, a (local) transaction is always started and ended in the same connection. With JTA, it needs to "take on" existing global transactions that was started by some other connections. <P>The RA will have the responsibility of <OL> <LI>setting up a Context with the appropriate transaction before calling store to do work.</LI> <LI>handling error on the context.</LI> <LI>restoring a previous context if it was switched out due to an XAResouce call to commit a transaction that is not what the XAResoruce is currently attached to. </LI> </OL> <P>Because of all these switching around, a Connection may be in a transaction-less state.  This happens between an XAResource.end call that detached the current global transaction from the Connection, and the next XAResource.start call that attach the next global transaction with the Connection. <BR>An (inferior) implementation is for the Connection object to start a local connection once it is detached from a global transaction.  If the user then uses the Connection immediately without a XAResource.start call, then this Connection behaves just like it did before JTA, i.e., with a local transaction.  If, on the other hand, an XAResource.start call happens next, then either the local transaction is "morphed" into a global transaction, or, if the start call is to attach the connection to a pre-existing global transaction, then the local transaction is thrown away and the Connection will take on the pre-exising global transaction. <BR>Another (superior) implementation is to make it possible for a Connection to be transaction-less.  When a Connection is first created by XAConnection.getConnection, or when a XAResource.end call detached a global transaction from the Connection, it is left in a transaction-less state. If a XAResource.start call happens next, then the Connection either start a new global transaction or it takes on an existing one.  If a call is made directly on the Connection before XAResource.start call happens, then the Connection starts a new local transaction.  This only affects Connections that was gotten thru the XAConnection.getConnection().  Connections gotten thru the DriverManager or a DataSource will have a local transaction automatically started, as is the behavior today.  When a Connection with a local transaction commits, the transaction is still around but it is chain to the next one - this is the current behavior.  This behavior is very desirable from a performance point of view, so it should be retained. However, a local transaction cannot "morph" into a global transaction, therefore when this Connection is attached to a global transaction, the local transaction is thrown away and a global one started <P>The RA will need to keep track of all global transactions.  This is done by (yet another) transaction table that lives in the RA.  This transaction table maps Xid to the ContextManager of the global transaction and whatever else a connection needs to talk to the transaction - I assume the Connection object currently have tendrils into various contexts and objects and these are things that need to be detached and attached when a Connection is hooked up with another transaction.  The reason for yet another transaction table instead of the one in store is because the one in store keeps track of local and internal transactions and is really quite overworked already. <P><B>Detailed design</B> <BR> First some ugly pictures.  Some links are not shown to reduce clutter.  Externally visible object is in <B>bold</B>. <P><PRE> When user ask for an XAConnection via a XADataSource, the following objects exists <BR> |-------------| |======= produces=&gt;| <B>XAResource</B>  | ||                 |-------------| ||                       | ||                     has A ||                       | ||  |--------------------- ||  V |--------------| produces |--------------| | <B>XADataSource</B> |=========&gt;| <B>XAConnection</B> |--------------|          |--------------| |                          | extends                    extends |                          | |                |-----------------------|   |----------------------| |                | DB2jPooledConnection |==&gt;| BrokeredConnection | |                |-----------------------|   |----------------------| |                          |       ^                  | |                        has A     |               has A |                          |       |                  | |-----------------|              |       -------------------- | EmbeddedDataSource |              | |-----------------|              | |                          | has A                        | |                          | V                          V |------------|           |----------------------|   |-----------------------| | JDBCDriver |=produces=&gt;| DetachableConnection |==&gt;| XATransactionResource | | LocalDriver|           |----------------------|   |                       | |------------|                   |                  |   points to :         | |                  |XATransactionController| |                  | ContextManager        | |                  | LCC                   | |                  | .. etc ..             | |                  |-----------------------| |                            | extends                     extends |                            | |-----------------|       |-----------------------| | EmbedConnection |-- ?--&gt;|  TransactionResource  | |-----------------|       |-----------------------| <BR><BR> When user ask for a PooledConnection via a PooledDataSource, the following objects exists <BR> |-------------------------------| | <B>EmbeddedConnectionPoolDataSource</B> | |-------------------------------| |                  || |                  || extends             produces |                  || |                  \/ |                |-----------------------|   |----------------------| |                | <B>DB2jPooledConnection</B> |==&gt;| <B>BrokeredConnection</B> | |                |-----------------------|   |----------------------| |                          |       ^                  | |                        has A     |               has A |                          |       |                  | |-----------------|              |       -------------------- | EmbeddedDataSource |              | |-----------------|              | |                          | has A                        | |                          | V                          V |------------|           |----------------------|   |-----------------------| | JDBCDriver |=produces=&gt;| EmbedConnection |==&gt;|  TransactionResource  | | LocalDriver|           |----------------------|   |-----------------------| |------------| <BR><BR> When user ask for a (normal) Connection via a DataSource, the following objects exists. The EmbeddedDataSource is just a wrapper for the JDBCDriver. <BR> |-----------------| | <B>EmbeddedDataSource</B> | |-----------------| | has A | V |------------|            |-----------------|     |-----------------------| | JDBCDriver |==produces=&gt;| <B>EmbedConnection</B> |- ?-&gt;| TransactionResource   | | LocalDriver|            |-----------------|     |-----------------------| |------------| </PRE> <P>XADataSource inherits DataSource methods from EmbeddedDataSource.  It also implements ResourceAdapter, whose main job is to keep track of run time global transactions.  A global transaction table maps XIDs to XATransactionResource.  XADataSource also has a XAResourceManager, which implements XAResource functionality in the Store. <P>XAConnection is the one thing that unites a global connection and the XAResource that delineates the global transaction.  This is where the real XAResource functionality is implemented.  All XAResource calls to the XAResource object as well as Connection call to the BrokeredConnection channels thrus the XAConnection, which makes sure only one thread can be accessing the DB2jPooledConnection at any given time. <P>XAResource and BrokeredConnection[23]0 are the two objects we give back to the TM and the user application respectively to control a distributed transaction.  According to the XA spec, the app server is supposed to make sure that these objects are not used the same time by multiple threads, but we don't trust the app server.  Therefore, we channel everthing back to the XAConnection. <P>The MT consideration is actually more complicated than this, because a XAResource is allowed to control any transaction, not just the one its XAConnection is current attached to.  So it is not sufficient to just synchronized on XAConnection to guarentee single thread access to the underlying transaction context.  To control some arbitrary global transaction, the TM can call XAResource to prepare any Xid.  To do that, the XAResource pass the request to the XAConnection, the XAConnection ask the XADataSource to find the XATransactionResource, sets up the thread's context, and call ask the XATransactionResource to prepare.  The XATransactionResource is synchronized to prevent some other thread from attaching, commiting, and in any way calling on the the same transaction context.  If any error is thrown, it is handled with the context of the transaction being prepared.  After the error is handled, the old context (the one where the XAResource is really attached to), is restored.  While this monkey business is going on, the thread that holds the connection the XAConnection is supposed to be attached to is blocked out.  It can resume after its XAConnection restored its context.  (Here is where I am not really sure what happens since that thread obviously doesn't know about all these hanky panky caused by the thread holding the XAResource commiting, preparing and rolling back some other irrelavant transactions, so how would its context be affected in any way?). <P>DB2jPooledConnection implements PooledConnection, is hands out these connection handles which allows some app server to do connection pooling. This is a very thin layer.  A connection handle implements a Connection by passing thru all calls to the underlaying connection.  In this case, it passes Connection call thru the DB2jPooledConnection to the DetachableConnection underneath. <P>EmbeddedDataSource implements JNDI and is a replacement for Driver. <P>The LocalDriver can now produce a DetachableConnection as well as a EmbedConnection (which is the pre-JTA Connection that cannot detach and attach to different transactions).  The way the LocalDriver knows to create a DetachableConnection versus a EmbedConnection is thru some extremely hackish URL settings.  This thing is very ugly and a more elegant way can (and should) no doubt be found. <P>DetachableConnection is a connection which can detach and attach to different XATransactionResource, and can be totally unattached to any transaction. <P>XATransactionResource is a bundle of things that sets up a connection with all the stuff it needs to actually talk to the database, do error handling, etc.  It is also the object that lives in the transaction table managed by the ResourceAdapter (XADataSource).  A XAResource (which may or may not be attached to a transaction) can commit, prepare, or rollback any global transaction that is not attached to an XAConnection.  To do that, the ResourceAdapter fishes out the XATransactionResource, set up the context, and do the commit processing/error handling on the current thread. <P>Local Connection is the same old local Connection except one difference.  Pre-JTA, a localConnection uses itself (or a root Connection) as the object to synchronized upon so that multiple threads getting hold of the same Connection object cannot simultaneously issue calls to the underlying transaction or context (since those things must be single thread access).  With JTA, the object of synchronization is the TransactionResource itself.  This part has not been well thought through and is probably wrong. <P>TransactionResource is a base class for XATransactionResource.  For a local transaction which cannot be detached from a connection, there is no need to encapsulate a bundle of things to set up a connection, so a TransactionResource (probably misnamed) has nothing and is used only for synchronization purposes.  This part has not been well thought throught and is probably wrong. <P>The non-XA PooledConnection is just a thin veneer over the normal connection.  I now have it over a Detachable connection just to simplify the inheritence (XAConnection need to extend PooledConnection and XAConnect needs to be detachable.  However, PooledConnection itself need not be detachable).  It could be changed around to have LocalDriver producing either EmbedConnection or XAConnection, and have the XAConnection implements detachable.  But the current way is simpler.

<p> This class contains a table function which can be used to read data from a Derby table. </p>
<p> Interface for Table Functions which can be told which columns need to be fetched plus simple bounds on those columns. </p> <p> This interface can be implemented by the ResultSet returned by the public static method which is bound to the Table Function. If that ResultSet implements this interface, then the initScan() method of this interface will be called before the scan of the ResultSet starts, that is, before calling any ResultSet method. </p> <p> ResultSets which implement this interface can perform more efficiently because they don't have to fetch all columns and rows. This can mean performance boosts for queries which only need a subset of the Table Function's columns and for queries which compare those columns to constant expressions using the &lt;, &lt;=, =, &gt;, &gt;=, and != operators. This can also mean performance boosts for LIKE and BETWEEN operations on Table Function columns. For more information, see the commentary on <a href="https://issues.apache.org/jira/browse/DERBY-4357">DERBY-4357</a>. </p>
<p> An expression to be pushed into a Table Function so that the Table Function can short-circuit its processing and return fewer rows. A restriction is represented as a binary tree. The non-leaf nodes are ANDs and ORs. The leaf nodes are ColumnQualifiers. A ColumnQualifier is a simple expression comparing a constant value to a column in the Table Function. </p>
A ResultColumn represents a result column in a SELECT, INSERT, or UPDATE statement.  In a SELECT statement, the result column just represents an expression in a row being returned to the client.  For INSERT and UPDATE statements, the result column represents an column in a stored table. So, a ResultColumn has to be bound differently depending on the type of statement it appears in. <P> The type of the ResultColumn can differ from its underlying expression, for example in certain joins the ResultColumn can be nullable even if its underlying column is not. In an INSERT or UPDATE the ResultColumn will represent the type of the column in the table, the type of the underlying expression will be the type of the source of the value to be insert or updated. The method columnTypeAndLengthMatch() can be used to detect when normalization is required between the expression and the type of ResultColumn. This class does not implement any type normalization (conversion), this is typically handled by a NormalizeResultSetNode.
A ResultColumnDescriptor describes a result column in a ResultSet. NOTE: These interfaces are intended to support JDBC. There are some JDBC methods on java.sql.ResultSetMetaData that have no equivalent here, mainly because they are of questionable use to us.  They are: getCatalogName() (will we support catalogs?), getColumnLabel(), isCaseSensitive(), isCurrency(), isDefinitelyWritable(), isReadOnly(), isSearchable(), isSigned(), isWritable()). The JDBC driver implements these itself, using the data type information and knowing data type characteristics.
A ResultColumnList is the target list of a SELECT, INSERT, or UPDATE.
The ResultDescription interface provides methods to get metadata on the results returned by a statement.
The ResultSet interface provides a method to tell whether a statement returns rows, and if so, a method to get the rows. It also provides a method to get metadata about the contents of the rows. It also provide a method to accept rows as input. <p> There is no single implementation of the ResultSet interface. Instead, the various support operations involved in executing statements implement this interface. <p> Although ExecRow is used on the interface, it is not available to users of the API. They should use Row, the exposed super-interface of ExecRow.  [I couldn't find another way to perform this mapping...] <p> Valid transitions: <ul> <li> open-&gt;close</li> <li> close-&gt;open</li> <li> close-&gt;finished</li> <li> finished-&gt;open</li> </ul>
Methods implemented by the common ResultSet class to handle certain events that may originate from the material or common layers.  Reply implementations may update result set state via this interface.
ResultSetFactory provides a wrapper around all of the result sets needed in an execution implementation. <p> For the activations to avoid searching for this module in their execute methods, the base activation supertype should implement a method that does the lookup and salts away this factory for the activation to use as it needs it.
A ResultSetNode represents a result set, that is, a set of rows.  It is analogous to a ResultSet in the LanguageModuleExternalInterface.  In fact, code generation for a a ResultSetNode will create a "new" call to a constructor for a ResultSet.


In general, required data is passed.
The ResultSetStatistics interface is used to provide run time statistics information on a specific ResultSet. <p> This interface extends Formatable so that all objects which implement this interface can be easily saved to the database.
ResultSetStatisticsFactory provides a wrapper around all of the result sets statistics objects needed in building the run time statistics.
This class represents a REVOKE statement.
This class performs actions that are ALWAYS performed for a REVOKE role statement at execution time.
This class represents a REVOKE role statement.
Implements the row level locking accessmanager.
Allows iterator over the role grant closure defined by the relation GRANT role-a TO role-b, or its inverse.
Allows iterator over the role grant closure defined by the relation <code>GRANT</code> role-a <code>TO</code> role-b, or its inverse. <p> The graph is represented as a <code>HashMap</code> where the key is the node and the value is a List grant descriptors representing outgoing arcs. The set constructed depends on whether <code>inverse</code> was specified in the constructor.
This class is used by rows in the SYS.SYSROLES system table. An instance contains information for exactly: One &lt;role definition&gt;, cf. ISO/IEC 9075-2:2003 section 12.4 <bold>or</bold> one &lt;grant role statement&gt;, section 12.5. A role definition is also modeled as a role grant (hence the class name), but with the special grantor "_SYSTEM", and with a grantee of the definer, in Derby this is always the current user. For a role definition, the WITH ADMIN flag is also set. The information contained in the isDef flag is usually redundant, but was added as a precaution against a real user named _SYSTEM, for example when upgrading an older database that did not forbid this.
This class provides rolling file OutputStream.  The file pattern, file size, and number of files can be customized. <p> This class borrows extensively from the java.util.logger.FileHandler class for its file handling ability and instead of handling logger messages it extends java.io.OutputStream. <p> A pattern consists of a string that includes the following special components that will be replaced at runtime: <ul> <li>    "/"    the local pathname separator <li>     "%t"   the system temporary directory <li>     "%h"   the value of the "user.home" system property <li>     "%d"   the value of the "derby.system.home" system property <li>     "%g"   the generation number to distinguish rotated logs <li>     "%u"   a unique number to resolve conflicts <li>     "%%"   translates to a single percent sign "%" </ul> If no "%g" field has been specified and the file count is greater than one, then the generation number will be added to the end of the generated filename, after a dot. <p> Thus for example a pattern of "%t/java%g.log" with a count of 2 would typically cause files to be written on Solaris to /var/tmp/java0.log and /var/tmp/java1.log whereas on Windows 95 they would be typically written to C:\TEMP\java0.log and C:\TEMP\java1.log <p> Generation numbers follow the sequence 0, 1, 2, etc. <p> Normally the "%u" unique field is set to 0.  However, if the <tt>FileHandler</tt> tries to open the filename and finds the file is currently in use by another process it will increment the unique number field and try again.  This will be repeated until <tt>FileHandler</tt> finds a file name that is  not currently in use. If there is a conflict and no "%u" field has been specified, it will be added at the end of the filename after a dot. (This will be after any automatically added generation number.) <p> Thus if three processes were all trying to output to fred%u.%g.txt then they  might end up using fred0.0.txt, fred1.0.txt, fred2.0.txt as the first file in their rotating sequences. <p> Note that the use of unique ids to avoid conflicts is only guaranteed to work reliably when using a local disk file system.
Creates and configures a RollingFileStream
Describe a routine (procedure or function) alias.
This node represents a routine signature.
This class describes rows in the SYS.SYSROUTINEPERMS system table, which keeps track of the routine (procedure and function) permissions that have been granted but not revoked.

The Row interface provides methods to get information about the columns in a result row. It uses simple, position (1-based) access to get to columns. Searching for columns by name should be done from the ResultSet interface, where metadata about the rows and columns is available. <p>
Perform row at a time DML operations of tables and maintain indexes.
Perform row at a time DML operations of tables and maintain indexes.
The result set generated by this node (RowCountResultSet) implements the filtering of rows needed for the <result offset clause> and the <fetch first clause>.  It sits on top of the normal SELECT's top result set, but under any ScrollInsensitiveResultSet. The latter's positioning is needed for the correct functioning of <result offset clause> and <fetch first clause> in the presence of scrollable and/or updatable result sets and CURRENT OF cursors.
This result set implements the filtering of rows needed for the <result offset clause> and the <fetch first clause>.  It sits on top of the normal SELECT's top result set, but under any ScrollInsensitiveResultSet needed for cursors. The latter positioning is needed for the correct functioning of <result offset clause> and <fetch first clause> in the presence of scrollable and/or updatable result sets and CURRENT OF cursors. It is only ever generated if at least one of the two clauses is present.
Allows clients to read and write row count estimates for conglomerates.
Holds the location of a row within a given conglomerate. A row location is not valid except in the conglomerate from which it was obtained.  They are used to identify rows for fetches, deletes, and updates through a conglomerate controller. <p> See the conglomerate implementation specification for information about the conditions under which a row location remains valid.
A RowLocationRetRowSource is the mechanism for iterating over a set of rows, loading those rows into a conglomerate, and returning the RowLocation of the inserted rows.
A RowLock represents a qualifier that is to be used when locking a Row through a RecordHandle. <BR> MT - Immutable
A locking policy that implements row level locking with isolation degree 1. This is an implementation of Gray's degree 1 isolation, read uncommitted, or often referred to as dirty reads.  Basically read operations are done with no locking. This locking policy is only to be used for read operations. The approach is to place all "write" container and row locks on the transaction group lock list.  Locks on this group will last until end of transaction. This implementation will still get table level intent locks.  This is to prevent hard cases where the container otherwise could be deleted while read uncommitted reader is still accessing it.  In order to not get table level intent locks some sort of other ddl level lock would have to be implemented. All "read" row locks will be not be requested. Note that write operations extend from the RowLocking3 implementations.
A locking policy that implements row level locking with isolation degree 2. The approach is to place all "write" container and row locks on the transaction group lock list.  Locks on this group will last until end of transaction.  All "read" container and row locks will be placed on a group list, key'd by the ContainerId of the lock.  Locks on this list will either be released explicitly by the caller, or will be released as a group when the unlockContainer() call is made. Note that write operations extend from the RowLocking3 implementations.
A locking policy that implements row level locking with isolation degree 2, never holding read locks after they are granted. Exactly the same as RowLocking2, except that read locks are acquired using zeroDuration locks, which are immediately released by the lock manager after they are granted.
A locking policy that implements row level locking with isolation degree 3. * We can inherit all the others methods of NoLocking since we hold the * container lock and row locks until the end of transaction.
A locking policy that implements row level locking with isolation degree 3.
A locking policy that implements row level locking with repeatable read isolation.  Since phantom protection with previous key locking is actually handled by the upper level access methods, the only difference in repeatable read is that read locks are of type RowLock.RS2.  This type will not conflict with a previous key insert lock.
<p> Row data type as described in the 2003 SQL spec in part 2, section 4.8. </p> /////////////////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////////////////
Class that represents a call to the ROW_NUMBER() window function.
This interface provides a representation of the ordering of rows in a ResultSet.

Just an easy way to pass information back and forth about current position of a row in a table. ************************************************************************ Public Methods of XXXX class: *************************************************************************
Takes a constant row value and returns it as a result set. <p> This class actually probably never underlies a select statement, but in case it might and because it has the same behavior as the ones that do, we have it implement CursorResultSet and give reasonable answers.
A RowResultSetNode represents the result set for a VALUES clause.
A RowSource is the mechanism for iterating over a set of rows.  The RowSource is the interface through which access recieved a set of rows from the client for the purpose of inserting into a single conglomerate. <p> A RowSource can come from many sources - from rows that are from fast path import, to rows coming out of a sort for index creation.
A row trigger executor is an object that executes a row trigger.  It is instantiated at execution time. There is one per row trigger.
A set of static utility methods to work with rows. <P> A row or partial row is described by two or three parameters. <OL> <LI>DataValueDescriptor[] row - an array of objects, one per column. <LI>FormatableBitSet validColumns - an indication of which objects in row map to which columns </OL> These objects can describe a complete row or a partial row. A partial row is one where a sub-set (e.g. columns 0, 4 and 7) of the columns are supplied for update, or requested to be fetched on a read.  Here's an example of code to set up a partial column list to fetch the 0th (type FOO), 4th (type BAR), and 7th (type MMM) columns from a row with 10 columns, note that the format for a partial row changed from a "packed" representation in the 3.0 release to a "sparse" representation in later releases: <blockquote><pre> // allocate/initialize the row DataValueDescriptor row = new DataValueDescriptor[10] row[0] = new FOO(); row[4] = new BAR(); row[7] = new MMM(); // allocate/initialize the bit set FormatableBitSet FormatableBitSet = new FormatableBitSet(10); FormatableBitSet.set(0); FormatableBitSet.set(4); FormatableBitSet.set(7); </blockquote></pre> <BR><B>Column mapping<B><BR> When validColumns is null: <UL> <LI> The number of columns is given by row.length <LI> Column N maps to row[N], where column numbers start at zero. </UL> <BR> When validColumns is not null, then <UL> <LI> The number of requested columns is given by the number of bits set in validColumns. <LI> Column N is not in the partial row if validColumns.isSet(N) returns false. <LI> Column N is in the partial row if validColumns.isSet(N) returns true. <LI> If column N is in the partial row then it maps to row[N]. If N &gt;= row.length then the column is taken as non existent for an insert or update, and not fetched on a fetch. </UL> If row.length is greater than the number of columns indicated by validColumns the extra entries are ignored.
the purpose of this class is to run Java-based test cases in a separate thread
the purpose of this class is to run IJ in a separate thread


A RunTimeStatistics object is a representation of the query execution plan and run time statistics for a java.sql.ResultSet. A query execution plan is a tree of execution nodes.  There are a number of possible node types.  Statistics are accumulated during execution at each node.  The types of statistics include the amount of time spent in specific operations (if STATISTICS TIMING is SET ON), the number of rows passed to the node by its child(ren) and the number of rows returned by the node to its parent.  (The exact statistics are specific to each node type.) <P> RunTimeStatistics is most meaningful for DML statements (SELECT, INSERT, DELETE and UPDATE).
RunTimeStatistics implemenation.
Class used for running a performance test from the command line. To learn how to run the tests, invoke this command: <pre> java org.apache.derbyTesting.perf.clients.Runner </pre>

<p> Machine to validate the operation of the sequence generator. This is a re-implementation of the sequence generator in a less efficient style whose correctness is easier to reason about. </p>
A SPSDescriptor describes a Stored Prepared Statement. It correlates to a row in SYS.SYSSTATEMENTS. <B>SYNCHRONIZATION</B>: Stored prepared statements may be cached.  Thus they may be shared by multiple threads.  It is very hard for two threads to try to muck with an sps simultaeously because all ddl (including sps recompilation) clears out the sps cache and invalidates whatever statement held a cached sps.  But it is possible for two statements to do a prepare execute statment <x> at the exact same time, so both try to do an sps.prepare() at the same time during code generation, so we synchronize most everything except getters on immutable objects just to be on the safe side.
This class implements a Cacheable for a DataDictionary cache of sps descriptors, with the lookup key being the name/schema of the sps. Assumes client passes in a string that includes the schema name. <p> The cache ensures that the class of the target sps is loaded if the sps is found in cache.  This is ensured by calling loadGeneratedClass() on the sps when it is added to the cache. Each subsequent user of the sps cache will do its own load/unload on the class.  Because the class manager/loader maintains reference counts on the classes it is handling, the user load/unload will just increment/decrement the use count.  Only when the sps is uncached will it be unloaded. /** * Check the consistency of the table descriptor held by this TDCacheable * versus an uncached table descriptor. * * @param uncachedSpsd  The uncached descriptor to compare to * @param identity      The identity of the table descriptor * @param reportInconsistent    A HeaderPrintWriter to send complaints to * * @return  true if the descriptors are the same, false if they're different * * @exception StandardException     Thrown on error */ private boolean checkConsistency(SPSDescriptor uncachedSpsd, Object identity, HeaderPrintWriter reportInconsistent) throws StandardException { boolean retval = true;  if (SanityManager.DEBUG) { if (uncachedSpsd == null) { reportInconsistent.println( "Inconsistent SPSNameCacheable: identity = " + identity + ", uncached table descriptor not found."); retval = false; } else { if ( (!uncachedSpsd.getText().equals(spsd.getText())) || (!uncachedSpsd.getUsingText().equals(spsd.getUsingText())) || (!uncachedSpsd.getQualifiedName().equals(spsd.getQualifiedName())) ) { reportInconsistent.println( "Inconsistent SPSNameCacheable: identity = " + identity + ", cached  SPS = " + spsd + ", uncached SPS = " + uncachedSpsd); retval = false; } } }  return retval; }
SQLBinary is the abstract class for the binary datatypes. <UL> <LI> CHAR FOR BIT DATA <LI> VARCHAR FOR BIT DATA <LI> LONG VARCHAR <LI> BLOB </UL> <P> Format : <encoded length><raw data> <BR> Length is encoded to support Cloudscape 5.x databases where the length was stored as the number of bits. The first bit of the first byte indicates if the format is an old (Cloudscape 5.x) style or a new Derby style. Derby then uses the next two bits to indicate how the length is encoded. <BR> <encoded length> is one of N styles. <UL> <LI> (5.x format zero) 4 byte Java format integer value 0 - either <raw data> is 0 bytes/bits  or an unknown number of bytes. <LI> (5.x format bits) 4 byte Java format integer value &gt;0 (positive) - number of bits in raw data, number of bytes in <raw data> is the minimum number of bytes required to store the number of bits. <LI> (Derby format) 1 byte encoded length (0 &lt;= L &lt;= 31) - number of bytes of raw data - encoded = 0x80 &amp; L <LI> (Derby format) 3 byte encoded length (32 &lt;= L &lt; 64k) - number of bytes of raw data - encoded = 0xA0 <L as Java format unsigned short> <LI> (Derby format) 5 byte encoded length (64k &lt;= L &lt; 2G) - number of bytes of raw data - encoded = 0xC0 <L as Java format integer> <LI> (future) to be determined L &gt;= 2G - encoded 0xE0 <encoding of L to be determined> (0xE0 is an esacape to allow any number of arbitary encodings in the future). </UL> <BR> When the value was written from a byte array the Derby encoded byte length format was always used from Derby 10.0 onwards (ie. all open source versions). <BR> When the value was written from a stream (e.g. PreparedStatement.setBinaryStream) then the Cloudscape '5.x format zero' was used by 10.0 and 10.1. The was due to the class RawToBinaryFormatStream always writing four zero bytes for the length before the data. <BR> The Cloudscape '5.x format bits' format I think was never used by Derby.
SQLBit represents the SQL type CHAR FOR BIT DATA
SQLBlob satisfies the DataValueDescriptor, interfaces (i.e., OrderableDataType). It uses the SQLLongVarbit implementation, which implements a String holder, e.g. for storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction. <p> Because LOB types are not orderable, we'll override those methods...
SQLBoolean satisfies the DataValueDescriptor interfaces (i.e., DataType). It implements a boolean column, e.g. for * storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction, as it affects the storage size and mechanism. <p> Because DataType is a subtype of DataType, SQLBoolean can play a role in either a DataType/Row or a DataType/Row, interchangeably. <p> We assume the store has a flag for nullness of the value, and simply return a 0-length array for the stored form when the value is null. <p> PERFORMANCE: There are likely alot of performance improvements possible for this implementation -- it new's Integer more than it probably wants to.
The SQLChar represents a CHAR value with UCS_BASIC collation. SQLChar may be used directly by any code when it is guaranteed that the required collation is UCS_BASIC, e.g. system columns. <p> The state may be in char[], a String, a Clob, or an unread stream, depending on how the datatype was created. <p> Stream notes: <p> When the datatype comes from the database layer and the length of the bytes necessary to store the datatype on disk exceeds the size of a page of the container holding the data then the store returns a stream rather than reading all the bytes into a char[] or String.  The hope is that the usual usage case is that data never need be expanded in the derby layer, and that client can just be given a stream that can be read a char at a time through the jdbc layer.  Even though SQLchar's can't ever be this big, this code is shared by all the various character datatypes including SQLClob which is expected to usually larger than a page. <p> The state can also be a stream in the case of insert/update where the client has used a jdbc interface to set the value as a stream rather than char[]. In this case the hope is that the usual usage case is that stream never need be read until it is passed to store, read once, and inserted into the database.
SQLClob represents a CLOB value with UCS_BASIC collation. CLOB supports LIKE operator only for collation.
This contains an instance of a SQL Date. <p> The date is stored as int (year &lt;&lt; 16 + month &lt;&lt; 8 + day) Null is represented by an encodedDate value of 0. Some of the static methods in this class are also used by SQLTime and SQLTimestamp so check those classes if you change the date encoding PERFORMANCE OPTIMIZATION: The java.sql.Date object is only instantiated when needed do to the overhead of Date.valueOf(), etc. methods.
SQLDecimal satisfies the DataValueDescriptor interfaces (i.e., OrderableDataType). It implements a numeric/decimal column, e.g. for * storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction, as it affects the storage size and mechanism. <p> Because OrderableDataType is a subtype of DataType, SQLDecimal can play a role in either a DataType/Row or a OrderableDataType/Row, interchangeably. <p> We assume the store has a flag for nullness of the value, and simply return a 0-length array for the stored form when the value is null.
SQLDouble satisfies the DataValueDescriptor interfaces (i.e., OrderableDataType). It implements a double column, e.g. for * storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction, as it affects the storage size and mechanism. <p> Because OrderableDataType is a subtype of DataType, SQLDouble can play a role in either a DataType/Row or a OrderableDataType/Row, interchangeably. <p> We assume the store has a flag for nullness of the value, and simply return a 0-length array for the stored form when the value is null. <p> PERFORMANCE: There are likely alot of performance improvements possible for this implementation -- it new's Double more than it probably wants to. <p> This is modeled after SQLInteger. <p> We don't let doubles get constructed with NaN or Infinity values, and check for those values where they can occur on operations, so the set* operations do not check for them coming in.
SQLException factory class to create jdbc 40 exception classes
Wrapper class for SQLExceptions
SQLInteger represents an INTEGER value.
SQLLongVarbit represents the SQL type LONG VARCHAR FOR BIT DATA It is an extension of SQLVarbit and is virtually indistinguishable other than normalization.
SQLLongint satisfies the DataValueDescriptor interfaces (i.e., OrderableDataType). It implements a bigint column, e.g. for * storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction, as it affects the storage size and mechanism. <p> Because OrderableDataType is a subtype of DataType, SQLLongint can play a role in either a DataType/Row or a OrderableDataType/Row, interchangeably. <p> We assume the store has a flag for nullness of the value, and simply return a 0-length array for the stored form when the value is null. <p> PERFORMANCE: There are likely alot of performance improvements possible for this implementation -- it new's Long more than it probably wants to.
SQLLongvarchar represents a LONG VARCHAR value with UCS_BASIC collation. SQLLongvarchar is mostly the same as SQLVarchar, so it is implemented as a subclass of SQLVarchar.  Only those methods with different behavior are implemented here.
SQLReal satisfies the DataValueDescriptor interfaces (i.e., OrderableDataType). It implements a real column, e.g. for storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction, as it affects the storage size and mechanism. <p> Because OrderableDataType is a subtype of ValueColumn, SQLReal can play a role in either a ValueColumn/Row or a OrderableDataType/Row, interchangeably. <p> We assume the store has a flag for nullness of the value, and simply return a 0-length array for the stored form when the value is null. <p> PERFORMANCE: There are likely alot of performance improvements possible for this implementation -- it new's Float more than it probably wants to. <p> This is called SQLReal even though it maps to the Java float type, to avoid confusion with whether it maps to the SQL float type or not. It doesn't, it maps to the SQL real type. <p> This is modeled after SQLSmallint.

An implementation of this interface encapsulates some of the SQL session context's state variables, cf. SQL 2003, section 4.37.3, notably those which we need to save and restore when entering a stored procedure or function (which can contain SQL and thus a nested connection), cf. 4.37.3, 4.27.3 and 4.34.1.1.  <p> Presently this set contains the following properties: <ul> <li>current role</li> <li>current schema</li> </ul> The standard specifies that the authorization stack be copied onto the new SQL session context before it is pushed (and possibly modifed) with a new cell for the authorization ids (user, role). In our implementation we merge these two stacks for now. Also, the authorization id of current user is not represented yet, since it can not be modified in a session; Derby can not run routines with definer's rights yet. <p> SQL session context is implemented as follows: Statements at root connection level use the instance held by the the lcc, nested connections maintain instances of SQLSessionContext, held by the activation of the calling statement. This forms a logical stack as required by the standard. The statement context also holds a reference to the current SQLSessionContext. <p> When a dynamic result set references e.g. current role, the value retrieved will always be that of the current role when the statement is logically executed (inside procedure/function), not the current value when the result set is accessed outside the stored procedure/function.  This works since the nested SQL session context is kept by the caller activation, so even though the statement context of the call has been popped, we can get at the final state of the nested SQL session context since the caller's activation is alive as long as dynamic result sets need it). <p> If more than one nested connection is used inside a shared procedure, they will share the same nested SQL session context. Since the same dynamic call context is involved, this seems correct.

SQLSmallint satisfies the DataValueDescriptor interfaces (i.e., OrderableDataType). It implements a smallint column, e.g. for storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction, as it affects the storage size and mechanism. <p> Because OrderableDataType is a subtype of ValueColumn, SQLSmallint can play a role in either a ValueColumn/Row or a OrderableDataType/Row, interchangeably. <p> We assume the store has a flag for nullness of the value, and simply return a 0-length array for the stored form when the value is null. <p> PERFORMANCE: There are likely alot of performance improvements possible for this implementation -- it new's Short more than it probably wants to.
This is a refactoring wrapper around the "real" SQLState.java, which has been relocated to org.apache.derby.shared.common.reference
This contains constants for all the standard SQL states as well as for those that are specific to Derby that our tests compare against to make sure the right error is thrown. It is important to use these constants rather than those in org.apache.derby.shared.common.reference.SQLState.java because (a) that class is not part of the public API and (b) that class contains message ids, not SQL states.
This contains an instance of a SQL Time Our current implementation doesn't implement time precision so the fractional seconds portion of the time is always 0.  The default when no time precision is specified is 0 fractional seconds.  A SQL Time without timezone information is assumed to be in the local time zone.  The local time is stored as is and doesn't change if the timezone changes. This is in conformance with the SQL99 standard.  The SQL92 standard indicates that the time is in GMT and changes with the timezone.  The SQL99 standard clarifies this to allow time without timezoned to be stored as the local time. <p> Time is stored as two ints.  The first int represents hour, minute, second and the second represents fractional seconds (currently 0 since we don't support time precision) encodedTime = -1 indicates null PERFORMANCE OPTIMIZATION: The java.sql.Time object is only instantiated on demand for performance reasons.
This contains an instance of a SQL Timestamp object. <p> SQLTimestamp is stored in 3 ints - an encoded date, an encoded time and nanoseconds encodedDate = 0 indicates a null WSCTimestamp SQLTimestamp is similar to SQLTimestamp, but it does conserves space by not keeping a GregorianCalendar object PERFORMANCE OPTIMIZATION: We only instantiate the value field when required due to the overhead of the Date methods. Thus, use isNull() instead of "value == null" and getTimestamp() instead of using value directly.
SQLTinyint satisfies the DataValueDescriptor interfaces (i.e., OrderableDataType). It implements a tinyint column, e.g. for storing a column value; it can be specified when constructed to not allow nulls. Nullability cannot be changed after construction, as it affects the storage size and mechanism. <p> Because OrderableDataType is a subtype of ValueColumn, SQLTinyint can play a role in either a ValueColumn/Row or a OrderableDataType/Row, interchangeably. <p> We assume the store has a flag for nullness of the value, and simply return a 0-length array for the stored form when the value is null.
Convert a SQL script to a JUnit test. Usage: java org.apache.derbyTesting.functionTests.util.SQLToJUnit <embedded_sql_out_file>
This node type converts a value in the SQL domain to a value in the Java domain.


SQLVarbit represents the SQL type VARCHAR FOR BIT DATA It is an extension of SQLBit and is virtually indistinguishable other than normalization.
SQLVarchar represents a VARCHAR value with UCS_BASIC collation. SQLVarchar is mostly the same as SQLChar, so it is implemented as a subclass of SQLChar.  Only those methods with different behavior are implemented here.
This class generates SQLWarning instances. It has an understanding of Derby's internal error/warning message Ids, and transforms these to localised error messages and appropriate SQLState.
This class is a decorator for the Scrollable Updatable Resultset tests.  It sets up a datamodel and populates it with data.
Factory for creating a SYSALIASES row. Here are the directions for adding a new system supplied alias. Misc: All system supplied aliases are class aliases at this point. Additional arrays will need to be added if we supply system aliases of other types. The preloadAliasIDs array is an array of hard coded UUIDs for the system supplied aliases. The preloadAliases array is the array of aliases for the system supplied aliases.  This array is in alphabetical order by package and class in Xena.  Each alias is the uppercase class name of the alias. The preloadJavaClassNames array is the array of full package.class names for the system supplied aliases.  This array is in alphabetical order by package and class in Xena. SYSALIASES_NUM_BOOT_ROWS is the number of boot rows in sys.sysaliases in a new database.
Factory for creating a SYSCHECKS row.
Factory for creating a SYSCOLPERMS row.
Factory for creating a SYSCOLUMNS row.
Factory for creating a SYSCONGLOMERATES row.
Factory for creating a SYSCONTRAINTS row.
Factory for creating a SYSDEPENDSS row.
Factory for creating a SYSDUMMY1 row.
Factory for creating a SYSFILES row.
Factory for creating a SYSFOREIGNKEYS row.
Factory for creating a SYSKEYS row.
Factory for creating a SYSPERMS row.
Factory for creating a SYSROLES row.
Factory for creating a SYSROUTINEPERMS row.
Factory for creating a SYSSCHEMAS row.
Factory for creating a SYSSEQUENCES row. The contract of this table is this: if the CURRENTVALUE column is null, then the sequence is exhausted and no more values can be generated from it.
Factory for creating a SYSSTATEMENTS row.
Factory for creating a SYSSTATISTICS row.
Factory for creating a SYSTABLEPERMS row.
Factory for creating a SYSTABLES row.
Factory for creating a SYSTRIGGERS row.
Factory for creating a SYSUSERS row.
Factory for creating a SYSVIEWS row.
Simple utility to generate samples from a mixture-of-Gaussian distribution.

A very simple, read-only, sample VTI. <p> This VTI is incomplete and has its quirks - it is intended for basic testing only! Supported getters: <ul> <li>getString</li> <li>getInt</li> </ul>
The SanityService provides assertion checking and debug control. <p> Assertions and debug checks can only be used for testing conditions that might occur in development code but not in production code. <b>They are compiled out of production code.</b> <p> Uses of assertions should not add AssertFailure catches or throws clauses; AssertFailure is under RuntimeException in the java exception hierarchy. Our outermost system block will bring the system down when it detects an assertion failure. <p> In addition to ASSERTs in code, classes can choose to implement an isConsistent method that would be used by ASSERTs, UnitTests, and any other code wanting to check the consistency of an object. <p> Assertions are meant to be used to verify the state of the system and bring the system down if the state is not correct. Debug checks are meant to display internal information about a running system. <p>

This class  describes actions that are ALWAYS performed for a Savepoint (rollback, release and set savepoint) Statement at Execution time.
A SavepointNode is the root of a QueryTree that represents a Savepoint (ROLLBACK savepoint, RELASE savepoint and SAVEPOINT) statement.
This ResultSet evaluates scalar, non distinct aggregates. It will scan the entire source result set and calculate the scalar aggregates when scanning the source during the first call to next().
Scan the the log which is implemented by a series of log files.n This log scan knows how to move across log file if it is positioned at the boundary of a log file and needs to getNextRecord. <PRE> 4 bytes - length of user data, i.e. N 8 bytes - long representing log instant N bytes of supplied data 4 bytes - length of user data, i.e. N </PRE>
A scan is the mechanism for iterating over the rows in a conglomerate, the scan controller is the interface through which access clients control the underlying scan.  An instance of a scan controller can be thought of as an open scan. <p> Scans are opened from a TransactionController. <P> A ScanController can handle partial rows. Partial rows are described in RowUtil. <BR> A scan controller is opened with a FormatableBitSet that describes the columns that need to be returned on a fetch call. This FormatableBitSet need not include any columns referenced in the qualifers, start and/or stop keys.
A ScanControllerRowSource is both a RowSource and a ScanController.  This interface is internal to Access for use in the case of RowSource which are implemented on top of a ScanController.
Inteface for scanning the log from outside the RawStore.
This object provides performance information related to an open scan. The information is accumulated during operations on a ScanController() and then copied into this object and returned by a call to ScanController.getStatistic().
The ScanManager interface contains those methods private to access method implementors necessary to implement Scans on Conglomerates.  Client of scans use the ScanController to interact with the scan. <P>
ScanQualifier provides additional methods for the Language layer on top of Qualifier.
Abstract <code>ResultSet</code> class for <code>NoPutResultSet</code>s which contain a scan. Returns rows that may be a column sub-set of the rows in the underlying object to be scanned. If accessedCols is not null then a sub-set of columns will be fetched from the underlying object (usually into the candidate row object), then the returned rows will be a compacted form of that row, with the not-fetched columns moved out. If accessedCols is null then the full row will be returned. <BR> Example: if accessedCols indicates that we want to retrieve columns 1 and 4, then candidate row will have space for 5 columns (because that's the size of the rows in the underlying object), but only cols "1" and "4" will have values: <BR> <pre> 0    1    2    3    4 [  - , COL1,  - ,  - , COL4 ] </pre> <BR> Rows returned by this ScanResultSet will have the values: <BR> <pre> 0     1 [ COL1, COL4 ] </pre>

Load the OE schema
This class represents a schema descriptor
Filter which passes Visitables only if the compiler is inside a named scope.
<p> Application for showcasing Derby features using an educational testing schema. </p> //////////////////////////////////////////////////////  MINIONS  //////////////////////////////////////////////////////
Provide insensitive scrolling functionality for the underlying result set.  We build a disk backed hash table of rows as the user scrolls forward, with the position as the key. For read-only result sets the hash table will containg the following columns: <pre> +-------------------------------+ | KEY                           | +-------------------------------+ | Row                           | +-------------------------------+ </pre> where key is the position of the row in the result set and row is the data. And for updatable result sets it will contain: <pre> +-------------------------------+ | KEY                           | [0] +-------------------------------+ | RowLocation                   | [POS_ROWLOCATION] +-------------------------------+ | Deleted                       | [POS_ROWDELETED] +-------------------------------+ | Updated                       | [POS_ROWUPDATED] +-------------------------------+ | Row                           | [extraColumns ... n] +-------------------------------+ </pre> where key is the position of the row in the result set, rowLocation is the row location of that row in the Heap, Deleted indicates whether the row has been deleted, Updated indicates whether the row has been updated, and row is the data.
A ScrollInsensitiveResultSetNode represents the insensitive scrolling cursor functionality for any child result set that needs one.
Parameters that are passed down during a recursive b-tree search. This class is intended to be used as a struct, primarily to make it easier to pass a number of search parameters around, and also to make it easy to re-use objects and not re-allocate.


Operations which can be secured. SQL authorization is one way to control who can access these operations.
Code to aid in checking the Security of Derby. This initial implementation only handles the emebdded code. Future work could expand to the client driver and network server.
Configures the wrapped test to be run with the specified security policy. <p> This setup class normally installs the default policy file. This can be overridden by specifying {@literal java.security.policy=<NONE>} (see {@linkplain #NO_POLICY}), and can also be overridden by installing a security manager explicitly before the default security manager is installed. <p> Individual tests/suites can be configured to be run without a security manager, with a specific policy file, or with a specific policy file merged with the default policy file. The last option is useful when you only need to extend the default policy with a few extra permissions to run a test.
This class provides helper functions for security-related features.

A SelectNode represents the result set for any of the basic DML operations: SELECT, INSERT, UPDATE, and DELETE.  (A RowResultSetNode will be used for an INSERT with a VALUES clause.)  For INSERT - SELECT, any of the fields in a SelectNode can be used (the SelectNode represents the SELECT statement in the INSERT - SELECT).  For UPDATE and DELETE, there will be one table in the fromList, and the groupByList fields will be null. For both INSERT and UPDATE, the resultColumns in the selectList will contain the names of the columns being inserted into or updated.

This class is used by rows in the SYS.SYSSEQUENCES system table. See the header comment of SYSSEQUENCESRowFactory for the contract of that table. In particular, if the CURRENTVALUE column is null, then the sequence has been exhausted and no more values can be generated from it.
<p> This is a generic machine for pre-allocating ranges of sequence numbers in order to improve concurrency. The public methods are synchronized and should be brief. The caller of the methods in this class is responsible for updating values on disk when the generator is exhausted or when it needs to allocate a new range of values. </p> <p> The most used method in this class is getCurrentValueAndAdvance(). This method returns the next number in the range managed by the sequence generator. This method will raise an exception if the sequence generator is exhausted. Otherwise getCurrentValueAndAdvance() hands back a tuple of return values: </p> <blockquote> ( <i>status, currentValue, lastAllocatedValue, numberOfValuesAllocated</i> ) </blockquote> <p> The <i>status</i> field takes the following values: </p> <ul> <li><b>RET_I_AM_CONFUSED</b> - This value should never be returned. If this value comes back, then the sequence generator is confused.</li> <li><b>RET_OK</b> - This means that the generator has successfully obtained a next value. That value is <i>currentValue</i>.</li> <li><b>RET_MARK_EXHAUSTED</b> - This means that the generator has reached the end of its legal range and is handing back its very last value. The caller must mark the catalogs to indicate that the range is exhausted. The very last value being handed back is <i>currentValue</i>.</li> <li><b>RET_ALLOCATE_NEW_VALUES</b> - This means that the generator has come to the end of its pre-allocated values. The caller needs to update the catalog to grab a new range of legal values and then call allocateNewRange() to tell the generator that the range was successfully allocated. The remaining values in the return tuple have these meanings: <ul> <li><i>currentValue</i> - This is what is expected to be the current value in the catalog before allocating a new range. If, in fact, this is not the value in the catalog, then we are racing with another session to drain values from the generator and update the disk. Do not update the catalog if the value there is not <i>currentValue</i>. Instead, assume that another session got in ahead of us and grabbed a new range of values. Simply call getCurrentValueAndAdvance() again.</li> <li><i>lastAllocatedValue</i> - This is the next value to write to the catalog.</li> <li><i>numberOfValuesAllocated</i> - This is the number of values which were allocated if we successfully updated the catalog. If we successfully updated the catalog, then we should call allocateNewRange(), handing it this value so that it can reset its range. As a sanity check, we also hand allocateNewRange() the <i>currentValue</i> that we were given. The allocateNewRange() method will assume we're racing another session and will ignore us if its sense of <i>currentValue</i> does not agree with ours.</li> </ul> </li> </ul> <p> It may happen that getCurrentValueAndAdvance() tells its caller to allocate a new range of sequence numbers in the system catalog. If the caller successfully allocates a new range, the caller should call allocateNewRange() to tell the generator to update its internal memory of that range. </p> <p> The peekAtCurrentValue() method is provided so that unused, pre-allocated values can be flushed when the sequence generator is being discarded. The caller updates the catalog with the value returned by peekAtCurrentValue(). The peekAtCurrentValue() method is also called by the syscs_peek_at_sequence() function which users should call rather than try to scan the underlying catalog themselves. </p>
<p> Machinery to test the concurrency of sequence/identity generators. </p>
<p> Logic to determine how many values to pre-allocate for a sequence. By default, Derby boosts concurrency by pre-allocating ranges of numbers for sequences. During orderly database shutdown, the unused numbers are reclaimed so that shutdown will not create holes in the sequences.  However, holes may appear if the application fails to shut down its databases before the JVM exits. </p> <p> Logic in this class is called every time Derby needs to pre-allocate a new range of sequence values. Users can override Derby's default behavior by writing their own implementation of this interface and then setting the following Derby property: </p> <pre> -Dderby.language.sequence.preallocator=com.acme.MySequencePreallocator </pre> <p> Classes which implement this interface must also provide a public 0-arg constructor so that Derby can instantiate them. Derby will instantiate a SequencePreallocator for every sequence. </p>
<p> Default Derby logic for determining how many values to pre-allocate for an identity column or sequence. </p>
SequenceReader - a background thread that checks the state of the sequence counter.
<p> An object cached in the data dictionary which manages new values for sequences. Note that this class must be public and have a 0-arg constructor in order to satisfy the Cacheable contract. </p> <p> This is the abstract superclass of specific implementations for specific sequences. For instance, one subclass handles the ANSI/ISO sequences stored in SYSSEQUENCES. Another subclass could handle the sequences stored in Derby's identity columns. </p> <p> This class does a couple tricky things: </p> <ul> <li>It pre-allocates a range of values from a sequence so that we don't have to change the on-disk value every time we get the next value for a sequence.</li> <li>When updating the on-disk value, we use a subtransaction of the user's execution transaction. If the special transaction cannot do its work immediately, without waiting for a lock, then a TOO MUCH CONTENTION error is raised. It is believed that this can only happen if someone holds locks on SYSSEQUENCES, either via sequence DDL or a scan of the catalog. The TOO MUCH CONTENTION error tells the user to not scan SYSSEQUENCES directly, but to instead use the SYSCS_UTIL.SYSCS_PEEK_AT_SEQUENCE() if the user needs the current value of the sequence generator.</li> </ul> <p> Here is the algorithm pursued when the caller asks for the next number in a sequence: </p> <ul> <li>We try to get the next number from a cache of pre-allocated numbers. The endpoint (last number in the pre-allocated range) was previously recorded in the catalog row which describes this sequence. If we are successful in getting the next number, we return it and all is well.</li> <li>Otherwise, we must allocate a new range by updating the catalog row. We should not be in contention with another connection because the update method is synchronized.</li> </ul>
Serializes and writes data sources to file, or prints information about a file assumed to be written by this program. <p> Four entities are written to the stream: <ol> <li>Derby version string - UTF</li> <li>Derby build number - UTF</li> <li>Derby data source - object</li> <li>Derby data source reference - object</li> </ol> <p> Both embedded and client data sources are attempted serialized, and the data source class names are obtained from a predefined list. If another data source implementation is added to Derby, its class name must be added to the list if this class is supposed to serialize it and write it to file. <p> Existing files are overwritten, and the file name is constructed like this: <tt>&lt;ClassName&gt;-&lt;modifiedVersionString&gt;.ser</tt> The version string is modified by replacing punctuation marks with underscores.
Change to a client server configuration based upon the current configuration at setup time. Previous configuration is restored at tearDown time. This only changes the configuration, it does not start any network server.
A context that is used during a service boot to stop cleanup on the stack at this point.
An event from the Framework describing a service lifecycle change. <p> <code>ServiceEvent</code> objects are delivered to <code>ServiceListener</code>s and <code>AllServiceListener</code>s when a change occurs in this service's lifecycle. A type code is used to identify the event type for future extendability. <p> OSGi Alliance reserves the right to extend the set of types.
Allows services to provide customized service objects in the OSGi environment. <p> When registering a service, a <code>ServiceFactory</code> object can be used instead of a service object, so that the bundle developer can gain control of the specific service object granted to a bundle that is using the service. <p> When this happens, the <code>BundleContext.getService(ServiceReference)</code> method calls the <code>ServiceFactory.getService</code> method to create a service object specifically for the requesting bundle. The service object returned by the <code>ServiceFactory</code> is cached by the Framework until the bundle releases its use of the service. <p> When the bundle's use count for the service equals zero (including the bundle stopping or the service being unregistered), the <code>ServiceFactory.ungetService</code> method is called. <p> <code>ServiceFactory</code> objects are only used by the Framework and are not made available to other bundles in the OSGi environment. The Framework may concurrently call a <code>ServiceFactory</code>.
A <code>ServiceEvent</code> listener. <code>ServiceListener</code> is a listener interface that may be implemented by a bundle developer. When a <code>ServiceEvent</code> is fired, it is synchronously delivered to a <code>ServiceListener</code>. The Framework may deliver <code>ServiceEvent</code> objects to a <code>ServiceListener</code> out of order and may concurrently call and/or reenter a <code>ServiceListener</code>. <p> A <code>ServiceListener</code> object is registered with the Framework using the <code>BundleContext.addServiceListener</code> method. <code>ServiceListener</code> objects are called with a <code>ServiceEvent</code> object when a service is registered, modified, or is in the process of unregistering. <p> <code>ServiceEvent</code> object delivery to <code>ServiceListener</code> objects is filtered by the filter specified when the listener was registered. If the Java Runtime Environment supports permissions, then additional filtering is done. <code>ServiceEvent</code> objects are only delivered to the listener if the bundle which defines the listener object's class has the appropriate <code>ServicePermission</code> to get the service using at least one of the named classes under which the service was registered. <p> <code>ServiceEvent</code> object delivery to <code>ServiceListener</code> objects is further filtered according to package sources as defined in {@link ServiceReference#isAssignableTo(Bundle, String)}.
A bundle's authority to register or get a service. <ul> <li>The <code>ServicePermission.REGISTER</code> action allows a bundle to register a service on the specified names. <li>The <code>ServicePermission.GET</code> action allows a bundle to detect a service and get it. </ul> Permission to get a service is required in order to detect events regarding the service. Untrusted bundles should not be able to detect the presence of certain services unless they have the appropriate <code>ServicePermission</code> to get the specific service. Stores a set of ServicePermission permissions.
wrapper class for basic daemon's clients
A reference to a service. <p> The Framework returns <code>ServiceReference</code> objects from the <code>BundleContext.getServiceReference</code> and <code>BundleContext.getServiceReferences</code> methods. <p> A <code>ServiceReference</code> object may be shared between bundles and can be used to examine the properties of the service and to get the service object. <p> Every service registered in the Framework has a unique <code>ServiceRegistration</code> object and may have multiple, distinct <code>ServiceReference</code> objects referring to it. <code>ServiceReference</code> objects associated with a <code>ServiceRegistration</code> object have the same <code>hashCode</code> and are considered equal (more specifically, their <code>equals()</code> method will return <code>true</code> when compared). <p> If the same service object is registered multiple times, <code>ServiceReference</code> objects associated with different <code>ServiceRegistration</code> objects are not equal.
A registered service. <p> The Framework returns a <code>ServiceRegistration</code> object when a <code>BundleContext.registerService</code> method invocation is successful. The <code>ServiceRegistration</code> object is for the private use of the registering bundle and should not be shared with other bundles. <p> The <code>ServiceRegistration</code> object may be used to update the properties of the service or to unregister the service.
To use a DaemonService, one implements the Serviceable interface.  Only one DaemonService will call this at any given time.  However, if this Serviceable object subscribes to or enqueues to the DeamonService multiple times, then multiple DaemonService threads may call this Serviceable object at the same time.  The Serviceable object must decide what synchronization it needs to provide depending on what work it needs to do. The Serviceable interface does not provide a way to pass a work object to identify what work needs to be done, it is assumed that the Serviceable object knows that.  If a Serviceable object has different work for the DaemonService to do, it must somehow encapsulate or identify the different work by an intermediary object which implements the Serviceable interface and which can an identify the different work and pass it along to the object that can deal with them.
Session stores information about the current session It is used so that a DRDAConnThread can work on any session.
This class describes actions that are performed for a set constraint at execution time. <p> Note that the dependency action we send is SET_CONSTRAINTS rather than ALTER_TABLE.  We do this because we want to distinguish SET_CONSTRAINTS from ALTER_TABLE for error messages.
A SetConstraintsNode is the root of a QueryTree that represents a SET CONSTRAINTS statement.
Checks the current Derby jars in the source tree directory, obtains the Derby version from them, and replaces the value of the placeholder version tags in the POM files. <p> After this method has been successfully run you should be ready to generate the Maven 2 artifacts for Derby. <p> The main task of this class is to replace the version tags in the Maven POM files. The can be done manually, but exact process would vary from platform to platform. Also, running a search-and-replace could potentially replace tags not supposed to be replaced. To make the Maven 2 artifact publish process simpler, this class was written. @NotThreadSafe
Takes the result set produced by an ordered UNION ALL of two tagged result sets and produces the INTERSECT or EXCEPT of the two input result sets. This also projects out the tag, the last column of the input rows.
A SetOperatorNode represents a UNION, INTERSECT, or EXCEPT in a DML statement. Binding and optimization preprocessing is the same for all of these operations, so they share bind methods in this abstract class. The class contains a boolean telling whether the operation should eliminate duplicate rows.
Represents shrinking of the reserved space of a particular row on a page. This operation is not undoable.
This class describes actions that are ALWAYS performed for a SET ROLE Statement at Execution time.
A SetRoleNode is the root of a QueryTree that represents a SET ROLE statement.
This class describes actions that are ALWAYS performed for a SET SCHEMA Statement at Execution time.
A SetSchemaNode is the root of a QueryTree that represents a SET SCHEMA statement.  It isn't replicated, but it generates a ConstantAction because it is basically easier than generating the code from scratch.
This class  describes actions that are ALWAYS performed for a SET TRANSACTION ISOLATION Statement at Execution time.
A SetTransactionIsolationNode is the root of a QueryTree that represents a SET TRANSACTION ISOLATION command
This is a wrapper class which invokes the Execution-time logic for SET TRANSACTION statements. The real Execution-time logic lives inside the executeConstantAction() method of the Execution constant.
creates database and builds single user table with indexes

This class is intended to be used a the qualifier class for ShExLockable.
This class holds a short.  This class exists for basic testing of user-defined types in JSQL.
A ShutdownException is a runtime exception that is used to notify code that the system has/is being shut down.



Test (slave) behaviour after killing the master server.
Test (slave) behaviour after shutdown of the master server via NetworkServerControl.

Test (master) behaviour after shutdown of the slave database.
Test (master) behaviour after shutdown of the slave server.
Test (master) behaviour after killing the slave server.
Test (master) behaviour after shutdown of the slave server via NetworkServerControl.
<p> This class shows which user declared SQL functions and procedures cannot be matched with Java methods. </p> <p> To run from the command-line, enter the following if running on J2SE: </p> <p> <code>java org.apache.derby.tools.SignatureChecker CONNECTION_URL_TO_DATABASE</code> <p> <p> And enter the following if running on J2ME: </p> <p> <code>java org.apache.derby.tools.SignatureChecker DATABASE_NAME</code> <p>
Converters from signed binary bytes to Java <code>short</code>, <code>int</code>, or <code>long</code>.
<p> This sample program is a minimal Java application showing JDBC access to a Derby database.</p> <p> Instructions for how to run this program are given in <A HREF=example.html>example.html</A>, by default located in the same directory as this source file ($DERBY_HOME/demo/programs/simple/).</p> <p> Derby applications can run against Derby running in an embedded or a client/server framework.</p> <p> When Derby runs in an embedded framework, the JDBC application and Derby run in the same Java Virtual Machine (JVM). The application starts up the Derby engine.</p> <p> When Derby runs in a client/server framework, the application runs in a different JVM from Derby. The connectivity framework (in this case the Derby Network Server) provides network connections. The client driver is loaded automatically.</p>

Implement the initial database population according to the TPC-C database population requirements in Clause 4.3 using simple Insert sql statements
<p> OptionalTool which adds support types and functions for using the JSON.simple toolkit at https://code.google.com/p/json-simple/. Creates the following schema objects in the current schema: </p> <ul> <li><b>JSONArray</b> - A UDT bound to the JSON.simple JSONArray class.</li> <li><b>toJSON</b> - A function for packaging up query results as a JSONArray. Each cell in the array is a row. Each row has key/value pairs for all columns returned by the query.</li> <li><b>readArrayFromString</b> - A function which turns a JSON document string into a JSONArray.</li> <li><b>readArrayFromFile</b> - A function which reads a file containing a JSON document and turns it into a JSONArray.</li> <li><b>readArrayFromURL</b> - A function which reads a JSON document from an URL address and turns it into a JSONArray.</li> </ul>
<p> Utility methods for simple JSON support. </p>
<p> This is a table function which turns a JSON array into a relational ResultSet. This table function relies on the JSON.simple JSONArray class found at https://code.google.com/p/json-simple/. Each object in the array is turned into a row. The shape of the row is declared by the CREATE FUNCTION ddl and the shape corresponds to the key names found in the row objects. Provided that the values in those objects have the expected type, the following ResultSet accessors can be called: </p> <ul> <li>getString()</li> <li>getBoolean()</li> <li>getByte()</li> <li>getShort()</li> <li>getInt()</li> <li>getLong()</li> <li>getFloat()</li> <li>getDouble()</li> <li>getObject()</li> <li>getBigDecimal()</li> </ul> <p> This table function relies on the JSONArray type loaded by the simpleJson optional tool. This table function can be combined with other JSONArray-creating functions provided by that tool. </p> <p> Here's an example of how to use this VTI on a JSON document read across the network using the readArrayFromURL function provided by the simpleJson tool: </p> <pre> call syscs_util.syscs_register_tool( 'simpleJson', true ); create function thermostatReadings( jsonDocument JSONArray ) returns table ( "id" int, "temperature" float, "fanOn" boolean ) language java parameter style derby_jdbc_result_set contains sql external name 'org.apache.derby.optional.api.SimpleJsonVTI.readArray'; select * from table ( thermostatReadings ( readArrayFromURL( 'https://thermostat.feed.org', 'UTF-8' ) ) ) t; </pre> <p> That returns a table like this: </p> <pre> id         |temperature             |fanOn ------------------------------------------ 1          |70.3                    |true 2          |65.5                    |false </pre> <p> Here's an example of how to use this VTI on a JSON document string with the assistance of the readArrayFromString function provided by the simpleJson tool: </p> <pre> select * from table ( thermostatReadings ( readArrayFromString ( '[ { "id": 1, "temperature": 70.3, "fanOn": true }, { "id": 2, "temperature": 65.5, "fanOn": false } ]' ) ) ) t; </pre>
The primary purpose of this program is to demonstrate how to obtain client connections using DriverManager or a DataSource and interact with Derby Network Server. In particular,this sample program 1)	obtains a client connection using the DriverManager 2)	obtains a client connection using a DataSource 3)	tests the database connections by executing a sample query and then exits the program Before running this program, please make sure that Derby Network Server is up and running. <P> Usage: java SimpleNetworkClientSample
In order for a database to be consistent, only one JVM is allowed to access it at a time. The embedded driver is loaded when the Network Server is started. Hence, the JVM that starts the Network Server can get an embedded connection to the same database that Network Server is accessing to serve the clients from other JVMs. This solution allows you to take advantage of the performance benefits of the embedded driver as well as allow for client connections from other JVMs to connect to the same database. In particular, this sample program 1) 	starts the Derby Network Server using a property and also loads the embedded driver 2)	checks if the Derby Network Server is up and running 3)	creates the database 'NSSimpleDB' if not already created 4)	obtains an embedded database connection 5)	tests the database connection by executing a sample query 6)	allows for client connections to connect to the server until the user decides to stop the server and exit the program 7)	closes the connections 8)	shuts down the Derby Network Server before exiting the program. Note: On running this program, there will be a NSSimpleDB database directory created if not present already, and there will be a derby.log file which contains messages from Derby. <P> Usage: java SimpleNetworkServerSample
Collection of simple transactions that can be executed against an order-entry database. These are not part of any standard TPC-C specification but are useful for running specific performance tests against Derby. Since they are not standard operations there is no ability to display the information. Any data selected by a query is always fetched by processing all the rows and all the columns using getXXX.
This node represents a unary upper or lower operator
A SingleChildResultSetNode represents a result set with a single child.
Class which generates and populates tables that can be used by {@code SingleRecordSelectClient} and {@code SingleRecordUpdateClient}. This tables contain rows with an int column (id) and a varchar(100) column (text). The id column is declared as primary key.
Client which performs single-record lookups on tables generated by {@code SingleRecordFiller}. Each time the client's {@code doWork()} method is called, it will pick one of the tables randomly, and select one random record in that table.
Client which updates a single record at a time on tables generated by {@code SingleRecordFiller}. Each time the client's {@code doWork()} method is called, it will pick one of the tables randomly, and update the text column of one random record in that table.
The Basic Services provide InfoStreams for reporting information. Two streams are provided: trace and error. It is configurable where these streams are directed. <p> Errors will be printed to the error stream in addition to being sent to the client. <p> By default both streams are sent to an error log for the system. When creating a message for a stream, you can create an initial entry with header information and then append to it as many times as desired. <p> Note: if character encodings are needed, the use of java.io.*OutputStream's should be replaced with java.io.*Writer's (assuming the Writer interface remains stable in JDK1.1)

This class implements the TimerFactory interface. It creates a singleton Timer instance. The class implements the ModuleControl interface, because it needs to cancel the Timer at system shutdown.
This interface extends the Cacheable interface (@see Cacheable) with a method that estimates the size of the Cacheable object, in bytes. CacheManagers constructed with the SizedCacheFactory interface regulate the total estimated cache size, in bytes. CacheManagers constructed with the CacheFactory use regulate the total number of cache entries.
Encapsulates the host name and the port number of the slave machine.
<p> This is an implementation of the replication slave controller service. The service is booted when this instance of Derby will have the replication slave role for this database. </p> <p> Note: The current version of the class is far from complete. Code to control the replication slave behavior will be added as more parts of the replication functionality is added to Derby. </p> ///////////////////////////////////////////////////////// END Inner Class                                       // /////////////////////////////////////////////////////////
SlaveDatabase is an instance of Database, and is booted instead of BasicDatabase if this database will have the replication slave role. SlaveDatabase differs from BasicDatabase in the following ways: 1: When starting a non-replicated database (i.e., BasicDatabase), only one thread is used to start all modules of the database. When booted in slave mode, the thread that boots the store module will be blocked during log recovery. To remedy this, SlaveDatabase runs the boot method of BasicDatabase in a separate thread. This ensures that the connection attempt that started slave replication mode will not hang. 2: While the database is in replication slave mode, the authentication services are not available because these require that the store module has been booted first. Calling getAuthenticationService when in slave mode will raise an exception. 3: While the database is in replication slave mode, connections are not accepted since the database cannot process transaction requests. Calling setupConnection when in slave mode will raise an exception. 4: If the failover command has been executed for this database, it is no longer in replication slave mode. When this has happened, SlaveDatabase works exactly as BasicDatabase.
<p> This is the interface for the replication slave controller service. The slave controller service is booted when this instance of Derby will have the replication slave role for this database. </p> <p> The replication slave service is responsible for managing all replication related functionality on the slave side of replication. This includes connecting to the master and apply log records received from the master. </p>
This class encapsulates a <code>Socket</code> connection and has methods that allow to read and write into the Object streams created from this connection.
The sort interface corresponds to an instance of an in-progress sort. Sorts are not persistent.
This class implements an in-memory ordered set based on the balanced binary tree algorithm from Knuth Vol. 3, Sec. 6.2.3, pp. 451-471. Nodes are always maintained in order, so that inserts and deletes can be intermixed. <P> This algorithm will insert/delete N elements in O(N log(N)) time using O(N) space.
Wrapping the output of a SortBuffer in a RowSource for the benefit of the createAndLoadConglomerate and loadConglomerate interface. Scan implements ScanController, this class just implements the RowSource interface.
A sort scan that just reads rows out of a sorter.
A sort controller is an interface for inserting rows into a sort. <p> A sort is created with the createSort method of TransactionController. The rows are read back with a scan controller returned from the openSortScan method of TranscationController.

The factory interface for all sort access methods.
This object provides performance information related to a sort. The information is accumulated during operations on a SortController() and then copied into this object and returned by a call to SortController.getSortInfo().
A SortObserver is an object that is used as a callback by the sorter.  It allows the sort client to do whatever they want from the context of a sort.  It contains 2 callback methods: <I>insertDuplicateKey()</I> and <I>insertNonDuplicateKey()</I>. On each <I>SortController.insert()</I>, one or the other of these methods will be called, depending on whether the given row has a key that has been seen before or not. <p> Some sample uses include: <UL><LI> <I>Sorts from Language</I>: Language typically recycles data type wrappers.  So the language layer uses SortObservers to clone rows that are kept by the sorter. </LI> <LI> <I>Distinct sorts</I>: The sorter will call the sort observer each time it identifies a duplicate row.  Based on what the sort observer returns to the sorter, the sorter will either retain (insert) the duplicate row, or discard the duplicate row.  All you have to do to implement a distinct sort is to tell the sorter to discard the row (return null from <I> insertDuplicateKey()</I>).  Also, if you want to throw an exception on a duplicate (e.g. create a unique index), you can just throw an exception from your SortObserver. </LI> <LI> <I>Aggregates</I>: Vector (grouped) aggregates typically require a sort.  Language can use a SortObserver to perform aggregations as duplicate elements are encountered.  Scalar aggregates can also be computed using a SortObserver. </LI> </UL> These are possible uses only.  You, kind reader, may do whatever you wish with this forgiving interface.
Takes a source result set, sends it to the sorter, and returns the results.  If distinct is true, removes all but one copy of duplicate rows using DistinctAggregator, which really doesn't aggregate anything at all -- the sorter assumes that the presence of an aggregator means that it should return a single row for each set with identical ordering columns. <p> If aggregate is true, then it feeds any number of aggregates to the sorter.  Each aggregate is an instance of GenericAggregator which knows which Aggregator to call to perform the aggregation. <p> Brief background on the sorter and aggregates: the sorter has some rudimentary knowledge about aggregates.  If it is passed aggregates, it will eliminate duplicates on the ordering columns.  In the process it will call the aggregator on each row that is discarded. <p> Note that a DISTINCT on the SELECT list and an aggregate cannot be processed by the same SortResultSet(), if there are both aggregates (distinct or otherwise) and a DISTINCT on the select list, then 2 separate SortResultSets are required (the DISTINCT is a sort on the output of the sort with the aggregation). <p> Currently, all rows are fed through the sorter.  This is true even if there is no sorting needed.  In this case we feed every row in and just pull every row out (this is an obvious area for a performance improvement).  We'll need to know if the rows are sorted before we can make any optimizations in this area. <p> <B>CLONING</B>: Cloning and sorts are an important topic. Currently we do a lot of cloning.  We clone the following: <UL> <LI> every row that is inserted into the sorter.  We need to clone the rows because the source result set might be reusing rows, and we need to be able to accumulate the entire result set in the sorter. </LI> <p> There are two cloning APIs: cloning by the sorter on rows that are not discarded as duplicates or cloning in the SortResultSet prior to inserting into the sorter. If we have any aggregates at all we always clone prior to inserting into the sorter.  We need to do this because we have to set up the aggregators before passing them into the sorter.  When we don't have aggregates we let the sorter to the cloning to avoid unnecessary clones on duplicate rows that are going to be discarded anyway.
Abstract base class for merge sort scans. Methods of SortScan.  Arranged alphabetically.
Manage the result information from a single call to ConglomerateController.getSpaceInfo(). <p>
Manage the result information from a single call to ConglomerateController.getSpaceInfo(). <p>
SpaceTable is a virtual table that shows the space usage of a particular table and its indexes. This virtual table can be invoked by calling it directly, and supplying the schema name and table name as arguments. <PRE> SELECT * FROM TABLE(SYSCS_DIAG.SPACE_TABLE('MYSCHEMA', 'MYTABLE')) T;  </PRE> If the schema name is not supplied, the default schema is used. <PRE> SELECT * FROM TABLE(SYSCS_DIAG.SPACE_TABLE('MYTABLE')) T; </PRE> <P> NOTE: Both the schema name and the table name must be any expression that evaluates to a string data type. If you created a schema or table name as a non-delimited identifier, you must present their names in all upper case. <P>The SpaceTable virtual table can be used to estimate whether space might be saved by compressing a table and its indexes. <P>The SpaceTable virtual table has the following columns: <UL> <LI>CONGLOMERATENAME varchar(128) - nullable.  The name of the conglomerate, which is either the table name or the index name. (Unlike the SYSCONGLOMERATES column of the same name, table ID's do not appear here).</LI> <LI>ISINDEX SMALLINT - not nullable.  Is not zero if the conglomerate is an index, 0 otherwise.</LI> <LI>NUMALLOCATEDPAGES bigint - not nullable.  The number of pages actively linked into the table.  The total number of pages in the file is the sum of NUMALLOCATEDPAGES + NUMFREEPAGES.</LI> <LI>NUMFREEPAGES bigint - not nullable. The number of free pages that belong to the table.  When a new page is to be linked into the table the system will move a page from the NUMFREEPAGES list to the NUMALLOCATEDPAGES list.  The total number of pages in the file is the sum of NUMALLOCATEDPAGES + NUMFREEPAGES.</LI> <LI>NUMUNFILLEDPAGES bigint - not nullable.  The number of unfilled pages that belong to the table. Unfilled pages are allocated pages that are not completely full. Note that the number of unfilled pages is an estimate and is not exact. Running the same query twice can give different results on this column. </LI> <LI>PAGESIZE integer - not nullable.  The size of the page in bytes for that conglomerate. </LI> <LI>ESTIMSPACESAVING bigint - not nullable.  The estimated space which could possibly be saved by compressing the conglomerate, in bytes.</LI> <LI>TABLEID char(36) - not nullable.  The UUID of the table.</LI> </UL> <P> To get space information on all schemas and tables, use a query such as <PRE> select v.* from SYS.SYSSCHEMAS s, SYS.SYSTABLES t, TABLE(SYSCS_DIAG.SPACE_TABLE(SCHEMANAME, TABLENAME)) v where s.SCHEMAID = t.SCHEMAID; </PRE>
Utility code that wraps a spawned process (Java Process object). <p> There are three main aspects handled by this class: <ul> <li>Draining the output streams of the process.<br/> Happens automatically, the output gathered can be accessed with {@linkplain #getFailMessage}, {@linkplain #getFullServerError}, {@linkplain #getFullServerOutput}, and {@linkplain #getNextServerOutput}</li> <li>Waiting for process completion, followed by cleanup (see {@linkplain #complete()} and {@linkplain #complete(long)})</li> <li>Forcibly destroying a process that live too long, for instance if inter-process communication hangs. This happens automatically if a threshold value is exceeded.</li> </ul> <p> <em>Implementation notes</em>: Active waiting is employed when waiting for the process to complete. This is considered acceptable since the expected usage pattern is to spawn the process, execute a set of tests, and then finally asking the process to shut down. Waiting for the process to complete is the last step, and a process typically lives only for a short period of time anyway (often only for seconds, seldom more than a few minutes). <br/> Forcibly destroying processes that live too long makes the test run continue even when facing inter-process communication hangs. The prime example is when both the client and the server are waiting for the other party to send data. Since the timeout is very high this feature is intended to avoid automated test runs from hanging indefinitely, for instance due to environmental issues affecting the process. @NotThreadSafe
Parse testJavaFlags for RunTest These are special properties that might be set for a suite or test, and they can be either ij properties or server properties which is why they need to be parsed
SpecialFunctionNode handles system SQL functions. A function value is either obtained by a method call off the LanguageConnectionContext or Activation. LanguageConnectionContext functions are state related to the connection. Activation functions are those related to the statement execution. Each SQL function takes no arguments and returns a SQLvalue. <P> Functions supported: <UL> <LI> USER <LI> CURRENT_USER <LI> CURRENT_ROLE <LI> SESSION_USER <LI> SYSTEM_USER <LI> CURRENT SCHEMA <LI> CURRENT ISOLATION <LI> IDENTITY_VAL_LOCAL </UL> <P> This node is used rather than some use of MethodCallNode for runtime performance. MethodCallNode does not provide a fast access to the current language connection or activation, since it is geared towards user defined routines.
This authentication service is a specific/user defined User authentication level support. <p> It calls the specific User authentication scheme defined by the user/ administrator.
This class is for strong-typing.  Dnc architected codes in the range +/- 4200 to 4299, plus one additional code for -4499.  SQL codes are architected by the product that issues them.
An intermediate exception encapsulation to provide code-reuse for common ResultSet data conversion exceptions. An intermediate exception encapsulation to provide code-reuse for common CrossConverters data conversion exceptions. The signature of the stored procedure SQLCAMessage I have come out so far is as follows: SQLCAMessage ( IN  SQLCode       INTEGER, IN  SQLErrml      SMALLINT, IN  SQLErrmc      VARCHAR(70), IN  SQLErrp       CHAR(8), IN  SQLErrd0      INTEGER, IN  SQLErrd1      INTEGER, IN  SQLErrd2      INTEGER, IN  SQLErrd3      INTEGER, IN  SQLErrd4      INTEGER, IN  SQLErrd5      INTEGER, IN  SQLWarn       CHAR(11), IN  SQLState      CHAR(5), IN  Locale        CHAR(5), IN  BufferSize    SMALLINT, IN  LineWidth     SMALLINT, OUT Message       VARCHAR(2400))  Some issues have been identified: 1. What would be the schema name of the stored procedue SQLCAMessage? 2. What is the format and type of the Locale parameter? If there does, I would really like to know the format of the locale in order to decide the type of the Locale parameter. Even there does not either, the Locale parameter probably still needs to be kept there for future extension, and we need to figure out the format of the locale. 3. What would be the format of the output message? Is this full message text ok or do we only need the explanation message corresponding to an SQL code. This somehow matters whether we need the Buffersize and Linewidth parameters for the stored procedure. 4. What if the invocation of stored procedure failed (due to, eg, connection dropping)? In this case, we probably need to return some client-side message.  Note that this class does NOT extend java.sql.SQLException.  This is because in JDBC 4 there will be multiple subclasses of SQLException defined by the spec.  So we can't also extend SQLException without having to create our own mirror hierarchy of subclasses.  When Derby is ready to throw an exception to the application, it catches SqlException and converts it to a java.sql.SQLException by calling the method getSQLException.  It is also possible that internal routines may call public methods. In these cases, it will need to wrap a java.sql.SQLException inside a Derby SqlException so that the internal method does not have to throw java.sql.SQLException.  Otherwise the chain of dependencies would quickly force the majority of internal methods to throw java.sql.SQLException. You can wrap a java.sql.SQLException inside a SqlException by using the constructor <code>new SqlException(java.sql.SQLException wrapMe)</code)
This represents a warning versus a full exception.  As with SqlException, this is an internal representation of java.sql.SQLWarning. Public JDBC methods need to convert an internal SqlWarning to a SQLWarning using <code>getSQLWarning()</code>
This class contains "utility" methods that work with XML-specific objects that are only available if JAXP and/or Xalan are in the classpath. NOTE: This class is only compiled with JDK 1.4 and higher since the XML-related classes that it uses (JAXP and Xalan) are not part of earlier JDKs. Having a separate class for this functionality is beneficial for two reasons: 1. Allows us to allocate XML objects and compile an XML query expression a single time per statement, instead of having to do it for every row against which the query is evaluated.  An instance of this class is created at compile time and then passed to the appropriate operator implementation method in XML.java. 2. By keeping all XML-specific references in this one class, we have a single "point of entry" to the XML objects--namely, the constructor for this class.  Thus, if we always make sure to check for the required XML classes _before_ calling this class's constructor, we can detect early on whether some classes (ex. Xalan) are missing, and can throw a friendly error up front, instead of a ClassNotFoundException somewhere deeper in the execution codepath.  The initial check for the required XML classes can be found in XML.checkXMLRequirements(). Note that we don't want to put references to XML-specific objects directly into XML.java because that class (XML.java) is instantiated anytime a table with an XML column is referenced. That would mean that if a user tried to select a non-XML column (ex. integer) from a table that had at least one XML column in it, the user would have to have JAXP and Xalan classes in his/her classpath--which we don't want.  Instead, by keeping all XML-specific objects in this one class, and then only instantiating this class when an XML operator is used (either implicitly or explicitly), we make it so that the user is only required to have XML-specific classes in his/her classpath _if_ s/he is trying to access or operate on XML values.


Implement the transactions following the TPC-C specification using client side prepared statements. Thus all the logic is contained within this class. The client, through this object, holds onto PreparedStatements for all the SQL for its lifetime. <P> This standard implementation is based upon the sample programs in the appendix of the TPC-C specification. <P> More specific direct (client side) implementations could extend this class overriding methods as needed. <P> Object is single threaded so it re-uses objects where possible to avoid the garbage collection due to the application affecting the results too much since the purpose of the framework is to test Derby's performance.
Implementation of InputStream which get EXTDTA from the DDMReader. This class can be used to stream LOBs from Network client to the Network server.
StandardException is the root of all exceptions that are handled in a standard fashion by the database code, mainly in the language code. <P> This class is abstract to ensure that an implementation only throws a specific exception (e.g. TransactionException) which is a sub-class <P> A method in an iterface in a protocol under com.ibm.db2j.protocol.Database must only throw a StandardException (if it needs to throw an exception). This indicates that the method can throw an exception and therefore its caller must ensure that any resources it allocates will be cleaned up in the event of an exception in the StandardException hierarchy. <P> Implementations of methods that throw StandardException can have throws clause that are more specific than StandardException.


Utilities for parsing runtimestats RESOLVE: This class should be internationalized.
The Statement interface provides a way of giving a statement to the language module, preparing the statement, and executing it. It also provides some support for stored statements. Simple, non-stored, non-parameterized statements can be executed with the execute() method. Parameterized statements must use prepare(). To get the stored query plan for a statement, use get(). <p> This interface will have different implementations for the execution-only and compile-and-execute versions of the product. In the execution-only version, some of the methods will do nothing but raise exceptions to indicate that they are not implemented. <p> There is a Statement factory in the Connection interface in the Database module, which uses the one provided in LanguageFactory.
StatementCache is a virtual table that shows the contents of the SQL statement cache. This virtual table can be invoked by calling it directly. <PRE> select * from new org.apache.derby.diag.StatementCache() t</PRE> <P>The StatementCache virtual table has the following columns: <UL> <LI> ID CHAR(36) - not nullable.  Internal identifier of the compiled statement. <LI> SCHEMANAME VARCHAR(128) - nullable.  Schema the statement was compiled in. <LI> SQL_TEXT VARCHAR(32672) - not nullable.  Text of the statement <LI> UNICODE BIT/BOOLEAN - not nullable.  Always true. <LI> VALID BIT/BOOLEAN - not nullable.  True if the statement is currently valid, false otherwise <LI> COMPILED_AT TIMESTAMP nullable - time statement was compiled, requires STATISTICS TIMING to be enabled. </UL> <P> The internal identifier of a cached statement matches the toString() method of a PreparedStatement object for a Derby database. <P> This class also provides a static method to empty the statement cache, StatementCache.emptyCache()
Utility class encapsulating the logic for interacting with the JDBC statement cache when creating new logical statements. <p> This class was introduced to share code between the pre-JDBC 4 and the JDBC 4+ versions of the JDBC classes. <p> The pattern for the {@code prepareX} methods is: <ol> <li>Generate a key for the statement to create.</li> <li>Consult cache to see if an existing statement can be used.</li> <li>Create new statement on physical connection if necessary.</li> <li>Return reference to existing or newly created statement.</li> </ol>
Methods implemented by the common Statement class to handle certain events that may originate from the material or common layers.  Reply implementations may update statement state via this interface.
This class describes a column permission used (required) by a statement.
StatementContext keeps the context for a statement.
StatementDuration is a virtual table which can be used to analyze the execution duration of the statements of "interest" in db2j.<!-- -->log or a specified file when db2j.<!-- -->language.<!-- -->logStatementText=true. <P>A limitation is that, for each transaction ID, a row will not be returned for the last	statement with that transaction id.  (Transaction IDs change within a connection after a commit or rollback, if the transaction that just ended modified data.) <P>The execution duration is the time between the beginning of execution of two successive statements.  There are a number of reasons why this time may not be accurate.  The duration could include time spent in the application waiting for user input, doing other work, etc. It may also only include a portion of the actual execution time, if the application executes a new statement before draining the previous open ResultSet.  StatementDuration can be used to get a rough sense of where the bottlenecks in an application's JDBC code are. <P>The StatementDuration virtual table has the following columns: <UL><LI>TS varchar(26) - not nullable.  The timestamp of the statement.</LI> <LI>THREADID varchar(80) - not nullable.  The thread name.</LI> <LI>XID varchar(15) - not nullable.  The transaction ID.</LI> <LI>LOGTEXT long varchar - nullable.  Text of the statement or commit or rollback.</LI> <LI>DURATION varchar(10) - not nullable.  Duration, in milliseconds, of the statement.</LI> </UL>
StatementGrabber looks through an input stream for the next JSQL statement.  A statement is considered to be any tokens up to the next semicolon or EOF. <p> Semicolons inside comments, strings, and delimited identifiers are not considered to be statement terminators but to be part of those tokens. <p> Comments currently recognized include the SQL comment, which begins with "--" and ends at the next EOL, and nested bracketed comments. <p> Strings and delimited identifiers are permitted to contain newlines; the actual IJ or JSQL parsers will report errors when those cases occur. <p> There are no escaped characters, i.e. "\n" is considered to be two characters, '\' and 'n'.
This class describes a generic permission (such as USAGE) required by a statement.
Utility class for direct operations. Maintains the connection and a hash table of PreparedStatements to simplify management of PreparedStatement objects by the business transactions.
A key representing a <code>java.sql.PreparedStatement</code> or a <code>java.sql.CallableStatement</code>. <p> The key takes a number of statement related attributes into account, and is used to insert and look up cached statement objects in the JDBC statement cache. <p> Key instances are created by a statement key factory. @Immutable
A factory for creating JDBC statement keys for use with the JDBC statement cache. @ThreadSafe
A StatementNode represents a single statement in the language.  It is the top node for any statement. <p> StatementNode controls the class generation for query tree nodes. History: 5/8/97	Rick Hilleags	Moved node-name-string to child classes.
This class describes a permission require by a statement.


In general, all required data is passed.
This class describes a role permission required by a statement.
This class describes a routine execute permission required by a statement.
This class describes a schema permission required by a statement.
This class describes a table permission required by a statement.
A statement trigger executor is an object that executes a statement trigger.  It is instantiated at execution time.  There is one per statement trigger.
Different types of statements
Utilities for dealing with statements.
This class has all the SQL statements use for the test
A StaticClassFieldReferenceNode represents a Java static field reference from a Class (as opposed to an Object).  Field references can be made in DML (as expressions).
Information that can be "compiled" once and then used over and over again at execution time.  This information is read only by both the caller and the user, thus can be shared by multiple threads/transactions once created. This information is obtained from the getStaticCompiledConglomInfo(conglomid) method call.  It can then be used in openConglomerate() and openScan() calls for increased performance.  The information is only valid until the next ddl operation is performed on the conglomerate.  It is up to the caller to provide an invalidation methodology. The static info would be valid until any ddl was executed on the conglomid, and would be up to the caller to throw away when that happened.  This ties in with what language already does for other invalidation of static info.  The type of info in this would be containerid and array of format id's from which templates can be created.  The info in this object is read only and can be shared among as many threads as necessary.
A StaticMethodCallNode represents a static method call from a Class (as opposed to from an Object). For a procedure the call requires that the arguments be ? parameters. The parameter is *logically* passed into the method call a number of different ways. <P> For a application call like CALL MYPROC(?) the logically Java method call is (in psuedo Java/SQL code) (examples with CHAR(10) parameter) <BR> Fixed length IN parameters - com.acme.MyProcedureMethod(?) <BR> Variable length IN parameters - com.acme.MyProcedureMethod(CAST (? AS CHAR(10)) <BR> Fixed length INOUT parameter - String[] holder = new String[] {?}; com.acme.MyProcedureMethod(holder); ? = holder[0] <BR> Variable length INOUT parameter - String[] holder = new String[] {CAST (? AS CHAR(10)}; com.acme.MyProcedureMethod(holder); ? = CAST (holder[0] AS CHAR(10)) <BR> Fixed length OUT parameter - String[] holder = new String[1]; com.acme.MyProcedureMethod(holder); ? = holder[0] <BR> Variable length INOUT parameter - String[] holder = new String[1]; com.acme.MyProcedureMethod(holder); ? = CAST (holder[0] AS CHAR(10)) <P> For static method calls there is no pre-definition of an IN or INOUT parameter, so a call to CallableStatement.registerOutParameter() makes the parameter an INOUT parameter, provided: - the parameter is passed directly to the method call (no casts or expressions). - the method's parameter type is a Java array type. Since this is a dynamic decision we compile in code to take both paths, based upon a boolean is INOUT which is derived from the ParameterValueSet. Code is logically (only single parameter String[] shown here). Note, no casts can exist here. boolean isINOUT = getParameterValueSet().getParameterMode(0) == PARAMETER_IN_OUT; if (isINOUT) { String[] holder = new String[] {?}; com.acme.MyProcedureMethod(holder); ? = holder[0] } else { com.acme.MyProcedureMethod(?) }
Class StaticValues: A location to store all the common static values used in this test
<P> This interface is used in the column SYS.SYSSTATISTICS.STATISTICS. It encapsulates information collected by the UPDATE STATISTICS command and is used internally by the Derby optimizer to estimate cost and selectivity of different query plans. <p>
Implementation of StatisticsDescriptor.

<p> This class implements the SQL Standard STDDEV_POP() aggregator, computing a population's standard deviation. It relies on VarPAggregator to compute the population variance and then applies the textbook definition of standard deviation: </p> <blockquote><pre><b> &radic;<span style="text-decoration:overline">var_pop()</span> </b></pre></blockquote>
<p> This class implements the SQL Standard STDDEV_SAMP() aggregator, computing a sample's standard deviation. It relies on VarSAggregator to compute the sample's variance and then applies the textbook definition of standard deviation: </p> <blockquote><pre><b> &radic;<span style="text-decoration:overline">var_samp()</span> </b></pre></blockquote>


Formatable for holding SQL data (which may be null).

The Statement interface is an extension of exec prepared statement that has some stored prepared specifics.
This interface provides basic storage functions needed for read only databases. Most storage implementations will be read-write and implement the WritableStorageFactory extension of this interface. <p> The database engine uses this interface to access storage. The normal database engine implements this interface using disk files and the standard java.io classes. <p> The storage factory must implement writable temporary files, even if the database is read-only or if the storage factory is read-only (i.e. it does not implement the WritableStorageFactory extension of this interface). Temporary files are those created under the temporary file directory. See {@link #getTempDir method getTempDir()}. <p>The database engine can be turned into a RAM based engine by providing a RAM based implementation of this interface. <p>There is one instance of the StorageFactory per database if the log files are kept in the database directory. If the log files are kept on a separate device then a second StorageFactory is instantiated to hold the log files. The database or log device name is set when the init method is called. The init method is called once per instance, before any other StorageFactory method. <p>The class implementing this interface must have a public niladic constructor. The init method will be called before any other method to set the database directory name, to tell the factory to create the database directory if necessary, and to allow the implementation to perform any initializations it requires. The database name set in the init method forms a separate name space. Different StorageFactory instances, with different database directory names, must ensure that their files do not clash. So, for instance, storageFactory1.newStorageFile( "x") must be a separate file from storageFactory2.newStorageFile( "x"). <p>The database engine will call this interface's methods from its own privilege blocks. This does not give a StorageFactory implementation carte blanche: a security manager can still forbid the implemeting class from executing a privileged action. However, the security manager will not look in the calling stack beyond the database engine. <p>Each StorageFactory instance may be concurrently used by multiple threads. Each StorageFactory implementation must be thread safe. <p>A StorageFactory implementation is plugged into the database engine via a sub-protocol. Sub-protocol <i>xxx</i> is tied to a StorageFactory implementation class via the derby.subSubProtocol.<i>xxx</i> system property. So, to use StorageFactory implementation class MyStorageFactory with database myDB you would set the system property "derby.subSubProtocol.mysf=MyStorageFactory" and use the URL "jdbc:derby:mysf:myDB" to connect to the database.
This class implements the PersistentService interface using a StorageFactory class. It handles all subSubProtocols except for cache.
This interface abstracts file naming. Any method in this interface that also appears in the java.io.File class should behave as the java.io.File method does. <p> When used by the database engine all files will be under either the database directory, specified by the databaseName argument of the {@link StorageFactory#init StorageFactory.init} method, or under the temporary file directory returned by the {@link StorageFactory#getTempDir StorageFactory.getTempDir} method. All relative path names are relative to the database directory. <p> The database engine will call this interface's methods from its own privilege blocks. <p> Different threads may operate on the same underlying file at the same time, either through the same or different StorageFile objects. The StiFile implementation must be capable of handling this. <p>
This interface abstracts an object that implements reading and writing on a random access file. It extends DataInput and DataOutput, so it implicitly contains all the methods of those interfaces. Any method in this interface that also appears in the java.io.RandomAccessFile class should behave as the java.io.RandomAccessFile method does. <p> Each StorageRandomAccessFile has an associated file pointer, a byte offset in the file. All reading and writing takes place at the file pointer offset and advances it. <p> An implementation of StorageRandomAccessFile need not be thread safe. The database engine single-threads access to each StorageRandomAccessFile instance. Two threads will not access the same StorageRandomAccessFile instance at the same time. <p>

The StoreCostController interface provides methods that an access client (most likely the system optimizer) can use to get store's estimated cost of various operations on the conglomerate the StoreCostController was opened for. <p> It is likely that the implementation of StoreCostController will open the conglomerate and will leave the conglomerate open until the StoreCostController is closed.  This represents a significant amount of work, so the caller if possible should attempt to open the StoreCostController once per unit of work and rather than close and reopen the controller.  For instance if the optimizer needs to cost 2 different scans against a single conglomerate, it should use one instance of the StoreCostController. <p> The locking behavior of the implementation of a StoreCostController is undefined, it may or may not get locks on the underlying conglomerate.  It may or may not hold locks until end of transaction. An optimal implementation will not get any locks on the underlying conglomerate, thus allowing concurrent access to the table by a executing query while another query is optimizing. <p>
Manage the result information from a single call to StoreCostController.getScanCost(). <p>
A read-only Clob representation operating on streams out of the Derby store module. <p> Note that the streams from the store are expected to have the following properties: <ol> <li>The first few bytes are used for length encoding. Currently the number of bytes is either 2 or 5. <li>A Derby-specific end-of-stream marker at the end of the stream can be present. The marker is expected to be <code>0xe0 0x00 0x00</code> </ol> End class StoreStreamClob
A class to provide static methods to manipulate fields in the field header. A class StoredPage uses to read/write field status and field data length. No attributes exist in this class, this class provides a set of static methods for writing field status and field data length, and for reading field status and field data length. <P><B>Stored Field Header Format</B><BR> The field header is broken into two sections. Only the Status byte is required to be there. <PRE> Field header format: +--------+-------------------+ | status | <fieldDataLength> | +--------+-------------------+ Overflow page and overflow id are stored as field data. If the overflow bit in status is set, the field data is the overflow information.  When the overflow bit is not set in status, then, fieldData is the actually user data for the field. That means, field header consists only field status, and field data length. A non-overflow field: +--------+-------------------+-------------+ | status | <fieldDataLength> | <fieldData> | +--------+-------------------+-------------+ An overflow field: +--------+-------------------+-----------------+--------------+ | status | <fieldDataLength> | <overflow page> | <overflowID> | +--------+-------------------+-----------------+--------------+ </PRE> <BR><B>status</B><BR> The status is 1 byte, it indicates the state of the field. A FieldHeader can be in the following states: NULL		- if the field is NULL, no field data length is stored OVERFLOW	- indicates the field has been overflowed to another page. overflow page and overflow ID is stored at the end of the user data. field data length must be a number greater or equal to 0, indicating the length of the field that is stored on the current page. The format looks like this: +--------+-----------------+---------------+------------+ |<status>|<fieldDataLength>|<overflow page>|<overflowID>| +--------+-----------------+---------------+------------+ overflowPage will be written as compressed long, overflowId will be written as compressed Int NONEXISTENT	- the field no longer exists, e.g. column has been dropped during an alter table EXTENSIBLE	- the field is of user defined data type. The field may be tagged. TAGGED		- the field is TAGGED if and only if it is EXTENSIBLE. FIXED		- the field is FIXED if and only if it is used in the log records for version 1.2 and higher. <BR><B>fieldDataLength</B><BR> The fieldDataLength is only set if the field is not NULL.  It is the length of the field that is stored on the current page. The fieldDataLength is a variable length CompressedInt. <BR><B>overflowPage and overflowID</B><BR> The overflowPage is a variable length CompressedLong, overflowID is a variable Length CompressedInt. They are only stored when the field state is OVERFLOW. And they are not stored in the field header. Instead, they are stored at the end of the field data. The reason we do that is to save a copy if the field has to overflow. <BR> MT - Mutable - Immutable identity - Thread Aware
A format id identifies a stored form of an object for the purposes of locating a class which can read the stored form and reconstruct the object using the java.io.Externalizable interface. <P>An important aspect of the format id concept is that it does not impose an implementation on the stored object. Rather, multiple implementations of an object (or interface) may share a format id. One implementation may store (write) an object and another may restore (read) the object. The implication of this is that a format id specifies the following properties of a stored object. <UL> <LI>The interface(s) the stored object must support. Any implementation which reads the object must support these interfaces. <LI>The format of the stored object. All implementations which support the format must be able to read and write it. </UL> <P>An object should implement the Formatable interface to support a stored format. In addition, the module which contains the object should register the object's class with the Monitor (See FormatIdUtil.register.) <P>When you add a format id to this file, please include the list of interfaces an implementation must support when it supports the format id. When Derby code reads a stored form it returns an object of a Class which supports the stored form. A reader may cast this object to any interface listed in this file. It is an error for the reader to cast the object to a class or interface not listed in this file. <P>When you implement a class that supports a format, add a comment that states the name of the class. The first implementation of a format defines the stored form. <P>This interface defines all the format ids for Derby. If you define a format id please be sure to declare it in this file. If you remove support for a one please document that the format id is deprecated. Never remove or re-use a format id.
StoredPage is a sub class of CachedPage that stores page data in a fixed size byte array and is designed to be written out to a file through a DataInput/DataOutput interface. A StoredPage can exist in its clean or dirty state without the FileContainer it was created from being in memory. <P><B>Page Format</B><BR> The page is broken into five sections <PRE> +----------+-------------+-------------------+-------------------+----------+ | formatId | page header | records           | slot offset table | checksum | +----------+-------------+-------------------+-------------------+----------+ </PRE> <BR><B>FormatId</B><BR> The formatId is a 4 bytes array, it contains the format Id of this page. <BR><B>Page Header</B><BR> The page header is a fixed size, 56 bytes <PRE> 1 byte  boolean           is page an overflow page 1 byte  byte              page status (a field maintained in base page) 8 bytes long              pageVersion (a field maintained in base page) 2 bytes unsigned short    number of slots in slot offset table 4 bytes integer           next record identifier 4 bytes integer           generation number of this page (Future Use) 4 bytes integer           previous generation of this page (Future Use) 8 bytes bipLocation       the location of the beforeimage page (Future Use) 2 bytes unsigned short    number of deleted rows on page. (new release 2.0) 2 bytes unsigned short    % of the page to keep free for updates 2 bytes short             spare for future use 4 bytes long              spare for future use (encryption uses to write random bytes here). 8 bytes long              spare for future use 8 bytes long              spare for future use </PRE> Note that spare space has been guaranteed to be writen with "0", so that future use of field should not either not use "0" as a valid data item or pick 0 as a valid default value so that on the fly upgrade can assume that 0 means field was never assigned. <BR><B>Records</B> The records section contains zero or more records, the format of each record follows. minimumRecordSize is the minimum user record size, excluding the space we use for the record header and field headers.  When a record is inserted, it is stored in a space at least as large as the sum of the minimumRecordSize and total header size. For example, If minimumRecordSize is 10 bytes, the user record is 7 bytes, we used 5 bytes for record and field headers, this record will take (10 + 5) bytes of space, extra 3 bytes is put into reserve. If minimumRecordSize is 10 bytes, user record is 17 bytes, we used 5 bytes for record and field headers, this record will take (17 + 5) bytes of space, no reserve space here. minimumRecordSize is defined by user on per container basis. The default for minimumRecordSize is set to 1. This implementation always keeps occupied bytes at the low end of the record section.  Thus removing (purging) a record moves all other records down, and their slots are also moved down. A page has no empty slot (an empty page has no slot) <BR><B>Record and Field Format</B> Record Header format is defined in the StoredRecordHeader class. <PRE> <BR><B>Fields</B> 1 byte    Boolean - is null, if true no more data follows. 4 bytes   Integer - length of field that follows (excludes these four bytes). StoredPage will use the static method provided by StoredFieldHeader to read/write field status and field data length. Field Header format is defined in the StoredFieldHeader class. <data> </PRE> <BR><B>Slot Offset Table</B><BR> The slot offset table is a table of 6 or 12 bytes per record, depending on the pageSize being less or greater than 64K: 2 bytes (unsigned short) or 4 bytes (int) page offset for the record that is assigned to the slot, and 2 bytes (unsigned short) or 4 bytes (int) for the length of the record on this page. 2 bytes (unsigned short) or 4 bytes (int) for the length of the reserved number of bytes for this record on this page. First slot is slot 0.  The slot table grows backwards. Slots are never left empty. <BR><B>Checksum</B><BR> 8 bytes of a java.util.zip.CRC32 checksum of the entire's page contents without the 8 bytes representing the checksum. <P><B>Page Access</B> The page data is accessed in this class by one of three methods. <OL> <LI>As a byte array using pageData (field in cachedPage). This is the fastest. <LI>As an ArrayInputStream (rawDataIn) and ArrayOutputStream (rawDataOut), this is used to set limits on any one reading the page logically. <LI>Logically through rawDataIn (ArrayInputStream) and logicalDataOut (FormatIdOutputStream), this provides the methods to write logical data (e.g. booleans and integers etc.) and the ObjectInput and ObjectOutput interfaces for DataValueDescriptor's. These logical streams are constructed using the array streams. </OL>
A class StoredPage uses to cache record headers by passing instances to BasePage, and to write stored versions of record headers. Format <PRE> 1 byte          - status compressed int  - record identifier compressed long - overflow page } only if hasOverflow() is true compressed int  - overflow id   }     "        "           " compressed int  - first field   } only if hasFirstField set - otherwise 0 compressed int  - number of fields in this portion - only if hasOverflow() is false OR hasFirstField is true - otherwise 0 </PRE>
Database implementation that drives the storeless engine as a Database service with no store.
PersistentService for the storeless engine. Type is 'storeless' which will correspond to the JDBC URL 'jdbc:derby:storeless'.
A Stream Container handle
The format of this stream file is: (RH) (FH) (field data) (FH) (field data) ........ (FH) (field data) Record header is stored once at the beginning of the file for all the rows stored in this file. Record Header indicates how many fields are in each row. Then we just stored all the column from each row. Field header stored on this file is fixed size with fieldDataLength size set to LARGE_SLOT_SIZE (4) bytes. NOTE: No locks are used in this container.  All transaction are not logged.
A handle to an open stream container, implememts StreamContainerHandle. <P> This class is an Observer to observe RawTransactions <BR> MT - Mutable - Immutable identity - Thread Aware
Generates stream headers encoding the length of the stream.
LogScan provides methods to read a log record and get its LogInstant in an already defined scan.  A logscan also needs to know how to advance to the next log record.
Streaming interface for a data value. The format of the stream is data type dependent and represents the on-disk format of the value. That is it is different to the value an application will see through JDBC with methods like getBinaryStream and getAsciiStream. <BR> If the value is NULL (DataValueDescriptor.isNull returns true then these methods should not be used to get the value.
This test runs the StressMultiTest with 10 threads for 1 minute.
This test runs the StressMultiTest with 50 threads for 59 minutes.
<p> This is a concrete VTI which is prepopulated with rows which are just arrays of string columns. </p>
<p> This is an abstract table function which assumes that all columns are strings and which coerces the strings to reasonable values for various getXXX() methods. Subclasses must implement the following ResultSet methods: </p> <ul> <li>next( )</li> <li>close()</li> </ul> <p> and the following protected method introduced by this class: </p> <ul> <li>getRawColumn( int columnNumber )</li> </ul>


A set of public static methods for dealing with Strings
* Sttest.java * 'Sttest' is short for 'single table test.' Sttest.java supplies * the main entry point and the top level code for controlling the * actions of the test, including the ddl for the table and indexes. * The purpose of the test is to exercise the store code by running * updates on the single table for an indefinitely long time, with * an indefinitely large number of user connections, each randomly * executing one of a small number of update procedures with random * data. The test sets a minimum and maximum number of rows, builds * the table up to the minimum number of rows, and from that point * either gradually grows the table to the max size, or gradually * shrinks it to the min size. Periodically memory use is reported, * and the table is compressed, to keep performance from deteriorating.
This interface is used to get information from a SubCheckConstraintDescriptor. A SubCheckConstraintDescriptor is used within the DataDictionary to get auxiliary constraint information from the system table that is auxiliary to sysconstraints.
This class is for testing whether methods in sub-classes are found.
This interface is used to get information from a SubConstraintDescriptor. A SubKeyConstraintDescriptor is used within the DataDictionary to get auxiliary constraint information from the system table that is auxiliary to sysconstraints.

This interface is used to get information from a SubKeyConstraintDescriptor. A SubKeyConstraintDescriptor is used within the DataDictionary to get auxiliary constraint information from the system table that is auxiliary to sysconstraints.
This class is for testing whether methods in sub-classes are found.
Class that submits Order Entry transactions to a database through an instance of an Operations class. This class is responsible for the mix of transactions and the generation of the random input values. Sub-classes can override the mix.
A SubqueryList represents a list of subqueries within a specific clause (select, where or having) in a DML statement.  It extends QueryTreeNodeVector.
A SubqueryNode represents a subquery.  Subqueries return values to their outer queries. An quantified subquery is one that appears under a quantified operator (like IN or EXISTS) - quantified subqueries can return more than one value per invocation. An expression subquery is one that is not directly under a quantified operator - expression subqueries are allowed to return at most one value per invocation (returning no value is considered to be equivalent to returning NULL). There are a large number of subquery types.  Because of the large number of types, and the large amount of shared code, we have decided to have 1 SubqueryNode without any subclasses.  The subquery type (and operator) is encoded in the subqueryType field. The query optimizer is responsible for optimizing subqueries, and also for transforming them so that code can be generated for them. The optimizer may eliminate some subqueries by transforming them into joins, or it may change the internal form of a subquery (for example, transforming 'where x in (select y from z where ...)' into 'where (select true from z where x = y and ...)'). Note that aggregates present some additional issues.  A transformation such as: <UL> where x in (SELECT <I>expression</I> FROM z) </UL> has to be treated specially if <I>expression</I> has an aggregate. We change it to: <UL> where x = (SELECT true FROM (SELECT MAX(x) FROM z) WHERE SQLCOL1 = y) </UL>
Replaces a <em>source</em> expression with a <em>target</em> expression.
<p> This VTI makes a table out of the output of the subversion log ("svn log") command. </p>
Aggregator for SUM().  Defers most of its work to OrderableAggregator.
Definition for the SUM()/AVG() aggregates.
A decorator that copies test resources from the classpath into the file system at setUp time. Resources are named relative to org/apache/derbyTesting/, e.g. the name passed into the constructor should be something like funtionTests/test/lang/mytest.sql <BR> Read only resources are placed into ${user.dir}/extin/name Read-write resources are placed into ${user.dir}/extinout/name write only output files can be created in ${user.dir}/extout These locations map to entries in the test policy file that have restricted permissions granted. All the three folders are created even if no files are copied into them. In each case the name of a file is the base name of the resource, no package structure is retained. A test may access such a resource using either files or URLs. The static utility methods must be used to obtain the location of any resource. In the future this decorator may create sub-folders to ensure that tests run in paralled do not interfere with each other. tearDown removes the three folders and their contents.

Flush all pages for a table on a commit
A synchronous <code>BundleEvent</code> listener. <code>SynchronousBundleListener</code> is a listener interface that may be implemented by a bundle developer. When a <code>BundleEvent</code> is fired, it is synchronously delivered to a <code>SynchronousBundleListener</code>. The Framework may deliver <code>BundleEvent</code> objects to a <code>SynchronousBundleListener</code> out of order and may concurrently call and/or reenter a <code>SynchronousBundleListener</code>. <p> A <code>SynchronousBundleListener</code> object is registered with the Framework using the {@link BundleContext#addBundleListener} method. <code>SynchronousBundleListener</code> objects are called with a <code>BundleEvent</code> object when a bundle has been installed, resolved, starting, started, stopping, stopped, updated, unresolved, or uninstalled. <p> Unlike normal <code>BundleListener</code> objects, <code>SynchronousBundleListener</code>s are synchronously called during bundle lifecycle processing. The bundle lifecycle processing will not proceed until all <code>SynchronousBundleListener</code>s have completed. <code>SynchronousBundleListener</code> objects will be called prior to <code>BundleListener</code> objects. <p> <code>AdminPermission[bundle,LISTENER]</code> is required to add or remove a <code>SynchronousBundleListener</code> object. This is a marker interface
Describe an S (Synonym) alias.



Abstract aggregator that is extended by all internal (system) aggregators.
Implements the description of a column in a system table.
Implements the description of a column in a system table.
A context that shuts the system down if it gets an StandardException with a severity greater than or equal to ExceptionSeverity.SYSTEM_SEVERITY or an exception that is not a StandardException.
This class represents access to system-wide Derby privileges. <P> <table border = "1"> <tr> <th>Permission <th>Description <th>Risk </tr> <tr> <th> "jmx" "control" <td> Controls the ability of JMX clients to control Derby and view security sensitive attributes through Derby's MBeans. <td> JMX clients may be able to change the state of the running system </tr> <tr> <th> "jmx" "monitor" <td> Controls the ability of JMX clients to monitor Derby through Derby's MBeans, such as viewing number of current connections and configuration settings. <em> Note: security related settings require</em> <code>control</code> <em>action on</em> <code>jmx</code> <td> JMX clients can see information about a runing system including software versions. </tr> </table>
This class represents Derby's notion of a principal, a concept of user identity with controlled access to Derby System Privileges. An authenticated user may have other identities which make sense in other code domains. <p> Note that principal names do NOT follow Authorization Identifier rules. For instance, although edward and edWard both match the normalized authorization identifier EDWARD, the instances <code>SystemPrincipal("edward")</code> and <code>SystemPrincipal("edWard")</code> represent different principals under the methods <code>getName()</code>, <code>equals()</code>, and <code>hashCode()</code>. <p> According to JAASRefGuide, Principal classes must implement Serializable.
Some system built-in procedures, and help routines.  Now used for network server. These procedures are built-in to the SYSIBM schema which match the DB2 SYSIBM procedures. Currently information on those can be found at url: ftp://ftp.software.ibm.com/ps/products/db2/info/vr8/pdf/letter/db2l2e80.pdf <P> Also used for builtin-routines, such as SYSFUN functions, when direct calls into Java libraries cannot be made.
This class implements a Cacheable for a DataDictionary cache of table descriptors.  It is an abstract class - there is more than one cache of table descriptors per data dictionary, and this class provides the implementation that's common to all of them.  The lookup key for the cache (the "identity" of the cache item) is provided by the subclass. Another design alternative was to make the table descriptors themselves the cacheable objects.  This was rejected because: we would have only one way of caching table descriptors, and we need at least two (by UUID and by name); the contents of a table descriptor would have to be split out into a separate class, so it could be used as the createParameter to the createIdentity() method; the releasing of the Cacheable would have to be done when at the end of compilation by traversing the tree - by creating a separate Cacheable object, we can release the object within the getTableDescriptor() method after getting the table descriptor out of it.
This class provides mechanism to call access Factory methods  from sql-j.


Utility functions for testing authorization.







// PT import javax.crypto.Cipher; import javax.crypto.spec.SecretKeySpec; import java.security.spec.KeySpec; import java.security.AlgorithmParameters; // import java.security.spec.AlgorithmParameterSpec; import javax.crypto.spec.IvParameterSpec; import java.security.GeneralSecurityException; import java.security.MessageDigest; import java.lang.reflect.*; To run, put the following line in derby.properties derby.module.test.T_Cipher=org.apache.derbyTesting.unitTests.crypto.T_Cipher and run java org.apache.derbyTesting.unitTests.harness.UnitTestMain
To run, put the following line in derby.properties derby.module.test.T_Cipher=org.apache.derbyTesting.unitTests.crypto.T_CipherBlowfish and run java org.apache.derbyTesting.unitTests.harness.UnitTestMain
To run, put the following line in derby.properties derby.module.test.T_Cipher=org.apache.derbyTesting.unitTests.crypto.T_CipherCFB and run java org.apache.derbyTesting.unitTests.harness.UnitTestMain
To run, put the following line in derby.properties derby.module.test.T_Cipher=org.apache.derbyTesting.unitTests.crypto.T_CipherDES and run java org.apache.derbyTesting.unitTests.harness.UnitTestMain
To run, put the following line in derby.properties derby.module.test.T_Cipher=org.apache.derbyTesting.unitTests.crypto.T_CipherECB and run java org.apache.derbyTesting.unitTests.harness.UnitTestMain
To run, put the following line in derby.properties derby.module.test.T_Cipher=org.apache.derbyTesting.unitTests.crypto.T_CipherOFB and run java org.apache.derbyTesting.unitTests.harness.UnitTestMain
used by unit tests, that needs  to simulate ColumnOrdering  data type parameter from the language layer.

This class has methods for corrupting a database. IT MUST NOT BE DISTRIBUTED WITH THE PRODUCT. NOTE: The entry points to this class are all static, for easy access via the query language.  Each of the static methods instantiates an object from the class and calls methods off of that object.  This allows the sharing of code across the various static methods.
This test exercices the DaemonFactory and DaemonService implementation
DEBUGGING: This T_Diagnosticable class provides a sample of how to use the "Diagnostic" facility.  The classes methods are built to be called by a "values" or a "call" statement from "ij".  Eventually there will be some sort of diagnostic monitor which will be used to call the various "D_*" routines.
Exception used to throw for errors in a unit test.
An Impl unittest for rawstore data that is based on the FileSystem
Abstract class which executes a unit test. <P>To write a test,	extend this class with a class which implements the two abstract methods: <UL> <LI>runTests <LI>setUp </UL>

Key for these objects is an array of objects value - Integer or String - implies what object should be used in the cache. waitms - time to wait in ms on a set or create (simulates the object being loaded into the cache). canFind - true of the object can be found on a set, false if it can't. (simulates a request for a non-existent object) raiseException - true if an exception should be raised during set or create identity
Unit test Lockable <BR> A simple Lockable that allows a single locker, but that locker can lock the object multiple times, standard Lockable behaviour.
A semaphore that implements Lockable for unit testing.

Protocol unit test for the LockManager.
A simple unit test for a MarkedLimitInputStream.
Abstract class which executes T_Generic. This splits the running of a test into two parts, the test setup and running the test. This allows the setup to be performed once, and then the test itself to be run for a number of iterations. The number iterations is set by the property derby.unittests.iterations and defaults to 1. <P> Statistics are provided about each iteration in the error log. The statistics are time for each iteration, used and total memory changes per iteration.
Abstract class which executes T_MultiIterations. This allows multiple threads running T_MultiIterations. This allows the setup to be performed once, and then the test itself to be run with multiple threads for a number of iterations. The number of threads and iterations are set by the property derby.unittests.numThreads and derby.unittests.iterations and default to 1. <P> Statistics are provided about each iteration in the error log. The statistics are time for each iteration, used and total memory changes per iteration.
A protocol unit test for the RawStore interface.
Implements a row of N columns of strings, or objects. Used for testing raw store functionality.
A implementation unit test for recovering log that has been damanged but salvagable. To run, create a derby.properties file in a new directory with the contents derby.module.test.recoverBadLog=org.apache.derbyTesting.unitTests.store.T_RecoverBadLog Execute in order To Test Bad Log due to partial write that are identified by checking the length in the beginning and end of the log record. java -DTestBadLogSetup=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog1=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog2=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog3=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog4=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog5=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog6=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog7=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog1=true org.apache.derbyTesting.unitTests.harness.UnitTestMain To Test Bad Log due to an incomplete out of order write that is identified by the checksum logic (simulated by	explicitly corrupting a middle of a log record at  the  end of log file after it is written). java -DTestBadLogSetup=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog1=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog2=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog3=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog4=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog5=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog6=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog7=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestBadLog1=true -DTestBadChecksumLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain
A implementation unit test for log full condition To run, create a derby.properties file in a new directory with the contents derby.module.test.recoverFullLog=org.apache.derbyTesting.unitTests.store.T_RecoverFullLog Execute in order java -DTestFillLog=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestLogSwitchFail=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestFullRecoveryFail=true org.apache.derbyTesting.unitTests.harness.UnitTestMain (run this serveral times, this simulate recovery running out of log) java -DTestFullRecover=true org.apache.derbyTesting.unitTests.harness.UnitTestMain
A protocol unit test for recovery. To run, create a derby.properties file in a new directory with the contents derby.module.test.recovery=org.apache.derbyTesting.unitTests.store.T_Recovery Execute java -DSetupRecovery=true org.apache.derbyTesting.unitTests.harness.UnitTestMain java -DTestRecovery=true org.apache.derbyTesting.unitTests.harness.UnitTestMain
A RowSource is the mechanism for iterating over a set of rows.  The RowSource is the interface through which access recieved a set of rows from the client for the purpose of inserting into a single container. <p> A RowSource can come from many sources - from rows that are from fast path import, to rows coming out of a sort for index creation.
This class implements a row which will be stored in a secondary index on a heap table. <p> This class creates a new DataValueDescriptor array which will be the row used to insert into the secondary index.  The fields of this object array are made up of references to DataValueDescriptors provided by the caller: the DataValueDescriptors in the template and a RowLocation. The interface is designed to support the standard access method interface where callers provide a single template and then read rows into that template over and over.  This class keeps a reference to the objects in the template and the rowlocation, so the state of this object changes whenever the caller changes the template. The caller provides a template which will contain a heap row, and a RowLocation which provides the location of the row within the heap table. <p> So for example to create an index from a base table by reading the base table and inserting each row one at a time into the secondary index you would do something like: DataValueDescriptors[] template = get_template_for_base_table(); RowLocation            rowloc   = ScanController_var.newRowLocationTemplate(); T_SecondaryIndexRow    indrow   = new T_SecondaryIndexRow(); indrow.init(template, rowloc, numcols_in_index); while (ScanController_variable.next()) { fetch(template) fetchLocation(rowloc) ConglomerateController_on_btree.insert(indrow.getRow()); }
This test implements serviceable for testing.  To facility testing, when this object is being serviced, it will synchronize on itself and notity all waiters.  Test driver may wait on this object and check timesServiced to make sure the background daemon has run.
Unit test for sorting.
A standard exception for testing. The messages for this exception are not localized or stored with the product.
Manage the result information from a single call to StoreCostController.getScanCost(). <p>
An Impl unittest for rawstore data that is based on the stream file
Transaction with context, a utility class for tests to create multiple interleaving transactions.
Test to ensure a implementation of the UUID module implements the protocol correctly.
Tracing can be done like so (commented out) import org.apache.derbyTesting.unitTests.util.MsgTrace;

Utility class to help test raw store functionality. If you write a raw store unit test, be that a protocol test or an implementation test, and find youself needing to do certain operations over and over again, chances are that functionality is either in here or should be added.  This class is here entirely for the convenience of people writing unit tests for the RawStore.


A poor mans structure used in DataDictionaryImpl.java. Used to save heapId, name pairs for non core tables.
This class represents a table descriptor. The external interface to this class is: <p> <ol> <li>external interface </li> <li>public String	getSchemaName();</li> <li>public String	getQualifiedName();</li> <li>public int	getTableType();</li> <li>public long getHeapConglomerateId() throws StandardException;</li> <li>public int getNumberOfColumns();		</li> <li>public FormatableBitSet getReferencedColumnMap();</li> <li>public void setReferencedColumnMap(FormatableBitSet referencedColumnMap);</li> <li>public int getMaxColumnID() throws StandardException;</li> <li>public void	setUUID(UUID uuid);</li> <li>public char	getLockGranularity();</li> <li>public void	setTableName(String newTableName);</li> <li>public void	setLockGranularity(char lockGranularity);</li> <li>public ExecRow getEmptyExecRow( ContextManager cm) throws StandardException;</li> <li>public boolean tableNameEquals(String otherSchemaName, String otherTableName);</li> <li>public ReferencedKeyConstraintDescriptor getPrimaryKey() throws StandardException;</li> <li>public void removeConglomerateDescriptor(ConglomerateDescriptor cd)	throws StandardException;</li> <li>public void removeConstraintDescriptor(ConstraintDescriptor cd)	throws StandardException;</li> <li>public void getAffectedIndexes(...) throws StandardException;</li> <li>public void	getAllRelevantTriggers(...) throws StandardException;</li> <li>public void getAllRelevantConstraints(...) throws StandardException</li> <li>public ColumnDescriptorList getColumnDescriptorList();</li> <li> public String[] getColumnNamesArray();</li> <li>public long[]   getAutoincIncrementArray();</li> <li>public ColumnDescriptor	getColumnDescriptor(String columnName);</li> <li>public ColumnDescriptor	getColumnDescriptor(int columnNumber);</li> <li>public ConglomerateDescriptor[]	getConglomerateDescriptors() throws StandardException;</li> <li>public ConglomerateDescriptor	getConglomerateDescriptor(long conglomerateNumber)	throws StandardException;</li> <li>public ConglomerateDescriptor	getConglomerateDescriptor(UUID conglomerateUUID) throws StandardException;</li> <li>public	IndexLister	getIndexLister() throws StandardException;</li> <li>public ViewDescriptor getViewDescriptor();</li> <li>public boolean tableHasAutoincrement();</li> <li>public boolean statisticsExist(ConglomerateDescriptor cd) throws StandardException;</li> <li>public double selectivityForConglomerate(...)throws StandardException;</li> </ol> <p>
A TableElementList represents the list of columns and other table elements such as constraints in a CREATE TABLE or ALTER TABLE statement.
A TableElementNode is an item in a TableElementList, and represents a single table element such as a column or constraint in a CREATE TABLE or ALTER TABLE statement.
A TableKey represents a immutable unique identifier for a SQL object. It has a schemaid and a name	.
A TableName represents a qualified name, externally represented as a schema name and an object name separated by a dot. This class is misnamed: it is used to represent the names of other object types in addition to tables.

A TableOperatorNode represents a relational operator like UNION, INTERSECT, JOIN, etc. that takes two tables as parameters and returns a table.  The parameters it takes are represented as ResultSetNodes. Currently, all known table operators are binary operators, so there are no subclasses of this node type called "BinaryTableOperatorNode" and "UnaryTableOperatorNode".
This class describes a row in the SYS.SYSTABLEPERMS system table, which stores the table permissions that have been granted but not revoked.

This class represents a set of privileges on one table.
Takes a table and a table filter and returns the table's rows satisfying the filter as a result set. There are several things we could do during object construction that are done in the open and next calls, to improve performance.
This class has been adapted from org.apache.derby.vti.VTITemplate because we do not want a test to depend on an engine class. This class implements most of the methods of the JDBC 1.2 interface java.sql.ResultSet, each one throwing a  SQLException with the name of the method. A concrete subclass can then just implement the methods not implemented here and override any methods it needs to implement for correct functionality. <P> The methods not implemented here are <UL> <LI>next() <LI>close() <LI>getMetaData() </UL> <P> For virtual tables the database engine only calls methods defined in the JDBC 1.2 definition of java.sql.ResultSet. <BR> Classes that implement a JDBC 2.0 conformant java.sql.ResultSet can be used as virtual tables.
Filter which passes Visitables which have been marked with a given tag.
<p> This little machine is constructed from an xml/html document/element and is used to parse content inside that element. This machine advances through the element, letting the operator look for substrings. This machine was created to read elements in JIRA's html reports, which are not well-formed xml documents. </p> <p> To operate the TagReader, keep positioning on substrings, following some pattern which allows you to navigate to the content you need. </p> ///////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////
The TargetResultSet interface is used to provide additional operations on result sets that are the target of a bulk insert or update.  This is useful because bulk insert is upside down - the insert is done via the store.
needsSync is never true - DONE An exception never marks the store as corrupt clean() does not stubbify preAllocate() does nothing - DONE getFileName() returns a file in the tmp directory - DONE flushAll does nothing - DONE file descriptor is never synced
this class is for temporary tables. The information kept here is necessary to implement the rollback and commit behavior for temporary tables. The temp tables will have following data structure TableDescriptor Declared in savepoint level Dropped in savepoint level Modified in savepoint level The actual logic LanguageConnectionContext will keep the "current savepoint level". At any point in time, this is the total number of savepoints defined for a transaction. At the start of any new transaction, the "current savepoint level" will be set to 0. Everytime a new user defined savepoint is set, store returns the total number of savepoints for the connection at that point. For eg, in a new transaction, "current savepoint level' will be 0. When the first savepoint is set, store will return 1 and "current savepoint level" will be set to 1. For next savepoint, store will return 2 and so on and so forth. When language calls rollback or release of a savepoint, store will again return the total number of savepoints for the connection after rollback or release and we will set "current savepoint level" to that number. For eg, start tran ("current savepoint level"=0) set savepoint 1 ("current savepoint level"=1) set savepoint 2 ("current savepoint level"=2) set savepoint 3 ("current savepoint level"=3) set savepoint 4 ("current savepoint level"=4) release savepoint 3 ("current savepoint level"=2) rollback savepoint 1 ("current savepoint level"=0) If the temporary table was declared with ON ROLLBACK DELETE ROWS and contents of that temporary table were modified in a transaction or within a savepoint unit, then we keep track of that by saving the savepoint level in dataModifiedInSavepointLevel. This information will be used at rollback of savepoint or transaction. Also, when a savepoint is released, we check if the table was modified in any of the savepoints that are getting released. If yes, then we put the current savepoint level as dataModifiedInSavepointLevel. eg start tran ("current savepoint level"=0) declare temp table 0 ON ROLLBACK DELETE ROWS("declared in savepoint level"=0, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) commit (temp table 0 ("declared in savepoint level"=-1, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1)) start tran ("current savepoint level = 0) temp table 0 ("declared in savepoint level"=-1, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) set savepoint 1("current savepoint level = 1") temp table 0 ("declared in savepoint level"=-1, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) set savepoint 2("current savepoint level = 2") delete 1 row from temp table 0 temp table 0 ("declared in savepoint level"=-1, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=2) release savepoint 2 ("current savepoint level"=1) and reset the modified in savepoint level as follows temp table 0 ("declared in savepoint level"=-1, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=1) rollback ("current savepoint level"=0) All the rows from the temp table 0 will be removed At the time of commit, we set dataModifiedInSavepointLevel to -1. At the time of rollback (transaction / savepoint), first we check if the table was modified in the unit of work getting rolled back. If yes, then we delete all the data from the temp table and we set dataModifiedInSavepointLevel to -1. When language calls release of a savepoint, store will again return the total number of savepoints in the system after release. We will go through all the temp tables and reset their declared or dropped or modified in savepoint level to the value returned by the release savepoint if those tables had their declared or dropped or modified in savepoint levels higher than what was returned by the release savepoint. eg start tran ("current savepoint level"=0) declare temp table 0 ("declared in savepoint level"=0, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) set savepoint 1("current savepoint level = 1") declare temp table 1 ("declared in savepoint level"=1, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) set savepoint 2("current savepoint level = 2") declare temp table 2 ("declared in savepoint level"=2, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) release savepoint 1 ("current savepoint level"=0) and reset the savepoint levels as follows temp table 1 ("declared in savepoint level"=0, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) temp table 2 ("declared in savepoint level"=0, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) set savepoint 3("current savepoint level = 1") rollback savepoint 3 ("current savepoint level"=0) and temp table info will look as follows temp table 0 ("declared in savepoint level"=0, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) temp table 1 ("declared in savepoint level"=0, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) temp table 2 ("declared in savepoint level"=0, "dropped in savepoint level"=-1, "dataModifiedInSavepointLevel"=-1) When you declare a temp table, it will have "declared in savepoint level" as the current savepoint level of the LanguageConnectionContext (which will be 0 in a transaction with no user-defined savepoints). The "dropped in savepoint level" for new temp tables will be set to -1. The "dataModifiedInSavepointLevel" for new temp tables will be set to -1 as well. When a temp table is dropped, we will first check if the table was declared in a savepoint level equal to the current savepoint level. If yes, then we will remove it from the temp tables list for the LanguageConnectionContext . eg start tran ("current savepoint level = 0") set savepoint 1("current savepoint level = 1") declare temp table 1 ("declared in savepoint level"=1, "dropped in savepoint level"=-1) drop temp table 1 (declared in savepoint level same as current savepoint level and hence will remove it from list of temp tables) If no, then we will set the dropped in savepoint level as the current savepoint level of the LanguageConnectionContext (which will be 0 in a transaction without savepoints and it also means that the table was declared in a previous transaction). At the time of commit, go through all the temp tables with "dropped in savepoint level" != -1 (meaning dropped in this transaction) and remove them from the temp tables list for the LanguageConnectionContext. All the rest of the temp tables with "dropped in savepoint level" = -1, we will set their "declared in savepoint level" to -1 and , "dataModifiedInSavepointLevel" to -1. eg start tran ("current savepoint level = 0) declare temp table t1("declared in savepoint level" = 0, "dropped in savepoint level"=-1) commit (temp table 1 ("declared in savepoint level"=-1, "dropped in savepoint level"=-1)) start tran ("current savepoint level = 0) drop temp table t1 ("declared in savepoint level" = -1, "dropped in savepoint level"=0) commit (temp table t1 will be removed from list of temp tables) At the time of rollback if rolling back transaction, first set the "current savepoint level" to 0 if rolling back to a savepoint, first set the "current savepoint level" to savepoint level returned by Store for the rollback to savepoint command Now go through all the temp tables. If "declared in savepoint level" of temp table is greater than or equal to "current savepoint level" (ie table was declared in this unit of work) And if table was not dropped in this unit of work ie "dropped in savepoint level" = -1 Then we should remove the table from the list of temp tables and drop the conglomerate created for it eg start tran ("current savepoint level = 0) declare temp table t2("declared in savepoint level" = 0, "dropped in savepoint level"=-1) rollback tran (temp table t2 will be removed from list of tables and conglomerate associated with it will be dropped) And if table was dropped in this unit of work ie "dropped in savepoint level" &gt;= "current savepoint level" Then we should remove the table from the list of temp tables eg start tran ("current savepoint level = 0) set savepoint 1("current savepoint level = 1") declare temp table t2("declared in savepoint level" = 1, "dropped in savepoint level"=-1) set savepoint 2("current savepoint level = 2") drop temp table t1 ("declared in savepoint level" = 1, "dropped in savepoint level"=2) rollback savepoint 1 ("current savepoint level = 0) temp table t1 will be removed from the list of temp tables Else if the "dropped in savepoint level" of temp table is greate than or equal to "current savepoint level" it mean that table was dropped in this unit of work (and was declared in an earlier savepoint unit / transaction) and we will restore it as part of rollback ie replace the existing entry for this table in valid temp tables list with restored temp table. At the end of restoring, "declared in savepoint level" will remain unchanged and "dropped in savepoint level" will be -1. eg start tran ("current savepoint level = 0) declare temp table t1 with definition 1("declared in savepoint level" = 0, "dropped in savepoint level"=-1, definition 1(stored in table descriptor)) commit (temp table t1 "declared in savepoint level" = -1, "dropped in savepoint level"=-1) start tran ("current savepoint level = 0) set savepoint 1("current savepoint level = 1") drop temp table t1 ("declared in savepoint level" = -1, "dropped in savepoint level"=1, definition 1(stored in table descriptor)) declare temp table t1 with definition 2(say different than definition 1) ("declared in savepoint level" = -1, "dropped in savepoint level"=1, definition 1(stored in table descriptor)) , ("declared in savepoint level" = 1, "dropped in savepoint level"=-1, definition 2(stored in table descriptor)) set savepoint 2("current savepoint level = 2") drop temp table t1("declared in savepoint level" = -1, "dropped in savepoint level"=1, definition 1(stored in table descriptor)) , ("declared in savepoint level" = 1, "dropped in savepoint level"=2, definition 2(stored in table descriptor)) rollback tran (Remove : temp table t1("declared in savepoint level" = 1, "dropped in savepoint level"=2, definition 2(stored in table descriptor) (Restore : temp table t1"declared in savepoint level" = -1, "dropped in savepoint level"=-1, definition 1(stored in table descriptor)) Else if the "dataModifiedInSavepointLevel" of temp table is greate than or equal to "current savepoint level" it means that table was declared in an earlier savepoint unit / transaction and was modified in the current UOW. And hence we will delete all the data from it.

A Clob representation where the Clob is stored either in memory or on disk. <p> Character positions given as input to methods in this class are always 1-based. Byte positions are always 0-based.
This is a class that is used to temporarily (non-persistently) hold rows that are used in language execution.  It will store them in an array, or a temporary conglomerate, depending on the number of rows. <p> It is used for deferred DML processing.
This is a class that is used to temporarily (non-persistently) hold rows that are used in language execution.  It will store them in an array, or a temporary conglomerate, depending on the number of rows. <p> It is used for deferred DML processing.
A result set to scan temporary row holders.  Ultimately, this may be returned to users, hence the extra junk from the ResultSet interface.
A TernaryOperatorNode represents a built-in ternary operators. This covers  built-in functions like {@code substr()}. Java operators are not represented here: the JSQL language allows Java methods to be called from expressions, but not Java operators.

Multi-threaded version of SimpleInsert. A number of threads execute the INSERT statements (each with their own connection). All the rows for a given warehouse are executed by one thread. The threads each get a warehouse to complete and then insert all the rows. Then they loop back to get the next warehouse. Warehouses are assigned to threads in a first come first served fashion. The main thread also inserts the ITEM table. <BR> By default the number of threads is the number of cpus on the machine, unless the scale to be loaded is larger than the number of threads. Then the number of threads will be equal to the scale. If the scale is one or the number of threads is one then the load is just like SimpleInsert. <BR> The number of threads can be set but if it it larger than the scale then the number of threads will be equal to the scale. <BR> It is assumed that foreign key constraints are created after the load.


This class is a wrapper of Process to provide a waitFor() method that forcibly terminates the process if it does not complete within the specified time. public class TimedProcess class WaitForProcess
Code to support Timeout error output.
This class provides access to Timer objects for various purposes. The scheme for creation of Timer objects is implementation-defined.
<p> A thread which sleeps for a specified time period, then wakes up and terminates the VM. </p>
The TimestampOperatorNode class implements the timestamp( date, time) function.

Describes the input token stream.
ToolScripts runs ij tool scripts (.sql files) in the tool package and compares the output to a canon file in the standard master package. <BR> Its suite() method returns a set of tests where each test is an instance of this class for an individual script wrapped in a clean database decorator. <BR> It can also be used as a command line program to run one or more tool based ij scripts as tests.
Utility functions shared across the optional tools.

A description of an instance of a module.

Provide support to transactions to manage sets of actions to perform at transaction boundaries. <P> Add rollback of savepoints? TODO: A
The TransactionController interface provides methods that an access client can use to control a transaction, which include the methods for gaining access to resources (conglomerates, scans, etc.) in the transaction controller's storage manager.  TransactionControllers are obtained from an AccessFactory via the getTransaction method. <P> Each transaction controller is associated with a transaction context which provides error cleanup when standard exceptions are thrown anywhere in the system.  The transaction context performs the following actions in response to cleanupOnError: <UL> <LI> If the error is an instance of StandardException that has a severity less than ExceptionSeverity.TRANSACTION_SEVERITY all resources remain unaffected. <LI> If the error is an instance of StandardException that has a severity equal to ExceptionSeverity.TRANSACTION_SEVERITY, then all resources are released.  An attempt to use any resource obtained from this transaction controller after such an error will result in an error.  The transaction controller itself remains valid, however. <LI> If the error is an instance of StandardException that has a severity greater than ExceptionSeverity.TRANSACTION_SEVERITY, then all resources are released and the context is popped from the stack.  Attempting to use this controller or any resources obtained from it will result in an error. </UL> Transactions are obtained from an AccessFactory.
This module is intended to be used only within the RawStore. RawStore functionality is accessed only through the RawStoreFactory interface. The transaction manager is responsible for: <UL> <LI>Generating unique transaction identifiers. <LI>Keeping a list of all open transactions within the raw store. </UL>
A transaction identifier that is only unique within a raw store, do not ever pass this out of raw store.  During reboot, all transaction Ids that have ever generated a log record will not be reused. However, if you put away the transaction Id of a read only transaction, then the is no guarentee that the transactionId won't be reused when the system reboots.  It is much safer to store away the ExternalTrasanctionId rather than the transactionId. The equals() method for TransactionId implements by value equality. MT - immutable need to write a value based HashCode() method.

An interface that must be implemented by a object that wants to be notified when a significant transaction event occurs. to support statement/savepoint rollback. void preSavepointRollback() throws StandardException;
The TransactionManager interface provides methods on the transaction needed by an access method implementer, but should not be visible to clients of a TransactionController. <p>
An instance of a TransactionResourceImpl is a bundle of things that connects a connection to the database - it is the transaction "context" in a generic sense.  It is also the object of synchronization used by the connection object to make sure only one thread is accessing the underlying transaction and context. <P>TransactionResourceImpl not only serves as a transaction "context", it also takes care of: <OL> <LI>context management: the pushing and popping of the context manager in and out of the global context service</LI> <LI>transaction demarcation: all calls to commit/abort/prepare/close a transaction must route thru the transaction resource. <LI>error handling</LI> </OL> <P>The only connection that have access to the TransactionResource is the root connection, all other nested connections (called proxyConnection) accesses the TransactionResource via the root connection.  The root connection may be a plain EmbedConnection, or a DetachableConnection (in case of a XATransaction).  A nested connection must be a ProxyConnection. A proxyConnection is not detachable and can itself be a XA connection - although an XATransaction may start nested local (proxy) connections. <P> this is an example of how all the objects in this package relate to each other.  In this example, the connection is nested 3 deep. DetachableConnection. <P><PRE> lcc  cm   database  jdbcDriver ^    ^    ^         ^ |    |    |         | |======================| | TransactionResource  | |======================| ^  | |  | |  |      |---------------rootConnection----------| |  |      |                                       | |  |      |- rootConnection-|                     | |  |      |                 |                     | |  V      V                 |                     | |========================|      |=================|      |=================| |    EmbedConnection     |      | EmbedConnection |      | EmbedConnection | |                        |&lt;-----|                 |&lt;-----|                 | | (DetachableConnection) |      | ProxyConnection |      | ProxyConnection | |========================|      |=================|      |=================| ^                 | ^             ^                        ^ |                 | |             |                        | ---rootConnection-- |             |                        | |             |                        | |             |                        | |======================|  |======================|  |======================| | ConnectionChild |  | ConnectionChild |  | ConnectionChild | |                      |  |                      |  |                      | |  (EmbedStatement)    |  |  (EmbedResultSet)    |  |  (...)               | |======================|  |======================|  |======================| </PRE> <P>A plain local connection <B>must</B> be attached (doubly linked with) to a TransactionResource at all times.  A detachable connection can be without a TransactionResource, and a TransactionResource for an XATransaction (called  XATransactionResource) can be without a connection.
A TransactionStatementNode represents any type of Transaction statement: SET TRANSACTION, COMMIT, and ROLLBACK.
TransactionTable is a virtual table that shows all transactions currently in the database. This virtual table can be invoked by calling it directly <PRE> select * from SYSCS_DIAG.TRANSACTION_TABLE </PRE> <P>The TransactionTable virtual table takes a snap shot of the transaction table while the system is in flux, so it is possible that some transactions may be in transition state while the snap shot is taken. We choose to do this rather then impose extraneous timing restrictions so that the use of this tool will not alter the normal timing and flow of execution in the application. <P>The TransactionTable virtual table has the following columns: <UL> <LI>XID varchar(15) - not nullable.  The transaction id, this can be joined with the LockTable virtual table's XID.</LI> <LI>GLOBAL_XID varchar(140) - nullable.  The global transaction id, only set if this transaction is a participant in a distributed transaction.</LI> <LI>USERNAME varchar(128) - nullable.  The user name, or APP by default. May appear null if the transaction is started by Derby.</LI> <LI>TYPE varchar(30) - not nullable. UserTransaction or an internal transaction spawned by Derby.</LI> <LI>STATUS varchar(8) - not nullable.  IDLE or ACTIVE.  A transaction is IDLE only when it is first created or right after it commits.  Any transaction that holds or has held any resource in the database is ACTIVE. Accessing the TransactionTable virtual table without using the class alias will not activate the transaction.</LI> <LI>FIRST_INSTANT varchar(20) - nullable.  If null, this is a read only transaction.  If not null, this is the first log record instant written by the transaction.</LI> <LI>SQL_TEXT VARCHAR(32672) - nullable.  if null, this transaction is currently not being executed in the database.  If not null, this is the SQL statement currently being executed in the database.</LI> </UL>
Transaction table entry is used to store all relevant information of a transaction into the transaction table for the use of checkpoint, recovery, Transaction management during Quiesce state, and for dumping transaction table.  Only works with the following classes: TransactionTable, XactFactory, Xact <BR> During run time, whenever any transaction is started, it is put into the transaction table.  Whenever any transaction is closed, it is removed from the transaction table.
This class is used by PlanExporter tool (DERBY-4587) as a data structure to keep the values retrieved after querying XPLAIN tables and few other properties of a plan node in a query plan.
A trigger. <p> We are dependent on TableDescriptors, SPSDescriptors (for our WHEN clause and our action).  Note that we don't strictly need to be dependent on out SPSes because we could just disallow anyone from dropping an sps of type 'T', but to keep dependencies uniform, we'll do be dependent. <p> We are a provider for DML (PreparedStatements or SPSes) The public methods for this class are: <ol> <li>getUUID <li>getName <li>getSchemaDescriptor <li>	public boolean listensForEvent(int event); <li>	public int getTriggerEventMask(); <li>	public Timestamp getCreationTimestamp(); <li>	public boolean isBeforeTrigger(); <li> public boolean isRowTrigger(); <li> public UUID getActionId(); <li> public SPSDescriptor getActionSPS(); <li>	public UUID getWhenClauseId(); <li>	public SPSDescriptor getWhenClauseSPS() <li> public String getWhenClauseText(); <li>	public TableDescriptor getTableDescriptor() <li> public ReferencedColumns getReferencedColumnsDescriptor() <li> public int[] getReferencedCols(); <li> public int[] getReferencedColsInTriggerAction(); <li> public boolean enforced(); <li> public void setEnabled(); <li> public void setDisabled(); <li> public boolean needsToFire(int stmtType, int[] modifiedCols) <li> public String getTriggerDefinition(); <li> public boolean getReferencingOld(); <li> public boolean getReferencingNew(); <li> public String getOldReferencingName(); <li> public String getNewReferencingName(); </ol>

This is a simple class that we use to track trigger events.  This is not expected to be used directly, instead there is a static TriggerEvent in TriggerEvents for each event found in this file.
Responsible for firing a trigger or set of triggers based on an event.
Static final trigger events.  One for each known trigger event.  Use these rather than constructing a new TriggerEvent.
A trigger execution context holds information that is available from the context of a trigger invocation.
This is a simple class used to store the run time information about a foreign key.  Used by DML to figure out what to check.
Provides information about about a a set of new rows created by a trigger action. <p> You can only use this class if no JDBC 2.0 or later calls are made against it.
Provides information about a set of rows before a trigger action changed them. <p> This class implements only JDBC 1.2, not JDBC 2.0.  You cannot compile this class with JDK1.2, since it implements only the JDBC 1.2 ResultSet interface and not the JDBC 2.0 ResultSet interface.  You can only use this class in a JDK 1.2 runtime environment if no JDBC 2.0 calls are made against it.
Rudimentary structure for containing information about a REFERENCING clause for a trigger.
Methods for testing triggers
Truncate a temp table on a commit, abort or rollback to savepoint
This is the superclass of all Descriptors. Users of DataDictionary should use the specific descriptor.
A TupleFilter is used to qualify rows from a tuple stream.
This class is a test where you are not able to create the lock file when booting an existing database.  The database will then become read-only.  The tests first creates a database and then shutdowns, turns off write access to the database directory and then boots the database again.  A non-default log directory is used since that uncovered a bug (DERBY-555).  (logDevice is set in the _app.properties file) NB! This test is not included in derbyall since it creates a read-only directory which will be annoying when trying to clean test directories.  When Java 6 can be used, it will be possible to turn on write access at the end of the test.
General Notes Descriptors are overriden using two distinct mechanisms - new TYPDEFNAM and TYPDEFOVR specifications which override environmental specifications originally established at connect time. - MDD/SDA pairs providing specific field level overrides for user data not conforming to the TYPDEFNAM and TYPDEFOVR specifications currently in effect.  Grouping triplets then refer to the new SDAs to specify the actual representation of the user data. - There are two types of early descriptor triplets, the T and the M triplets. Early --------------------------- Environmental   Grp Row Arr TTTTTTTTTTTTT   MMMMMMMMMMM - The T triplets are established by the TYPDEFNAM and TYPDEFOVR values. These can be overridden for any command or reply by specifying a new value for TYPDEFNAM and TYPDEFOVR. - The M triplets are established by the MGRLVL parameter on EXCSAT. They define PROTOCOL information units such as the SQLCA. These grouping and structuring triplets cannot be overriden Any change would mean a change in what information was exchanged rather than just how that information would be represented. - There are two types of late descriptor triplets, the O and U triplets. Late --------------------------- Environmental   Grp Row Arr OOOOOOOOOOOOO   UUUUUUUUUUU - The O triplets provide specific overrides. - The U triplets define actual user data, sometimes in combination with PROTOCOL information units.  The U triplets reference O triplets and both T triplets and M triplets (which in turn reference T triplets). - Rules for assigning LIDs to O triplets - select LID within range of 1 to 255. - select LID which doesn't interfere with references to early triplets or O triplets.  requirements - if this object handles overrides, they are only in effect for one command or the reply to one command.  Make sure that the correct "in effect" overrides are used especially when MDD overrides are present. - raise error if no CCSIDMBC or CCSIDDBC has been specified for mixed or double byte data.  Return SQLSTATE 57017 with 0 as source CCSID token. Possible errors: - check for valid lid at SQLAM level. - if the lid is greater than the max supported lid then the descriptor is invalid for the supported SQLAM level. Error Checking and Reporting Notes taken from PROTOCOL manual. - If the receiver of an FDODSC finds it in error, the error must be reported with a DDM message DSCINVRM.  If the descriptor passes PROTOCOL validity checks, but the data does not match the descriptors, the mismatch must be reported with a DDM message DTAMCHRM. so descriptor must be correct and valid and then the data must match the descriptor. - Possible General Errors - 01 FD:OCA Triplet not used in PROTOCOL descriptors or Type code invalid. - 02 Triplet Sequence Error: the two possible sequences are: 1.) GDA,(CPT,)RLO<,RLO><== normal case with no overrrides 2.) MDD,SDA,(MDD,SDA,)MDD,GDA,(CPT,)\ MDD,RLO<,MDD,RLO> where () indicates an optional repeating group and <> indicates a field allowed only when arrays are expected. - 03 An array description is required, and this one does not describe an array (probably too many or too few RLO triplets). - 04 A row description is required, and this one does not describe a row probably too many or too few RLO triplets. - 05 Late Environmental Descriptor just received not supported (probably due to non-support of requested overrides). - 06 Malformed triplet; missing required parameter. - 07 Parameter value not acceptable. - Possible MDD Errors - 11 MDD present is not recognized as PROTOCOL Descriptor - 12 MDD class not recognized as valid PROTOCOL class. - 13 MDD type not recognized as valid PROTOCOL type. - Possible SDA Errors - 21 Representation incompatible with PROTOCOL type (in prior MDD). - 22 CCSID not supported - Possible GDA/CPT Errors - 32 GDA references a LID that is not an SDA or GDA. - 33 GDA length override exceeds limits. - 34 GDA precision exceeds limits. - 35 GDA scale > precision or scale negative - 36 GDA length override missing or incompatible with protocol type. - Possible RLO Errors - 41 RLO references a LID that is not an RLO or GDA - 42 RLO fails to reference a required GDA or RLO (for example, QRYDSC must include a reference to SQLCAGRP).  Nullable SQL and PROTOCOL types are all odd numbers and nullable type is one number higher than the related non-nullable type  earlyTTriplets late0Triplets  typdef end

This interface defines methods associated with a TypeId that are used by the compiler.
Factory interface for the compilation part of datatypes.

TypeDescriptor represents a type in a system catalog, a persistent type. Examples are columns in tables and parameters for routines. A TypeDescriptor is immutable.

TypeId describes the static information about a SQL type independent of any specific attributes of the type such as length. So the TypeId for CHARACTER describes the fundamental information about CHARACTER. A specific type (e.g. CHARACTER(10)) is described by a TypeDescriptor for a catlog type and a DataTypeDescriptor for a runtime type. (note a DataTypeDescriptor adds runtime attributes to the TypeDescriptor it has). <P> A TypeId is immutable. <P> The equals(Object) method can be used to determine if two typeIds are for the same type, which defines type id equality.
Derby interface for identifying the format id for the stored form of an object. Objects of different classes may have the same format id if: <UL> <LI> The objects read and write the same stored forms. <LI> The object's getTypeId() method returns the same identifier. <LI> The objects support all the interfaces the type implies. </UL>

An implementation of interface CharStream, where the stream is assumed to contain only Unicode characters. NOTE: This class was modified to support the ability to get all the characters in the input stream between two tokens.  - Jeff
Describe an A (Abstract Data Type) alias. For the first release of USer Defined Types, this is a vacuous object. Future revs may add real information to this object. The UDTAliasInfo maintains a version stamp so that it can evolve its persistent form over time.
This class takes a string used for a connection URL and checks for correctness. To turn off output in ij, use the command line property of -DURLCheck=false. param anURL	 The URL used to connect to a database.
This class provides a http based implementation of the StorageFile interface. It is used by the database engine to access persistent data and transaction logs under the http and https subsubprotocols.
This class provides a http based implementation of the StorageFactory interface. It is used by the database engine to access persistent data and transaction logs under the http and https subsubprotocols.
A class that is used to store java.lang.Strings and provide ordering capability.
Class for reading characters from streams encoded in the modified UTF-8 format. <p> Note that we often operate on a special Derby stream. A Derby stream is possibly different from a "normal" stream in two ways; an encoded length is inserted at the head of the stream, and if the encoded length is <code>0</code> a Derby-specific end of stream marker is appended to the data. <p> If the underlying stream is capable of repositioning itself on request, this class supports multiple readers on the same source stream in such a way that the various readers do not interfere with each other (except for serializing access). Each reader instance will have its own pointer into the stream, and request that the stream repositions itself before calling read/skip on the stream.
Utility methods for handling UTF-8 encoded byte streams. <p> Note that when the {@code skip} methods mention detection of invalid UTF-8 encodings, it only checks the first byte of a character. For multibyte encodings, the second and third byte are not checked for correctness, just skipped and ignored. @ThreadSafe End class UTF8Util

An interface for accessing Derby UUIDs, unique identifiers. <p>The values in the system catalog held in ID columns with a type of CHAR(36) are the string representations of these UUIDs. <p>A UUID implements equals() and hashCode based on value equality.
Internal comment (not for user documentation): Although this is an abstract interface, I believe that the underlying implementation of UUID will have to be DCE UUID. This is because the string versions of UUIDs get stored in the source code.  In other words, no matter what implementation is used for UUIDs, strings that look like this <blockquote><pre> E4900B90-DA0E-11d0-BAFE-0060973F0942 </blockquote></pre> will always have to be turned into universally unique objects by the recreateUUID method Generates and recreates unique identifiers. An example of such an identifier is: <blockquote><pre> E4900B90-DA0E-11d0-BAFE-0060973F0942 </blockquote></pre> These resemble DCE UUIDs, but use a different implementation. <P> The string format is designed to be the same as the string format produced by Microsoft's UUIDGEN program, although at present the bit fields are probably not the same.
The upgrade tests use jar files containing older version databases. These need to be "unjarred" in order to do the tests.
This node represents a unary arithmetic operator
This node is the superclass  for all unary comparison operators, such as is null and is not null.
This class implements the timestamp( x) and date(x) functions. These two functions implement a few special cases of string conversions beyond the normal string to date/timestamp casts.

A UnaryOperatorNode represents a built-in unary operator as defined by the ANSI/ISO SQL standard.  This covers operators like +, -, NOT, and IS NULL. Java operators are not represented here: the JSQL language allows Java methods to be called from expressions, but not Java operators.
A class that provides interface to be called with undo of an Insert happens in raw store. ************************************************************************ Public Methods of XXXX class: *************************************************************************
An Undoable operation is an operation that changed the state of the RawStore in the context of a transaction and this change can be rolled back.
A UnionNode represents a UNION in a DML statement.  It contains a boolean telling whether the union operation should eliminate duplicate rows.
Takes two result sets and returns their union (all). (Any duplicate elimination is performed above this ResultSet.)
Unique index aggregator.  Enforces uniqueness when creating a unique index or constraint.
Utility class that generates a sequence of unique numbers in random order. Example of how to use the generator to print all the numbers from 0 to 9 in random order: <pre> UniqueRandomSequence sequence = new UniqueRandomSequence(10); while (sequence.hasMore()) { System.out.println(sequence.nextValue()); } </pre>
This is a descriptor for something that is a SQL object that has the following properties: <UL> <LI> resides in a schema </LI> <LI> has a name (that is unique when combined with schema) </LI> <LI> has a unique identifier (UUID) </LI> </UL> UUIDS.
Extension for Tuple Descriptors that have UUIDS.
Method factory to support sorting of Almost unique index. This class overrides getMergeSort of ExternalSortFactory to return UniqueWithDuplicateNullsMergeSort.
UniqueWithDuplicateNullsIndexSortObserver is implementation of BasicSortObserver for eliminating non null duplicates from the backing index of unique constraint. This class is implemented to check for special case of distinct sorting where duplicate keys are allowed only if there is a null in the key part.
This class extends and customizes MergeSort to support unique indexes with duplicate nulls. It overrides compare method to consider keypart - 1 parts of the keys while comparing (only for non null keys).

JUnit test which checks that only expected methods throw SQLFeatureNotSupporteException. As currently compiled, this class does not object to a handful of mandatory LOB-supporting methods which Derby does not implement. You can expose these methods by setting the STRICT_ENFORCEMENT constant to true.
An UntypedNullConstantNode represents a SQL NULL before it has been bound.  The bind() operation will replace the UntypedNullConstantNodes with typed ConstantNodes.
Updatable blob stream is a wrapper stream over dvd stream and LOBInputStream. It detects if blob data has moved from dvd to clob control. If this happens, it will update itself to point to LOBInputStream and reflect changes made to the Blob after the current position of the stream.
This class  describes compiled constants that are passed into Updatable VTIResultSets. CLASS METHODS
An abstract implementation of PreparedStatement (JDBC 3.0) that is useful when writing a read-write (updatable) virtual table interface (VTI). This class implements the methods of the JDBC3.0 version of PreparedStatement each one throwing a SQLException with the name of the method. A concrete subclass can then just implement the methods not implemented here and override any methods it needs to implement for correct functionality.
This class  describes compiled constants that are passed into UpdateResultSets.
Represents the update of a particular field of a row on a page. <PRE>
UpdateLoader implements then functionality of derby.database.classpath. It manages the ClassLoaders (instances of JarLoader) for each installed jar file. Jar files are installed through the sqlj.install_jar procedure. <BR> Each JarLoader delegates any request through standard mechanisms to load a class to this object, which will then ask each jarLoader in order of derby.database.classpath to load the class through an internal api. This means if the third jar in derby.database.classpath tries to load a class, say from the class for a procedure's method making some reference to it, then the request is delegated to UpdateLoader. UpdateLoader will then try to load the class from each of the jars in order of derby.database.classpath using the jar's installed JarLoader.
An UpdateNode represents an UPDATE statement.  It is the top node of the query tree for that statement. For positioned update, there may be no from table specified. The from table will be derived from the cursor specification of the named cursor. end of UpdateNode
Represents the update of a particular row on a page. <PRE>
Update the rows from the specified base table. This will cause constraints to be checked and triggers to be executed based on the c's and t's compiled into the update plan.
This class extends from the UpdateSensitiveLOBLocatorInputStream and creates and returns an implementation of the Blob specific locator InputStream. It also over-rides the reCreateStream method which re-creates the underlying Blob locator stream whenever a update happens on the Blob object associated with this stream.
This class extends from the UpdateSensitiveLOBLocatorInputStream and creates and returns an implementation of the Clob specific locator InputStream. It also over-rides the reCreateStream method which re-creates the underlying Clob locator stream whenever a update happens on the Clob object associated with this stream.
Wraps a Buffered Clob locator reader and watches out for updates on the Clob associated with it. Before a read operation is performed on the Reader this stream verifies that the underlying Clob has not changed and if it has it recreates the specific streams.
Super-class of the Update sensitive locator streams. Before a read operation if performed on the stream this stream verifies that the underlying LOB has not changed and if it has it recreates the specific streams. Since Locator streams are specific to Blob and Clob the sub-classes would take care of creating the appropriate streams.

Update the rows from the source into the specified base table.
Abstract class to provide support for test fixtures for upgrade change testing.
<p> This class factors out the machinery  needed to wrap a class loader around the jar files for an old release. </p>
Utility class for creating a set of tests that comprise a complete upgrade run through all five phases. This includes handling the class loading for the old jar files and setting up the configuration to use a single use database and DataSource for connections.

Definition for user-defined aggregates.
The UserAuthenticator interface provides operations to authenticate a user's credentials in order to successfully connect to a database. Any user authentication schemes could be implemented using this interface and registered at start-up time. <p> If an application requires its own authentication scheme, then it can implement this interface and register as the authentication scheme that Derby should call upon connection requests to the system. See the dcoumentation for the property <I>derby.authentication.provider</I> <p> A typical example would be to implement user authentication using LDAP, Sun NIS+, or even Windows User Domain, using this interface. <p> <i>Note</i>: Additional connection attributes can be specified on the database connection URL and/or Properties object on jdbc connection. Values for these attributes can be retrieved at runtime by the (specialized) authentication scheme to further help user authentication, if one needs additional info other than user, password, and database name.

Aggregator for user-defined aggregates. Wraps the application-supplied implementation of org.apache.derby.agg.Aggregator.

<p> This type id describes a user defined type. There are 2 kinds of user defined types in Derby: </p> <ul> <li><b>Old-fashioned</b> - In the original Cloudscape code, it was possible to declare a column's type to be the name of a Java class. Unlike ANSI UDTs, these user defined types were not schema objects themselves and they didn't have schema-qualified names. Some of the system tables have columns whose datatypes are old-fashioned user defined types. E.g., SYS.SYSALIASES.ALIASINFO.</li> <li><b>ANSI</b> - As part of the work on <a href="https://issues.apache.org/jira/browse/DERBY-651">DERBY-651</a>, we added ANSI UDTs. These are user defined types which are declared via the CREATE TYPE statement. These have schema-qualified names. The CREATE TYPE statement basically binds a schema-qualified name to the name of a Java class.</li> </ul>
A Descriptor for a user stored in SYSUSERS.
This contains an instance of a user-defined type, that is, a java object.
User type constants.  These are created by built-in types that use user types as their implementation. This could also potentially be used by an optimizer that wanted to store plans for frequently-used parameter values. This is also used to represent nulls in user types, which occurs when NULL is inserted into or supplied as the update value for a user type column.

<p> Utility methods for JDBC 4.2 </p>
General non-JDBC related utilities. Some of these were relocated from TestUtil.
Self-contained utilities. Don't reference any other driver classes, except Configuration, from within this class.
<p> Utility methods for JDBC 4.2. </p>
A storage factory for virtual files, where the contents of the files are stored in main memory. <p> Note that data store deletion may happen inside one of two different methods; either in {@code shutdown} or in {@code init}. This is due to the current implementation and the fact that dropping a database is done through the file IO interface by deleting the service root. As the deletion then becomes a two step process, someone else may boot the database again before the reference to the store has been removed. To avoid this, the {@code init}-method will never initialize with a store scheduled for deletion. I have only seen this issue in heavily loaded multithreaded environments (2 CPUs/cores should be enough to reproduce).

This contains all the opcodes for the JVM as defined in The Java Virtual Machine Specification. REMIND: might want a debugging version of this, that stored the stack depth and operand expectations.
This class implements a Cacheable for a Byte code generator cache of VMTypeIds.  It maps a Java class or type name to a VM type ID.
<p> Context parameter which is passed to an AwareVTI. </p>
<P> VTICosting is the interface that the query optimizer uses to cost Table Functions. The methods on this interface provide the optimizer with the following information: </P> <UL> <LI> The estimated number of rows returned by the Table Function in a single instantiation. <LI> The estimated cost to instantiate and iterate through the Table Function. <LI> Whether or not the Table Function can be instantiated multiple times within a single query execution. </UL> <P> The optimizer places a Table Function in the join order after making some assumptions: </P> <UL> <LI><B>Cost</B> - The optimizer hard-codes a guess about how expensive it is to materialize a Table Function. </LI> <LI><B>Count</B> - The optimizer also hard-codes a guess about how many rows a Table Function returns. </LI> <LI><B>Repeatability</B> - The optimizer assumes that the same results come back each time you invoke a Table Function. </LI> </Ul> <P> The class which contains your Table Function can override these assumptions and improve the join order as follows: </P> <UL> <LI><B>Implement</B> - The class must implement <a href="./VTICosting.html">VTICosting</a>. </LI> <LI><B>Construct</B> - The class must contain a public, no-arg constructor. </LI> </Ul> <P> The methods in this interface take a <a href="./VTIEnvironment.html">VTIEnvironment</a> argument. This is a state variable created by the optimizer. The methods in this interface can use this state variable to pass information to one another and learn other details of the operating environment. </P>
This class applies a VTI modification deferral policy to a statement to see whether it should be deferred. end of class VTIDeferModPolicy
<P> VTIEnvironment is the state variable created by the optimizer to help it place a Table Function in the join order. The methods of <a href="./VTICosting.html">VTICosting</a> use this state variable in order to pass information to each other and learn other details of the operating environment. </P>
An abstract implementation of ResultSetMetaData (JDBC 1.2) that is useful when writing a VTI (virtual table interface). This class implements most of the methods of ResultSetMetaData, each one throwing a SQLException with the name of the method. A concrete subclass can then just implement the methods not implemented here and override any methods it needs to implement for correct functionality. <P> The methods not implemented here are <UL> <LI>getColumnCount() <LI>getColumnType() </UL> <BR> For virtual tables the database engine only calls methods defined in the JDBC 1.2 definition of java.sql.ResultSetMetaData. <BR> Classes that implement a JDBC 2.0 conformant java.sql.ResultSetMetaData can be used as the meta data for virtual tables. <BR> Developers can use the VTIMetaDataTemplate20 instead of this class when developing in a Java 2 environment.

An abstract implementation of ResultSet that is useful when writing table functions, read-only VTIs (virtual table interface), and the ResultSets returned by executeQuery in read-write VTI classes. This class implements most of the methods of the JDBC 4.0 interface java.sql.ResultSet, each one throwing a  SQLException with the name of the method. A concrete subclass can then just implement the methods not implemented here and override any methods it needs to implement for correct functionality. <P> The methods not implemented here are <UL> <LI>next() <LI>close() </UL> <P> For table functions and virtual tables, the database engine only calls methods defined in the JDBC 2.0 definition of java.sql.ResultSet.
Special result set used when checking deferred CHECK constraints.  Activated by a special {@code --DERBY_PROPERTY validateCheckConstraint=<baseTableUUIDString>} override on a SELECT query, cf DeferredConstraintsMemory#validateCheck.  It relies on having a correct row location set prior to invoking {@code getNewtRowCore}, cf. the special code path in {@code ProjectRestrictResultSet#getNextRowCore} activated by {@code #validatingCheckConstraint}.
A ValueNode is an abstract class for all nodes that can represent data values, that is, constants, columns, and expressions.
A ValueNodeList represents a list of ValueNodes within a specific predicate e.g. IN list, NOT IN list or BETWEEN in a DML statement.
Basic implementation of ExecRow.
<p> This class implements the SQL Standard VAR_POP() aggregator, computing a population's variance.  It uses the IBM formula described <a href="http://www-01.ibm.com/support/knowledgecenter/ssw_ibm_i_71/db2/rbafzcolvar.htm">here</a>: </p> <blockquote><pre><b> sum(x<sub>i</sub><sup>2</sup>)/n - m<sup>2</sup> where n is the number of items in the population m is the population average x<sub>1</sub> ... x<sub>n</sub> are the items in the population </b></pre></blockquote> <p> The IBM formula can be computed without buffering up an arbitrarily long list of items. The IBM formula is algebraically equivalent to the textbook formula for population variance: </p> <blockquote><pre><b> sum( (x<sub>i</sub> - m)<sup>2</sup> )/n </b></pre></blockquote>
<p> This class implements the SQL Standard VAR_SAMP() aggregator, computing the variance over a sample.  It uses the IBM formula described <a href="http://www-01.ibm.com/support/knowledgecenter/ssw_ibm_i_71/db2/rbafzcolvarsamp.htm">here</a>: </p> <blockquote><pre><b> [ sum(x<sub>i</sub><sup>2</sup>) - sum(x<sub>i</sub>)<sup>2</sup>/n ]/(n-1) where n is the number of items in the population x<sub>1</sub> ... x<sub>n</sub> are the items in the population </b></pre></blockquote>
<p> Varargs routines used by VarargsTest. </p>

The VariableSizeDataValue interface corresponds to Datatypes that have adjustable width. The following methods are defined herein: setWidth()
If a RCL (SELECT list) contains an aggregate, then we must verify that the RCL (SELECT list) is valid. For ungrouped queries, the RCL must be composed entirely of valid aggregate expressions - in this case, no column references outside of an aggregate. For grouped aggregates, the RCL must be composed of grouping columns or valid aggregate expressions - in this case, the only column references allowed outside of an aggregate are grouping columns.
JUnit test which checks that all methods specified by the interfaces in JDBC 4.0 are implemented. The test requires JVM 1.6 to run. Even though this class uses JDBC it extends BaseTestCase as it handles getting connections itself and thus does not need the utility methods or connecion handlng provided by BaseJDBCTestCase.

Generates a set of client-server combinations to run the compatibility tests for. <p> Due to the requirement for running with a variety of Derby versions, the compatibility test suite is run as multiple processes. The test is controlled from the main process (the process in which the test/suite is started), and this process spawns additional processes for each server version and each client version. In some cases it also has to spawn additional processes to accomplish other tasks. <p> For development purposes the default MATS suite is sufficient for ongoing work. Eventually, and at least before cutting a new release, the full development suite should be run, since it will test the trunk against all previous releases. The other suites will test old releases against each other, and as such they are of less interest since the old releases don't change. Note however that these suites can be used to test releases on branches where this version of the compatibility test doesn't exist (just add the JARs to the release repository and configure includes or excludes to suite your needs). <p> <strong>NOTE 1</strong>: The set of combinations computed by this class depends on the number of old releases available on the local computer. If there are no old releases available a warning will be emitted, but the test won't fail (it will test trunk vs trunk). <p> <strong>NOTE 2</strong>: trunk is defined as a distribution, although it hasn't been released yet. The reason is simple: we always want to test trunk for incompatibilities against older versions.
This interface defines a Standard MBean for exposing the version information of a running Derby component. Refer to the getters of the interface for defined attributes. All attributes are read-only. The MBean does not define any operations. <P> Key properties for registered MBean: <UL> <LI> <code>type=Version</code> <LI> <code>jar={derby.jar|derbynet.jar}</code> <LI> <code>system=</code><em>runtime system identifier</em> (see overview) </UL> <P> If a security manager is installed these permissions are required: <UL> <LI> <code>SystemPermission("server", "monitor")</code> for version information specific to derbynet.jar <LI> <code>SystemPermission("engine", "monitor")</code> for version information specific to derby.jar </UL>
This is the implementation of ViewDescriptor. Users of View descriptors should only use the following methods: <ol> <li> getUUID <li> setUUID <li> getViewText <li> setViewName <li> getCheckOptionType <li> getCompSchemaId </ol>
A VirtualColumnNode represents a virtual column reference to a column in a row returned by an underlying ResultSetNode. The underlying column could be in a base table,  view (which could expand into a complex expression), subquery in the FROM clause, temp table, expression result, etc. By the time we get to code generation, all VirtualColumnNodes should stand only for references to columns in a base table within a FromBaseTable.
Represents a file in the virtual file system. <p> A virtual file is not created until one of the following methods are invoked: <ul> <li>{@code createNewFile} <li>{@code getOutputStream} <li>{@code getRandomAccessFile} <li>{@code mkdir} <li>{@code mkdirs} </ul> <p> When a method that requires access to the file data or to know if the file exists or not, the associated data store is consulted.
This class acts as a conduit of information between the lock manager and the outside world.  Once a virtual lock table is initialized, it contains a snap shot of all the locks currently held in the lock manager.  A VTI can then be written to query the content of the lock table. <P> Each lock held by the lock manager is represented by a Hashtable.  The key to each Hashtable entry is a lock attribute that is of interest to the outside world, such as transaction id, type, mode, etc.
A random access file capable of reading and writing from/into a virtual file whose data is represented by a {@code BlockedByteArray}. <p> If the file is opened in read-only mode and the caller invokes one of the write methods, it will fail with a {@code NullPointerException}.
A Visitable is something that can be visited by a Visitor
Filter for qualifying Visitables.
A visitor is an object that traverses the querytree and performs some action.
This class is a VTI for loading data into the Wisconsin benchmark schema. See The Benchmark Handbook, Second Edition (edited by Jim Gray).
This error is never seen outside of the btree implementation.  It is raised when an operation could not procede because it would have had to wait.
An Order Entry warehouse. <P> Fields map to definition in TPC-C for the WAREHOUSE table. The Java names of fields do not include the W_ prefix and are in lower case. <BR> The columns that map to an address are extracted out as a Address object with the corresponding Java field address. <BR> All fields have Java bean setters and getters. <BR> Fields that are DECIMAL in the database map to String in Java (rather than BigDecimal) to allow running on J2ME/CDC/Foundation. <P> Primary key maps to {id}. <P> A Warehouse object may sparsely populated, when returned from a business transaction it is only guaranteed to contain  the information required to display the result of that transaction.
Cooked up VTI to test SYSCS_BULK_INSERT
This class represents an OLAP window definition.
Superclass of any window function call.
A WindowList represents the list of windows (definitions) for a table expression, either defined explicitly in a WINDOW clause, or inline in the SELECT list or ORDER BY clause.
Superclass of window definition and window reference.
Represents a reference to an explicitly defined window
WindowResultSet This ResultSet handles a window function ResultSet. The ResultSet is opened using openCore().  Each row is fetched and any restrictions evaluated for each row in a do-while loop in getNextRowCore(). The ResultSet is closed using closeCore().
A WindowResultSetNode represents a result set for a window partitioning on a select. Modeled on the code in GroupByNode.
Class which creates and populates the tables used by {@code IndexJoinClient}. These are the same tables as the ones used by the functional Wisconsin test found in the lang suite.
WorkHorseForCollatorDatatypes class holds on to RuleBasedCollator, and the base SQLChar object for the collation sensitive SQLChar, SQLVarchar, SQLLongvarchar and SQLClob. This class uses RuleBasedCollator and SQLChar object in the collation sensitive methods to do the comparison. The reason for encapsulating this here is that the collation version of SQLChar, SQLVarchar, SQLLongvarchar and SQLClob do not all have to duplicate the code for collation sensitive methods. Instead, they can simply delegate the work to methods defined in this class.
A wrapper around the getObject() overloads added by JDBC 4.1. We can eliminate this class after Java 7 goes GA and we are allowed to use the Java 7 compiler to build our released versions of derbyTesting.jar.
A wrapper around the abort(Executor) method added by JDBC 4.1. We can eliminate this class after Java 7 goes GA and we are allowed to use the Java 7 compiler to build our released versions of derbyTesting.jar.
A wrapper around the new DatabaseMetaData methods added by JDBC 4.1. We can eliminate this class after Java 7 goes GA and we are allowed to use the Java 7 compiler to build our released versions of derbyTesting.jar.
A wrapper around the methods added by JDBC 4.1. We can eliminate this class after Java 7 goes GA and we are allowed to use the Java 7 compiler to build our released versions of derbyTesting.jar.
A wrapper around the methods added by JDBC 4.1. We can eliminate this class after Java 7 goes GA and we are allowed to use the Java 7 compiler to build our released versions of derbyTesting.jar.
A wrapper around the new Statement methods added by JDBC 4.1.
A wrapper around the new DatabaseMetaData methods added by JDBC 4.2.
This interface extends StorageFactory to provide read/write access to storage. <p> The database engine will call this interface's methods from its own privilege blocks. <p> Each WritableStorageFactory instance may be concurrently used by multiple threads.
This abstract class describes compiled constants that are passed into Delete, Insert, and Update ResultSets. This class and its sub-classes are not really implementations of ConstantAction, since they are not executed. A better name for these classes would be 'Constants'. E.g. WriteCursorConstants, DeleteConstants. Ideally one day the split will occur.



Connection factory using javax.sql.XADataSource. Returns a connection in local mode obtained from getXAConnection().getConnection().
This class contains database state specific to XA, specifically the XAResource that will be used for XA commands.
This interface allows access to commit,prepare,abort global transactions as part of a two phase commit protocol.  These interfaces have been chosen to be exact implementations required to implement the XAResource interfaces as part of the JTA standard extension. <P> It is expected that the following interfaces are only used during the recovery portion of 2 phase commit, when the transaction manager is cleaning up after a runtime crash - it is expected that no current context managers exist for the Xid's being operated on.  The "online" two phase commit protocol will be implemented by calls directly on a TransactionController. <P> The XAResource interface is a Java mapping of the industry standard XA resource manager interface.  Please refer to: X/Open CAE Specification - Distributed Transaction Processing: The XA Specification, X/Open Document No. XO/CAE/91/300 or ISBN 1 872630 24 3. <P>
The Statement returned by an Connection returned by a XAConnection needs to float across the underlying real connections. We do this by implementing a wrapper statement.
This interface allows access to commit,prepare,abort global transactions as part of a two phase commit protocol, during runtime. These interfaces have been chosen to be exact implementations required to implement the XAResource interfaces as part of the JTA standard extension. <P> It is expected that the following interfaces are only used during the runtime portion of a 2 phase commit connection. <P> If a runtime exception causes a transaction abort (of a transaction that has not been successfully prepared), then the transaction will act as if xa_rollback() had been called.  The transaction will be aborted and any other call other than destroy will throw exceptions. <P> The XAResource interface is a Java mapping of the industry standard XA resource manager interface.  Please refer to: X/Open CAE Specification - Distributed Transaction Processing: The XA Specification, X/Open Document No. XO/CAE/91/300 or ISBN 1 872630 24 3. <P> NOTE - all calls to this interface assume that the caller has insured that there is no active work being done on the local instance of the transaction in question.  RESOLVE - not sure whether this means that the connection associated with the transaction must be closed, or if it just means that synchronization has been provided to provide correct MT behavior from above.

The XAXactId class is a specific implementation of the JTA Xid interface.  It is only used by the TransactionTable.restore() interface to return an array of Xid's back to the caller, as part of serving the XAresource.restore() interface. <P> It is NOT the object that is stored in the log.  One reason for this is that the Formattable and Xid interface's define two different return values for the getFormatId() interface.
This type implements the XMLDataValue interface and thus is the type on which all XML related operations are executed. The first and simplest XML store implementation is a UTF-8 based one--all XML data is stored on disk as a UTF-8 string, just like the other Derby string types.  In order to make it possible for smarter XML implementations to exist in the future, this class always writes an "XML implementation id" to disk before writing the rest of its data.  When reading the data, the impl id is read first and serves as an indicator of how the rest of the data should be read. So long as there's only one implementation (UTF-8) the impl id can be ignored; but when smarter implementations are written, the impl id will be the key to figuring out how an XML value should be read, written, and processed.


Optimizer tracer which produces output in an xml format.
Run all of the XML JUnit tests as a single suite. This suite is included in lang._Suite but is at this level to allow easy running of just the XML tests.
This class implements TypeCompiler for the XML type.
This is the Default Visitor which produces explain information like the old getRuntimeStatistics() approach. <br/> It exists to support backward-compatibility. The only thing this visitor does, is to log the output of the statistics to the default log stream. (the file derby.log)
This is the module implementation of the XPLAINFactoryIF. It gets lazy-loaded when needed. The factory method determines which visitor to use. The visitor is cached in this factory for later reuse.
This is the factory interface of the XPLAINFactory facility. It extends the possibilities and provides a convenient protocol to explain queries on basis of the query execution plan. This plan manfifests in Derby in the different ResultSets and their associated statistics. The introduction of this factory interface makes it possible to switch to another implementation or to easily extend the API.



This class describes a Tuple for the XPLAIN_SORT_PROPS System Table.


This is the Visitor, which explains the information and stores the statistics in the system catalogs. It traverses the result set statistics tree and extracts the information.

This class contains helper methods, which support the System Table Visitor.
Classes, which implement this interface have the ability to explain the gathered ResultSetStatistics. A Visitor pattern is used to traverse the ResultSetStatistics tree and to extract the required information. Classes implementing this interface are responsible about what they extract and what will be done with the extracted information. This approach allows easy representaion extensions of the statistics, e.g. an XML representation.
This interface has to be implemented by object structures, which want to get explained. The current implementation let the ResultSetStatistics extend this Interface to be explainable.

A transaction has five states <OL> <LI> CLOSED - cannot be used <LI> IDLE - no reads have been performed by the transaction. <LI> ACTIVE - at least one read has been attempted by the transaction <LI> UPDATE - at least one update has been attempted by the transaction <LI> PREPARED - the transaction is ready to commit (FUTURE). </OL> <BR>Transaction identifiers are re-used for transactions that do not enter the UPDATE state during their lifetime.
The context associated with the transaction. This object stores the context associated with the raw store transaction on the stack.  It stores info about the transaction opened within a context manager (ie. typically a single user) for a single RawStoreFactory.

Use this class for a short hand representation of the transaction.  This value is only guarentee to be unique within one continuous operation of the raw store, in other words, every reboot may reuse the same value. Whereas GlobalXactId is unique for all times across all raw store, a XactId is only unique within a particular rawstore and may be reused. XactId keeps track of the outstanding transactionId and is responsible for dispensing new transactionIds
The XactXAResourceManager implements the Access XAResource interface, which provides offline control over two phase commit transactions.  It is expected to be used by TM's (transaction manager's), to recover if systems fail while transactions are still in-doubt (prepared). <P> This interface allows access to commit,prepare,abort global transactions as part of a two phase commit protocol.  These interfaces have been chosen to be exact implementations required to implement the XAResource interfaces as part of the JTA standard extension. <P> It is expected that the following interfaces are only used during the recovery portion of 2 phase commit, when the transaction manager is cleaning up after a runtime crash - it is expected that no current context managers exist for the Xid's being operated on.  The "online" two phase commit protocol will be implemented by calls directly on a TransactionController. <P> The XAResource interface is a Java mapping of the industry standard XA resource manager interface.  Please refer to: X/Open CAE Specification - Distributed Transaction Processing: The XA Specification, X/Open Document No. XO/CAE/91/300 or ISBN 1 872630 24 3. ************************************************************************ Public Methods of XXXX class: *************************************************************************
<p> This is a VTI designed to read XML files which are structured like row sets. <p> XML files parsed by this VTI are always processed with external entity expansion disabled and secure parser processing enabled. <p> There are two invocation formats provided by this VTI. <p> One form of this VTI takes the following arguments. This form is useful when all of the columns in the row can be constructed from data nested INSIDE the row Element. </p> <ul> <li>xmlResourceName - The name of an xml file.</li> <li>rowTag - The tag of the element which contains the row-structured content.</li> <li>childTags - The attributes and descendant elements inside the row element which should be treated as columns.</li> </ul> <p> Here is a sample declaration of this first form of the XmlVTI: </p> <pre> create function findbugs( xmlResourceName varchar( 32672 ), rowTag varchar( 32672 ), childTags varchar( 32672 )... ) returns table ( className   varchar( 32672 ), bugCount    int ) language java parameter style derby_jdbc_result_set no sql external name 'org.apache.derby.vti.XmlVTI.xmlVTI'; </pre> <p> ...and here is a sample invocation: </p> <pre> create view findbugs as select * from table ( findbugs ( 'findbugs.xml', 'ClassStats', 'class', 'bugs' ) ) v; select * from findbugs where bugCount != 0; </pre> <p> A second form of this VTI takes the following arguments. This form is useful when some of the columns in the row are "inherited" from outer elements inside which the row element nests: </p> <ul> <li>xmlResourceName - The name of an xml file.</li> <li>rowTag - The tag of the element which contains the row-structured content.</li> <li>parentTags - Attributes and elements (to be treated as columns) from outer elements in which the rowTag is nested.</li> <li>childTags - Attributes and elements (to be treated as columns) inside the row element.</li> </ul> <p> Here is a sample declaration of this second form of the XmlVTI. Using the second form involves declaring an ArrayList type and a factory method too: </p> <pre> create type ArrayList external name 'java.util.ArrayList' language java; create function asList( cell varchar( 32672 ) ... ) returns ArrayList language java parameter style derby no sql external name 'org.apache.derby.vti.XmlVTI.asList'; create function optTrace ( xmlResourceName varchar( 32672 ), rowTag varchar( 32672 ), parentTags ArrayList, childTags ArrayList ) returns table ( stmtID    int, queryID   int, complete  boolean, summary   varchar( 32672 ), type        varchar( 50 ), estimatedCost        double, estimatedRowCount    int ) language java parameter style derby_jdbc_result_set no sql external name 'org.apache.derby.vti.XmlVTI.xmlVTI'; create view optTrace as select * from table ( optTrace ( '/Users/me/derby/mainline/z.xml', 'planCost', asList( 'stmtID', 'queryID', 'complete' ), asList( 'summary', 'type', 'estimatedCost', 'estimatedRowCount' ) ) ) v ; select * from optTrace where stmtID = 6 and complete order by estimatedCost; </pre>
extends Properties
End class _Suite
Tests trunk against all available versions of old Derby releases. <p> This is different from the MATS in that it also tests old releases on branches and not only the latest release on each branch.
Tests all the newest branch releases and trunk against each other. <p> This suite is limited in that only the latest releases of old branches are included in the test set. It is still testing old releases against each other, which is somewhat uninteresting since they the old releases don't change.
Tests all available versions of Derby against each other.
Test of backup restore through java program JDBC calls. Enhanced the test from bug5229 repro.
Test to make sure checkpoint or occuring as expected. Check is done by looking at the timestamp for "log.ctrl" file, If modified time is more than what it was in the last lookup means , we know that checkpoint occured. Other thing that is counted is in this program is number of log switches.
A tool that generates a list of required classes from a set of properties files. The value of any property within a property file that starts with 'derby.module.' is taken as a class name. That class name and all the clases it requires are listed to System.out, to facilitate building a zip file. Classes that start with 'java.' or 'javax.' are not listed and are not checked for dependent classes. <P> If the class name starts with 'com.ibm.db2j.' then a messages.properties file is searched for corresponding to that class, if one exists then is is added to the list of files printed. <P> The search path for the classes is $CLASSPATH <P> If the system property cloudscapeOnly is set to true then only classes and message.properties files are listed that start with com.ibm.db2j. <P> The output for each class or properties file is a relative file name that uses '/' as the file separator. e.g. com/ibm/db2j/core/Setup.class <P> The output order of the classes & files is random. <P> Usage: java [-DignoreWebLogic=true] [-Dverbose=true] [-DcloudscapeOnly=true] [-DruntimeOnly=true] [-Ddb2jtools=true] [-DportingOnly=true] [-Doutputfile=<filename>] org.apache.derbyBuild.classlister property_file [ property_file ... ]
The purpose of this test and col_rec2 test is to create a territory based database and create some objects with collation sensitive character types. Then, make the database crash so that during the recovery, store engine has to do collation related operations. Those collation related operations are going to require that we use correct Collator object. DERBY-3302 demonstrated a npe during this operation because Derby was relying on database context to get the correct Collator object. But database context is not available at this point in the recovery. With the fix for DERBY-3302, the Collator object will now be obtained from collation sensitive datatypes itself rather than looking at database context which is not available at this point in recovery. This particular class will do the steps of create a territory based database and create some objects with collation sensitive character types. Then, make the database crash. col_rec2.java will do the part of rebooting the crashed db which will require store to go through recovery.
The purpose of this test and col_rec1 test is to create a territory based database and create some objects with collation sensitive character types. Then, make the database crash so that during the recovery, store engine has to do collation related operations. Those collation related operations are going to require that we use correct Collator object. DERBY-3302 demonstrated a npe during this operation because Derby was relying on database context to get the correct Collator object. But database context is not available at this point in the recovery. With the fix for DERBY-3302, the Collator object will now be obtained from collation sensitive datatypes itself rather than looking at database context which is not available at this point in recovery. col_rec1 class will do the steps of create a territory based database and create some objects with collation sensitive character types. Then, make the database crash. This test will do the part of rebooting the crashed db which will require store to go through recovery.
This Program Test getConnection()/getStatement().
<p>This class is for whatever java is in the current classpath
Validate BC calls.
Test various data manager limits like in db2 here.
* * dbcleanup * * Preliminary version: *	gets rid of all the items in a database except those that *	are present when a fresh database is created.  There are *	some gaps still-- sync objects, and I have not done SYSFILES. *	I have probably missed other things as well.  At present this *	is hardwired for jdbc:derby:wombat, the focus of our *	attention in the embedded tests. *
Simple program to archive a database up in a jar file within the test harness.





Basic tests for exercising the {@code org.apache.derby.iapi.tools.run} class found in {@code derbyrun.jar}.
The purpose of this test is to reproduce JIRA DERBY-662: Sometimes during redo the system would incorrectly remove the file associated with a table.  The bug required the following conditions to reproduce: 1) The OS/filesystem must be case insensitive such that a request to delete a file named C2080.dat would also remove c2080.dat.  This is true in windows default file systems, not true in unix/linux filesystems that I am aware of. 2) The system must be shutdown not in a clean manner, such that a subsequent access of the database causes a REDO recovery action of a drop table statement.  This means that a drop table statement must have happened since the last checkpoint in the log file.  Examples of things that cause checkpoints are: o clean shutdown from ij using the "exit" command o clean shutdown of database using the "shutdown=true" url o calling the checkpoint system procedure o generating enough log activity to cause a regularly scheduled checkpoint. 3) If the conglomerate number of the above described drop table is TABLE_1, then for a problem to occur there must also exist in the database a table such that it's HEX(TABLE_2) = TABLE_1 4) Either TABLE_2 must not be accessed during REDO prior to the REDO operation of the drop of TABLE_1 or there must be enough other table references during the REDO phase to push the caching of of the open of TABLE_2 out of cache. If all of the above conditions are met then during REDO the system will incorrectly delete TABLE_2 while trying to redo the drop of TABLE_1. <p> This test reproduces the problem by doing the following: 1) create 500 tables, need enough tables to insure that conglomerate number 2080 (c820.dat) and 8320 (c2080.dat) exist. 2) checkpoint the database so that create does not happen during REDO 3) drop table with conglomerate number 2080, mapping to c820.dat.  It looks it up in the catalog in case conglomerate number assignment changes for some reason. 4) exit the database without a clean shudown, this is the default for test suites which run multiple tests in a single db - no clean shutdown is done. Since we only do a single drop since the last checkpoint, test will cause the drop during the subsequent REDO. 5) run next test program dropcrash2, which will cause redo of the drop.  At this point the bug will cause file c2080.dat to be incorrectly deleted and thus accesses to conglomerate 8320 will throw container does not exist errors. 6) check the consistency of the database which will find the container does not exist error.

This test tests the JDBC Statement executeUpdate method. Since IJ will eventually just use execute rather then executeUpdate, I want to make sure that executeUpdate is minimally covered.
<p>This class is for IBM's jdk 1.3.
<p>This class is for IBM's jdk 1.4.
<p>This class is for IBM's jdk 1.5.
<p>This class is for IBM's jdk 1.6.
<p>This class is for IBM's jdk 1.7.
<p>This class is for IBM's jdk 1.8.
ij is Derby's interactive JDBC scripting tool. It is a simple utility for running scripts against a Derby database. You can also use it interactively to run ad hoc queries. ij provides several commands for ease in accessing a variety of JDBC features. <P> To run from the command line enter the following: <p> java [options] org.apache.derby.tools.ij [arguments] <P> ij is can also be used with any database server that supports a JDBC driver.

ijException is used to get messages from the ij parser to the main ij loop. Because this is not under the protocol/impl umbrella, it does not have available to it the message service. At this time, all messages are hard-coded in this file. A more serviceable solution may need to be found.
This is an impl for just returning errors from JDBC statements. Used by Async to capture its result for WaitFor.
Used for fatal IJ exceptions
This is an impl for a statement execution; the result is either an update count or result set depending on what was executed.
This impl is intended to be used with multiple resultsets, where the execution of the statement is already complete.
This is a wrapper for results coming out of the ij parser.
This is an empty impl for reuse of code.
This impl is intended to be used with a resultset, where the execution of the statement is already complete.
This is an impl for when 1 row of a result set is the intended use of it.  The caller *must not* do a "next" on the result set.  It's up to them to make sure that doesn't happen.
This is an impl for a statement execution; the result is either an update count or result set depending on what was executed.

This is an impl for a simple Vector of objects.
This is an impl for just returning warnings from JDBC objects we don't want the caller to touch. They are already cleared from the underlying objects, doing clearSQLWarnings here is redundant.

<p>This class is for IBM's J9 jdk 1.3.
<p>This class is for IBM's J9 jdk 1.3.1 subset - 2.2
<p>This class is for IBM's J9 jdk 1.3., foundation class library; v 2.2 (wctme5.7) Having the following method overload the one in jvm.java causes problems when running the junit tests - they *do* successfully run with securityManager. Foundation class tests actually run ok with security manager - except when useprocess is false. This is caused by a bug in the jvm. See also DERBY-885 and DERBY-1785. protected void setSecurityProps() { System.out.println("Note: J9 (foundation) tests do not run with security manager"); }
<p>This class is for IBM's J9 jdk 1.3., foundation class library; v 2.4 (weme6.2) protected void setSecurityProps() { System.out.println("Note: J9 (foundation) tests do not run with security manager"); }
<p>This class is for IBM's J9 jdk 1.5 subset - 2.4
<p>This class is for JDK1.3.
<p>This class is for JDK1.4.
<p>This class is for JDK1.5.




<p>This class provides the interface and mechanism for plugging VMs into the system.  Typically you only need to add a new implementation if your supported attributes or command line building are different from those that exist. <p>this class has fields for all options that a JDK VM can take, that is the reference point for all others.  Note some VMs (like jview) don't take all options and will ignore them (like -mx).  Defining the system property "verbose" to 1 will give you warnings for ignored properties in a properly implemented subclass. <p> here is the canonical output from java -help for options we take: <pre> -noasyncgc        don't allow asynchronous garbage collection -verbosegc        print a message when garbage collection occurs -noclassgc        disable class garbage collection -ss<number>       set the maximum native stack size for any thread -oss<number>      set the maximum Java stack size for any thread -ms<number>       set the initial Java heap size -mx<number>       set the maximum Java heap size -classpath <directories separated by semicolons> list directories in which to look for classes -prof[:<file>]    output profiling data to .\java.prof or .\<file> -verify           verify all classes when read in -noverify         do not verify any class -nojit            turn off the jit -Dprop=name       define property; can be specified more than once </pre>
This test tries to push byte code generation to the limit. It has to be run with a large amount of memory which is set with jvmflags in largeCodeGen_app.properties There are only a few types of cases now. Other areas need to be tested such as large in clauses, etc.
A quick and dirty class for generating a properties file from the maint property in DBMS.properties and release.properties. Useful for getting the values of the third and fourth parts of the version number into Ant as separate properties. It puts the third value into the output properties as the property "interim", and the fourth value as "point". Usage: java maintversion2props input_properties_file output_properties_file

This Program Test SetMaxFieldsize()/getMaxFieldsize(). and the getXXX calls that are affected.



outparams30 contains java procedures using java.math.BigDecimal. These are moved to this class to enable tests using other procedures in outparams.java to run in J2ME/CDC/FP. TODO To change the template for this generated type comment go to Window - Preferences - Java - Code Style - Code Templates
Usage: java propertyConfig <master file> <config> <output file> <B> e.g., java propertyConfig dbms.properties cloudsync cloudsync.dbms.properties <P> This program takes a master property file, and using the configuration specification, generate an output file that only contains the properties for that particular configuration. <P> For the different types of legitamite configurations, please see org.apache.derby.modules.properties <P> PropertySplitter will look at cloudscape.config.<tag> to see which configuration a particular module belongs to. <B>E.g., cloudscape.config.dataComm.listen=cloudtarget,cloudsync <B>this means all properties associated with dataComm.listen will be in the output properties file only if we are generating for the cloudsync or cloudtarget configuration.  They will not be in the output properties file if we are generating for the cloud or cloudscape configuration.
Test of additional methods in JDBC2.0 result set meta-data. This program simply calls each of the additional result set meta-data methods, one by one, and prints the results.

<p> The run class facilitates running the various Derby utilities with the java -jar command. For example: <p> java -jar derbyrun.jar ij [-p propertiesfile] [sql script]<br> java -jar derbyrun.jar sysinfo [-cp ...] [-cp help]<br> java -jar derbyrun.jar dblook [args] (or no arguments for usage)<br> java -jar derbyrun.jar server [args] (or no arguments for usage)<br> java -jar derbyrun.jar SignatureChecker [args] (or no arguments for usage)<br>
* * shutdown * *	force a shutdown after a test complete to guarantee shutdown *	which doesn't always seem to happen with useprocess=false *
This is from a bug found by a beta customer.
Test of strings longer than 64K. This is the wrapper class used by the test harness.

Repro for DERBY-1939.  In effect what we have to do is execute a query (using a PreparedStatement) for which the optimizer will choose to do a Hash Join using an IndexToBaseRow result result.  But that's not enough--at execution time, we then have to force a situation where the Hash Table "spills" to disk, and only then will the error occur. In order to get the optimizer to choose the necessary plan we have a moderately complex query that has a predicate which can be pushed to table T1.  T1 in turn has an index declared on the appropriate column.  The optimizer will then choose to do a Hash Join between T2 and T1 and will use the index on T1, as desired. Then, in order to force the "spill" to disk, we use the Derby property "maxMemoryPerTable" and set it to a "magic" value that a) is large enough to allow the optimizer to choose a Hash Join, but b) is small enough to cause hash-table-spill-over at execution time.  It took a while find out what value this property should have given the data in the tables, but having found it we can now reliably reproduce the failure.
The purpose of this test is to reproduce JIRA DERBY-715: Sometimes a deadlock would be incorrectly reported as a timeout.  The bug seemed to always reproduce at least once if the following test was run (at least one of the iterations in the loop would get an incorrect timeout vs. a deadlock).
The purpose of this test space reclamation of long rows and long columns. This addresses DERBY-670. The main issue is that previous to fixes for DERBY-670, space reclamation was only automatically queued when the last row on a page was deleted.  In the case of long columns, the actual row on the main page can be quite small as the long data is streamed onto other pages.  So the table can grow unexpectedly quite large before the default space reclamation kicks in.  The change queues space reclamation in the case of long columns (blob/clob), immediately post commit of the single delete. The testing strategy is to loop doing insert, delete, commit of a blob for a number of iterations and check that the actual size of the table is reasonable.  A sleep will be added to allow time for post commit to catch up as the test may be run in a number of environments with varying performance of background activities.
This class displays system information to system out. To run from the command-line, enter the following: <p> <code>java org.apache.derby.tools.sysinfo</code> <p> <p> Also available on this class are methods which allow you to determine the version of the code for the system without actually booting a database. Please note that this is the Derby version of the .jar files, not of your databases. <p> The numbering scheme for released Derby products is <b><code>m1.m2.m3 </code></b> where <b><code>m1</code></b> is the major release version, <b><code>m2</code></b> is the minor release version, and <b><code>m3</code></b> is the maintenance level. Versions of the product with the same major and minor version numbers are considered feature compatible. <p>Valid major and minor versions are always greater than zero. Valid maintenance versions are greater than or equal to zero.
This class defines miscelanious test java methods to be called from sql. These are not generic methods, typically  used by a particular tests.
This class understands the message protocol and looks up SQLExceptions based on keys, so that the Local JDBC driver's messages can be localized. REMIND: May want to investigate putting some of this in the protocol side, for the errors that any Derby JDBC driver might return. The ASSERT mechanism is a wrapper of the basic services, to ensure that failed asserts at this level will behave well in a JDBC environment. In the past, this class was sent on the wire to the client and because it has the message protcol stuff and also the detailed stack trace as one of it's member variable, the client.jar files was really big. To get around this problem, now we have added EmbedSQLException which is just a java sql exception with the stack trace information variable transient so it doesn't get transported to the client side and thus reducing the size of client.jar The bug for this fix was 1850. The p4 number for it will have the details of all the files impacted and the actual changes made.
This class is utilities specific to the two ij Main's. This factoring enables sharing the functionality for single and dual connection ij runs.

An interface for running xa tests. The real implementation is only loaded if the requisite javax classes are in the classpath.
The real xa helper class.  Load this class only if we know the javax classes are in the class path.
