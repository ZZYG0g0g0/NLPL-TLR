The compiler calls this method just before walking a query tree. The compiler calls this method when it's done walking a tree. Initialize the Visitor before processing any trees. User-written code calls this method before poking the Visitor into the LanguageConnectionContext. For example, an implementation of this method might open a trace file. Final call to the Visitor. User-written code calls this method when it is done inspecting query trees. For instance, an implementation of this method might release resources, closing files it has opened.
Returns true if locks by anyone are blocking anyone else Return true if locks are held in this space <BR> MT - thread safe Return true if locks are held in this group and this space. <BR> MT - thread safe Clear a limit set by setLimit. Create an object which can be used as a compatibility space within this lock manager. Create the lock table that contains the mapping from <code>Lockable</code>s to locks. Get the lock timeout in milliseconds. A negative number means that there is no timeout. * Property related methods EXCLUDE-END-lockdiag- * Methods of PropertySetCallback * Methods of LockFactory Lock a specific object with a timeout. <BR> MT - thread safe EXCLUDE-START-lockdiag- Routines to support lock diagnostics VTIs for the benefit of VirtualLockTable package Check if we should not wait for locks, given the specified timeout and compatibility space. If the timeout is {@code C_LockFactory.NO_WAIT} or the {@code LockOwner} has the {@code noWait} flag set, we shouldn't wait for locks. Transfer a set of locks from one group to another. <BR> MT - thread safe Unlock a specific object <BR> MT - thread safe Unlock a group of objects. <BR> MT - thread safe
closing the connection to the database  Create XML data based on the query that's passed in. The query should have exactly one parameter, which will be initialized to the statement id before the query is executed. <p> This method creates the queries such that after execution of the query it will return XML data fragments. </P> Escape characters that have a special meaning in XML. This method is needed since in the case of XML attributes we have to filter the quotation (&quot;) marks that is compulsory. eg: scanned_object="A &quot;quoted&quot;  table name";     Generating the XML tree   marking the depth of each element  Check if there is a schema in the database that matches the schema name that was passed in to this instance. Set the schema of the current connection to the XPLAIN schema in which the statistics can be found.
Backup the database to backupDir. <P>Please see Derby on line documentation on backup and restore. Backup the database to a backup directory and enable the log archive mode that will keep the archived log files required for roll-forward from this version backup. Checkpoints the database, that is, flushes all dirty data to disk. Records a checkpoint in the transaction log, if there is a log. Database creation has finished. DERBY-5996(Create readme files (cautioning users against modifying database files) at database hard upgrade time) This gets called during hard upgrade. It will create 3 readme files one in database directory, one in "seg0" directory and one in log directory. These readme files warn users against touching any of files associated with derby database disables the log archival process, i.e No old log files will be kept around for a roll-forward recovery. Start the failover for this database. Find an access method that implements a format type. Find an access method that implements an implementation type. Freeze the database temporarily so a backup can be taken. <P>Please see Derby on line documentation on backup and restore. Get a transaction. If a new transaction is implicitly created, give it name transName. Get the LockFactory to use with this store. ************************************************************************ methods that are Property related. ************************************************************************* ************************************************************************ methods that are transaction related. ************************************************************************* Get a transaction controller with which to manipulate data within the access manager.  Implicitly creates an access context if one does not already exist. Return a snap shot of all transactions in the db. <p> Take a snap shot of all transactions currently in the database and make a record of their information. Return the XAResourceManager associated with this AccessFactory. <p> Returns an object which can be used to implement the "offline" 2 phase commit interaction between the accessfactory and outstanding transaction managers taking care of in-doubt transactions. XAResourceManager Is the store read-only. Register an access method that this access manager can use. ************************************************************************ methods that implement functionality on the org.apache.derby.iapi.db API ************************************************************************* Start the replication master role for this database Start a global transaction. <p> Get a transaction controller with which to manipulate data within the access manager.  Implicitly creates an access context. <p> Must only be called if no other transaction context exists in the current context manager.  If another transaction exists in the context an exception will be thrown. <p> The (format_id, global_id, branch_id) triplet is meant to come exactly from a javax.transaction.xa.Xid.  We don't use Xid so that the system can be delivered on a non-1.2 vm system and not require the javax classes in the path. <p> If the global transaction id given matches an existing in-doubt global transaction in the current system, then a StandardException will be thrown with a state of SQLState.STORE_XA_XAER_DUPID. <p> XATransactionController Stop the replication master role for this database. Unfreeze the database after a backup has been taken. <P>Please see Derby on line documentation on backup and restore. Wait until the thread handling the post commit work finihes the work assigned to it.

Copy all information from the given AccessPath to this one. Get whatever was last set as the conglomerate descriptor. Returns null if nothing was set since the last call to startOptimizing() Get the cost estimate for this AccessPath.  This is the last one set by setCostEstimate. Return whether or not the optimizer is considering a covering index scan on this AccessPath. Get the join strategy, as set by setJoinStrategy(). Get the lock mode, as last set in setLockMode(). Return whether or not the optimizer is considering a non-matching index scan on this AccessPath. We expect to call this during generation, after access path selection is complete. Get the optimizer associated with this access path. Sets the "name" of the access path. if the access path represents an index then set the name to the name of the index. if it is an index created for a constraint, use the constraint name. This is called only for base tables. Set the conglomerate descriptor for this access path. Set the given cost estimate in this AccessPath.  Generally, this will be the CostEstimate for the plan currently under consideration. Set whether or not to consider a covering index scan on the optimizable. Remember the given join strategy Set the lock mode Set whether or not to consider a non-matching index scan on this AccessPath.

Copy an InputStream into an array of bytes and return an InputStream against those bytes. The input stream is copied until EOF is returned. This is useful to provide streams to applications in order to isolate them from Derby's internals. Return an InputStream that wraps the valid byte array. Note that no copy is made of the byte array from the input stream, it is up to the caller to ensure the correct co-ordination. The caller promises to set their variable to null before any other calls to write to this stream are made. Or promises to throw away references to the stream before passing the array reference out of its control. Read the complete contents of the passed input stream into this byte array.
Add a warning to the activation Temporary tables can be declared with ON COMMIT DELETE ROWS. But if the table has a held curosr open at commit time, data should not be deleted from the table. This method, (gets called at commit time) checks if this activation held cursor and if so, does that cursor reference the passed temp table name. Check the validity of the current executing statement. Needs to be called after a statement has obtained the relevant table locks on the Generated plans have a current row field for ease in defining the methods and finding them dynamically. The interface is used to set the row before a dynamic method that uses it is invoked. <p> When all processing on the currentRow has been completed, callers should call activation.clearCurrentRow(resultSetNumber) to ensure that no unnecessary references are retained to rows. This will allow the rows no longer in use to be collected by the garbage collecter. RESOLVE - this method belongs on an internal, not external, interface Clear the ConglomerateController to be used for an update or delete. (Saves opening the ConglomerateController twice.) Clear the info for the index to be re-used for update/delete. (ScanController and conglomerate number.) clear the parent resultset hash table; Clear the activation's warnings. Closing an activation statement marks it as unusable. Any other requests made on it will fail.  An activation should be marked closed when it is expected to not be used any longer, i.e. when the connection for it is closed, or it has suffered some sort of severe error. This will also close its result set and release any resources it holds e.g. for parameters. <P> Any class that implements this must be prepared to be executed from garbage collection, ie. there is no matching context stack. When the prepared statement is executed, it passes execution on to the activation execution was requested for. Returns the column positions array of columns requested in auto-generated keys resultset for this avtivation. Returns null if no specific column requested by positions <p> Returns the column names array of columns requested in auto-generated keys resultset for this avtivation. Returns null if no specific column requested by names <p> Returns true if auto-generated keys resultset request was made for this avtivation. <p> Get the top ConstantAction on the stack without changing the stack. Get the current row at the given index. Called by generated code to get the next number in an ANSI/ISO sequence and advance the sequence. Raises an exception if the sequence was declared NO CYCLE and its range is exhausted. Return the cursor name of this activation. This will differ from its ResultSet's cursor name if it has been altered with setCursorName. Thus this always returns the cursor name of the next execution of this activation. The cursor name of the current execution must be obtained from the ResultSet. or this.getResultSet.getCursorName() [with null checking]. <p> Statements that do not support cursors will return a null. <p> Get the TableDescriptor for the target of DDL. Get the DataValueFactory Return the set of dynamical created result sets, for procedures. Base implementation returns null, a generated class for a procedure overwrites this with a real implementation. Get the ExecutionFactory Get whether or not this activation is for create table. (NOTE: We can do certain optimizations for create table that we can't do for other DDL.) Get the ConglomerateController, if any, that has already been opened for the heap when scaning for an update or delete. (Saves opening the ConglomerateController twice.) Get the conglomerate number of the index, if any, that has already been opened for scaning for an update or delete. (Saves opening the ScanController twice.) Get the ScanController, if any, that has already been opened for the index when scaning for an update or delete. (Saves opening the ScanController twice.) Get the language connection context associated with this activation Return the maximum number of dynamical created result sets from the procedure definition. Base implementation returns 0, a generated class for a procedure overwrites this with a real implementation. Get the maximum # of rows.  (# of rows that can be returned by a ResultSet.  0 means no limit.) Get the number of subqueries in the entire query. Gets the ParameterValueSet for this execution of the statement. This activation is created in a dynamic call context, or substatement execution context; get its caller's or superstatement's activation. get the reference to parent table ResultSets, that will be needed by the referential action dependent table scans. Get the prepared statement that this activation is for. Get the result description for this activation, if it has one. Returns the current result set for this activation, i.e. the one returned by the last execute() call.  If there has been no execute call or the activation has been reset or closed, a null is returned. Return the holdability of this activation. <p> Get the saved RowLocation. Get the current SQL session context if in a nested connection of a stored routine or in a substatement. Get the ResultSet for the target of an update/delete to a VTI. Get the Execution TransactionController associated with this activation/lcc. Returns the chained list of warnings. Returns null if there are no warnings. Tell this activation that the given ResultSet was found to have the given number of rows.  This is used during execution to determine whether a table has grown or shrunk.  If a table's size changes significantly, the activation may invalidate its PreparedStatement to force recompilation. Note that the association of row counts with ResultSets is kept in the activation class, not in the activation itself.  This means that this method must be synchronized. This method is not required to check the number of rows on each call.  Because of synchronization, this check is likely to be expensive, so it may only check every hundred calls or so. Find out if the activation is closed or not. Is this Activation for a cursor? Is the activation in use? Returns true if this Activation is only going to be used for one execution. Mark the activation as unused. Pop the ConstantAction stack, returning the element which was just popped off the stack. Push a ConstantAction to be returned by getConstantAction(). Returns the newConstantAction. Resets the activation to the "pre-execution" state - that is, the state where it can be used to begin a new execution. Frees local buffers, stops scans, resets counters to zero, sets current date and time to an unitialized state, etc. Set the auto-generated keys resultset mode to true for this activation. The specific columns for auto-generated keys resultset can be requested by passing column positions array The specific columns for auto-generated keys resultset can be requested by passing column names array Both the parameters would be null if user didn't request specific keys. Otherwise, the user could request specific columns by passing column positions or names array but not both. Generated plans have a current row field for ease in defining the methods and finding them dynamically. The interface is used to set the row before a dynamic method that uses it is invoked. <p> When all processing on the currentRow has been completed, callers should call activation.clearCurrentRow(resultSetNumber) to ensure that no unnecessary references are retained to rows. This will allow the rows no longer in use to be collected by the garbage collecter. JDBC requires that all select statements be converted into cursors, and that the cursor name be settable for each execution of a select statement. The Language Module will support this, so that the JDBC driver will not have to parse JSQL text. This method will have no effect when called on non-select statements. <p> There will be a JSQL statement to disable the "cursorization" of all select statements. For non-cursorized select statements, this method will have no effect. <p> This has no effect if the activation has been closed. <p> Save the TableDescriptor for the target of DDL so that it can be passed between the various ConstantActions during execution. Mark the Activation as being for create table. (NOTE: We can do certain optimizations for create table that we can't do for other DDL.) beetle 3865: updateable cursor using index.  A way of communication between cursor activation and update activation. Set the ConglomerateController to be used for an update or delete. (Saves opening the ConglomerateController twice.) Set the conglomerate number of the index to be used for an update or delete, when scanning an index that will also be updated (Saves opening the ScanController twice.) Set the ScanController to be used for an update or delete, when scanning an index that will also be updated (Saves opening the ScanController twice.) Set the maximum # of rows.  (# of rows that can be returned by a ResultSet.  0 means no limit.) Sets the parameter values for this execution of the statement. <p> Has no effect if the activation has been closed. <p> NOTE: The setParameters() method is currently unimplemented. A statement with parameters will generate its own ParameterValueSet, which can be gotten with the getParameterValueSet() method (above). The idea behind setParameters() is to improve performance when operating across a network by allowing all the parameters to be set in one call, as opposed to one call per parameter. This activation is created in a dynamic call context or a substatement execution context, chain its parent statements activation.. store a reference to the parent table result sets Set current resultset holdability. Set this Activation for a single execution. E.g. a java.sql.Statement execution. Save the ResultSet for the target of an update/delete to a VTI. Set up and return the current SQL session context for all immediately nested connections stemming from the call or function invocation of the statement corresponding to this activation (push=true) or for a substatement, which shares the parents statement's session context (push=false).
/////////////////////////////////////////////////////////////////////  CURSOR SUPPORT  ///////////////////////////////////////////////////////////////////// Updatable cursors need to add a getter method for use in BaseActivation to access the result set that identifies target rows for a positioned update or delete. <p> The code that is generated is: <pre><verbatim> public CursorResultSet getTargetResultSet() { return targetResultSet; } public CursorResultSet getCursorResultSet() { return cursorResultSet; } </verbatim></pre> An execute method always ends in a return statement, returning the result set that has been constructed.  We want to do some bookkeeping on that statement, so we generate the return given the result set. Upon entry the only word on the stack is the result set expression The base class for activations is BaseActivation /////////////////////////////////////////////////////////////////////  CURRENT DATE/TIME SUPPORT  ///////////////////////////////////////////////////////////////////// The first time a current datetime is needed, create the class level support for it. The first half of the logic is in our parent class. /////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////// Get the package name that this generated class lives in Get the number of ExecRows to allocate Updatable cursors need to add a field and its initialization for use in BaseActivation to access the result set that identifies cursor result rows for a positioned update or delete. <p> The code that is generated is: <pre><verbatim> private CursorResultSet cursorResultSet; </verbatim></pre> The expression that is generated is: <pre><verbatim> (ResultSet) (cursorResultSet = (CursorResultSet) #expression#) </verbatim></pre> The expression must be the top stack word when this method is called. Updatable cursors need to add a field and its initialization for use in BaseActivation to access the result set that identifies target rows for a positioned update or delete. <p> The code that is generated is: <pre><verbatim> private CursorResultSet targetResultSet; </verbatim></pre> The expression that is generated is: <pre><verbatim> (ResultSet) (targetResultSet = (CursorResultSet) #expression#) </verbatim></pre> Generate the assignment for numSubqueries = x /////////////////////////////////////////////////////////////////////  EXECUTE METHODS  /////////////////////////////////////////////////////////////////////
Clear the potentially granted flag. MT - single thread required Set the potentially granted flag, returns true if the flag changed its state. MT - single thread required Wait for a lock to be granted, returns when the lock is granted. <P> The sleep wakeup scheme depends on the two booleans wakeUpNow and potentiallyGranted. MT - Single thread required - and assumed to be the thread requesting the lock. Wake up anyone sleeping on this lock. MT - Thread Safe
(non-Javadoc) @see org.eclipse.ui.IActionDelegate#run(org.eclipse.jface.action.IAction) (non-Javadoc) @see org.eclipse.ui.IActionDelegate#selectionChanged(org.eclipse.jface.action.IAction, org.eclipse.jface.viewers.ISelection) (non-Javadoc) @see org.eclipse.ui.IObjectActionDelegate#setActivePart(org.eclipse.jface.action.IAction, org.eclipse.ui.IWorkbenchPart)
Reset the fields to allow object re-use.
Create a permission name from a Bundle Determines the equality of two <code>AdminPermission</code> objects. Returns the canonical string representation of the <code>AdminPermission</code> actions. <p> Always returns present <code>AdminPermission</code> actions in the following order: <code>class</code>, <code>execute</code>, <code>extensionLifecycle</code>, <code>lifecycle</code>, <code>listener</code>, <code>metadata</code>, <code>resolve</code>, <code>resource</code>, <code>startlevel</code>, <code>context</code>. Returns the hash code value for this object. Determines if the specified permission is implied by this object. This method throws an exception if the specified permission was not constructed with a bundle. <p> This method returns <code>true</code> if the specified permission is an AdminPermission AND <ul> <li>this object's filter matches the specified permission's bundle ID, bundle symbolic name, bundle location and bundle signer distinguished name chain OR</li> <li>this object's filter is "*"</li> </ul> AND this object's actions include all of the specified permission's actions. <p> Special case: if the specified permission was constructed with "*" filter, then this method returns <code>true</code> if this object's filter is "*" and this object's actions include all of the specified permission's actions Returns a new <code>PermissionCollection</code> object suitable for storing <code>AdminPermission</code>s.
For now, it looks like the only time we accumulate chain breaking exceptions is for disconnect exceptions. Called only for disconnect event ---------------------------------------------------------------------------- Checks whether a data type is supported for <code>setObject(int, Object, int)</code> and <code>setObject(int, Object, int, int)</code>. Close client resources associated with this agent, such as socket and streams for the net. flush() means to send all chained requests. -------------------- entry points ------------------------------------------
/////////////////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////////////////// Get the formatID which corresponds to this class. /////////////////////////////////////////////////////////////////////////////////  AliasInfo BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  Formatable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Read this object from a stream of stored objects. /////////////////////////////////////////////////////////////////////////////////  BIND TIME LOGIC  ///////////////////////////////////////////////////////////////////////////////// Set the collation type for string input and return types. This is used by dblook to reconstruct the aggregate-specific parts of the ddl needed to recreate this alias. Write this object to a stream of stored objects.
Get the aggregator that performs the aggregation on the input datatype at execution time.  If the input type can be handled, return a type descriptor with the resultant type information and fill in the string buffer with the name of the class that is used to perform the aggregation over the input type. If the aggregate cannot be performed on this type, then a null should be returned. <p> The aggregator class must implement a zero argument constructor.  The aggregator class can be the same class as the AggregateDefinition if it implements both interfaces. <p> The result datatype may be the same as the input datatype or a different datatype.  To create your own type descriptor to return to this method, see <i>com.ibm.db2j.types.TypeFactory</i>.
Bind this operator.  Determine the type of the subexpression, and pass that into the UserAggregate. * Make sure the aggregator class is ok Do code generation for this unary operator.  Should never be called for an aggregate -- it should be converted into something else by code generation time. Get the AggregateDefinition. Get the class that implements that aggregator for this node. Get the class that implements that aggregator for this node. Get the generated ResultColumn where this aggregate now resides after a call to replaceAggregatesWithColumnReference(). Get the generated ColumnReference to this aggregate after the parent called replaceAggregatesWithColumnReference(). Get the result column that has a new aggregator. This aggregator will be fed into the sorter. Get the aggregate expression in a new result column. Get the null aggregate result expression column. Get the SQL name of the aggregate * Instantiate the aggregate definition. Indicate whether this aggregate is distinct or not. Return true if this is a user-defined aggregate Replace aggregates in the expression tree with a ColumnReference to that aggregate, append the aggregate to the supplied RCL (assumed to be from the child ResultSetNode) and return the ColumnReference. This is useful for pushing aggregates in the Having clause down to the user's select at parse time.  It is also used for moving around Aggregates in the select list when creating the Group By node.  In that case it is called <B> after </B> bind time, so we need to create the column differently. Resolve a user-defined aggregate. Construct an AliasDescriptor for a modern builtin aggregate. initialize fields for user defined aggregate Print a string ref of this node.
Called prior to inserting a duplicate sort key.  We do aggregation here. Called prior to inserting a distinct sort key.
ValueNode override. QueryTreeNode override. Prints the sub-nodes of this object.
Accumulate the next scalar value Initialize the Aggregator <p> For merging another partial result into this Aggregator. This lets the SQL interpreter divide the incoming rows into subsets, aggregating each subset in isolation, and then merging the partial results together. This method can be called when performing a grouped aggregation with a large number of groups. While processing such a query, Derby may write intermediate grouped results to disk. The intermediate results may be retrieved and merged with later results if Derby encounters later rows which belong to groups whose intermediate results have been written to disk. This situation can occur with a query like the following: </p> <pre> select a, mode( b ) from mode_inputs group by a order by a </pre> Return the result scalar value
Get the name of the aggergate (e.g. MAX) Get the name of the class that implements the user aggregator for this class. Get the column number for the aggregator column. Get the column number for the input (addend) column. Get the column number for the output (result) column. Get the result description for the input value to this aggregate. Get the formatID which corresponds to this class. Is the aggergate distinct Read this object from a stream of stored objects. Get a string for the object ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out
Get the formatID which corresponds to this class. Indicate whether i have a distinct or not.  ////////////////////////////////////////////  FORMATABLE  ////////////////////////////////////////////
Drop the routine or synonym. For a routine its permission descriptors will be dropped as well. For a synonym its TableDescriptor will be dropped as well. Methods so that we can put AliasDescriptors on hashed lists Determine if two AliasDescriptors are the same. Gests the AliasInfo for the alias. Gets the type of the alias. Get the provider's type.  Provider interface     Gets the java class name of the alias. Gets the name of the alias. Gets the name space of the alias. Get the provider's UUID Return the name of this Provider.  (Useful for errors.)  Gets the full, qualified name of the alias. Gets the SchemaDescriptor for this alias. Gets the name of the schema that the alias lives in. Gets the UUID  of the schema for this method alias. Return the specific name for this object. Gets whether or not the alias is a system alias. Interface methods Gets the UUID  of the method alias. Get a hashcode for this AliasDescriptor Functions are persistent unless they are in the SYSFUN schema. Report whether this descriptor describes a Table Function. ////////////////////////////////////////////////////  DEPENDENT INTERFACE  //////////////////////////////////////////////////// Check that all of the dependent's dependencies are valid. Mark the dependent as invalid (due to at least one of its dependencies being invalid).  Always an error for an alias -- should never have gotten here. Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). /** * Sets the ID of the method alias * * @param aliasID	The UUID of the method alias to be set in the descriptor * * @return	Nothing */ public void setAliasID(UUID aliasID) { this.aliasID = aliasID; } Convert the AliasDescriptor to a String.
Get the name of the static method that the alias represents at the source database.  (Only meaningful for method aliases ) Return true if this alias is a Table Function.
Generate the {@code suites.All} test suite. Sub-suites should be added to {@link AllPackages#suite()} and not here.
Get a class's set of tests from its suite method through reflection. Ignore errors caused by the class version of the test class being higher than what's supported on this platform. <p> Get a list of test classes to add. The classes that have been compiled with target level equal to the lowest supported level are included as {@code java.lang.Class} objects. Classes compiled with higher target levels are included as {@code java.lang.String}s with the class names so that this method does not fail with class not found errors on some platforms. </p> <p> To construct a test suite from these classes, the classes' static {@code suite()} methods have to be called. </p> Get the class names of all the top-level JUnit test suites that are included in {@code suites.All}. Invoke the static {@code suite()} method on a test class. Print the class names of all the test suites included in {@code suites.All}.
Make a copy of this ResultColumn in a new ResultColumn Return the full table name qualification for this node

Statically calculates how many pages this extent can manage given the availspace number of bytes to store this extent in if read/writeExternal changes, this must change too methods specific to allocExtent write operation that is called underneath the log page goes thru the following transition: ALLOCATED_PAGE <-> deallocated page -> free page <-> ALLOCATED_PAGE Allocate this page - this is called underneath the log record Compress free pages at end of this extent. <p> Search backward from end of extent and prepare data structures to return pages at end of extent to the OS. Returns the lowest page that can be returned to the OS. <p> Deallocate logical page pagenum - this is called underneath the log record. pagenum must be a page managed by this extent and it must be valid Get the number of used page in this extent Get the first logical page number managed by this extent. read operation that is called above the log Get a page number that is free Get the last logical page number managed by this extent. page preallocation get the last preallocated pagenumber managed by this alloc page Get the logical page number that is bigger than prevPageNumber and is a valid page.  If no such page in this extent, return ContainerHandle.INVALID_PAGE_HANDLE Get the physical offset of pagenum. If deallocOK is true, then even if pagenum is deallocated, it is OK. If deallocOK is false, then an exception is thrown if pagenum is deallocated. An exception is always thrown if pagenum is a free page Return the status of a particular page translate bit position in map to page number. <p> Get the total number of pages in this extent Get the number of unfilled pages in this extent Get a page number that is unfilled, pagenum is the last page that was rejected. Return the status of this extent preallocated N pages, passed in the last preallocated page number. Keep track of unfilled pages, if the extent changed, returns true. Undo the compress space operation. <p> Undo of this operation doesn't really "undo" the operation, it just makes sure the data structures are ok after the undo.  We are guaranteed at the point of the transaction doing the Undo of the compress space operation fixes up the bit maps to only point at pages within the new_highest_page range. <p> Prior to logging the compress space operation all pages greater than There are only 2 possibilities at this point: 1) the truncate of pages greater than new_highest_page happened before the abort took place.  W 2)
Extract the container information from epage. borrowed space management Write the container information into the container information area. Add a page which is managed by this alloc page. Return the page number of the newly added page. <BR> MT - thread aware (latched) Chain the next page number and offset underneath a log record compress out empty pages at end of container. <p> Call the extent to update the data structure make the bit map look like contiguous free pages at the end of the extent no longer exist. Similar to preallocate do the operation unlogged, need to force the change to the extent before actually removing the space from the file. <p> The sequence is: 1) update extent data structure 2) force extent changes to disk 3) truncate pages If the system crashes between 1 and 2 then no changes are on disk. If the system crashes between 2 and 3 then there are extra pages in the file that extent does not know about, this is the same case as preallocation which the code already handes.  It will handle any set of pages from 0 to all of the intended pages being truncated.  The next allocate looks at actual size of file as does the right thing. <p> MT - expect Container level X lock Compress free pages. <p> Compress the free pages at the end of the range maintained by this allocation page.  All pages being compressed should be FREE. Only pages in the last allocation page can be compressed. <p> Methods of cachedPage - create, read and write up a page Overwriting StoredPage's CachedPage methods Create a new alloc page. Deallocate page Return a copy of the allocExtent to be cached by the container. the container must take care to maintain its coherency by invalidating the cache before any update. get the last pagenumber currently managed by this alloc page get the last preallocated pagenumber managed by this alloc page overwriting StoredPage methods get the largest page number this alloc page can manage. This is the different from the last pagenumber currently managed by this alloc page unless the alloc page is full and all the pages have been allocated format Id must fit in 4 bytes Return my format identifier. Initialize in memory structure using the buffer in pageData specific methods to AllocPage Return the next free page number after given page number Preallocate user page if needed. Do the actual page allocation/deallocation/ree underneath a log operation. Change the page status to new status Handle undo of compress space operation. update unfilled page information We will be using inputExtent's unfilledPage bitmap as the new bitmap, so caller of this routine need to not touch the bitmap after this call (in other words, call this ONLY in allocationCache invalidate and throw away the reference to the bitImpl) Write the page out
Loggable methods Allocate/deallocate/free this page number Return my format identifier.  method to support BeforeImageLogging debug Undoable methods Allocate/deallocate/free this page number.
Set the allocation status of pageNumber to doStatus.  To undo this operation, set the allocation status of pageNumber to undoStatus Chain one allocation page to the next. Compress free pages. <p> Compress the free pages at the end of the range maintained by this allocation page.  All pages being compressed should be FREE. Only pages in the last allocation page can be compressed. <p>
dump the allocation cache information Get the page number for the allocation page that is managing this page number returns estimated number of allocated pages Get the last (allocated) page of the container Get the last valid page of the file container.  A valid page is one that is not deallocated or freed. Get the next page (after pageNumber) that is valid Get the page status of a page shorthand to grow the 4 arrays to the desired size invalidate all extents invalidate the extent that is managed by this alloc page invalidate the last extent reset the allocation cache in case when filecontainer object is reused shorthand to set the 4 array values Set the page number to be unfilled Validate the cache, find all alloc pages and fill in the arrays
This is the guts of the Execution-time logic for ALTER CONSTRAINT.
Workhorse for adding a new column to a table.  Clear the state of this constant action.  For the trigger, get the trigger action sql provided by the user in the create trigger sql. This sql is saved in the system table. Since a column has been dropped from the trigger table, the trigger action sql may not be valid anymore. To establish that, we need to regenerate the internal representation of that sql and bind it again.  This method is called both on the WHEN clause (if one exists) and the triggered SQL statement of the trigger action.  Return true if the trigger was dropped by this method (if cascade is true and it turns out the trigger depends on the column being dropped), or false otherwise. Get rid of duplicates from a set of index conglomerate numbers and index descriptors. routine to process compress table or ALTER TABLE <t> DROP COLUMN <c>; <p> Uses class level variable "compressTable" to determine if processing compress table or drop column: if (!compressTable) must be drop column. <p> Handles rebuilding of base conglomerate and all necessary indexes. Iterate through the received list of CreateIndexConstantActions and execute each one, It's possible that one or more of the constant actions in the list has been rendered "unneeded" by the time we get here (because the index that the constant action was going to create is no longer needed), so we have to check for that. Defragment rows in the given table. <p> Scans the rows at the end of a table and moves them to free spots towards the beginning of the table.  In the same transaction all associated indexes are updated to reflect the new location of the base table row. <p> After a defragment pass, if was possible, there will be a set of empty pages at the end of the table which can be returned to the operating system by calling truncateEnd().  The allocation bit maps will be set so that new inserts will tend to go to empty and half filled pages starting from the front of the conglomerate. Workhorse for dropping a column from a table. This routine drops a column from a table, taking care to properly handle the various related schema objects. The syntax which gets you here is: ALTER TABLE tbl DROP [COLUMN] col [CASCADE|RESTRICT] The keyword COLUMN is optional, and if you don't specify CASCADE or RESTRICT, the default is CASCADE (the default is chosen in the parser, not here). If you specify RESTRICT, then the column drop should be rejected if it would cause a dependent schema object to become invalid. If you specify CASCADE, then the column drop should additionally drop other schema objects which have become invalid. You may not drop the last (only) column in a table. Schema objects of interest include: - views - triggers - constraints - check constraints - primary key constraints - foreign key constraints - unique key constraints - not null constraints - privileges - indexes - default values Dropping a column may also change the column position numbers of other columns in the table, which may require fixup of schema objects (such as triggers and column privileges) which refer to columns by column position number. Indexes are a bit interesting. The official SQL spec doesn't talk about indexes; they are considered to be an imlementation-specific performance optimization. The current Derby behavior is that: - CASCADE/RESTRICT doesn't matter for indexes - when a column is dropped, it is removed from any indexes which contain it. - if that column was the only column in the index, the entire index is dropped. Drop statistics of either all the indexes on the table or only one specific index depending on what user has requested. INTERFACE METHODS Run this constant action. This is the guts of the Execution-time logic for ALTER TABLE. Delete old index row and insert new index row in input index. <p> Get info on the indexes on the table being compressed. computes the minimum/maximum value in a column of a table.  Get the ran max or min range bound for an autoincrement column. class implementation Return the "semi" row count of a table.  We are only interested in whether the table has 0, 1 or &gt; 1 rows. RowSource interface  Workhorse for modifying column level constraints. Right now it is restricted to modifying a null constraint to a not null constraint. Workhorse for modifying the default value of a column. Change an identity from ALWAYS to BY DEFAULT (or vice versa) RowLocationRetRowSource interface   Purge committed deleted rows from conglomerate. <p> Scans the table and purges any committed deleted rows from the table.  If all rows on a page are purged then page is also reclaimed. <p>  Set up to update all of the indexes on a table when doing a bulk insert on an empty table. OBJECT METHODS Truncate end of conglomerate. <p> Returns the contiguous free space at the end of the table back to the operating system.  Takes care of space allocation bit maps, and OS call to return the actual space. <p> TRUNCATE TABLE  TABLENAME; (quickly removes all the rows from table and it's correctponding indexes). Truncate is implemented by dropping the existing conglomerates(heap,indexes) and recreating a new ones  with the properties of dropped conglomerates. Currently Store does not have support to truncate existing conglomerated until store supports it , this is the only way to do it. Error Cases: Truncate error cases same as other DDL's statements except 1)Truncate is not allowed when the table is references by another table. 2)Truncate is not allowed when there are enabled delete triggers on the table. Note: Because conglomerate number is changed during recreate process all the statements will be marked as invalide and they will get recompiled internally on their next execution. This is okay because truncate makes the number of rows to zero it may be good idea to recompile them becuase plans are likely to be incorrect. Recompile is done internally by Derby, user does not have any effect. Update all of the indexes on a table when doing a bulk insert on an empty table. Update a new column with its default. We could do the scan ourself here, but instead we get a nested connection and issue the appropriate update statement. Update statistics of either all the indexes on the table or only one specific index depending on what user has requested. Make sure that the columns are non null If any column is nullable, check that the data is null.
Accept the visitor for all visitable children of this node. We inherit the generate() method from DDLStatementNode. Bind this AlterTableNode.  This means doing any static error checking that can be done before actually creating the table. For example, verifying that the user is not trying to add a non-nullable column. Generate the ColumnInfo argument for the constant action. Return the number of constraints. Create the Constant information that will drive the guts of Execution. Generate arguments to constant action. Called by makeConstantAction() in this class and in our subclass RepAlterTableNode. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Return true if the node references SESSION schema tables (temporary or permanent) Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Generate code for no short-circuiting AND operator. Used to evaluate check constraints where at least one is deferrable, since we need to know exactly which constraint(s) violated the checks.
Bind this logical operator.  All that has to be done for binding a logical operator is to bind the operands, check that both operands are BooleanDataValue, and set the result type to BooleanDataValue. Finish putting an expression into conjunctive normal form.  An expression tree in conjunctive normal form meets the following criteria: o  If the expression tree is not null, the top level will be a chain of AndNodes terminating in a true BooleanConstantNode. o  The left child of an AndNode will never be an AndNode. o  Any right-linked chain that includes an AndNode will be entirely composed of AndNodes terminated by a true BooleanConstantNode. o  The left child of an OrNode will never be an OrNode. o  Any right-linked chain that includes an OrNode will be entirely composed of OrNodes terminated by a false BooleanConstantNode. o  ValueNodes other than AndNodes and OrNodes are considered leaf nodes for purposes of expression normalization. In other words, we won't do any normalization under those nodes. In addition, we track whether or not we are under a top level AndNode. SubqueryNodes need to know this for subquery flattening. Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. Do bind() by hand for an AndNode that was generated after bind(), eg by putAndsOnTop(). (Set the data type and nullability info.) Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Do the 1st step in putting an expression into conjunctive normal form.  This step ensures that the top level of the expression is a chain of AndNodes terminated by a true BooleanConstantNode. Verify that changeToCNF() did its job correctly.  Verify that: o  AndNode  - rightOperand is not instanceof OrNode leftOperand is not instanceof AndNode o  OrNode	- rightOperand is not instanceof AndNode leftOperand is not instanceof OrNode Verify that putAndsOnTop() did its job correctly.  Verify that the top level of the expression is a chain of AndNodes terminated by a true BooleanConstantNode.
Mixed types  illegal ambiguity bad return type  BIGINT  legal resolutions unresolvable bad return type  BINARY  legal resolutions  BLOB  legal resolutions bad return type /////////////////////////////////////////////////////////////////////////////////  CONSTANTS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////  This block of methods is used to test whether Derby matches primitives and wrapper objects according to the ANSI rules. The ANSI rules are described in DERBY-3652.  /////////////////////////////////////////////////////////////////  BOOLEAN  legal resolutions unresolvable  CHAR  legal resolutions  CLOB  legal resolutions  DATE  legal resolutions  DECIMAL  legal resolutions illegal ambiguity bad return type  DOUBLE  legal resolutions unresolvable illegal ambiguity bad return type  INTEGER  legal resolutions unresolvable bad return type  LONGVARBINARY  legal resolutions  LONGVARCHAR  legal resolutions  NUMERIC  legal resolutions illegal ambiguity bad return type  REAL  legal resolutions unresolvable illegal ambiguity outside the spec. these should not resolve. bad return type  SMALLINT  legal resolutions unresolvable  TIME  legal resolutions  TIMESTAMP  legal resolutions bad return type  VARBINARY  legal resolutions  BLOB arguments   CLOB arguments   VARCHAR  legal resolutions  Procedure with OUT parameters of wrapper type
If the result set has been opened, close the open scan. Return the requested value computed from the next row.  Return the total amount of time spent in this ResultSet  ResultSet interface (leftover from NoPutResultSet)  open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
/////////////////////////////////////////////////////////////////////////////////  FUNCTION ENTRY POINT (BOUND BY THE CREATE FUNCTION STATEMENT)  ///////////////////////////////////////////////////////////////////////////////// Create from a file name identifying the server log file /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> The Apache Server's logs format timestamps thusly: "01/Jul/2002:17:31:19 +0200" </p> /////////////////////////////////////////////////////////////////////////////////  OVERRIDES  ///////////////////////////////////////////////////////////////////////////////// <p> The Apache Server's logs represent nulls as "-". </p> <p> The Apache Server's logs format timestamps thusly: "01/Jul/2002:17:31:19 +0200" </p>
Check if the application requester is the same as this one Get the type of the client. get the Application requester manager level The timestamp length may be truncated for old versions of Derby. See DERBY-2602. Check if provided JCC version level is greaterThanOrEqualTo current level Is this an AppRequester that supports XA return true if XAMGR &gt;= 7, false otherwise Check whether two objects are not equal when 1 of the objects could be null set Application requester manager level if the manager level is less than the minimum manager level, set the manager level to zero (saying we can't handle this level), this will be returned to the application requester and he can decide whether or not to proceed For CCSIDMGR, if the target server supports the CCSID manager but not the CCSID requested, the value returned is FFFF For now, we won't support the CCSIDMGR since JCC doesn't request it. Get the maximum length supported for an exception's message parameter string. Tells whether the client sends a trailing Derby-specific status byte when transferring EXTDTA objects. Return true if the client contains the fix for DERBY-5236, which allows DDMWriter.writeLDString() to write strings that need up to 64K-1 bytes when represented in UTF-8. Otherwise, writeLDString() should use the old maximum length, which is 32700 bytes. Check if the client expects QRYCLSIMP to be supported when the protocol is LMTBLKPRC. Returns true if Derby's client driver supports SECMEC_USRSSBPWD DRDA security mechanism. Returns whether our AppRequester's UNICODEMGR supports UTF8 (CCSID 1208)

///////////////////////////////////////////////////////////////////////  TABLE FUNCTION  /////////////////////////////////////////////////////////////////////// <p> Entry point for creating an ArchiveVTI which is bound to a Derby table function by a CREATE FUNCTION statement which looks like this: </p> <pre> create function t1( archiveSuffix varchar( 32672 ) ) returns table ( keyCol int, aCol int, bCol int ) language java parameter style derby_jdbc_result_set reads sql data external name 'org.apache.derbyTesting.functionTests.tests.lang.ArchiveVTI.archiveVTI' </pre> <p> Get this database session's connection to the database. </p> ///////////////////////////////////////////////////////////////////////  AwareVTI BEHAVIOR  /////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  UTILITY METHODS  ////////////////////////////////////////////////////////////////////// <p> Get cursors on all the tables which we are going to union together. </p> //////////////////////////////////////////////////////////////////////  RestrictedVTI BEHAVIOR  ////////////////////////////////////////////////////////////////////// <p> Compile the query against the next table and use its ResultSet until it's drained. </p> //////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  //////////////////////////////////////////////////////////////////////
Clears the limit by setting the limit to be the entire byte array. Return a reference to the array of bytes this stream is going to read from so that caller may load it with stuff * Methods of InputStream Read a compressed int from the stream. <p> Read a compressed int from the stream, which is assumed to have been written by a call to CompressNumber.writeInt(). <p> Code from CompressedNumber is inlined here so that these fields can be read from the array with a minimum of function calls. <p> The format of a compressed int is as follows: Formats are (with x representing value bits): <PRE> 1 Byte- 00xxxxxx                            val &lt;= 63 (0x3f) 2 Byte- 01xxxxxx xxxxxxxx                   val &gt; 63 &amp;&amp; &lt;= 16383 (0x3fff) 4 byte- 1xxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx val &gt; 16383 &amp;&amp; &lt;= MAX_INT </PRE> Read a compressed long from the stream. <p> Read a compressed long from the stream, which is assumed to have been written by a call to CompressNumber.writeLong(). <p> Code from CompressedNumber is inlined here so that these fields can be read from the array with a minimum of function calls. <p> The format of a compressed int is as follows: Formats are (with x representing value bits): <PRE> value &gl;= 16383 (0x3fff): 2 byte - 00xxxxxx xxxxxxxx value &gt; 16383 &amp;&amp; &lt;= 0x3fffffff: 4 byte - 01xxxxxx xxxxxxxx xxxxxxxx xxxxxxxx value &lt; 0x3fffffff &amp;&lt; &lt;= MAX_LONG: 8 byte - 1xxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx </PRE> read in a Derby UTF formated string into a char[]. <p> This routine inline's the code to read a UTF format string from a byte[] array (pageData), into a char[] array.  The string will be read into the char[] array passed into this routine through rawData_array[0] if it is big enough.  If it is not big enough a new char[] will be alocated and returned to the caller by putting it into rawData_array[0]. <p> To see detailed description of the Derby UTF format see the writeExternal() routine of SQLChar. <p> The routine returns the number of char's read into the returned char[], note that this length may smaller than the actual length of the char[] array. <p> The stream must be positioned on the first user byte when this method is invoked. * Methods of DataInput * Public methods Set the array of bytes to be read. Position is set to zero. * Methods of Limit A setLimit which also sets the position to be offset. Skip as many bytes as possible, but no more than {@code count}. Skip as many bytes as possible, but no more than {@code n}.
* Methods of LengthOutputStream Set the position of the stream pointer. * Methods of OutputStream
Make the contents of an array available as a read-only list. If the array is null, an empty list will be returned. Copy a (possibly null) array of booleans Copy a (possibly null) array of bytes Copy a (possibly null) array of ints /////////////////////////////////////////////////////////////////  Methods to copy arrays.  ///////////////////////////////////////////////////////////////// Copy an array of objects; the original array could be null Copy a (possibly null) array of longs Copy a (possibly null) 2-dimensional array of ints Read an array of objects out of a stream. Read the length of an array of objects in an object stream. Read an array of integers from an ObjectInput. This allocates the array. Read an array of integers from an ObjectInput. This allocates the array. Read an array of integers from an ObjectInput. This allocates the array. Reads an array of objects from the stream. Read an array of strings from an ObjectInput. This allocates the array. Write an array of objects and length to an output stream. Does equivalent of writeArrayLength() followed by writeArrayItems() Write an array of objects to an output stream. /////////////////////////////////////////////////////////////////  Methods for Arrays of OBJECTS.  Cannot be used for an array of primitives, see below for something for primitives  ///////////////////////////////////////////////////////////////// Write the length of an array of objects to an output stream. The length /////////////////////////////////////////////////////////////////  Methods for Arrays of BOOLEANS  ///////////////////////////////////////////////////////////////// Write an array of booleans to an ObjectOutput. This writes the array in a format readBooleanArray understands. /////////////////////////////////////////////////////////////////  Methods for Arrays of INTs  ///////////////////////////////////////////////////////////////// Write an array of integers to an ObjectOutput. This writes the array in a format readIntArray understands. /////////////////////////////////////////////////////////////////  Methods for Arrays of LONGs  ///////////////////////////////////////////////////////////////// Write an array of longs to an ObjectOutput. This writes the array in a format readLongArray understands.

ConnectionEventListener methods Tell the caller if we received Connection closed event Tell the caller if we received Connection error event Clear the event received flags for this listener.
Dumps stack traces for all the threads if the JVM supports it. The result is returned as a string, ready to print. If the JVM doesn't have the method Thread.getAllStackTraces i.e, we are on a JVM &lt; 1.5, or  if we don't have the permissions: java.lang.RuntimePermission "getStackTrace" and "modifyThreadGroup", a message saying so is returned instead. Returns the thread dump stored in this AssertFailure as a string. Overrides printStackTrace() in java.lang.Throwable to include the thread dump after the normal stack trace. Overrides printStackTrace(PrintStream s) in java.lang.Throwable to include the thread dump after the normal stack trace. Overrides printStackTrace(PrintWriter s) in java.lang.Throwable to include the thread dump after the normal stack trace. Tells if generating a thread dump is supported in the running JVM.

Will be used to calculate the shipping interval based on the fill information obtained from the log buffer. This method uses the following steps to arrive at the shipping interval, a) FI &gt;= FI_HIGH return -1 (signifies that the waiting time should be 0) b) FI &gt;  FI_LOW and FI &lt; FI_HIGH return minShippingInterval c) FI &lt;= FI_LOW return maxShippingInterval. Transmits all the log records in the log buffer to the slave. updates the information about the latest instance of the log record that has been flushed to the disk. Calling this method has no effect in this asynchronous implementation of the log shipper. Transmits a chunk of log record from the log buffer to the slave, used by the master controller when the log buffer is full and some space needs to be freed for further log records. Get the highest log instant shipped so far Load relevant system properties: max and min log shipping interval Ships log records from the log buffer to the slave being replicated to. The log shipping happens between shipping intervals of time, the shipping interval being derived from the fill information (an indicator of load in the log buffer) obtained from the log buffer. The shipping can also be triggered in the following situations, 1) Based on notifications from the log buffer, where the fill information is again used as the basis to decide whether a shipping should happen or not 2) On a forceFlush triggered by the log buffer becoming full and the LogBufferFullException being thrown. Retrieves a chunk of log records, if available, from the log buffer and transmits them to the slave. Used for both periodic and forced shipping. Stop shipping log records. If a ship is currently in progress it will not be interrupted, shipping will stop only after the current shipment is done. Used to notify the log shipper that a log buffer element is full. This method would basically use the following steps to decide on the action to be taken when a notification from the log shipper is received, a) Get FI from log buffer b) If FI &gt;= FI_HIGH b.1) notify the log shipper thread. c) Else If the time elapsed since last ship is greater than minShippingInterval c.1) notify the log shipper thread.

This is exact.


Authenticate a User inside Derby. <p> Get the name of the credentials database used to authenticate system-wide operations. This returns null for all implementations except NATIVE authentication. </p>
* Methods of AuthenticationService Authenticate a User inside JBMS.T his is an overload method. We're passed-in a Properties object containing user credentials information (as well as database name if user needs to be validated for a certain database access). /* * Methods of module control - To be overriden Start this module.  In this case, nothing needs to be done. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Find the data dictionary for the current connection. Get all the database properties. Returns a property if it was set at the database or system level. Treated as SERVICE property by default. Privileged module lookup. Must be package protected so that user code can't call this entry point. <p> Get the name of the database if we are performing authentication at the database level. </p> Privileged service name lookup. Must be private so that user code can't call this entry point. <p> Get a transaction for performing authentication at the database level. </p> <p> This method hashes a clear user password using a Single Hash algorithm such as SHA-1 (SHA equivalent) (it is a 160 bits digest) </p> <p> The digest is returned as an object string. </p> <p> This method is only used by the SHA-1 authentication scheme. </p> <p> Hash a password using the default message digest algorithm for this system before it's stored in the database. </p> <p> If the data dictionary supports the configurable hash authentication scheme, and the property {@code derby.authentication.builtin.algorithm} is a non-empty string, the password will be hashed using the algorithm specified by that property. Otherwise, we fall back to the new authentication scheme based on SHA-1. The algorithm used is encoded in the returned token so that the code that validates a user's credentials knows which algorithm to use. </p> * Methods of PropertySetCallback  Parse the value of the password lifetime property. Return null if it is bad. Parse the value of the password expiration threshold property. Return null if it is bad. Class implementation  Strong Password Substitution (USRSSBPWD). This method generates a password substitute to authenticate a client which is using a DRDA security mechanism such as SECMEC_USRSSBPWD. Depending how the user is defined in Derby and if BUILTIN is used, the stored password can be in clear-text (system level) or encrypted (hashed - *not decryptable*)) (database level) - If the user has authenticated at the network level via SECMEC_USRSSBPWD, it means we're presented with a password substitute and we need to generate a substitute password coming from the store to compare with the one passed-in. The substitution algorithm used is the same as the one used in the SHA-1 authentication scheme ({@link PasswordHasher#ID_PATTERN_SHA1_SCHEME}), so in the case of database passwords stored using that scheme, we can simply compare the received hash with the stored hash. If the configurable hash authentication scheme {@link PasswordHasher#ID_PATTERN_CONFIGURABLE_HASH_SCHEME} is used, we have no way to find out if the received hash matches the stored password, since we cannot decrypt the hashed passwords and re-apply another hash algorithm. Therefore, strong password substitution only works if the database-level passwords are stored with the SHA-1 scheme. NOTE: A lot of this logic could be shared with the DRDA decryption and client encryption managers - This will be done _once_ code sharing along with its rules are defined between the Derby engine, client and network code (PENDING). Substitution algorithm works as follow: PW_TOKEN = SHA-1(PW, ID) The password (PW) and user name (ID) can be of any length greater than or equal to 1 byte. The client generates a 20-byte password substitute (PW_SUB) as follows: PW_SUB = SHA-1(PW_TOKEN, RDr, RDs, ID, PWSEQs) w/ (RDs) as the random client seed and (RDr) as the server one. See PWDSSB - Strong Password Substitution Security Mechanism (DRDA Vol.3 - P.650) <p> Convert a string into a byte array in hex format. </p> <p> For each character (b) two bytes are generated, the first byte represents the high nibble (4 bits) in hexadecimal ({@code b & 0xf0}), the second byte represents the low nibble ({@code b & 0x0f}). </p> <p> The character at {@code str.charAt(0)} is represented by the first two bytes in the returned String. </p> <p> New code is encouraged to use {@code String.getBytes(String)} or similar methods instead, since this method does not preserve all bits for characters whose codepoint exceeds 8 bits. This method is preserved for compatibility with the SHA-1 authentication scheme. </p>
Verify the connected user is authorized to perform the requested operation. This variation should only be used with operations that do not use tables or routines. If the operation involves tables or routines then use the variation of the authorize method that takes an Activation parameter. The activation holds the table, column, and routine lists. Verify the connected user possesses the indicated permissions Verify the connected user is authorized to perform the requested operation. Get the readOnly status for this authorizer's connection. Refresh this authorizer to reflect a change in the database permissions. Set the readOnly status for this authorizer's connection.
Reads a single byte from the underlying stream. Reads a number of bytes from the underlying stream and stores them in the specified byte array. Reads a number of bytes from the underlying stream and stores them in the specified byte array at the specified offset. Checks if positionedStream's position was changed since last used, sets the position to right position if its changed. Skips up to the specified number of bytes from the underlying stream.
flush a counter to disk; i.e write the current value of the counter into the row in SYSCOLUMNS. get the column position in the table for which this counter has been created. get the current value of the counter. An uninitialized counter means the current value is NULL. return the identity of the counter. get the start value make a unique key for the counter. make a unique key for the counter. reset to the counter to the beginning or the end. update the counter to its next value. update the counter.
* Methods from java.sql.Driver. Accept anything that starts with <CODE>jdbc:derby:</CODE>. Connect to the URL if possible /////////////////////////////////////////////////////////////////////  Support for booting and shutting down the engine.  ///////////////////////////////////////////////////////////////////// * Retrieve the driver which is specific to our JDBC level. * We defer real work to this specific driver. Returns the driver's major version number. Returns the driver's minor version number. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Returns an array of DriverPropertyInfo objects describing possible properties. * Return true if the engine has been booted. Report whether the Driver is a genuine JDBC COMPLIANT (tm) driver. Load the most capable driver available. But if the vm level doesn't support it, then we fall back on a lower-level driver. * Record which driver module actually booted. * Unregister the driver and the AutoloadedDriver if exists. This happens when the engine is forcibly shut down.
This method is called by the page manager when it's about to evict a page which is holding an aux object, or when a rollback occurred on the page.  The aux object should release its resources.  The aux object can assume that no one else has access to it via the raw store during this method call. After this method returns the raw store throws away any reference to this object.
Return the result of the aggregation.  If the count is zero, then we haven't averaged anything yet, so we return null.  Otherwise, return the running average as a double. ///////////////////////////////////////////////////////////  FORMATABLE INTERFACE  /////////////////////////////////////////////////////////// Get the formatID which corresponds to this class.   ///////////////////////////////////////////////////////////  EXTERNALIZABLE INTERFACE  ///////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  /////////////////////////////////////////////////////////////// Get the table function context Set the table function context
* Methods of B2I. Create an empty secondary index b-tree, using the generic b-tree to do the generic part of the creation process. This routine opens the newly created container, adds a single page, and makes this page the root by inserting a LeafControlRow onto this page at slot 0 and marking in that control row that the page is a root page. The following properties are specific to the b-tree secondary index: <UL> <LI> "baseConglomerateId" (integer).  The conglomerate id of the base conglomerate is never actually accessed by the b-tree secondary index implementation, it only serves as a namespace for row locks. This property is required. <LI> "rowLocationColumn" (integer).  The zero-based index into the row which the b-tree secondary index will assume holds a @see RowLocation of the base row in the base conglomerate.  This value will be used for acquiring locks.  In this implementation RowLocationColumn must be the last key column. This property is required. </UL> A secondary index i (a, b) on table t (a, b, c) would have rows which looked like (a, b, row_location).  baseConglomerateId is set to the conglomerate id of t.  rowLocationColumns is set to 2.  allowsDuplicates would be set to false, @see BTree#create.  To create a unique secondary index set uniquenessColumns to 2, this means that the btree code will compare the key values but not the row id when determing uniqueness.  To create a nonunique secondary index set uniquenessColumns to 3, this would mean that the uniqueness test would include the row location and since all row locations will be unique  all rows inserted into the index will be differentiated (at least) by row location. Open a b-tree compress scan. <p> B2I does not support a compress scan. <p> Drop this b-tree secondary index. * Methods of Conglomerate Retrieve the maximum value row in an ordered conglomerate. <p> Returns true and fetches the rightmost row of an ordered conglomerate into "fetchRow" if there is at least one row in the conglomerate.  If there are no rows in the conglomerate it returns false. <p> Non-ordered conglomerates will not implement this interface, calls will generate a StandardException. <p> RESOLVE - this interface is temporary, long term equivalent (and more) functionality will be provided by the openBackwardScan() interface. ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Protected locking implmentations of abtract BTree routines: getBtreeLockingPolicy lockTable ************************************************************************* Create a new btree locking policy from scratch. Return static information about the conglomerate to be included in a a compiled plan. <p> The static info would be valid until any ddl was executed on the conglomid, and would be up to the caller to throw away when that happened.  This ties in with what language already does for other invalidation of static info.  The type of info in this would be containerid and array of format id's from which templates can be created. The info in this object is read only and can be shared among as many threads as necessary. <p> * Methods of Storable (via Conglomerate via BTree). * This class is responsible for re/storing its * own state and calling its superclass to store its'. Storable interface, implies Externalizable, TypedFormat Return my format identifier. Bulk Load a B-tree secondary index. Lock the base table. <p> Assumes that segment of the base container is the same as the segment of the btree segment. <p> RESOLVE - we really want to get the lock without opening the container. raw store will be providing this. Open a b-tree controller. Open a b-tree secondary index scan controller. Return an open StoreCostController for the conglomerate. <p> Return an open StoreCostController which can be used to ask about the estimated row counts and costs of ScanController and ConglomerateController operations, on the given conglomerate. <p> Restore the in-memory representation from the stream. <p> ************************************************************************ Private methods of B2I, arranged alphabetically. ************************************************************************* Store the stored representation of the column value in the stream. <p> For more detailed description of the ACCESS_B2I_V3_ID and ACCESS_B2I_V5_ID formats see documentation at top of file. Store the stored representation of the column value in the stream. <p> For more detailed description of the ACCESS_B2I_V3_ID format see documentation at top of file. Store the stored representation of the column value in the stream. <p> For more detailed description of the ACCESS_B2I_V3_ID and ACCESS_B2I_V4_ID formats see documentation at top of file.
* Methods of ConglomerateController. Close the conglomerate controller. <p> Any changes to this method will probably have to be reflected in close as well. <p> Currently delegates to OpenBTree.  If the btree controller ends up not having any state of its own, we can remove this method (the VM will dispatch to OpenBTree), gaining some small efficiency.  For now, this method remains for clarity. Insert a row into the conglomerate.

* Methods of ModuleControl. Create the conglomerate and return a conglomerate object for it. * Methods of MethodFactory (via ConglomerateFactory) Return the default properties for this kind of conglomerate. * Methods of ConglomerateFactory Return the conglomerate factory id. <p> Return a number in the range of 0-15 which identifies this factory. Code which names conglomerates depends on this range currently, but could be easily changed to handle larger ranges.   One hex digit seemed reasonable for the number of conglomerate types being currently considered (heap, btree, gist, gist btree, gist rtree, hash, others? ). <p> Privileged Monitor lookup. Must be private so that user code can't call this entry point. Interface to be called when an undo of an insert is processed. <p> Implementer of this class provides interface to be called by the raw store when an undo of an insert is processed.  Initial implementation will be by Access layer to queue space reclaiming events if necessary when a rows is logically "deleted" as part of undo of the original insert.  This undo can happen a lot for many applications if they generate expected and handled duplicate key errors. <p> Caller may decide to call or not based on deleted row count of the page, or if overflow rows/columns are present. Return the primary format that this access method supports. The btree currently only supports one format. Return the primary implementation type for this access method. The btree only has one implementation type, "BTREE". Return Conglomerate object for conglomerate with conglomid. <p> Return the Conglomerate Object.  This is implementation specific. Examples of what will be done is using the id to find the file where the conglomerate is located, and then executing implementation specific code to instantiate an object from reading a "special" row from a known location in the file.  In the btree case the btree conglomerate is stored as a column in the control row on the root page. <p> This operation is costly so it is likely an implementation using this will cache the conglomerate row in memory so that subsequent accesses need not perform this operation. <p> The btree object returned by this routine may be installed in a cache so the object must not change. Return whether this access method supports the format supplied in the argument. The btree currently only supports one format. Return whether this access method implements the implementation type given in the argument string. The btree only has one implementation type, "BTREE".
Close the scan. Close the scan, a commit or abort is about to happen. Initialize the scan for use. <p> Any changes to this method may have to be reflected in close as well. <p> The btree init opens the container (super.init), and stores away the state of the qualifiers.  The actual searching for the first position is delayed until the first next() call. Open the container after it has been closed previously. <p> Open the container, obtaining necessary locks.  Most work is actually done by RawStore.openContainer().  Will only reopen() if the container is not already open.
Close the scan. Close the scan, a commit or abort is about to happen. Initialize the scan for use. <p> Any changes to this method may have to be reflected in close as well. <p> The btree init opens the container (super.init), and stores away the state of the qualifiers.  The actual searching for the first position is delayed until the first next() call.
************************************************************************ Abstract Protected lockNonScan*() locking methods of BTree: lockNonScanPreviousRow   - lock the row previous to the current lockNonScanRow           - lock the input row ************************************************************************* Lock the row previous to the input row. <p> See BTree.lockPreviousRow() for more info. Lock the in memory row. <p> See BTree.lockRow() for more info. ************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Abstract Protected lockScan*() locking methods of BTree: lockScanRow              - lock row unlockScanRecordAfterRead- unlock the scan record ************************************************************************* Lock a btree row to determine if it is a committed deleted row. <p> Request an exclusive lock on the row located at the given slot, NOWAIT. Return true if the lock is granted, otherwise false. <p> Lock a row as part of doing the scan. <p> Lock the row at the given slot (or the previous row if slot is 0). <p> If this routine returns true all locks were acquired while maintaining the latch on leaf.  If this routine returns false, locks may or may not have been acquired, and the routine should be called again after the client has researched the tree to reget the latch on the appropriate page. Release read lock on a row.
************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Abstract Protected lockScan*() locking methods of BTree: lockScanRow              - lock row, only if row is forUpdate and not a previous key lock. unlockScanRecordAfterRead- unlock the scan record if we locked it in lockScanRow. ************************************************************************* Lock a row as part of doing the scan. <p> Lock the row at the given slot (or the previous row if slot is 0). <p> If this routine returns true all locks were acquired while maintaining the latch on leaf.  If this routine returns false, locks may or may not have been acquired, and the routine should be called again after the client has researched the tree to reget the latch on the appropriate page. Release read lock on a row. Because this is read uncommitted no S row locks will have been requested, thus none need be released.  The only locks that need to be released are U locks requested if the scan was opened for update.
************************************************************************ Public Methods of This class: ************************************************************************* Release read lock on a row.
************************************************************************ Protected methods of This class: ************************************************************************* Lock a row as part of doing the scan. <p> Lock the row at the given slot (or the previous row if slot is 0). <p> If this routine returns true all locks were acquired while maintaining the latch on leaf.  If this routine returns false, locks may or may not have been acquired, and the routine should be called again after the client has researched the tree to reget the latch on the appropriate page. ************************************************************************ Abstract Protected lockNonScan*() locking methods of BTree: lockNonScanPreviousRow   - lock the row previous to the current lockNonScanRow           - lock the input row ************************************************************************* Lock the row previous to the input row. <p> See BTreeLockingPolicy.lockNonScanPreviousRow Lock the in memory row. <p> See BTree.lockRow() for more info. ************************************************************************ Private methods of This class: ************************************************************************* Lock key previous to first key in btree. <p> In the previous key locking protocol repeatable read and phantom protection is guaranteed by locking a range of keys in the btree. The range is defined by the key previous to the first key you look at and all subsequent keys you look at.  The first key in the index is a special case, as there are no keys previous to it.  In that case a special key is declared the "previous key" to the first key in the btree and is locked instead. <p> In this implementation that first key is defined to be in the base container, page ContainerHandle.FIRST_PAGE_NUMBER, record id PREVIOUS_KEY_HANDLE. <p> Note that the previous key is the same for all indexes on a given conglomerate.  It seemed better for all locks on a base table to have the same containerid, rather than having some locks generated from a btree have a containerid from base table and some having a containerid from the btree.  If this turns out to be a problem we could either have 2 different containerid's, be more creative with the record id, or even add more to the lock key. Lock a btree row (row is at given slot in page). <p> Lock the row at the given slot in the page.  Meant to be used if caller only has the slot on the page to be locked, and has not read the row yet.  This routine fetches the row location field from the page, and then locks that rowlocation in the base container. <p> Lock a btree row, enforcing the standard lock/latch protocol. On return the row is locked.  Return status indicates if the lock was waited for, which will mean a latch was dropped while waiting. In general a false status means that the caller will either have to research the tree unless some protocol has been implemented that insures that the row will not have moved while the latch was dropped. <p> This routine request a row lock NOWAIT on the in-memory row "current_row.".  If the lock is granted the routine will return true. If the lock cannot be granted NOWAIT, then the routine will release the latch on "current_leaf" and "aux_leaf" (if aux_leaf is non-null), and then it will request a WAIT lock on the row. <p> ************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Abstract Protected lockScan*() locking methods of BTree: lockScanRow              - lock row unlockScanRecordAfterRead- unlock the scan record ************************************************************************* Lock a btree row to determine if it is a committed deleted row. <p> Lock a row as part of doing the scan. <p> Lock the row at the given slot (or the previous row if slot is 0). <p> If this routine returns true all locks were acquired while maintaining the latch on leaf.  If this routine returns false, locks may or may not have been acquired, and the routine should be called again after the client has researched the tree to reget the latch on the appropriate page. move left in btree and lock previous key. <p> Enter routine with "current_leaf" latched.  This routine implements the left travel ladder locking protocol to search the leaf pages from right to left for the previous key to 1st key on current_leaf. There are 2 cases: 1) the previous page has keys, in which case the last key on that page is locked, other wise search continues on the next page to the left. 2) there are no keys on the current page and there is no page to the left.  In this case the special "leftmost key" lock is gotten by calling lockPreviousToFirstKey(). Left laddar locking is used if all latches can be obtained immediately with NOWAIT.  This means that current latch is held while asking for left latch NOWAIT, and if left latch is granted then subsequently current latch can be released.  If this protocol is followed and all latches are granted then caller is guaranteed that the correct previous key has been locked and current_page latch remains.  The NOWAIT protocol is used to avoid latch/latch deadlocks.  The overall protocol is that one never holds a latch while waiting on another unless the direction of travel is down and to the right. <p> If along the search a latch has to be waited on then latches are released and a wait is performed, and "false" status is returned to caller.  In this case the routine can no longer be sure of it's current position and may have to retry the whole operation. Release read lock on a row. For serializable, there is no work to do.
************************************************************************ Public Methods of This class: ************************************************************************* Lock a row as part of doing the scan. <p> Lock the row at the given slot (or the previous row if slot is 0). <p> If this routine returns true all locks were acquired while maintaining the latch on leaf.  If this routine returns false, locks may or may not have been acquired, and the routine should be called again after the client has researched the tree to reget the latch on the appropriate page. Unlock a record after it has been locked for read. <p> In repeatable read only unlock records which "did not qualify".  For example in a query like "select * from foo where a = 1" on a table with no index it is only necessary to hold locks on rows where a=1, but in the process of finding those rows the system will get locks on other rows to verify they are committed before applying the qualifier.  Those locks can be released under repeatable read isolation. <p> if it is forUpdate then get S lock and release U lock, else there is nothing to do in serializable - we keep the S locks until end of transaction.
************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Public Methods of StaticCompiledOpenConglomInfo Interface: ************************************************************************* return the "Conglomerate". <p> For secondaryindex compiled info return the secondary index conglomerate. <p> Return my format identifier. ************************************************************************ Public Methods of Storable Interface (via StaticCompiledOpenConglomInfo): This class is responsible for re/storing its own state. ************************************************************************* Return whether the value is null or not. The containerid being zero is what determines nullness;  subclasses are not expected to override this method. Restore the in-memory representation from the stream. Restore the in-memory representation to the null value. The containerid being zero is what determines nullness;  subclasses are not expected to override this method. Store the stored representation of the column value in the stream. It might be easier to simply store the properties - which would certainly make upgrading easier.

Find the page and record to undo.  If no logical undo is necessary, i.e., row has not moved, then just return the latched page where undo should go.  If the record has moved, it has a new recordId on the new page, this routine needs to call pageOp.resetRecord with the new RecordHandle so that the logging system can update the compensation Operation with the new location. Return my format identifier. Restore the in-memory representation from the stream. This object has no state, so nothing to restore. This object has no state, so nothing to write.
Return my format identifier. Store the stored representation of the column value in the stream. <p> For more detailed description of the format see documentation at top of file.
************************************************************************ Public Methods required by Storable interface, implies Externalizable, TypedFormat: ************************************************************************* Return my format identifier. <p> This identifier was used for B2I in all Derby versions prior to and including 10.2.  Databases hard upgraded to a version subsequent to 10.2 will write the new format, see B2I.  Databases created in a version subsequent to 10.2 will also write the new formate, see B2I. Store the stored representation of the column value in the stream. <p> For more detailed description of the format see documentation at top of file.
ClassBuilder interface  add a field to this class. Fields cannot be initialized here, they must be initialized in the static initializer code (static fields) or in the constructors. <p> static fields also added to this list, with the modifier set appropriately. Add the fact that some class limit was exceeded while generating the class. Text is the simple string passed in. Add the fact that some class limit was exceeded while generating the class. We create a set of them and report at the end, this allows the generated class file to still be dumped. At the time the class is completed and bytecode generated, if there are no constructors then the default no-arg constructor will be defined. the class's unqualified name  class interface  Let those that need to get to the classModify tool to alter the class definition. a constructor. Once it is created, thrown exceptions, statements, and local variable declarations must be added to it. It is put into its defining class when it is created. <verbatim> Java: #modifiers #className() {} // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. // className is taken from definingClass.getName() </verbatim> <p> This is used to start a constructor as well; pass in null for the returnType when used in that manner. <p> See Modifiers a method. Once it is created, thrown exceptions, statements, and local variable declarations must be added to it. It is put into its defining class when it is created. <verbatim> Java: #modifiers #returnType #methodName() {} // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. </verbatim> <p> This is used to start a constructor as well; pass in null for the returnType when used in that manner. See java.lang.reflect.Modifiers a method with parameters. Once it is created, thrown exceptions, statements, and local variable declarations must be added to it. It is put into its defining class when it is created. <verbatim> Java: #modifiers #returnType #methodName() {} // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. </verbatim> <p> This is used to start a constructor as well; pass in null for the returnType when used in that manner. See java.lang.reflect.Modifiers

ModuleControl interface  Start this module. We need a read/write version of the class utilities * CacheableFactory interface  JavaFactory interface  a class.  Once it is created, fields, methods, interfaces, static initialization code, and constructors can be added to it. <verbatim> Java: package #packageName; #modifiers #className extends #superClass { } // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. </verbatim> See java.lang.reflect.Modifiers Privileged startup. Must be private so that user code can't call this entry point. Stop this module.  In this case, nothing needs to be done. /////////////////////////////////////////  UTILITIES specific to this implementation  ////////////////////////////////////////// Get the VM Type ID that corresponds with the given java type name. This uses the cache of VM type ids. Map vm types as strings to vm types as the VM handles, with int ids. Used in mapping opcodes based on type of operand/stack entry available.

Write a instruction that uses a constant pool entry as an operand, add a limit exceeded message if the number of constant pool entries has exceeded the limit. a throwable can be added to the end of the list of thrownExceptions. Call a sub-method created by getNewSubMethod handling parameters correctly. when the method has had all of its parameters and thrown exceptions defined, and its statement block has been completed, it can be completed and its class file information generated. <p> further alterations of the method will not be reflected in the code generated for it. class interface In their giveCode methods, the parts of the method body will want to get to the constant pool to add their constants. We really only want them treating it like a constant pool inclusion mechanism, we could write a wrapper to limit it to that.   MethodBuilder interface  Return the logical name of the method. The current myEntry refers to the sub method we are currently overflowing to. Those sub-methods are hidden from any caller. Create a sub-method from this method to allow the code builder to split a single logical method into multiple methods to avoid the 64k per-method code size limit. The sub method with inherit the thrown exceptions of this method. Push the contents of the described static field onto the stack. Check to see if the current method byte code is nearing the limit of 65535. If it is start overflowing to a new method. <P> Overflow is handled for a method named e23 as: <CODE> public Object e23() { ... existing code // split point return e23_0(); } private Object e23_0() { ... first set overflowed code // split point return e23_1(); } private Object e23_1() { ... second set overflowed code // method complete return result; } </CODE> <P> These overflow methods are hidden from the code using this MethodBuilder, it continues to think that it is building a single method with the original name. <BR> Restrictions: <UL> <LI> Only handles methods with no arguments <LI> Stack depth must be zero </UL> Push an integer value. Uses the special integer opcodes for the constants -1 to 5, BIPUSH for values that fit in a byte and SIPUSH for values that fit in a short. Otherwise uses LDC with a constant pool entry. Push a long value onto the stack. For the values zero and one the LCONST_0 and LCONST_1 instructions are used. For values betwee Short.MIN_VALUE and Short.MAX_VALUE inclusive an byte/short/int value is pushed using push(int, Type) followed by an I2L instruction. This saves using a constant pool entry for such values. All other values use a constant pool entry. For values in the range of an Integer an integer constant pool entry is created to allow sharing with integer constants and to reduce constant pool slot entries. Create an array instance Stack ... =&gt; ...,arrayref Pop the top stack value and store it in the instance field of this class. Pop the top stack value and store it in the field. This call requires the instance to be pushed by the caller. Upon entry the top word(s) on the stack is the value to be put into the field. Ie. we have <PRE> word </PRE> Before the call we need <PRE> word this word </PRE> word2,word1 -&gt; word2, word1, word2 So that we are left with word after the put. come in with ref, value Set the field but don't duplicate its value so nothing is left on the stack after this call. Attempt to split a large method by pushing code out to several sub-methods. Performs a number of steps. <OL> <LI> Split at zero stack depth. <LI> Split at non-zero stack depth (FUTURE) </OL> If the class has already exceeded some limit in building the class file format structures then don't attempt to split. Most likely the number of constant pool entries has been exceeded and thus the built class file no longer has integrity. The split code relies on being able to read the in-memory version of the class file in order to determine descriptors for methods and fields. Tell if statement number in this method builder hits limit.  This method builder keeps a counter of how many statements are added to it. Caller should call this function every time it tries to add a statement to this method builder (counter is increased by 1), then the function returns whether the accumulated statement number hits a limit. The reason of doing this is that Java compiler has a limit of 64K code size for each method.  We might hit this limit if an extremely long insert statement is issued, for example (see beetle 4293).  Counting statement number is an approximation without too much overhead.  Class implementation  sets exceptionBytes to the attribute_info needed for a method's Exceptions attribute. The ClassUtilities take care of the header 6 bytes for us, so they are not included here. See The Java Virtual Machine Specification Section 4.7.5, Exceptions attribute.

builds the JVM method descriptor for this method as defined in JVM Spec 4.3.3, Method Descriptors. static String get(Expression[] vmParameters, String vmReturnType, BCJava factory) { int count = vmParameters.length; String[] vmParameterTypes; if (count == 0) { vmParameterTypes = BCMethodDescriptor.EMPTY; } else { vmParameterTypes = new String[count]; for (int i =0; i < count; i++) { vmParameterTypes[i] = ((BCExpr) vmParameters[i]).vmType(); } } return new BCMethodDescriptor(vmParameterTypes, vmReturnType, factory).toString(); }
Return table name The setup creates a Connection to the database, and creates a table with blob columns. Teardown test. Rollback connection and close it.
************************************************************************ Public Methods of Conglomerate Interface: ************************************************************************* Add a column to the conglomerate. <p> Currently B2I does not support this operation. input template column. Do the generic part of creating a b-tree conglomerate.  This method is called from the concrete subclass (which may also read some properties). <p> This method processes all properties which are generic to all BTree's.  It creates the container for the btree. <p> The following properties are generic to a b-tree conglomerate.  : <UL> <LI>"allowDuplicates" (boolean).  If set to true the table will allow rows which are duplicate in key column's 0 through (nUniqueColumns - 1). Currently only supports "false". This property is optional, defaults to false. <LI>"nKeyFields"  (integer) Columns 0 through (nKeyFields - 1) will be included in key of the conglomerate. This implementation requires that "nKeyFields" must be the same as the number of fields in the conglomerate, including the rowLocationColumn. Other implementations may relax this restriction to allow non-key fields in the index. This property is required. <LI>"nUniqueColumns" (integer) Columns 0 through "nUniqueColumns" will be used to check for uniqueness.  So for a standard SQL non-unique index implementation set "nUniqueColumns" to the same value as "nKeyFields"; and for a unique index set "nUniqueColumns" to "nKeyFields" - 1 (ie. don't include the rowLocationColumn in the uniqueness check). This property is required. <LI>"maintainParentLinks" (boolean) Whether the b-tree pages maintain the page number of their parent.  Only used for consistency checking.  It takes a certain amount more effort to maintain these links, but they're really handy for ensuring that the index is consistent. This property is optional, defaults to true. </UL> ************************************************************************ Private/Protected methods of BTree: ************************************************************************* Create a branch row template for this conglomerate. <p> Reads the format id's of each of the columns and manufactures object of the given type for each.  It then uses these "empty" objects to create a template row.  The object passed in is then added to the last column of the row. ************************************************************************ Public methods of BTree: ************************************************************************* Create a template for this conglomerate. <p> Reads the format id's of each of the columns and manufactures object of the given type for each.  It then uses these "empty" objects to create a template row. <p> This method is public so that B2IUndo() can call it. Drop this btree. This must be done by a concrete implementation. * Private Methods of BTree. * Public Methods of BTree. ************************************************************************ Abstract Protected locking methods of BTree: getBtreeLockingPolicy lockScan unlockScan lockPreviousRow lockRowOnPage lockRow lockTable ************************************************************************* Create a new btree locking policy from scratch. Return dynamic information about the conglomerate to be dynamically reused in repeated execution of a statement. <p> The dynamic info is a set of variables to be used in a given ScanController or ConglomerateController.  It can only be used in one controller at a time.  It is up to the caller to insure the correct thread access to this info.  The type of info in this is a scratch template for btree traversal, other scratch variables for qualifier evaluation, ... <p> Get the id of the container of the conglomerate. <p> Will have to change when a conglomerate could have more than one container.  The ContainerKey is a combination of the container id and segment id. ************************************************************************ Public Methods of Storable Interface (via Conglomerate): This class is responsible for re/storing its own state. ************************************************************************* Return whether the value is null or not. The containerid being zero is what determines nullness;  subclasses are not expected to override this method. Is this conglomerate temporary? <p> Is this a "unique" index? Returns if the index type is uniqueWithDuplicateNulls. Load a b-tree.  This must be done by a concrete implementation. Lock the base table. <p> Assumes that segment of the base container is the same as the segment of the btree segment. <p> RESOLVE - we really want to get the lock without opening the container. raw store will be providing this. Open a b-tree controller. This must be done by a concrete implementation. Restore the in-memory representation from the stream. Restore the in-memory representation to the null value. The containerid being zero is what determines nullness;  subclasses are not expected to override this method. Set if the index is unique only for non null keys ************************************************************************ Public toString() Method: ************************************************************************* Store the stored representation of the column value in the stream. It might be easier to simply store the properties - which would certainly make upgrading easier.
* Methods of ConglomerateController Close the conglomerate controller. <p> Any changes to this method will probably have to be reflected in close as well. <p> Currently delegates to OpenBTree.  If the btree controller ends up not having any state of its own, we can remove this method (the VM will dispatch to OpenBTree), gaining some small efficiency.  For now, this method remains for clarity. Close conglomerate controller as part of terminating a transaction. <p> Use this call to close the conglomerate controller resources as part of committing or aborting a transaction.  The normal close() routine may do some cleanup that is either unnecessary, or not correct due to the unknown condition of the controller following a transaction ending error. Use this call when closing all controllers as part of an abort of a transaction. <p> This call is meant to only be used internally by the Storage system, clients of the storage system should use the simple close() interface. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this. Compares immidiate left and right records to check for duplicates. This methods compares new record (being inserted) with the record in immidate left and right postion to see if its duplicate (only for almost unique index and for non null keys) Compares the new record with the one at slot or the one right to it. If the slot is last slot in the page it will move to the right to sibling of the leaf and will compare with the record from the last slot. Compares the oldrow with the one at 'slot' or the one left to it. If the slot is first slot it will move to the left sibiling of the 'leaf' and will compare with the record from the last slot. Compares two rows for insert. If the two rows are not equal, {@link #NO_MATCH} is returned. Otherwise, it tries to get a lock on the row in the tree. If the lock is obtained without waiting, {@link #MATCH_FOUND} is returned (even if the row has been deleted). Otherwise, {@link #RESCAN_REQUIRED} is returned to indicate that the latches have been released and the B-tree must be rescanned. If {@code MATCH_FOUND} is returned, the caller should check whether the row has been deleted. If so, it may have to move to check the adjacent rows to be sure that there is no non-deleted duplicate row. If {@code MATCH_FOUND} or {@code RESCAN_REQUIRED} is returned, the transaction will hold an update lock on the specified record when the method returns. <b>Note!</b> This method should only be called when the index is almost unique (that is, a non-unique index backing a unique constraint). * Methods of ConglomerateController which are not supported. Delete a row from the conglomerate. Insert a row into the conglomerate. Just insert the row on the current page/slot if it fits. <p> Create room to insert a row to the right of the largest key in table. <p> Perform a split pass on the tree which will move the largest key in leaf right to a new leaf, splitting parent branch pages as necessary. Fetch the row at the given location. Fetch the row at the given location. Request set of properties associated with a table. <p> Returns a property object containing all properties that the store knows about, which are stored persistently by the store.  This set of properties may vary from implementation to implementation of the store. <p> This call is meant to be used only for internal query of the properties by jbms, for instance by language during bulk insert so that it can create a new conglomerate which exactly matches the properties that the original container was created with.  This call should not be used by the user interface to present properties to users as it may contain properties that are meant to be internal to jbms.  Some properties are meant only to be specified by jbms code and not by users on the command line. <p> Note that not all properties passed into createConglomerate() are stored persistently, and that set may vary by store implementation. Request the system properties associated with a table. <p> Request the value of properties that are associated with a table.  The following properties can be requested: derby.storage.pageSize derby.storage.pageReservedSpace derby.storage.minimumRecordSize derby.storage.initialPages <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(ConglomerateController cc) { Properties prop = new Properties(); prop.put("derby.storage.pageSize", ""); cc.getTableProperties(prop); System.out.println( "table's page size = " + prop.getProperty("derby.storage.pageSize"); } * public Methods of BTreeController Initialize the controller for use. <p> Any changes to this method will probably have to be reflected in close as well. <p> Currently delegates to OpenBTree.  If the btree controller ends up not having any state of its own, we can remove this method (the VM will dispatch to OpenBTree), gaining some small efficiency.  For now, this method remains for clarity. Insert a row into the conglomerate. Insert a row into the conglomerate, and store its location in the provided template row location. Unimplemented by btree. Return whether this is a keyed conglomerate. <p> All b-trees are keyed. Load rows from rowSource into the opened btree. <p> Efficiently load rows into the already opened btree.  The btree must be table locked, as no row locks will be requested by this routine. On exit from this routine the conglomerate will be closed (on both error or success). <p> This routine does an almost bottom up build of a btree.  It assumes all rows arrive in sorted order, and inserts them directly into the next (to the right) spot in the current leaf until there is no space. Then it calls the generic split code to add the next leaf (RESOLVE - in the future we could optimize this to split bottom up rather than top down for create index). Lock the given row location. <p> Should only be called by access. <p> This call can be made on a ConglomerateController that was opened for locking only. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this. Return a row location object of the correct type to be used in calls to insertAndFetchLocation. * private Methods of BTreeController Attempt to reclaim committed deleted rows from the page. <p> Get exclusive latch on page, and then loop backward through page searching for deleted rows which are committed.  The routine assumes that it is called from a transaction which cannot have deleted any rows on the page.  For each deleted row on the page it attempts to get an exclusive lock on the deleted row, NOWAIT. If it succeeds, and since this row did not delete the row then the row must have been deleted by a transaction which has committed, so it is safe to purge the row.  It then purges the row from the page. <p> Note that this routine may remove all rows from the page, it will not attempt a merge in this situation.  This is because this routine is called from split which is attempting an insert on the given page, so it would be a waste to merge the page only to split it again to allow the insert of the row causing the split. Replace the entire row at the given location. Start an internal transaction and do the split. <p> This routine starts a new transaction, and handles any errors that may come during the transaction.  This transation must not obtain any locks as they are likely to conflict with the current user transaction. <p> If attempt_to_reclaim_deleted_rows is true this routine will attempt to reclaim space on the leaf page input, by purging committed deleted rows from the leaf.  If it succeeds in purging at least one row, then it will commit the internal transaction and return without actually performing a split.
Public Methods of This class: Close the controller. <p> Close the open controller.  This method always succeeds, and never throws any exceptions. Callers must not use the StoreCostController Cost controller after closing it; they are strongly advised to clear out the scan controller reference after closing. <p> Return the cost of exact key lookup. <p> Return the estimated cost of calling ScanController.fetch() on the current conglomerate, with start and stop positions set such that an exact match is expected. <p> This call returns the cost of a fetchNext() performed on a scan which has been positioned with a start position which specifies exact match on all keys in the row. <p> Example: <p> In the case of a btree this call can be used to determine the cost of doing an exact probe into btree, giving all key columns.  This cost can be used if the client knows it will be doing an exact key probe but does not have the key's at optimize time to use to make a call to getScanCost() <p> Return the cost of calling ConglomerateController.fetch(). <p> Return the estimated cost of calling ConglomerateController.fetch() on the current conglomerate.  This gives the cost of finding a record in the conglomerate given the exact RowLocation of the record in question. <p> The validColumns parameter describes what kind of row is being fetched, ie. it may be cheaper to fetch a partial row than a complete row. <p> Calculate the cost of a scan. <p> Cause this object to calculate the cost of performing the described scan.  The interface is setup such that first a call is made to calcualteScanCost(), and then subsequent calls to accessor routines are made to get various pieces of information about the cost of the scan. <p> For the purposes of costing this routine is going to assume that a page will remain in cache between the time one next()/fetchNext() call and a subsequent next()/fetchNext() call is made within a scan. <p> The result of costing the scan is placed in the "cost_result". The cost of the scan is stored by calling cost_result.setEstimatedCost(cost). The estimated row count is stored by calling cost_result.setEstimatedRowCount(row_count). <p> The estimated cost of the scan assumes the caller will execute a fetchNext() loop for every row that qualifies between start and stop position.  Note that this cost is different than execution a next(),fetch() loop; or if the scan is going to be terminated by client prior to reaching the stop condition. <p> The estimated number of rows returned from the scan assumes the caller will execute a fetchNext() loop for every row that qualifies between start and stop position. <p> Private/Protected methods of This class: Initialize the cost controller. <p> Save initialize parameters away, and open the underlying container. <p> Return an "empty" row location object of the correct type. <p>
Fetch the next N rows from the table. <p> Utility routine used by both fetchSet() and fetchNextGroup(). * Private/Protected methods of This class, sorted alphabetically Position scan at "start" position. <p> Positions the scan to the slot just before the first record to be returned from the scan.  Returns the start page latched, and sets "current_slot" to the slot number.
************************************************************************ Abstract Protected lockNonScan*() locking methods of BTree: lockNonScanPreviousRow   - lock the row previous to the current lockNonScanRow           - lock the input row lockNonScanRowOnPage     - lock the given row on the page. ************************************************************************* Lock the previous key. <p> Given the current latched page and slot number, lock the logically previous key in the table.  There are 3 cases: <p> slotnumber &gt; 1                       - just lock (slotnumber - 1) (slotnumber == 1) &amp;&amp; (leftmost leaf) - this is the first key in the table, so lock a "magic" FIRSTKEY. (slotnumber == 1) &amp;&amp; !(leftmost leaf)- traverse left in the tree looking for a previous key. <p> On successful return from this routine appropriate locking will have been done.  All locks and latches are requested nowait, if any lock/latch cannot be granted this routine releases the current_leaf latch and any latches it may have acquired and returns "false." <p> All extra latches that may have been gotten will have been released. <p> This routine will find the "previous row" to the (current_leaf, current_slot), walking left in the tree as necessary, and first request the lock on that row NOWAIT.  If that lock can not be granted, then it will release all latches that it has acquired up to that point including the latched current_leaf passed into the routine, and request the lock WAIT.  Once the lock has been granted the routine will return and it is up to the caller to research the tree to find where the current position may have ended up.  For instance in the case of insert once the current latch is released, the correct page to do the insert may no longer be where the original scan found it. <p> If routine returns true, lock was granted NOWAIT, current leaf remains latched, and was never unlatched.  If routine returns false, lock was granted WAIT, current leaf is not latched, row may have moved in the btree so caller must research to find the row. Lock a btree row (row in memory).  Meant to be used if caller has the entire row objectified. <p> Lock a btree row, enforcing the standard lock/latch protocol. On return the row is locked.  Return status indicates if the lock was waited for, which will mean a latch was dropped while waiting. In general a false status means that the caller will either have to research the tree unless some protocol has been implemented that insures that the row will not have moved while the latch was dropped. <p> This routine request a row lock NOWAIT on the in-memory row "current_row.".  If the lock is granted the routine will return true. If the lock cannot be granted NOWAIT, then the routine will release the latch on "current_leaf" (if current_leaf is non-null) and "aux_leaf" (if aux_leaf is non-null), and then it will request a WAIT lock on the row. Lock the row at the given slot. <p> If this routine returns true all locks were acquired while maintaining the latch on leaf.  If this routine returns false, locks may or may not have been acquired, and the routine should be called again after the client has researched the tree to reget the latch on the appropriate page. ************************************************************************ Abstract Protected lockScan*() locking methods of BTree: lockScanRow              - lock row unlockScanRecordAfterRead- unlock the scan record ************************************************************************* Lock a btree row to determine if it is a committed deleted row. <p> Request an exclusive lock on the row located at the given slot, NOWAIT. Return true if the lock is granted, otherwise false. <p> Lock a row as part of doing the scan. <p> Lock the row at the given slot (or the previous row if slot is 0). <p> If this routine returns true all locks were acquired while maintaining the latch on leaf.  If this routine returns false, locks may or may not have been acquired, and the routine should be called again after the client has researched the tree to reget the latch on the appropriate page. Release read lock on a row.
************************************************************************ Public Methods of This class: ************************************************************************* Fetch the maximum row in the table. Call positionAtStartPosition() to quickly position on rightmost row of rightmost leaf of tree. Search last page for last non deleted row, and if one is found return it as max. If no row found on last page, or could not find row without losing latch then call fetchMaxRowFromBeginning() to search from left to right for maximum value in index. ************************************************************************ Protected implementation of abstract methods of BTreeScan class: ************************************************************************* disallow fetchRows on this scan type, caller should only be able to call fetchMax(). <p> ************************************************************************ Private methods of This class: ************************************************************************* Move the current position to the page to the left of the current page, right after the last slot on that page. If we have to wait for a latch, give up the latch on the current leaf and give up. The caller will have to reposition and retry. Position scan at "start" position of the MAX scan. <p> Positions the scan to the slot just after the last record on the rightmost leaf of the index.  Returns the rightmost leaf page latched, the rightmost row on the page locked and sets "current_slot" to the slot number just right of the last row on the page. <p>
Open index for either table level or row level update. <p> perform the work described in the postcommit work. <p> In this implementation the only work that can be executed by this post commit processor is this class itself. <p> Reclaim space taken up by committed deleted rows. <p> This routine assumes it has been called by an internal transaction which has performed no work so far, and that it has an exclusive table lock. These assumptions mean that any deleted rows encountered must be from committed transactions (otherwise we could not have gotten the exclusive table lock). <p> This routine handles purging committed deletes while holding a table level exclusive lock.  See purgeRowLevelCommittedDeletes() for row level purging. Attempt to reclaim committed deleted rows from the page with row locking. <p> Get exclusive latch on page, and then loop backward through page searching for deleted rows which are committed. This routine is called only from post commit processing so it will never see rows deleted by the current transaction. For each deleted row on the page it attempts to get an exclusive lock on the deleted row, NOWAIT. If it succeeds, and since this transaction did not delete the row then the row must have been deleted by a transaction which has committed, so it is safe to purge the row.  It then purges the row from the page. <p> The latch on the leaf page containing the purged rows must be kept until after the transaction has been committed or aborted in order to insure proper undo of the purges can take place.  Otherwise another transaction could use the space freed by the purge and then prevent the purge from being able to undo. Private/Protected methods of This class: Public Methods of This class: Public Methods of Serviceable class: The urgency of this post commit work. <p> This determines where this Serviceable is put in the post commit queue.  Post commit work in the btree can be safely delayed until there is not user work to do. @return true, if this work needs to be done on a user thread immediately
Get a fetch descriptor that can be used to fetch the missing columns in a partial key. The fetch descriptor is only created on the first call to this method. The returned descriptor will be cached, so subsequent calls will return the same descriptor and the arguments to this method should be the same between invokations. Get a template into which the position key can be copied. The value is cached, so two calls to this method on the same object will return the same object. ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* Save this position by key and release the latch on the current leaf.
* Methods of ScanController Close the scan. * Methods of ScanManager Close the scan, a commit or abort is about to happen. Delete the row at the current position of the scan. A call to allow client to indicate that current row does not qualify. <p> Indicates to the ScanController that the current row does not qualify for the scan.  If the isolation level of the scan allows, this may result in the scan releasing the lock on this row. <p> Note that some scan implimentations may not support releasing locks on non-qualifying rows, or may delay releasing the lock until sometime later in the scan (ie. it may be necessary to keep the lock until either the scan is repositioned on the next row or page). <p> This call should only be made while the scan is positioned on a current valid row. Returns true if the current position of the scan still qualifies under the set of qualifiers passed to the openScan().  When called this routine will reapply all qualifiers against the row currently positioned and return true if the row still qualifies.  If the row has been deleted or no longer passes the qualifiers then this routine will return false. <p> This case can come about if the current scan or another scan on the same table in the same transaction deleted the row or changed columns referenced by the qualifier after the next() call which positioned the scan at this row. <p> Note that for comglomerates which don't support update, like btree's, there is no need to recheck the qualifiers. <p> The results of a fetch() performed on a scan positioned on a deleted row are undefined. <p> Fetch the row at the current position of the Scan. Fetch the row at the current position of the Scan. * Methods of ScanController, which are not supported by btree. Fetch the location of the current position in the scan. Fetch the row at the next position of the Scan. If there is a valid next position in the scan then the value in the template storable row is replaced with the value of the row at the current scan position.  The columns of the template row must be of the same type as the actual columns in the underlying conglomerate. The resulting contents of templateRow after a fetchNext() which returns false is undefined. The result of calling fetchNext(row) is exactly logically equivalent to making a next() call followed by a fetch(row) call.  This interface allows implementations to optimize the 2 calls if possible. Fetch the next N rows from the table. <p> The client allocates an array of N rows and passes it into the fetchNextSet() call.  This routine does the equivalent of N fetchNext() calls, filling in each of the rows in the array. Locking is performed exactly as if the N fetchNext() calls had been made. <p> It is up to Access how many rows to return.  fetchNextSet() will return how many rows were filled in.  If fetchNextSet() returns 0 then the scan is complete, (ie. the scan is in the same state as if fetchNext() had returned false).  If the scan is not complete then fetchNext() will return (1 &lt;= row_count &lt;= N). <p> The current position of the scan is undefined if fetchNextSet() is used (ie. mixing fetch()/fetchNext() and fetchNextSet() calls in a single scan does not work).  This is because a fetchNextSet() request for 5 rows from a heap where the first 2 rows qualify, but no other rows qualify will result in the scan being positioned at the end of the table, while if 5 rows did qualify the scan will be positioned on the 5th row. <p> Qualifiers, start and stop positioning of the openscan are applied just as in a normal scan. <p> The columns of the row will be the standard columns returned as part of a scan, as described by the validColumns - see openScan for description. <p> Expected usage: // allocate an array of 5 empty row templates DataValueDescriptor[][] row_array = allocate_row_array(5); int row_cnt = 0; scan = openScan(); while ((row_cnt = scan.fetchNextSet(row_array) != 0) { // I got "row_cnt" rows from the scan.  These rows will be // found in row_array[0] through row_array[row_cnt - 1] } <p> RESOLVE - This interface is being provided so that we can prototype the performance results it can achieve.  If it looks like this interface is useful, it is very likely we will look into a better way to tie together the now 4 different fetch interfaces: fetch, fetchNext(), fetchNextGroup(), and fetchSet(). * Private/Protected methods of This class, sorted alphabetically Fetch the next N rows from the table. <p> Utility routine used by both fetchSet() and fetchNextGroup(). Insert all rows that qualify for the current scan into the input Hash table. <p> This routine scans executes the entire scan as described in the openScan call.  For every qualifying unique row value an entry is placed into the HashTable. For unique row values the entry in the BackingStoreHashtable has a key value of the object stored in row[key_column_number], and the value of the data is row.  For row values with duplicates, the key value is also row[key_column_number], but the value of the data is a Vector of rows.  The caller will have to call "instanceof" on the data value object if duplicates are expected, to determine if the data value of the Hashtable entry is a row or is a Vector of rows. <p> Note, that for this routine to work efficiently the caller must ensure that the object in row[key_column_number] implements the hashCode and equals method as appropriate for it's datatype. <p> It is expected that this call will be the first and only call made in an openscan.  Qualifiers and stop position of the openscan are applied just as in a normal scan.  This call is logically equivalent to the caller performing the following: import java.util.Hashtable; hash_table = new Hashtable(); while (next()) { row = create_new_row(); fetch(row); if ((duplicate_value = hash_table.put(row[key_column_number], row)) != null) { Vector row_vec; // inserted a duplicate if ((duplicate_value instanceof vector)) { row_vec = (Vector) duplicate_value; } else { // allocate vector to hold duplicates row_vec = new Vector(2); // insert original row into vector row_vec.addElement(duplicate_value); // put the vector as the data rather than the row hash_table.put(row[key_column_number], row_vec); } // insert new row into vector row_vec.addElement(row); } } <p> The columns of the row will be the standard columns returned as part of a scan, as described by the validColumns - see openScan for description. RESOLVE - is this ok?  or should I hard code somehow the row to be the first column and the row location? <p> Currently it is only possible to hash on the first column in the conglomerate, in the future we may change the interface to allow hashing either on a different column or maybe on a combination of columns. <p> No overflow to external storage is provided, so calling this routine on a 1 gigabyte conglomerate will incur at least 1 gigabyte of memory (probably failing with a java out of memory condition).  If this routine gets an out of memory condition, or if "max_rowcnt" is exceeded then then the routine will give up, empty the Hashtable, and return "false." <p> On exit from this routine, whether the fetchSet() succeeded or not the scan is complete, it is positioned just the same as if the scan had been drained by calling "next()" until it returns false (ie. fetchNext() and next() calls will return false). reopenScan() can be called to restart the scan. <p> RESOLVE - until we get row counts what should we do for sizing the the size, capasity, and load factor of the hash table. For now it is up to the caller to create the Hashtable, Access does not reset any parameters. <p> RESOLVE - I am not sure if access should be in charge of allocating the new row objects.  I know that I can do this in the case of btree's, but I don't think I can do this in heaps. Maybe this is solved by work to be done on the sort interface. Fetch the row at the current position of the Scan without applying the qualifiers. Return ScanInfo object which describes performance of scan. <p> Return ScanInfo object which contains information about the current scan. <p> * Public Methods of BTreeScan Initialize the scan for use. <p> Any changes to this method may have to be reflected in close as well. <p> The btree init opens the container (super.init), and stores away the state of the qualifiers.  The actual searching for the first position is delayed until the first next() call. Shared initialization code between init() and reopenScan(). <p> Basically save away input parameters describing qualifications for the scan, and do some error checking. Returns true if the current position of the scan is at a deleted row.  This case can come about if the current scan or another scan on the same table in the same transaction deleted the row after the next() call which positioned the scan at this row. The results of a fetch() performed on a scan positioned on a deleted row are undefined. Check if a B-tree page is empty. The control row, which is always present, is not counted.  Return whether this is a keyed conglomerate. <p> Return a row location object of the correct type to be used in calls to fetchLocation. Move to the next position in the scan. Do work necessary to close a scan. <p> This routine can only be called "inline" from other btree routines, as it counts on the state of the pos to be correct. <p> Closing a scan from close() must handle long jumps from exceptions where the state of pos may not be correct.  The easiest case is a lock timeout which has caused us not to have a latch on a page, but pos still thinks there is a latch.  This is the easiest but other exceptions can also caused the same state at close() time. Do any necessary work to complete the scan. Position scan to 0 slot on next page. <p> Position to next page, keeping latch on previous page until we have latch on next page.  This routine releases the latch on current_page once it has successfully gotten the latch on the next page. <p> Position the scan after the last row on the previous page. Hold the latch on the current page until the previous page has been latched. If the immediate left sibling is empty, move further until a non-empty page is found or there are no more leaves to be found. The latch on the current page will be held until a non-empty left sibling page is found. </p> <p> This method never waits for a latch, as waiting for latches while holding another latch is only allowed when moving forward in the B-tree. Waiting while moving backward may result in deadlocks with scanners going forward. A {@code WaitError} is thrown if the previous page cannot be latched without waiting. {@code scan_position.current_leaf} will point to the same page as before the method was called in the case where a {@code WaitError} is thrown, and the page will still be latched. </p>  Position scan at "start" position for a forward scan. <p> Positions the scan to the slot just before the first record to be returned from the scan.  Returns the start page latched, and sets "current_slot" to the slot number. <p> Position scan at "start" position. <p> Positions the scan to the slot just before the first record to be returned from the scan.  Returns the start page latched, and sets "current_slot" to the slot number. process_qualifier - Determine if a row meets all qualifier conditions. <p> Check all qualifiers in the qualifier array against row.  Return true if all compares specified by the qualifier array return true, else return false. <p> It is up to caller to make sure qualifier list is non-null. Reposition the current scan.  This call is semantically the same as if the current scan had been closed and a openScan() had been called instead. The scan is reopened with against the same conglomerate, and the scan is reopened with the same "hold" and "forUpdate" parameters passed in the original openScan.  The previous template row continues to be used. Reposition the current scan.  This call is semantically the same as if the current scan had been closed and a openScan() had been called instead. The scan is reopened against the same conglomerate, and the scan is reopened with the same "scan column list", "hold" and "forUpdate" parameters passed in the original openScan. <p> The statistics gathered by the scan are not reset to 0 by a reopenScan(), rather they continue to accumulate. <p> Note that this operation is currently only supported on Heap conglomerates. Also note that order of rows within are heap are not guaranteed, so for instance positioning at a RowLocation in the "middle" of a heap, then inserting more data, then continuing the scan is not guaranteed to see the new rows - they may be put in the "beginning" of the heap. Replace the entire row at the current position of the scan. Unimplemented interface by btree, will throw an exception. Reposition the scan leaving and reentering the access layer. <p> When a scan leaves access it saves the RecordHandle of the record on the page.  There are 2 cases to consider when trying to reposition the scan when re-entering access: o ROW has not moved off the page. If the row has not moved then the RecordHandle we have saved away is valid, and we just call RawStore to reposition on that RecordHandle (RawStore takes care of the row moving within the page). o ROW has moved off the page. This can only happen in the case of a btree split.  In that case the splitter will have caused all scans positioned on this page within the same transaction to save a copy of the row that the scan was positioned on.  Then to reposition the scan it is necessary to research the tree from the top using the copy of the row. There are a few cases where it is possible that the key no longer exists in the table.  In the case of a scan held open across commit it is easy to imagine that the row the scan was positioned on could be deleted and subsequently purged from the table all before the scan resumes.  Also in the case of read uncommitted the scan holds no lock on the current row, so it could be purged - in the following scenario for instance:  read uncommitted transaction 1 opens scan and positions on row (1,2), transaction 2 deletes (1,2) and commits, transaction 1 inserts (1,3) which goes to same page as (1,2) and is going to cause a split, transaction 1 saves scan position as key, and then purges row (1, 2), when transaction 1 resumes scan (1, 2) no longer exists.  missing_row_for_key_ok parameter is added as a sanity check to make sure it ok that repositioning does not go to same row that we were repositioned on. Shortcut for for savePositionAndReleasePage(null,null). Save the current scan position by key and release the latch on the leaf that's being scanned. This method should be called if the latch on a leaf needs to be released in the middle of the scan. The scan can later reposition to the saved position by calling {@code reposition()}. * Standard toString() method.  Prints out current position in scan.
Return all information gathered about the scan. <p> This routine returns a list of properties which contains all information gathered about the scan.  If a Property is passed in, then that property list is appeneded to, otherwise a new property object is created and returned. <p> Not all scans may support all properties, if the property is not supported then it will not be returned.  The following is a list of properties that may be returned: numPagesVisited - the number of pages visited during the scan.  For btree scans this number only includes the leaf pages visited. numRowsVisited - the number of rows visited during the scan.  This number includes all rows, including: those marked deleted, those that don't meet qualification, ... numRowsQualified - the number of undeleted rows, which met the qualification. treeHeight (btree's only) - for btree's the height of the tree.  A tree with one page has a height of 1.  Total number of pages visited in a btree scan is (treeHeight - 1 + numPagesVisited). numColumnsFetched - the number of columns Fetched - partial scans will result in fetching less columns than the total number in the scan. columnsFetched - The FormatableBitSet.toString() method called on the validColumns arg. to the scan, unless validColumns was set to null, and in that case we will return "all". NOTE - this list will be expanded as more information about the scan is gathered and returned.
Initialize the load generator. Print average number of transactions per second. Start steady state. Start warmup. Stop the load generator.
Clean the first entry in the queue. If there is more work, re-request service from the daemon service. Notify the daemon service that the cleaner needs to be serviced. Try to schedule a clean operation in the background cleaner. Request that the cleaner tries to shrink the cache the next time it wakes up. Indicate that we want to be serviced ASAP. Indicate that we don't want the work to happen immediately in the user thread. Stop subscribing to the daemon service.


Close the BackingStoreHashtable. <p> Perform any necessary cleanup after finishing with the hashtable.  Will deallocate/dereference objects as necessary.  If the table has gone to disk this will drop any on disk files used to support the hash table. <p> ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: *************************************************************************
Do the work to add one row to the hash table. <p> Return a cloned copy of the row. ************************************************************************ Public Methods of This class: ************************************************************************* Close the BackingStoreHashtable. <p> Perform any necessary cleanup after finishing with the hashtable.  Will deallocate/dereference objects as necessary.  If the table has gone to disk this will drop any on disk files used to support the hash table. <p> end of doSpaceAccounting <p> Return an Enumeration that can be used to scan the entire table. The objects in the Enumeration can be either of the following: </p> <ul> <li>a row - This is a single row with a unique hash key.</li> <li>or a bucket of rows - This is a list of rows which all have the same hash key.</li> </ul> <p> The situation is a little more complicated because the row representation is different depending on whether the row includes a RowLocation. If includeRowLocations()== true, then the row is a LocatedRow. Otherwise, the row is an array of DataValueDescriptor. Putting all of this together, if the row contains a RowLocation, then the objects in the Enumeration returned by this method can be either of the following: </p> <ul> <li>a LocatedRow</li> <li>or a List&lt;LocatedRow&gt;</li> </ul> <p> But if the row does not contain a RowLocation, then the objects in the Enumeration returned by this method can be either of the following: </p> <ul> <li>a DataValueDescriptor[]</li> <li>or a List&lt;DataValueDescriptor[]&gt;</li> </ul> <p> RESOLVE - is it worth it to support this routine when we have a disk overflow hash table? </p> <p> Get data associated with given key. </p> <p> There are 2 different types of objects returned from this routine. </p> <p> In both cases, the key value is either the object stored in row[key_column_numbers[0]], if key_column_numbers.length is 1, otherwise it is a KeyHasher containing the objects stored in row[key_column_numbers[0, 1, ...]]. For every qualifying unique row value an entry is placed into the hash table. </p> <p> For row values with duplicates, the value of the data is a list of rows. </p> <p> The situation is a little more complicated because the row representation is different depending on whether the row includes a RowLocation. If includeRowLocations() == true, then the row is a LocatedRow. Otherwise, the row is an array of DataValueDescriptor. Putting all of this together, if the row contains a RowLocation, then the objects returned by this method can be either of the following: </p> <ul> <li>a LocatedRow</li> <li>or a List&lt;LocatedRow&gt;</li> </ul> <p> But if the row does not contain a RowLocation, then the objects returned by this method can be either of the following: </p> <ul> <li>a DataValueDescriptor[]</li> <li>or a List&lt;DataValueDescriptor[]&gt;</li> </ul> <p> The caller will have to call "instanceof" on the data value object if duplicates are expected, to determine if the data value of the hash table entry is a row or is a list of rows. </p> <p> See the javadoc for elements() for more information on the objects returned by this method. </p> <p> The BackingStoreHashtable "owns" the objects returned from the get() routine.  They remain valid until the next access to the BackingStoreHashtable.  If the client needs to keep references to these objects, it should clone copies of the objects.  A valid BackingStoreHashtable can place all rows into a disk based conglomerate, declare a row buffer and then reuse that row buffer for every get() call. </p> Return runtime stats to caller by adding them to prop. <p> Take a value which will go into the hash table and return an estimate of how much memory that value will consume. The hash value could be either an array of columns or a LocatedRow. ************************************************************************ Private/Protected methods of This class: ************************************************************************* Call method to either get next row or next row with non-null key columns. Currently, RowLocation information is not included in rows siphoned out of a RowSource. That functionality is only supported if the rows come from the scan of a base table. Return true if we should include RowLocations with the rows stored in this hash table. <p> Make a full set of columns from an object which is either already an array of column or otherwise a LocatedRow. The full set of columns is what's stored on disk when we spill to disk. This is the inverse of makeInMemoryRow(). </p> <p> Construct a full set of columns, which may need to end with the row location.The full set of columns is what's stored on disk when we spill to disk. </p> <p> Make an in-memory row from an on-disk row. This is the inverse of makeDiskRow(). </p> <p> Turn a list of disk rows into a list of in-memory rows. The on disk rows are always of type DataValueDescriptor[]. But the in-memory rows could be of type LocatedRow. </p> Put a row into the hash table. <p> The in memory hash table will need to keep a reference to the row after the put call has returned.  If "needsToClone" is true then the hash table will make a copy of the row and put that, else if "needsToClone" is false then the hash table will keep a reference to the row passed in and no copy will be made. <p> If routine returns false, then no reference is kept to the duplicate row which was rejected (thus allowing caller to reuse the object). remove a row from the hash table. <p> a remove of a duplicate removes the entire duplicate list. Set the auxillary runtime stats. <p> getRuntimeStats() will return both the auxillary stats and any BackingStoreHashtable() specific stats.  Note that each call to setAuxillaryRuntimeStats() overwrites the Property set that was set previously. Return a shallow cloned row Return number of unique rows in the hash table. <p> Determine whether a new row should be spilled to disk and, if so, do it. end of spillToDisk
Notifies backup thread is active and then does compress and takes Backup
Return a string of the specified length that can be used to increase the size of the rows. The string only contains x's. The rows have a defined minimum size in bytes, whereas the string length is in characters. For now, we assume that one character maps to one byte on the disk as long as the string only contains ASCII characters. Create the tables. Drop the tables if they exits. Populate the database. Fill the tables with rows.
Perform a single transaction with a profile like the one specified in Clause 1.2 of the TPC-B specification. Generate a random account id based on the specified branch. Per Clause 5.3.5 of the TPC-B specification, the accounts should be fetched from the selected branch 85% of the time (or always if that's the only branch), and from another branch the rest of the time. Find the branch the specified teller belongs to. Generate a random delta value between -99999 and +99999, both inclusive (TPC-B specification, Clause 5.3.6). The delta value specifies how much the balance should increase or decrease. Generate a random teller id. Initialize the connection and the statements used by the test.
Wait until {@code numThreads} have called {@code await()} on this barrier, then proceed.
Allocate an array of qualifiers and initialize in Qualifier[][]  class interface  Temporary tables can be declared with ON COMMIT DELETE ROWS. But if the table has a held curosr open at commit time, data should not be deleted from the table. This method, (gets called at commit time) checks if this activation held cursor and if so, does that cursor reference the passed temp table name. Check that a positioned statement is executing against a cursor from the same PreparedStatement (plan) that the positioned statement was original compiled against. Only called from generated code for positioned UPDATE and DELETE statements. See CurrentOfNode. Clear the current row for the specified ResultSet.   * prepared statement use the same activation for * multiple execution. For each excution we create new * set of temporary resultsets, we should clear this hash table. * otherwise we will refer to the released resources.  Closing an activation marks it as unusable. Any other requests made on it will fail.  An activation should be marked closed when it is expected to not be used any longer, i.e. when the connection for it is closed, or it has suffered some sort of severe error. This should also remove it from the language connection context. A generated class can create its own closeActivationAction method to invoke special logic when the activation is closed. Create the ResultSet tree for this statement. Create the ResultSet tree for this statement, and possibly perform extra checks or initialization required by specific sub-classes.  Activation interface     Return a calendar for use by this activation. Calendar objects are not thread safe, the one returned is purely for use by this activation and it is assumed that is it single threded through the single active thread in a connection model.  Get the Current ContextManager. Used to get a proxy for the current connection. Get the current row at the given index. Called by generated code to get the next number in an ANSI/ISO sequence and advance the sequence. Raises an exception if the sequence was declared NO CYCLE and its range is exhausted. get the cursor name.  For something that isn't a cursor, this is used as a string name of the result set for messages from things like the dependency manager. <p> Activations that do support cursors will override this. Used in CurrentOfResultSet to get to the cursor result set for a cursor.  Overridden by activations generated for updatable cursors.  Those activations capture the cursor result set in a field in their execute() method, and then return the value of that field in their version of this method. Compute the DB2 compatible length of a value. end of getDB2Length  Used by activations to generate data values.  Most DML statements will use this method.  Possibly some DDL statements will, as well. Dependable interface implementation  Real implementations of this method are provided by a generated class. Used in activations for generating rows.     * Code originally in the parent class BaseExpressionActivation Get the language connection factory associated with this connection Real implementations of this method are provided by a generated class.  Privileged Monitor lookup. Must be package private so that user code can't call this entry point. Get the number of subqueries in the entire query.   This method can help reduce the amount of generated code by changing instances of this.pvs.getParameter(position) to this.getParameter(position) return the parameters. Get the activation of the calling statement or parent statement. get the reference to parent table ResultSets, that will be needed by the referential action dependent table scans.   class implementation  Used in the execute method of activations for generating the result sets that they concatenate together. Get the saved RowLocation. Return the current SQL session context for all immediately nested connections stemming from the call or function invocation of the statement corresponding to this activation. <p/> Substatements (e.g. used in rs.updateRow), inherit the SQL session context via its parent activation. Called by generated code to compute the next autoincrement value. Used in CurrentOfResultSet to get to the target result set for a cursor. Overridden by activations generated for updatable cursors.  Those activations capture the target result set in a field in their execute() method, and then return the value of that field in their version of this method.   Find out if the activation closed or not.  Is the activation in use?  Returns true if this Activation is only going to be used for one execution. Dependent interface implementation   Mark the activation as unused. This method is used to materialize a resultset if can actually fit in the memory specified by "maxMemoryPerTable" system property.  It converts the result set into union(union(union...(union(row, row), row), ...row), row).  It returns this in-memory converted resultset, or the original result set if not converted. See beetle 4373 for details. Optimization implemented as part of Beetle: 4373 can cause severe stack overflow problems. See JIRA entry DERBY-634. With default MAX_MEMORY_PER_TABLE of 1MG, it is possible that this optimization could attempt to cache upto 250K rows as nested union results. At runtime, this would cause stack overflow. As Jeff mentioned in DERBY-634, right way to optimize original problem would have been to address subquery materialization during optimization phase, through hash joins. Recent Army's optimizer work through DEBRY-781 and related work introduced a way to materialize subquery results correctly and needs to be extended to cover this case. While his optimization needs to be made more generic and stable, I propose to avoid this regression by limiting size of the materialized resultset created here to be less than MAX_MEMORY_PER_TABLE and MAX_DYNAMIC_MATERIALIZED_ROWS. @param	rs	input result set @return	materialized resultset, or original rs if it can't be materialized   Reinitialize data structures added by the sub-classes before each execution of the statement. The default implementation does nothing. Sub-classes should override this method if they need to perform operations before each execution. Reinitialize all Qualifiers in an array of Qualifiers. This is a partial implementation of reset. Subclasses will want to reset information they are aware of, such as parameters. <p> All subclasses must call super.reset() and then do their cleanup. <p> The execute call must set the resultSet field to be the resultSet that it has returned.  Set a column position in an array of column positions. Remember the row for the specified ResultSet. remember the cursor name   beetle 3865: updateable cursor using index.  A way of communication between cursor activation and update activation.  GeneratedByteCode interface      how do we do/do we want any sanity checking for the number of parameters expected? This activation is created in a dynamic call context or a substatement execution context, make note of its parent statements activation (a). Set a Qualifier in a 2 dimensional array of Qualifiers. Set a single Qualifier into one slot of a 2 dimensional array of Qualifiers.  @see Qualifier for detailed description of layout of the 2-d array. Set this Activation for a single execution. Link this activation with its PreparedStatement. It can be called with null to break the link with the PreparedStatement.  Find out if it's time to check the row counts of the tables involved in this query. Various activation methods need to disallow their invocation if the activation is closed. This lets them check and throw without generating alot of code. <p> The code to write to generate the call to this is approximately: <verbatim> // jf is a JavaFactory CallableExpression ce = jf.newMethodCall( jf.thisExpression(), BaseActivation.CLASS_NAME, "throwIfClosed", "void", acb.exprArray(jf.newStringLiteral(...some literal here...))); //mb is a MethodBuilder mb.addStatement(jf.newStatement(ce)); </verbatim> The java code to write to call this is: <verbatim> this.throwIfClosed(...some literal here...); </verbatim> In both cases, "...some literal here..." gets replaced with an expression of type String that evaluates to the name of the operation that is being checked, like "execute" or "reset". Throw an exception if any parameters are uninitialized.
Do the code generation for this node. Should never be called. Get the name of this column Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Get the user-supplied schema name for this column's table. This will be null if the user did not supply a name (for example, select t.a from t). Another example for null return value (for example, select b.a from t as b). But for following query select app.t.a from t, this will return APP Get the user-supplied table name of this column.  This will be null if the user did not supply a name (for example, select a from t). The method will return B for this example, select b.a from t as b The method will return T for this example, select t.a from t {@inheritDoc } Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Add a page to this container. <BR> MT - thread aware - The add page operation involves 2 transactions, one is the user transaction (the transaction which owns the passed in handle), the other one is a NestedTopTransaction created by this BaseContainer. The nestedTopTransaction is used by the underlying container to change high contention structures, such as link list anchor or bit map pages. The nestedTopTransaction commits or aborts before this routine returns. The user transaction is used to latch the newly created page. Backup the container to the specified path. Can the container be updated. The container will have no pre-allocate threshold, i.e., if the implementation supports it, page preallocation will happen the next time a new page is allocated. * Implementation specific methods Release free space to the OS. <P> As is possible release any free space to the operating system.  This will usually mean releasing any free pages located at the end of the file using the java truncate() interface. Deallocate a page from the container. Mark the container as drop or not drop depending on the input value. Creates encrypted or decrypted version of the container. * portions of Cacheable interface, interface is actually implemented by * sub-class. This section also contains methods related to this interface. Flush all outstanding changes in this container to persistent storage. Return a BasePage that represents the given alloc page number in this container. Get an allocation page and latch it. Return a BasePage that represents any page - alloc page, valid page, free page, dealloced page etc.  The only requirement is that the page is initialized... Get any page and latch it . Request the system properties associated with a container. <p> Request the value of properties that are associated with a container. The following properties can be requested: derby.storage.pageSize derby.storage.pageReservedSpace derby.storage.minimumRecordSize <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(BaseContainer base) { Properties prop = new Properties(); prop.put("derby.storage.pageSize", ""); base.getContainerProperties(prop); System.out.println( "container's page size = " + prop.getProperty("derby.storage.pageSize"); } Get the logged container version. Get the special dealloc lock on the page - the lock is gotten by the transaction that owns the container handle  Cost estimates  Get the first page in the container. Get the first valid page. Result is latched. Get only a valid, non-overflow page.  If page number is either invalid or overflow, returns null Get the next page in the container. Get the next valid page and latch it Return a BasePage that represents the given page number in this container. The resulting page is latched. Get a potentially suitable page for insert and latch it. Get the reusable RecordId sequence number for the container. This sequence number should be incremented every time there is an operation which may cause RecorIds to be reused. This method can be used by clients to check if a RecordId they obtained is still guaranteed to be valid. If the sequence number has changed, the RecordId may have been reused for another row. public int getPageSize() { return pageSize(); } *	Methods that need to be provided by a sub-class. Get information about space used by the container. Increment the reusable RecordId sequence number. utility to latch a page Discontinue use of this container. Note that the unlockContainer call made from this method may not release any locks. The container lock may be held until the end of the transaction. protected void setPageSize(int pageSize) { identity.setPageSize(pageSize); } Not interested in participating in the diagnostic virtual lock table. * Methods from Lockable, just require a single exclusive locker Log all information on the container creation necessary to recreate teh container during a load tran. Create a new page in the container. The container is about to be modified. Loggable actions use this to make sure the container gets cleaned if a checkpoint is taken after any log record is sent to the log stream but before the container is actually dirtied. The container will be grown vastly, prepare for it. ReCreate a page for rollforward recovery. <p> During redo recovery it is possible for the system to try to redo the creation of a page (ie. going from non-existence to version 0). It first trys to read the page from disk, but a few different types of errors can occur: o the page does not exist at all on disk, this can happen during rollforward recovery applied to a backup where the file was copied and the page was added to the file during the time frame of the backup but after the physical file was copied. o space in the file exists, but it was never initalized.  This can happen if you happen to crash at just the right moment during the allocation process.  Also on some OS's it is possible to read from a part of the file that was not ever written - resulting in garbage from the store's point of view (often the result is all 0's). All these errors are easy to recover from as the system can easily create a version 0 from scratch and write it to disk. Because the system does not sync allocation of data pages, it is also possible at this point that whlie writing the version 0 to disk to create it we may encounter an out of disk space error (caught in this routine as a StandardException from the create() call.  We can't recovery from this without help from outside, so the caught exception is nested and a new exception thrown which the recovery system will output to the user asking them to check their disk for space/errors. The arguments passed in need to be sufficient for the page cache to materialize a brand new page and write it to disk. Remove the container and reclaim its space.  Once executed, this operation cannot be undone - as opposed to dropContainer which only marks the container as dropped and can be rolled back. <BR><B> This operation should only be called by post commit clean up </B> Remove a page from this container.  The page will be unlatched by this routine before it returns. Unlike addPage, this method done as part of the user transaction. The removed page is not usable by anyone until the user transaction comits. If the user transaction rolls back, the removed page is un-removed. <BR> MT - thread aware - * Methods to be used by sub-classes. Set the container's dropped state  Lock the container and mark the container as in-use by this container handle.
* Methods from ContainerHandle Add a page to the container The page returned will be observing me. Add a page to the container, if flag == ContainerHandle.ADD_PAGE_BULK, tell the container about it. The page returned will be observing me. Backup the container to the specified path. * Implementation specific methods for myself and my sub-classes   Release free space to the OS. <P> As is possible release any free space to the operating system.  This will usually mean releasing any free pages located at the end of the file using the java truncate() interface.  {@inheritDoc }  * Implementation specific methods, these are public so that they can be * called in other packages that are specific implementations of Data, ie. * a directory at the level * * com.ibm.db2j.impl.Database.Storage.RawStore.Data.* Get this page with no check - any page type or status is fine. Caller must be prepared to handle freed, deallocated,or alloc page Called by recovery ONLY. Request the system properties associated with a container. <p> Request the value of properties that are associated with a container. The following properties can be requested: derby.storage.pageSize derby.storage.pageReservedSpace derby.storage.minimumRecordSize <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(BaseContainerHandle ch) { Properties prop = new Properties(); prop.put("derby.storage.pageSize", ""); ch.getContainerProperties(prop); System.out.println( "conatainer's page size = " + prop.getProperty("derby.storage.pageSize"); } * Methods of RawContainerHandle - methods are called underneath the log Get the container status.   cost estimation   Return my locking policy, may be different from the Transaction's default locking policy. Get the mode I was opened with. Get the reusable recordId sequence number. Get information about space used by the container. Return the RawTransaction I was opened in.    Log all information necessary to recreate the container during a load tran. Return a record handle that is initialized to the given page number and record id. Preallocate numPage if possible. The container is about to be modified. Loggable actions use this to make sure the container gets cleaned if a checkpoint is taken after any log record is sent to the log stream but before the container is actually dirtied. ReCreate a page for rollforward recovery. <p> During redo recovery it is possible for the system to try to redo the creation of a page (ie. going from non-existence to version 0). It first trys to read the page from disk, but a few different types of errors can occur: o the page does not exist at all on disk, this can happen during rollforward recovery applied to a backup where the file was copied and the page was added to the file during the time frame of the backup but after the physical file was copied. o space in the file exists, but it was never initalized.  This can happen if you happen to crash at just the right moment during the allocation process.  Also on some OS's it is possible to read from a part of the file that was not ever written - resulting in garbage from the store's point of view (often the result is all 0's). All these errors are easy to recover from as the system can easily create a version 0 from scratch and write it to disk. Because the system does not sync allocation of data pages, it is also possible at this point that whlie writing the version 0 to disk to create it we may encounter an out of disk space error (caught in this routine as a StandardException from the create() call.  We can't recovery from this without help from outside, so the caught exception is nested and a new exception thrown which the recovery system will output to the user asking them to check their disk for space/errors. remove the container Remove a page from the container.  *	Methods of DerbyObserver Called when the transaction is about to complete. Was I opened for updates? <p> <BR> MT - thread safe Attach me to a container. If this method returns false then I cannot be used anymore, and any reference to me must be discarded.
Add and load a stream container Add a container with a specified page size to a segment. Find all the all the containers stored in the seg0 directory and backup each container to the specified backup location. end of boot Privileged startup. Must be private so that user code can't call this entry point. Return values of system properties that identify the JVM. Will catch SecurityExceptions and note them for displaying information. end of buildjvmVersion Return values of system properties that identify the OS. Will catch SecurityExceptions and note them for displaying information. * Methods of ModuleControl Implement checkpoint operation, write/sync all pages in cache. <p> The derby write ahead log algorithm uses checkpoint of the data cache to determine points of the log no longer required by restart recovery. <p> This implementation uses the 2 cache interfaces to force all dirty pages to disk: WRITE DIRTY PAGES TO OS: In the first step all pages in the page cache are written, but not synced (pagecache.cleanAll).  The cachemanager cleanAll() interface guarantees that every dirty page that exists when this call is first made will have it's clean() method called. The data cache (CachedPage.clean()), will call writePage but not sync the page. By using the java write then sync, the checkpoint is usually doing async I/O, allowing the OS to schedule multiple I/O's to the file as efficiently as it can. Note that it has been observed that checkpoints can flood the I/O system because these writes are not synced, see DERBY-799 - checkpoint should probably somehow restrict the rate it sends out those I/O's - it was observed a simple sleep every N writes fixed most of the problem. FORCE THOSE DIRTY WRITES TO DISK: To force the I/O's to disk, the system calls each open dirty file and uses the java interface to sync any outstanding dirty pages to disk (containerCache.cleanAll()).  The open container cache does this work in RAFContainer.clean() by writing it's header out and syncing the file.  (Note if any change is made to checkpoint to sync the writes vs. syncing the file, one probably still needs to write the container header out and sync it). Database creation finished {@inheritDoc } Drop a container. <P><B>Synchronisation</B> <P> This call will mark the container as dropped and then obtain an CX lock (table level exclusive lock) on the container. Once a container has been marked as dropped it cannot be retrieved by an openContainer() call unless explicitly with droppedOK. <P> Once the exclusive lock has been obtained the container is removed and all its pages deallocated. The container will be fully removed at the commit time of the transaction. Drop a stream container. <P><B>Synchronisation</B> <P> This call will remove the container. {@inheritDoc } Add a file to the list of files to be removed post recovery. Find the largest containerid is seg 0. <p> Do a file list of the files in seg0 and return the highest numbered file found. <p> Until I figure out some reliable place to store this information across a boot of the system, this is what is used following a boot to assign the next conglomerate id when a new conglomerate is created.  It is only called at most once, and then the value is cached by calling store code. <p> Ask the log factory to flush up to this log instant. Return an alternate path to container file relative to the root directory. The alternate path uses upper case 'C','D', and 'DAT' instead of lower case - there have been cases of people copying the database and somehow upper casing all the file names. The intended use is as a bug fix for track 3444. get all the names of the files in seg 0. MT - This method needs to be synchronized to avoid conflicts with other privileged actions execution in this class. Return the path to a container file. <p> Return the path to a container file that is relative to the root directory. <p> The format of the name of an existing container file is: segNNN/cXXX.dat The format of the name of a stub describing a dropped container file is: segNNN/dXXX.dat NNN = segment number, currently 0 is where normal db files are found. XXX = The hex representation of the container number The store will always create containers with this format name, but the store will also recognize the following two formats when attempting to open files - as some copy tools have uppercased our filesnames when moving across operating systems: The format of the name of an existing container file is: segNNN/CXXX.DAT The format of the name of a stub describing a dropped container file is: segNNN/DXXX.DAT <p> Returns the encryption block size used by the algorithm at time of creation of an encrypted database Return my unique identifier check to see if we are the only JBMS opened against this database. <BR>This method does nothing if this database is read only or we cannot access files directly on the database directory. <BR>We first see if a file named db.lck exists on the top database directory (i.e., the directory where service.properties lives).  If it doesn't exist, we create it and write to it our identity which is generated per boot of the JBMS. <BR>If the db.lck file already exists when we boot this database, we try to delete it first, assuming that an opened RandomAccessFile can act as a file lock against delete.  If that succeeds, we may hold a file lock against subsequent JBMS that tries to attach to this database before we exit. <BR>We test to see if we think an opened file will prevent it from being deleted, if so, we will hold on to the open file descriptor and use it as a filelock.  If not, and we started out deleting an existing db.lck file, we issue a warning message to the info stream that we are about to attached to a database which may already have another JBMS attached to it. Then we overwrite that db.lck file with our identity. <BR>Upon shutdown, we delete the db.lck file.  If the system crash instead of shutdown cleanly, it will be cleaned up the next time the system boots Get the loggable allocation action associated with this implementation Get the loggable page action that is associated with this implementation @return the PageActions @exception StandardExceptions Standard Derby Error Policy Return an id which can be used to create a container. <p> Return an id number with is greater than any existing container in the current database.  Caller will use this to allocate future container numbers - most likely caching the value and then incrementing it as it is used. <p> Privileged Monitor lookup. Must be private so that user code can't call this entry point.  Does this factory support this service type. Notify through set handler that an undo of an insert has happened. <p> When an undo of an event is executed by the raw store UndoHandler.insertUndoNotify() will be called, allowing upper level callers to execute code as necessary.  The initial need is for the access layer to be able to queue post commit reclaim space in the case of inserts which are aborted (including the normal case of inserts failed for duplicate key violations) (see DERBY-4057) Longer descrption of routine. <p> Is the store read-only. Return a jar file by asking the class's class loader for the location where the class was loaded from. If no value, it returns null Return true if the Lucene plugin is loaded Really this is just a convience routine for callers that might not have access to a log factory. * CacheableFactory Produces new container objects. <p> Concrete implementations of a DataFactory must implement this routine to indicate what kind of containers are produced. This class produces file-based containers - RAFContainer objects for files that support random access and InputStreamContainer object for others, such as data files in JARs. <p> Creates a RAFContainer object. This method is overridden in BaseDataFileFactoryJ4 to produce RAFContainer4 objects instead of RAFContainer objects. * Methods of DataFactory   open an exsisting streamContainer Called after recovery is performed. Called from within a privilege block end of privGetJBMSLockOnDB end of privReleaseJBMSLockOnDB end of privRestoreDataDirectory return a secure random number re-Create a container during redo recovery. called ONLY during recovery load tran. * Called by post commit daemon, calling ReclaimSpace.performWork() Delete the stub files that are not required for recovery. A stub file is not required to be around if the recovery is not going to see any log record that belongs to that container. Since the stub files are created as a post commit operation, they are not necessary during undo operation of the recovery. To remove a stub file we have to be sure that it was created before the redoLWM in the check point record. We can be sure that the stub is not required if the log instant when it was created is less than the redoLWM. {@inheritDoc } Remove stubs in this database.  Stubs are committed deleted containers removes the data directory(seg*) from database home directory and restores it from backup location. This function gets called only when any of the folling attributes are specified on connection URL: Attribute.CREATE_FROM (Create database from backup if it does not exist) Attribute.RESTORE_FROM (Delete the whole database if it exists and then restore * it from backup) Attribute.ROLL_FORWARD_RECOVERY_FROM:(Perform Rollforward Recovery; except for the log directory everthing else is replced  by the copy  from backup. log files in the backup are copied to the existing online log directory. In all the cases, data directory(seg*) is replaced by the data directory directory from backup when this function is called. PrivilegedExceptionAction method end of run {@inheritDoc } * Implementation specific methods Register a handler class for insert undo events. <p> Register a class to be called when an undo of an insert is executed. When an undo of an event is executed by the raw store UndoHandler.insertUndoNotify() will be called, allowing upper level callers to execute code as necessary.  The initial need is for the access layer to be able to queue post commit reclaim space in the case of inserts which are aborted (including the normal case of inserts failed for duplicate key violations) (see DERBY-4057) <p> Set up the cache cleaner for the container cache and the page cache. Privileged startup. Must be private so that user code can't call this entry point. end of stop keeps track of information about the stub files of the  committed deleted containers. We use the info to delete them at checkpoints. In addition to the file info , we also keep track of the identity of the container; which helps to remove entry in the cache and the log instant when the stub was created, which helps us to figure out whether we require the stub file for the crash recovery. We maintain the information in a hashtable: key(LOG INSTANT) Values: File handle , and ContainerIdentity.
Overrides newRAFContainer in BaseDataFileFactory to produce RAFContainer4 objects capable of exploiting the NIO API available in Java 1.4+
<p> Get the maximum value of 4 input values.  If less than 4 values, input {@code null} for the unused parameters and place them at the end. If more than 4 input values, call this multiple times to accumulate results.  Also have judge's type as parameter to have a base upon which the comparison is based.  An example use is for code generation in bug 3858. </p> <p> If all the input values are SQL NULL, return SQL NULL. Otherwise, return the maximum value of the non-NULL inputs. </p> <p> Get the minimum value of 4 input values.  If less than 4 values, input {@code null} for the unused parameters and place them at the end. If more than 4 input values, call this multiple times to accumulate results.  Also have judge's type as parameter to have a base upon which the comparison is based.  An example use is for code generation in bug 3858. </p> <p> If all the input values are SQL NULL, return SQL NULL. Otherwise, return the minimum value of the non-NULL inputs. </p>
Push the first set of common arguments for obtaining a scan ResultSet from ResultSetFactory. The first 11 arguments are common for these ResultSet getters <UL> <LI> ResultSetFactory.getBulkTableScanResultSet <LI> ResultSetFactory.getHashScanResultSet <LI> ResultSetFactory.getTableScanResultSet <LI> ResultSetFactory.getRaDependentTableScanResultSet </UL>   Can this join strategy be used on the outermost table of a join.
Get InputStream for application properties file Returns nul if it does not exist. Boot a module. If the module implements ModuleControl then its boot() method is called. Otherwise all the boot code is assumed to take place in its constructor. Boot all persistent services that can be located at run time. <BR> This method enumerates through all the service providers that are active and calls bootPersistentServices(PersistentService) to boot all the services that that provider knows about. Boot all persistent services that can be located by a single service provider <BR> This method enumerates through all the service providers that are active and calls bootPersistentServices(PersistentService) to boot all the services that that provider knows about. Boot (start or create) a service (persistent or non-persistent). If the module implements ModuleSupportable then call its canSupport() method to see if it can or should run in this setup. If it doesn't then it can always run. Obtain a class that supports the given identifier. Create a persistent service. * Methods related to service providers. * * A service provider implements PersistentService and * abstracts out: * *    Finding all serivces that should be started at boot time. *    Finding the service.properties file for a service *    Creating a service's root. * * A monitor can have any number of service providers installed, * any module that implements PersistentService is treated specially * and stored only in the serviceProviders hashtable, indexed by * its getType() method. * * Once all the implementations have loaded the service providers * are checked to see if they run in the current environment. Determine which of the set of service providers (PersistentService objects) are supported in the current environment. If a PersistentService implementation does not implement ModuleControl then it is assumed it does support the current environment. Otherwise the canSupport() method makes the determination. Any providers that are not supported are removed from the list. * non-public methods. Find a class that implements the required index, return the index into the implementations vector of that class. Returns -1 if no class could be found. Find a provider and start  a service. Find the service provider from a name that includes a service type, ie. is of the form 'type:name'. If type is less than 3 characters then it is assumed to be of type directory, i.e. a windows driver letter. * BundleFinder private Hashtable localeBundles; Get the locale from the ContextManager and then find the bundle based upon that locale. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. end of getDefaultImplementations Get the complete set of module properties by loading in contents of all the org/apache/derby/modules.properties files. This must be executed in a privileged block otherwise when running in a security manager environment no properties will be returned. Create an implementation set. Look through the properties object for all properties that start with derby.module and add the value into the vector. If no implementations are listed in the properties object then null is returned. Return a PersistentService implementation to handle the subSubProtocol. end of getPersistentService end of getPersistentServiceImplementation Return an array of the service identifiers that are running and implement the passed in protocol (java interface class name). Return the name of the service that the passed in module lives in. end of getServiceProvider Return the PersistentService object for a service. Will return null if the service does not exist. Find the StorageFactory class name that handles the subSub protocol. Looks in the system property set and the set defined during boot. end of getStorageFactoryClassName Methods of ModuleFactory includes BootStrap and Runnable Returns the Timer factory for this system. * Methods of com.ibm.db2j.system.System Return the UUID factory for this system.  Returns null if there isn't one. See com.ibm.db2j.system.System Initialize the monitor wrt the current environemnt. Returns false if the monitor cannot be initialized, true otherwise. load a module instance. Look through the implementations for a module that implements the required factory interface and can handle the properties given. The module's start or create method is not called. Return a new instance of class {@code classObject} using a no-param constructor. Obtain an new instance of a class that supports the given identifier.  Removes a PersistentService. Could be used for drop database. * Class methods Return a property set that has the runtime properties removed. Should only be called if reportOn is true apart from report/Exception(). Set the locale for the service *outside* of boot time. Set the locale for the service at boot time. The passed in properties must be the one passed to the boot method. Privileged startup. Must be private so that user code can't call this entry point. Returns true if the system is already booted or in the process of shutting down. Shut down a service that was started by this Monitor. Will cause the stop() method to be called on each loaded module. Start a module. Start a non-persistent service. Start a peristent service. Boot a service under the control of the provider * Locale handling Privileged shutdown of the ContextService. Must be private so that user code can't call this entry point.
Returns false if an insert is not to be allowed in the page. Append an overflow pointer to a partly logged row, to point to a long column that just been logged. <BR> MT - latched - page latch must be held increment the version by one and return the new version. Is the given slot number on the page? <BR> MT - latched Initialized this page for reuse or first use Try to compact this record.  Deleted record are treated the same way as nondeleted record.  This page must not be an overflow page.  The record may already have been purged from the page. <P> <B>Locking Policy</B> <P> No locks are obtained. <BR> MT - latched <P> <B>NOTE : CAVEAT </B><BR> This operation will physically get rid of any reserved space this record may have, or it may compact the record by merging strung out row pieces together.  Since the freed reserved space is immediately usable by other transactions which latched the page, it is only safe to use this operation if the caller knows that it has exclusive access to the page for the duration of the transaction, i.e., effectively holding a page lock on the page, AND that the record has no uncommitted updates. Subclass implementation of compactRecord.  Copy num_rows from srcPage, src_slot into this page starting at dest_slot. This is destination page of the the copy half of copy and Purge. Mark this page as being deallocated  Returns true if the entire record of that slot fits inside of this page.  Returns false if part of the record on this slot overflows to other pages, either due to long row or long column. <BR> MT - latched    * Cacheable methods Find the slot for the first record on the page with an id greater than the passed in identifier. <BR> Returns the slot of the first record on the page with an id greater than the one passed in.  Usefulness of this functionality depends on the clients use of the raw store interfaces.  If all "new" records are always inserted at the end of the page, and the raw store continues to guarantee that all record id's will be allocated in increasing order on a given page, then a page is always sorted in record id order.  For instance current heap tables function this way.  If the client ever inserts at a particular slot number, rather than at the "end" then the record id's will not be sorted. <BR> In the case where all record id's are always sorted on a page, then this routine can be used by scan's which "lose" their position because the row they have as a position was purged.  They can reposition their scan at the "next" row after the row that is now missing from the table. <BR> This method returns the record regardless of its deleted status. <BR> MT - latched Find the slot for the record with the passed in identifier. <BR> This method returns the record regardless of its deleted status. <BR> The "slotHint" argument is a hint about what slot the record id might be in.  Callers may save the last slot where the record was across latch/unlatches to the page, and then pass that slot back as a hint - if the page has not shuffled slots since the last reference then the hint will succeed and a linear search is saved.  If the caller has no idea where it may be, then FIRST_SLOT_NUMBER is passed in and a linear search is performed. <BR> MT - latched Get the aux object. <BR> MT - latched - It is required the caller throws away the returned reference when the page is unlatched. OK to hand object outside to cache.. * Methods that read/store records/fields based upon calling methods * a sub-calls provides to do the actual storage work. * Page LastLog Instant control  returns the page data array, that is actually written to the disk. * Implementation specific methods Get the Page identifer <BR> MT - RESOLVE  Get the page status, one of the values in the above page status flag * Page Version control Return the current page version. * Methods that our super-class (BasePage) requires we implement. * Here we only implement the methods that correspond to the logical * operations that require logging, any other methods that are storage * specific we leave to our sub-class. * * All operations that are logged must bump this page's version number * and update this page's last log instant. * These should be sanity checked on each logAndDo (similarly, it should * be checked in CompensationOperation.doMe) * Methods that any sub-class must implement. These allow generic log operations. Get the stored length of a record. This must match the amount of data written by logColumn and logField. <BR> MT - latched - page latch must be held Return the total number of bytes reserved by the record at this slot. <BR> MT - latched, page is latched when this methods is called.  Return the total number of bytes used, reserved, or wasted by the record at this slot. <BR> MT - latched, page is latched when this methods is called. Mark this page as being allocated and initialize it to a pristine page initialize a page for the first time or for reuse All subtypes are expected to overwrite this method if it has something to clean up Initialized the BasePage. <p> Initialize the object, ie. perform work normally perfomed in constructor.  Called by setIdentity() and createIdentity(). Must be called by a sub-class before calling setHeaderAtSlot.  Insert a row allowing overflow. If handle is supplied then the record at that hanlde will be updated to indicate it is a partial row and it has an overflow portion.  Routine to insert a long column. <p> This code inserts a long column as a linked list of rows on overflow pages.  This list is pointed to by a small pointer in the main page row column.  The operation does the following: allocate new overflow page insert single row filling overflow page while (more of column exists) allocate new overflow page insert single row with next piece of row update previous piece to point to this new piece of row Same code is called both from an initial insert of a long column and from a subsequent update that results in a long column. get record count without checking for latch get record count without checking for latch  no need to check for slot on page, call already checked Check whether the page is latched. Return true if the page is an overflow page, false if not. For implementation that don't have overflow pages, return false. Check if a B-tree scan positioned on this page needs to reposition. Log a to be stored column. <BR> MT - latched - page latch must be held Log a currently stored field. The logged version of the field must be readable by storeField. <BR> MT - latched - page latch must be held Log a to be stored long column.  return -1 when done. <BR> MT - latched - page latch must be held Log a currently stored record to the output stream. The logged version of the record must be readable by storeRecord. <BR> MT - latched - page latch must be held Log the row that will be stored at the given slot to the given OutputStream. The logged form of the Row must be readable by storeRecord. <BR> MT - latched - page latch must be held Create a new record identifier. <BR> MT - latched, page is latched when this methods is called. Create a new record identifier, the passed in one is the last one created. Use this method to collect and reserve multiple recordIds in one stroke.  Given the same input recordId, the subclass MUST return the same recordId every time. <BR> MT - latched, page is latched when this methods is called. Create a new record identifier, and bump to next recordid. <BR> MT - latched, page is latched when this methods is called.  The page or its header is about to be modified. Loggable actions use this to make sure the page gets cleaned if a checkpoint is taken after any log record is sent to the log stream but before the page is actually dirtied. Move page state from UNLATCHED to PRELATCH. setExclusive*() routines do the work of completing the latch - using the preLatch status. Purge one or more rows on a non-overflow page. Purge a record from the page. <BR> MT - latched - page latch must be held Purge all the overflow columns and overflow rows of the record at slot.   Release the exclusive latch on the page. <BR> MT - latched Remove record at slot. <p> Remove the slot at the in-memory slot table, i.e., slots from 0 to deleteSlot-1 is untouched, deleteSlot is removed from in memory slot table, deleteSlot+1 .. recordCount()-1 move to down one slot. <BR> MT - latched Reserve the required number of bytes for the record in the specified slot. <BR> MT - latched - page latch must be held Read portion of a log record at the given slot into the given byteHolder. <BR> MT - latched, page is latched when this methods is called. * abstract methods that an implementation must provide. * * <BR> MT - latched, page is latched when these methods are called. Read the record at the given slot into the given row. <P> This reads and initializes the columns in the row array from the raw bytes stored in the page associated with the given slot.  If validColumns is non-null then it will only read those columns indicated by the bit set, otherwise it will try to read into every column in row[]. <P> If there are more columns than entries in row[] then it just stops after every entry in row[] is full. <P> If there are more entries in row[] than exist on disk, the requested excess columns will be set to null by calling the column's object's restoreToNull() routine (ie.  ((Object) column).restoreToNull() ). <P> If a qualifier list is provided then the row will only be read from disk if all of the qualifiers evaluate true.  Some of the columns may have been read into row[] in the process of evaluating the qualifier. <BR> MT - latched, page is latched when this methods is called. Restore a storable row from a InputStream that was used to store the row after a logRecord call. <BR> MT - latched - page latch must be held Set the aux object. <BR> MT - single thread required. Calls via the Page interface will have the page latched, thus providing single threadedness. Otherwise calls via this class are only made when the class has no-identity, thus only a single thread can see the object. Set the number of rows in the container - the page uses this to decide whether it needs to aggressive set the container's row count when it changes. Mark the record at the passed in slot as deleted. return code comes from StoredRecordHeader class: return	1, if delete status from not deleted to deleted return -1, if delete status from deleted to not deleted return  0, if status unchanged. <BR> <B>Any sub-class must call this method when deleting a record.</B> <BR> MT - latched Set the delete status of a record from the page. <BR> MT - latched - page latch must be held Get an exclusive latch on the page. <BR> MT - thread safe Get an exclusive latch on the page, but only if I don't have to wait. <BR> MT - thread safe * Manipulation of the in-memory version of the slot table. Must be called by any non-abstract sub-class to initialise the slot table. Set page status based on passed in status flag. Set the page status underneath a log record <BR> MT - latched - page latch must be held set it when the page is read from disk. <BR> MT - single thread required - Only called while the page has no identity which requires that only a single caller can be accessing it. Set a hint in this page to make B-tree scans positioned on it reposition before they continue. This method is typically called when rows are removed from a B-tree leaf page (for instance in a page split). Set the reserved space for this row to value. Shift all records in the in-memory slot table up one slot, starting at and including the record in slot 'low' A new slot is added to accomdate the move. <BR> MT - latched Is this page/deleted row a candidate for immediate reclaim space. <p> Used by access methods after executing a delete on "slot_just_deleted" to ask whether a post commit should be queued to try to reclaim space after the delete commits. <p> Also used by access methods after undo of an insert. <p> Will return true if the number of non-deleted rows on the page is &lt;= "num_non_deleted_rows".  For instance 0 means schedule reclaim only if all rows are deleted, 1 if all rows but one are deleted. <p> Will return true if the row just deleted is either a long row or long column.  In this case doing a reclaim space on the single row may reclaim multiple pages of free space, so better to do it now rather than wait for all rows on page to be deleted.  This case is to address the worst case scenario of all rows with long columns, but very short rows otherwise.  In this case there could be 1000's of rows on the main page with many gigabytes of data on overflow pages in deleted space that would not be reclaimed until all rows on the page were deleted. Skip a previously stored field written by logField or logColumn. <BR> MT - latched - page latch must be held * Debugging methods Debugging, print slot table information Is there space for copying this many rows which takes this many bytes on the page <BR> MT - latched, page is latched when this methods is called. Read a previously stored field written by logField or logColumn and store it on the data page at thge given slot with the given record identifier and field number. Any previously stored field is replaced. <BR> MT - latched - page latch must be held Read a previously stored record written by logRecord or logRow and store it on the data page at the given slot with the given record identifier. Any previously stored record must be replaced. <BR> MT - latched - page latch must be held Returns true if an insert is allowed in the page and the page is relatively unfilled - let specific implementation decide what relatively unfilled means Unlatch the page. * Methods of Observer. This object is set to observe the BaseContainerHandle it was obtained by, that handle will notify its observers when it is being closed. In that case we will release the latch on the page held by that container. <BR> MT - latched   Update the overflow pointer for a long column <BR> MT - latched - page latch must be held Update the overflow pointer for a long row <BR> MT - latched - page latch must be held
end of createTempDir Create and returns a temporary file in temporary file system of database. Get the canonical name of the database. This is a name that uniquely identifies it. It is system dependent. The normal, disk based implementation uses method java.io.File.getCanonicalPath on the directory holding the database to construct the canonical name. Get the pathname separator character used by the StorageFile implementation. Get the abstract name of the directory that holds temporary files. Classes implementing the StorageFactory interface must have a null constructor.  This method is called when the database is booted up to initialize the class. It should perform all actions necessary to start the basic storage, such as creating a temporary file directory. The init method will be called once, before any other method is called, and will not be called again. end of init This method is used to determine whether the storage is fast (RAM based) or slow (disk based). It may be used by the database engine to determine the default size of the page cache. Construct a persistent StorageFile from a path name. Construct a persistent StorageFile from a directory and path name. Construct a persistent StorageFile from a directory and path name. Construct a StorageFile from a path name. Construct a StorageFile from a directory and file name. Construct a StorageFile from a directory and file name. Set the canonicalName. May need adjustment due to DERBY-5096 Determine whether the storage supports random access. If random access is not supported then it will only be accessed using InputStreams and OutputStreams (if the database is writable).
Retrieve the the position of the ColumnReference or ResultColumn for which we most recently found a base table number. Reset the state of this visitor. Set a new JBitSet to serve as the holder for base table numbers we find while walking.   //////////////////////////////////////////////  VISITOR INTERFACE  //////////////////////////////////////////////
Return the method name to get a Derby DataValueDescriptor object of the correct type and set it to a specific value. The method named will be called with two arguments, a value to set the returned value to and a holder object if pushCollationForDataValue() returns false. Otherwise three arguments, the third being the collationType. This implementation returns "getDataValue" to map to the overloaded methods DataValueFactory.getDataValue(type, dvd type) The caller will have pushed a DataValueFactory and  value of that can be converted to the correct type, e.g. int for a SQL INTEGER. Thus upon entry the stack looks like: ...,dvf,value If field is not null then it is used as the holder of the generated DataValueDescriptor to avoid object creations on multiple passes through this code. The field may contain null or a valid value. This method then sets up to call the required method on DataValueFactory using the dataValueMethodName(). The value left on the stack will be a DataValueDescriptor of the correct type: If the field contained a valid value then generated code will return that value rather than a newly created object. If field was not-null then the generated code will set the value of field to be the return from the DataValueFactory method call. Thus if the field was empty (set to null) when this code is executed it will contain the newly generated value, otherwise it will be reset to the same value. ...,dvd The caller will have pushed a DataValueFactory and a null or a value of the correct type (interfaceName()). Thus upon entry the stack looks like on of: ...,dvf,ref ...,dvf,null This method then sets up to call the required method on DataValueFactory using the nullMethodName(). The value left on the stack will be a DataValueDescriptor of the correct type: ...,dvd Get the method name for getting out the corresponding primitive Java type. Get the StoredFormatId from the corresponding TypeId. Get the TypeCompiler that corresponds to the given TypeId. Get the TypeId that corresponds to this TypeCompiler. Return the method name to get a Derby DataValueDescriptor object of the correct type set to SQL NULL. The method named will be called with one argument: a holder object if pushCollationForDataValue() returns false, otherwise two arguments, the second being the collationType. Tell whether this numeric type can be converted to the given type. Tell whether this numeric type can be stored into from the given type. Return true if the collationType is to be passed to the methods generated by generateNull and generateDataValue.  Set the TypeCompiler that corresponds to the given TypeId. Determine whether thisType is storable in otherType due to otherType being a user type.
wrap a string in quotes we want equals to say if these are the same type id or not. Get the jdbc type id for this type.  JDBC type can be found in java.sql.Types. Returns the SQL name of the datatype. If it is a Derby user-defined type, it returns the full Java path name for the datatype, meaning the dot-separated path including the package names. If it is a UDT, returns "schemaName"."unqualifiedName". Get the schema name of this type. Non-null only for UDTs Get the formatID which corresponds to this class. Get the unqualified name of this type. Except for UDTs, this is the same value as getSQLTypeName() Hashcode which works with equals. Return true if this is this type id describes an ANSI UDT Read this object from a stream of stored objects. strip the bracketing quotes from a string Converts this TypeId, given a data type descriptor (including length/precision), to a string. E.g. VARCHAR(30) For most data types, we just return the SQL type name. Format this BaseTypeIdImpl as a String Does this type id represent a user type? Write this object to a stream of stored objects.
* UserAuthenticator methods. Authenticate the passed-in user's credentials.   ModuleControl implementation (overriden)  Check if we should activate this authentication service. Hash a password using the same algorithm as we used to generate the stored password token.
Returns the maximum number of JDBC prepared statements a connection is allowed to cache. ---------------------------interface methods----------------------------   Internally used method. Read an object from the ObjectInputStream. <p> This implementation differs from the default one by initiating state validation of the object created. Specifies the maximum size of the statement cache. Make sure the state of the de-serialized object is valid.
Compute a DNC log writer before a connection is created. Called on for connection requests.  The java.io.PrintWriter overrides the traceFile setting.  If neither traceFile, nor logWriter, nor traceDirectory are set, then null is returned. ----------------------supplemental methods------------------------------ ---------------------- helper methods ----------------------------------- The java.io.PrintWriter overrides the traceFile setting. If neither traceFile nor jdbc logWriter are set, then null is returned. logWriterInUseSuffix used only for trace directories to indicate whether log writer is use is from xads, cpds, sds, ds, driver, config, reset. This method handles all the override semantics.  The logWriter overrides the traceFile, and traceDirectory settings.  If neither traceFile, nor logWriter, nor traceDirectory are set, then null is returned. Constructs the JDBC connection URL from the state of the data source. Returns the SSL mode specified by the property object. Attempt to establish a database connection in a non-pooling, non-distributed environment. Attempt to establish a database connection in a non-pooling, non-distributed environment.   //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// ---------------------------- password ----------------------------------  The password property is defined in subclasses, but the method getPassword (java.util.Properties properties) is in this class to eliminate dependencies on j2ee for connections that go thru the driver manager. Helper methods Minion method that establishes the initial physical connection using DS properties instead of CPDS properties. Parses the string and returns the corresponding constant for the SSL mode denoted. <p> Valid values are <tt>off</tt>, <tt>basic</tt> and <tt>peerAuthentication</tt>. Return the security mechanism. If security mechanism has not been set explicitly on datasource, then upgrade the security mechanism to a more secure one if possible. Return the security mechanism for this datasource object. If security mechanism has not been set explicitly on datasource, then upgrade the security mechanism to a more secure one if possible. We use the NET layer constants to avoid a mapping for the NET driver. Return security mechanism if it is set, else upgrade the security mechanism if possible and return the upgraded security mechanism  Returns the SSL encryption mode specified for the data source. Check if derby.client.traceDirectory is provided as a JVM property. If yes, then we use that value. If not, then we look for traceDirectory in the the properties parameter. Check if derby.client.traceLevel is provided as a JVM property. If yes, then we use that value. If not, then we look for traceLevel in the the properties parameter. This method has logic to upgrade security mechanism to a better (more secure) one if it is possible. Currently derby server only has support for USRIDPWD, USRIDONL, EUSRIDPWD and USRSSBPWD (10.2+) - this method only considers these possibilities. USRIDPWD, EUSRIDPWD and USRSSBPWD require a password, USRIDONL is the only security mechanism which does not require password. 1. if password is not available, then security mechanism possible is USRIDONL 2. if password is available,then USRIDPWD is returned. Method that establishes the initial physical connection using DS properties instead of CPDS properties. Handles common error situations that can happen when trying to obtain a physical connection to the server, and which require special handling. <p> If this method returns normally, the exception wasn't handled and should be handled elsewhere or be re-thrown. JDBC 4.0 java.sql.Wrapper interface methods Check whether this instance wraps an object that implements the interface specified by {@code iface}. Returns the maximum number of JDBC prepared statements a connection is allowed to cache. <p> A basic data source will always return zero. If statement caching is required, use a {@link javax.sql.ConnectionPoolDataSource}. <p> This method is used internally by Derby to determine if statement pooling is to be enabled or not. Not part of public API, so not present in {@link org.apache.derby.jdbc.ClientDataSourceInterface}. Read the value of the passed system property. If we are running under the Java security manager and permission to read the property is missing,a null is returned, and no diagnostic is given (DERBY-6620). Set this property to pass in more Derby specific connection URL attributes. <BR> Any attributes that can be set using a property of this DataSource implementation (e.g user, password) should not be set in connectionAttributes. Conflicting settings in connectionAttributes and properties of the DataSource will lead to unexpected behaviour. Set this property to create a new database.  If this property is not set, the database (identified by databaseName) is assumed to be already existing. Properties to be seen by Bean - access thru reflection. -- Standard JDBC DataSource Properties Sets the security mechanism. Set this property if one wishes to shutdown the database identified by databaseName. Specifies the SSL encryption mode to use. <p> Valid values are <tt>off</tt>, <tt>basic</tt> and <tt>peerAuthentication</tt>. tokenize "property=value;property=value..." and returns new properties object This method is used both by ClientDriver to parse the url and ClientDataSource.setConnectionAttributes Returns {@code this} if this class implements the specified interface. --- private helper methods The dataSource keeps individual fields for the values that are relevant to the client. These need to be updated when set connection attributes is called.

Get rid of all queued up Serviceable tasks. Privileged Monitor lookup. Must be package private so that user code can't call this entry point. class specific methods Daemon Service method pause the daemon.  Wait till it is no running before it returns Returns true if awakened by some notification, false if wake up by timer Runnable methods Change the priority of the current thread, but only if it was created by {@link ModuleFactory#getDaemonThread}. Finish what we are doing and at the next convenient moment, get rid of the thread and make the daemon object goes away if possible. remember we are calling from another thread Removes a client from the list of subscribed clients. The call does not wait for the daemon to finish the work it is currently performing. Therefore, the client must tolerate that its <code>performWork()</code> method could be invoked even after the call to <code>unsubscribe()</code> has returned (but not more than once). *Wait until the work in the high priority queue is done. *Note: Used by tests only to make sure all the work *assigned to the daemon is completed. BasicDaemon method let everybody else run first
<p> Backup Lucene indexes to the backup directory. This assumes that the rest of the database has been backup up and sanity checks have been run. </p> Privileged startup. Must be private so that user code can't call this entry point. ModuleControl interface methods specific to this class  Privileged startup. Must be private so that user code can't call this entry point. Get the set of database properties from the set stored on disk outside of service.properties. * Methods to allow sub-classes to offer alternate implementations. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. LocaleFinder methods  Return the DataDictionary for this database, set up at boot time.  * Methods related to  ModuleControl Database interface Return the engine type that this Database implementation supports. This implementation supports the standard database. Return the UUID of this database. * Methods of JarReader Methods from org.apache.derby.database.Database Get the location of the Lucene indexes Privileged Monitor lookup. Must be private so that user code can't call this entry point. * Return an Object instead of a ResourceAdapter * so that XA classes are only used where needed; * caller must cast to ResourceAdapter. <p> Get the database StorageFactory. </p>   * Methods of PropertySetCallback Is the database active (open). //////////////////////////////////////////////////////////////////////  SUPPORT FOR BACKING UP LUCENE DIRECTORY  ////////////////////////////////////////////////////////////////////// <p> Return true if the Lucene plugin is loaded. </p>  Start the replication master role for this database Stop the replication master role for this database. Only a SlaveDatabase can be in replication slave mode. Always throws an exception
return the dependent for this dependency. return the provider for this dependency.  Dependency interface  return the provider's key for this dependency.
DependencyManager interface  adds a dependency from the dependent on the provider. This will be considered to be the default type of dependency, when dependency types show up. <p> Implementations of addDependency should be fast -- performing alot of extra actions to add a dependency would be a detriment. Adds the dependency to the data dictionary or the in-memory dependency map. <p> The action taken is detmermined by whether the dependent and/or the provider are persistent.  class implementation  Add a new dependency to the specified table if it does not already exist in that table. Adds the dependency as an in-memory dependency. Adds the dependency as a stored dependency. <p> We expect that transactional locking (in the data dictionary) is enough to protect us from concurrent changes when adding stored dependencies. Adding synchronization here and accessing the data dictionary (which is transactional) may cause deadlocks.  Erases all of the dependencies the dependent has, be they valid or invalid, of any dependency type.  This action is usually performed as the first step in revalidating a dependent; it first erases all the old dependencies, then revalidates itself generating a list of new dependencies, and then marks itself valid if all its new dependencies are valid. <p> There might be a future want to clear all dependencies for a particular provider, e.g. when destroying the provider. However, at present, they are assumed to stick around and it is the responsibility of the dependent to erase them when revalidating against the new version of the provider. <p> clearDependencies will delete dependencies if they are stored; the delete is finalized at the next commit.  Clear the specified in memory dependency. This is useful for clean-up when an exception occurs. (We clear all in-memory dependencies added in the current StatementContext.) removes a dependency for a given provider. assumes that the dependent removal is being dealt with elsewhere. Won't assume that the dependent only appears once in the list. @GuardedBy("this") Copy dependencies from one dependent to another.  A version of invalidateFor that does not provide synchronization among invalidators. Count the number of active dependencies, both stored and in memory, in the system. drops a single dependency Returns a string representation of the SQL action, hence no need to internationalize, which is causing the invokation of the Dependency Manager. Turn a list of DependencyDescriptors into a list of Dependencies. Returns an enumeration of all dependencies that this provider is supporting for any dependent at all (even invalid ones). Includes all dependency types. Returns the LanguageConnectionContext to use.   Returns a list of all providers that this dependent has (even invalid ones). Includes all dependency types. mark all dependencies on the named provider as invalid. When invalidation types show up, this will use the default invalidation type. The dependencies will still exist once they are marked invalid; clearDependencies should be used to remove dependencies that a dependent has or provider gives. <p> Implementations of this can take a little time, but are not really expected to recompile things against any changes made to the provider that caused the invalidation. The dependency system makes no guarantees about the state of the provider -- implementations can call this before or after actually changing the provider to its new state. <p> Implementations should throw StandardException if the invalidation should be disallowed.
Minion helper method. Create and return a pooled connection Implementation of ConnectionPoolDataSource interface methods
Most of our customers would be using JNDI to get the data sources. Since we don't have a JNDI in the test setup to test this, we are adding this method to fake it. This is getting used in XAJNDITest so we can compare the two data sources. Return a handle to the internal driver, possibly instantiating it first if it hasn't been booted or if it has been shut down. Privileged service lookup. Must be private so that user code can't call this entry point. Return the value of the {@code attributesAsPassword} property, cf. {@link #setAttributesAsPassword}. DataSource methods - keep these non-final so that others can extend Derby's classes if they choose to. Attempt to establish a database connection. Attempt to establish a database connection with the given username and password.  If the {@code attributeAsPassword} property is set to true then the password argument is taken to be a list of connection attributes with the same format as the {@code connectionAttributes} property. Get a user connection: minion method.      Get the log writer for this data source. <p/> The log writer is a character output stream to which all logging and tracing messages for this data source object instance will be printed.  This includes messages printed by the methods of this object, messages printed by methods of other objects manufactured by this object, and so on.  Messages printed to a data source specific log writer are not printed to the log writer associated with the {@code java.sql.Drivermanager} class. When a data source object is created the log writer is initially null, in other words, logging is disabled. DataSource methods Gets the maximum time in seconds that this data source can wait while attempting to connect to a database.  A value of zero means that the timeout is the default system timeout if there is one; otherwise it means that there is no timeout. When a data source object is created, the login timeout is initially zero. See {@link #setLoginTimeout}. //////////////////////////////////////////////////////////////////  SECURITY  ////////////////////////////////////////////////////////////////// Privileged Monitor lookup. Must be private so that user code can't call this entry point. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Cannot add @Override here: the compiler balks since this is compiled with compiler level 1.6 which doesn't specify this method in the JDBC interface (4.0). Add as soon as we move to minimum 1.7.  Return database name with ant attributes stripped off.   We don't really need this in the tests (see equals), but unsafe not to define hashCode if we define equals. JDBC 4.0 java.sql.Wrapper interface methods Returns false unless {@code interFace} is implemented. Set {@code attributeAsPassword} property to enable passing connection request attributes in the password argument of {@link #getConnection(String,String)}. If the property is set to {@code true} then the {@code password} argument of the {@link #getConnection(String, String)} method call is taken to be a list of connection attributes with the same format as the {@code connectionAttributes} property. Set this property to pass in more Derby specific connection URL attributes. <br> Any attributes that can be set using a property of this DataSource implementation (e.g user, password) should not be set in connection attributes. Conflicting settings in connection attributes and properties of the DataSource will lead to unexpected behaviour. Properties to be seen by Bean - access thru reflection. Set this property to create a new database.  If this property is not set, the database (identified by {@code databaseName}) is assumed to be already existing. Set the data source name.  The property is not mandatory.  It is used for informational purposes only. Properties to be seen by Bean - access through reflection. Set the database name.  Setting this property is mandatory.  If a database named wombat at g:/db needs to be accessed, database name should be set to "g:/db/wombat".  The database will be booted if it is not already running in the system. Set the data source descripton. This property is not mandatory. It is used for informational purposes only. Set the log writer for this data source. <p/> The log writer is a character output stream to which all logging and tracing messages for this data source object instance will be printed.  This includes messages printed by the methods of this object, messages printed by methods of other objects manufactured by this object, and so on.  Messages printed to a data source specific log writer are not printed to the log writer associated with the {@code java.sql.Drivermanager} class. When a data source object is created the log writer is initially null, in other words, logging is disabled. Sets the maximum time in seconds that this data source will wait while attempting to connect to a database.  A value of zero specifies that the timeout is the default system timeout if there is one; otherwise it specifies that there is no timeout. When a data source object is created, the login timeout is initially zero. <p/> <b>Derby currently ignores this property.</b> Set the {@code password} property for the data source. <p/> This is user's password for any data source {@code getConnection()} call that takes no arguments. Set this property if you wish to shutdown the database identified by {@code databaseName}. Set the {@code user} property for the data source. <p/> This is user name for any data source {@code getConnection()} call that takes no arguments. Return a resource adapter. Use {@code ra} if non-null and active, else get the one for the data base. Returns {@code this} if this class implements the specified interface. Update {@link #jdbcurl} from attributes set.
Instantiate and return an EmbedXAConnection from this instance of EmbeddedXADataSource. Minion method.  Implementation of XADataSource interface methods {@inheritDoc } <p/> Also clears {@link #ra}.

Flushes stream, and optionally also closes it if constructed with canClose equal to true. HeaderPrintWriter interface (partial; remaining methods come from the PrintWriter supertype).
Attach this result set to the top statement context on the stack. Result sets can be directly read from the JDBC layer. The JDBC layer will push and pop a statement context around each ResultSet.getNext(). There's no guarantee that the statement context used for the last getNext() will be the context used for the current getNext(). The last statement context may have been popped off the stack and so will not be available for cleanup if an error occurs. To make sure that we will be cleaned up, we always attach ourselves to the top context. The fun and games occur in nested contexts: using JDBC result sets inside user code that is itself invoked from queries or CALL statements. Checks whether the currently executing statement has been cancelled. This is done by checking the statement's allocated StatementContext object. Determine if the cursor is before the first row in the result set. <p> Pretty-print the inner ResultSet fields inside an outer ResultSet. Returns the outerNode. </p> Clean up on error Dump out the time information for run time stats. <p> Find all fields of type ResultSet. </p>  Returns the row at the absolute position from the query, and returns NULL when there is no such position. (Negative position means from the end of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: An exception will be thrown on 0.  Get the Timestamp for the beginning of execution. Get a compacted version of the candidate row according to the columns specified in the bit map. Share the holders between rows. If there is no bit map, use the candidate row as the compact row. Also, create an array of ints mapping base column positions to compact column positions, to make it cheaper to copy columns to the compact row, if we ever have to do it again. Return the current time in milliseconds, if DEBUG and RunTimeStats is on, else return 0.  (Only pay price of system call if need to.) Return the elapsed time in milliseconds, between now and the beginTime, if DEBUG and RunTimeStats is on, else return 0. (Only pay price of system call if need to.) Get the Timestamp for the end of execution.  Get the execution time in milliseconds. ////////////////////////////////////////////////////  UTILS  //////////////////////////////////////////////////// Get a execution factory Returns the first row from the query, and returns NULL when there are no rows. Cache the language connection context. Return it. Returns the last row from the query, and returns NULL when there are no rows. Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction and projection parameters are evaluated for each row. NOTE: This method should only be called on the top ResultSet of a ResultSet tree to ensure that the entire ResultSet tree gets closed down on an error.  the getNextRowCore() method will be called for all other ResultSets in the tree.   Returns the previous row from the query, and returns NULL when there are no more previous rows. Returns the row at the relative position from the current cursor position, and returns NULL when there is no such position. (Negative position means toward the beginning of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: 0 is valid. NOTE: An exception is thrown if the cursor is not currently positioned on a row. The following methods are common to almost all sub-classes. They are overriden in selected cases. Returns the description of the table's rows Returns the row number of the current row.  Row numbers start from 1 and go to 'n'.  Corresponds to row numbering used to position current row in the result set (as per JDBC).   Get the current transaction controller. Report if closed. Is this ResultSet or it's source result set for update This method will be overriden in the inherited Classes if it is true Mark the ResultSet as the topmost one in the ResultSet tree. Useful for closing down the ResultSet on an error. ResultSet interface open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... NOTE: This method should only be called on the top ResultSet of a ResultSet tree to ensure that the entire ResultSet tree gets closed down on an error.  the openCore() method will be called for all other ResultSets in the tree. Allow sub-classes to record the total time spent in their constructor time. NoPutResultSet interface This is the default implementation of reopenCore(). It simply does a close() followed by an open().  If there are optimizations to be made (caching, etc), this is a good place to do it -- this will be overridden by a number of resultSet imlementations.  and SHOULD be overridden by any node that can get between a base table and a join.   Returns true. Sets the current position to after the last row and returns NULL because there is no current row. Sets the current position to before the first row and returns NULL because there is no current row. Copy columns from the candidate row from the store to the given compact row. If there is no column map, just use the candidate row. This method assumes the above method (getCompactRow()) was called first. getCompactRow() sets up the baseColumnMap. Strip the package location from a class name <p> Pretty-print a ResultSet as an xml child of a parent node. Return the node representing the ResultSet child. </p>
ProviderInfo methods    Get the formatID which corresponds to this class. Formatable methods Read this object from a stream of stored objects. Object methods. Write this object to a stream of stored objects.
Private helper method for fixture testDERBY5289TriggerUpgradeFormat to check and cleanup date in each phase. This test creates a table with LOB column and insets large data into that column. There is a trigger defined on this table but the trigger does not need access to the LOB column. In 10.8 and prior releases, even though we don't need the LOB column to execute the trigger, we still read all the columns from the trigger table when the trigger fired. With 10.9, only the columns required by the firing triggers are read from the trigger table and hence for our test here, LOB column will not be materialized. In 10.8 and prior releases, the trigger defined in this test can run into OOM errors depending on how much heap is available to the upgrade test. But in 10.9 and higher, that won't happen because LOB is never read into memory for the trigger being used by this test. Stored procedure that clears all SPS plans in the database. It does the same as SYSCS_UTIL.SYSCS_INVALIDATE_STORED_STATEMENTS, but we need to create our own procedure since the built-in procedure might not be available in soft upgrade. Start of helper methods for testExhuastivePermutationOfTriggerColumns //////////////////////  make power set of N  ////////////////////// Test case that drops all trigger plans. Should be run at the end of soft upgrade if the old version suffers from DERBY-4835 or DERBY-5289. Otherwise, the database may fail to boot again with the old version. Test for combination of DERBY-5120 and DERBY-5044. ALTER TABLE DROP COLUMN will detect the trigger dependency in this test only in a release with both DERBY-5120 and DERBY-5044 fixes. Ensure that after hard upgrade (with the old version) we can no longer connect to the database. Get a count of number of rows in SYS.SYSDEPENDS //////////////////////////////////////////////  create all permutations of an array of numbers  ////////////////////////////////////////////// Prepare tables and data for DERBY-5120 and DERBY-5044 Make sure that the rows lost from sysdepends with earlier release are restored when the db is in soft upgrade mode or when it has been hard upgraded to this release. DERBY-5120 Make sure table created in soft upgrade mode can be accessed after shutdown.  DERBY-2931 Verify that recompilation of a stale meta-data query works in soft upgrade. Before DERBY-4753, it used to fail with a syntax error because the recompilation didn't accept internal syntax. DERBY-5044(ALTER TABLE DROP COLUMN will not detect triggers defined on other tables with their trigger action using the column being dropped) ALTER TABLE DROP COLUMN should detect triggers defined on other table but using the table being altered in their trigger action. If the column getting dropped is used in such a trigger, then ALTER TABLE DROP COLUMN .. RESTRICT should fail and ALTER TABLE DROP COLUMN .. CASCADE should drop such triggers. Following test is for checking the upgrade scenario for DERBY-5044 and DERBY-5120. Make sure that the rows lost from sysdepends with earlier release are restored when the db is in soft upgrade mode or when it has been hard upgraded to this release. DERBY-5120 Changes made for DERBY-1482 caused corruption which is being logged under DERBY-5121. The issue is that the generated trigger action sql could be looking for columns (by positions, not names) in incorrect positions. With DERBY-1482, trigger assumed that the runtime resultset that they will get will only have trigger columns and trigger action columns used through the REFERENCING column. That is an incorrect assumption because the resultset could have more columns if the triggering sql requires more columns. DERBY-1482 changes are in 10.7 and higher codelines. Because of this bug, the changes for DERBY-1482 have been backed out from 10.7 and 10.8 codelines so they now match 10.6 and earlier releases. This in other words means that the resultset presented to the trigger will have all the columns from the trigger table and the trigger action generated sql should look for the columns in the trigger table by their absolution column position in the trigger table. This disabling of code will make sure that all the future triggers get created correctly. The existing triggers at the time of upgrade (to the releases with DERBY-1482 backout changes in them) will get marked invalid and when they fire next time around, the regenerated sql for them will be generated again and they will start behaving correctly. So, it is highly recommended that we upgrade 10.7.1.1 to next point release of 10.7 or to 10.8 DERBY-5289 Upgrade could fail during upgrade with triggers due to failure reading serializable or SQLData object Regression test case for DERBY-6314, which caused upgrade to fail if a metadata query had been executed with the old version of the database. Test general DML. Just execute some INSERT/UPDATE/DELETE statements in all phases to see that generally the database works. DERBY-5249 table created with primary and foreign key can't be dropped Test currently disabled. Remove the x from the name to enable the test once the bug is fixed. This test has been contributed by Rick Hillegas for DERBY-5121 The test exhaustively walks through all subsets and permutations of columns for a trigger which inserts into a side table based on updates to a master table. Test table with index can be read after shutdown DERBY-2931 Simple test of the old version from the meta data. Simple test of the triggers. Added for DERBY-4835 Execute the trigger which will fire the triggers. Check the data to make sure that the triggers fired correctly. End of helper methods for testExhuastivePermutationOfTriggerColumns Test that triggers that use XML operators work after upgrade. The first fix for DERBY-3870 broke upgrade of such triggers because the old execution plans failed to deserialize on the new version.
Called prior to inserting a duplicate sort key. Called prior to inserting a distinct sort key.
Clone this UUID. * Methods of UUID Implement value equality. Return my format identifier. Provide a hashCode which is compatible with the equals() method. Read this in Read a long value, msb first, from its character representation in the string reader, using '-' or end of string to delimit. Private workhorse of the string making routines. Produce a string representation of this UUID which is suitable for use as a unique ANSI identifier. Produce a string representation of this UUID which can be passed to UUIDFactory.recreateUUID later on to reconstruct it.  The funny representation is designed to (sort of) match the format of Microsoft's UUIDGEN utility. Write this out.
* Methods of UUID Generate a new UUID. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Recreate a UUID previously generated UUID value.


Loggable methods Apply the change indicated by this operation and optional data. BeginXact method the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. BeginXact has no optional data to write out Return my format identifier. BeginXact is both a FIRST and a RAWSTORE log record Always redo a BeginXact. BeginXact has no resource to release DEBUG: Print self.
Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. Do code generation for this BETWEEN operator. Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed.
This method checks that the SQL type can be converted to Decimal This method is a wrapper for the CallableStatement method getBigDecimal(int parameterIndex). The wrapper method needs the parameterType as an input since ParameterMetaData is not available in JSR169. This method is a wrapper for the ResultSet method getBigDecimal(int columnIndex). This method is a wrapper for ResultSet method getBigDecimal(String columnName). This method is a wrapper for ResultSet method getObject(int columnIndex) This method is a wrapper for ResultSet method getObject(String columnName) This method is a wrapper for the PreparedStatement method setBigDecimal(int parameterIndex,BigDecimal x) This method is a wrapper for the PreparedStatement method setObject(int parameterIndex, Object x) This method is a wrapper for ResultSet method updateBigDecimal(int columnIndex, BigDecimal x) This method is a wrapper for ResultSet method updateBigDecimal(String columnName, BigDecimal x)
Bind this operator
Test the type compatability of the operands and set the type info for this node.  This method is useful both during binding and when we generate nodes within the language module outside of the parser. Bind this comparison operator.  All that has to be done for binding a comparison operator is to bind the operands, check the compatibility of the types, and set the result type to SQLBoolean. Finish putting an expression into conjunctive normal form.  An expression tree in conjunctive normal form meets the following criteria: o  If the expression tree is not null, the top level will be a chain of AndNodes terminating in a true BooleanConstantNode. o  The left child of an AndNode will never be an AndNode. o  Any right-linked chain that includes an AndNode will be entirely composed of AndNodes terminated by a true BooleanConstantNode. o  The left child of an OrNode will never be an OrNode. o  Any right-linked chain that includes an OrNode will be entirely composed of OrNodes terminated by a false BooleanConstantNode. o  ValueNodes other than AndNodes and OrNodes are considered leaf nodes for purposes of expression normalization. In other words, we won't do any normalization under those nodes. In addition, we track whether or not we are under a top level AndNode. SubqueryNodes need to know this for subquery flattening. Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree.  Return whether or not to use the between selectivity for this node. Was this node generated in a query rewrite? Negate the comparison. <p> Return a node equivalent to this node, but with the left and right operands swapped. The node type may also be changed if the operator is not symmetric. </p> <p> This method may for instance be used to normalize a predicate by moving constants to the right-hand side of the comparison. Example: {@code 1 = A} will be transformed to {@code A = 1}, and {@code 10 < B} will be transformed to {@code B > 10}. </p> Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Use between selectivity when calculating the selectivity. This node was generated as part of a query rewrite. Bypass the normal comparability checks.
Accept the visitor for all visitable children of this node. Test the type compatability of the operands and set the type info for this node.  This method is useful both during binding and when we generate nodes within the language module outside of the parser. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views.  Get the leftOperand Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query CONSTANT			- immutable Get the rightOperandList Return whether or not this expression tree represents a constant expression.  Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set the leftOperand to the specified ValueNode Set the rightOperandList to the specified ValueNodeList Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Bind this logical operator.  All that has to be done for binding a logical operator is to bind the operands, check that both operands are BooleanDataValue, and set the result type to BooleanDataValue. Do code generation for this logical binary operator. This is used for AND and OR. the IsNode extends this class but overrides generateExpression. Verify that eliminateNots() did its job correctly.  Verify that there are no NotNodes above the top level comparison operators and boolean expressions.
Accept the visitor for all visitable children of this node. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Bind an XMLEXISTS or XMLQUERY operator.  Makes sure the operand type and target type are both correct and sets the result type. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views.  generate a SQL-&gt;Java-&gt;SQL conversion tree above the left and right operand of this Binary Operator Node if needed. Subclasses can override the default behavior. Do code generation for this binary operator. Get the leftOperand Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query CONSTANT			- immutable Determine the type the binary method is called on. By default, based on the receiver. Override in nodes that use methods on super-interfaces of the receiver's interface, such as comparisons. Get the rightOperand Return whether or not this expression tree represents a constant expression.  Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set the leftOperand to the specified ValueNode Set the interface type for the left and right arguments. Used when we don't know the interface type until later in binding. Set the methodName. Set the operator. Set the rightOperand to the specified ValueNode Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Compare this Orderable with a given Orderable for the purpose of qualification and sorting.  The caller gets to determine how nulls should be treated - they can either be ordered values or unknown values. Compare this Orderable with a given Orderable for the purpose of index positioning.  This method treats nulls as ordered values - that is, it treats SQL null as equal to null and less than all other values.
Public Methods of This class: Short one line description of routine. <p> Longer descrption of routine. <p> Public Methods of Storable interface - Externalizable, TypedFormat: Private/Protected methods of This class: Short one line description of routine. <p> Longer descrption of routine. <p> Return whether the value is null or not. The containerid being zero is what determines nullness;  subclasses are not expected to override this method. Restore the in-memory representation from the stream. Restore the in-memory representation to the null value. The containerid being zero is what determines nullness;  subclasses are not expected to override this method. Store the stored representation of the column value in the stream. <p> A BinaryOrderableWrapper is never used to store data out, only to read data from disk and compare it to another byte stream.
Return 50% if this is a comparison with a boolean column, a negative selectivity otherwise. Create a set of table numbers to search when trying to find which (if either) of this operator's operands reference the received target table.  At the minimum this set should contain the target table's own table number.  After that, if we're _not_ attempting to push this operator (or more specifically, the predicate to which this operator belongs) to the target table, we go on to search the subtree beneath the target table and add any base table numbers to the searchable list. Determine whether there is a column from the given table on one side of this operator, and if so, which side is it on?  See if the node always evaluates to true or false, and return a Boolean constant node if it does. @see BinaryOperatorNode#genSQLJavaSQLTree @see BinaryComparisonOperatorNode#genSQLJavaSQLTree Determine whether this comparison operator is a useful stop key with knowledge of whether the key column is on the left or right.        Get the absolute 0-based column position of the ColumnReference from the conglomerate for this Optimizable.     If this rel op was created for an IN-list probe predicate then return the underlying InListOperatorNode.  Will return null if this rel op is a "legitimate" relational operator (as opposed to a disguised IN-list).  With the exception of nullability checking via the isInListProbeNode() method, all access to this.inListProbeSource MUST come through this method, as this method ensures that the left operand of the inListProbeSource is set correctly before returning it. Return the node type that must be used in order to construct an equivalent expression if the operands are swapped. For symmetric operators ({@code =} and {@code <>}), the same node type is returned. Otherwise, the direction of the operator is switched in order to preserve the meaning (for instance, a node representing less-than will return the node type for greater-than). Returns the negation of this operator; negation of Equals is NotEquals. map current node to its negation    The methods generated for this node all are on Orderable. Overrides this method in BooleanOperatorNode for code generation purposes. Take a ResultSetNode and return a column reference that is scoped for for the received ResultSetNode, where "scoped" means that the column reference points to a specific column in the RSN.  This is used for remapping predicates from an outer query down to a subquery. For example, assume we have the following query: select * from (select i,j from t1 union select i,j from t2) X1, (select a,b from t3 union select a,b from t4) X2 where X1.j = X2.b; Then assume that this BinaryRelationalOperatorNode represents the "X1.j = X2.b" predicate and that the childRSN we received as a parameter represents one of the subqueries to which we want to push the predicate; let's say it's: select i,j from t1 Then what we want to do in this method is map one of the operands X1.j or X2.b (depending on the 'whichSide' parameter) to the childRSN, if possible.  Note that in our example, "X2.b" should _NOT_ be mapped because it doesn't apply to the childRSN for the subquery "select i,j from t1"; thus we should leave it as it is.  "X1.j", however, _does_ need to be scoped, and so this method will return a ColumnReference pointing to "T1.j" (or whatever the corresponding column in T1 is). ASSUMPTION: We should only get to this method if we know that exactly one operand in the predicate to which this operator belongs can and should be mapped to the received childRSN.   Return an equivalent node with the operands swapped, and possibly with the operator type changed in order to preserve the meaning of the expression.  Return whether or not this binary relational predicate requires an implicit (var)char conversion.  This is important when considering hash join since this type of equality predicate is not currently supported for a hash join. Initialize the fields used for retrieving base tables in subtrees, which allows us to do a more extensive search for table references.  If the fields have already been created, then just reset their values.     Return true if a key column for the given table is found on the left side of this operator, false if it is found on the right side of this operator. NOTE: This method assumes that a key column will be found on one side or the other.  If you don't know whether a key column exists, use the columnOnOneSide() method (below). Create a Boolean constant node with a specified value.   return the selectivity of this predicate.  is this is useful start key? for example a predicate of the from <em>column Lessthan 5</em> is not a useful start key but is a useful stop key. However <em>5 Lessthan column </em> is a useful start key.    Determine whether or not the received ValueNode (which will usually be a ColumnReference) references either the received optTable or else a base table in the subtree beneath that optTable.
Return the length of the value in thie stream in bytes. If the value is unknown then -1 is returned.
This generates the proper constant.  It is implemented by every specific constant node (e.g. IntConstantNode). Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null. This is useful for bind time resolution of VTIs. RESOLVE: What do we do for primitives?
The SQL concatenation '||' operator. Stuff a BitDataValue with a Blob.
Tell whether this type (bit) is compatible with the given type. Tell whether this type (bit) can be converted to the given type.    Tell whether this type (bit) can be stored into from the given type.
Clear the bit at the specified position Check to see if the specified bit is set Convert a byte array to a human-readable String for debugging purposes. Set the bit at the specified position
Read the next <code>len</code> bytes of the <code>Blob</code> value from the server.
Write the <code>byte[]</code> to the <code>Blob</code> value on the server; starting from the current position of this stream.

Tries to optimize the block size by setting it equal to the the page size used by the database. <p> Since we don't have a way of knowing which page size will be used, wait to set the block size until the first write request and see how many bytes are written then. Returns an input stream serving the data in the blocked byte array. Returns an output stream writing data into the blocked byte array. Increases the capacity of this blocked byte array by allocating more blocks. @GuardedBy("this") Returns the number of bytes allocated. Returns the byte at the given position. Reads up to {@code len} bytes. Releases this array. Changes the allocated length of the data. <p> If the new length is larger than the current length, the blocked byte array will be extended with new blocks. If the new length is smaller, existing (allocated) blocks will be removed if possible. Writes the given byte into the blocked byte array. Writes the given bytes into the blocked byte array.
Closes the stream. Returns the current position. Reads a single byte. Reads up to {@code len} bytes. Sets the position.
Closes the stream. Returns the current position. Sets the position. Writes the specified bytes into the underlying blocked byte array. Writes the single byte into the underlying blocked byte array.
Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. This generates the proper constant.  It is implemented by every specific constant node (e.g. IntConstantNode). Return the value from this BooleanConstantNode public boolean	getBoolean() { return booleanValue; } Return the length public int	getLength() throws StandardException { return value.getLength(); } Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null. This is useful for bind time resolution of VTIs. RESOLVE: What do we do for primitives? Return the value as a string. Does this represent a false constant. Does this represent a true constant. The default selectivity for value nodes is 50%.  This is overridden in specific cases, such as the RelationalOperators. Set the value in this ConstantNode. end of setValue
The SQL AND operator.  This provides SQL semantics for AND with unknown truth values - consult any standard SQL reference for an explanation. Tell whether a BooleanDataValue has the given value.  This is useful for short-circuiting. Return an immutable BooleanDataValue with the same value as this. The SQL IS operator - consult any standard SQL reference for an explanation. Implements the following truth table: otherValue | TRUE    | FALSE   | UNKNOWN this    |---------------------------- | TRUE    | TRUE    | FALSE   | FALSE FALSE   | FALSE   | TRUE    | FALSE UNKNOWN | FALSE   | FALSE   | TRUE Implements NOT IS. This reverses the sense of the is() call. The SQL OR operator.  This provides SQL semantics for OR with unknown truth values - consult any standard SQL reference for an explanation. * NOTE: The NOT operator is translated to "= FALSE", which does the same * thing. Set the value of this BooleanDataValue. Throw an exception with the given SQLState if this BooleanDataValue is false. This method is useful for evaluating constraints. If this value is false and we have a deferred constraint, remember the violation and proceed, else throw.  See also {@link org.apache.derby.impl.sql.compile.AndNoShortCircuitNode}.
Tell whether this type (boolean) is compatible with the given type. Tell whether this type (boolean) can be converted to the given type.   Get the method name for getting out the corresponding primitive Java type.

Allocate a new leaf page to the conglomerate. * Debug/consistency check Methods of ControlRow: * Perform consistency checks for a branch page.  The checks * specific to a branch page are: * <menu> * <li> The rows on the page are indeed branch rows, and *      they all have the correct number of fields (which *      is the b-tree's key fields plus one for the child *      page number. * <li> The child pages pointed to by the left child pointer *      and the index rows are linked together in the same *      order that they appear on the page. * <li> The child pages themselves are all consistent. * </menu> * This method also performs the consistency checks that * are common to both leaf and branch pages (see * ControlRow.checkGeneric). * * @exception StandardException Standard exception policy. * Non - Debug/consistency check Methods of ControlRow: Perform page specific initialization. <p> * A branch page that has just been allocated as part * of a split has index rows and a left child pointer * that were copied from another page.  The parent * link on the corresponding pages will still point to * the original page.  This method fixes their parent * pointers so that they point to the curren page like * they're supposed to. * <P> * Note that maintaining the parent link is kind of a * pain, and will slow down applications.  It's only * needed for consistency checks, so we may want to * have implementations that don't bother to maintain it. * <P> * This Return the left child pointer for the page. <p> Leaf pages don't have children, so they override this and return null. * Return the left child page number for the page.  Leaf pages * don't have left children, so they override this and return * null. Get the number of columns in the control row. <p> Control rows all share the first columns as defined by this class and then add columns to the end of the control row.  For instance a branch control row add a child page pointer field. <p> Return the right child pointer for the page. <p> Leaf pages don't have children, so they override this and return null. Return a new template for reading a data row from the current page. <p> Default implementation for rows which are the same as the conglomerates template, sub-classes can alter if underlying template is different (for instance branch rows add an extra field at the end). TypedFormat: Return my format identifier. Private methods of BranchControlRow * Add a level to the tree by moving the current branch-root page up * one level and adding a new page as it's left child.  On exit the * current root page remains the root of the tree. * <P> * On entry, the current branch root page is expected to be latched. * On exit, all latches will have been released. * <P> * Latch order: *    o ROOT: on entry current root is latched. *            No other latches should be held. *    o ROOT_OLDCHILD: Get and Latch root's left child page. *    o ROOT_NEWCHILD: Allocate a new branch page with latch. *    o Conditionally fix up the parent links on the pages pointed at *      by the newly allocated page.  Loop through slots on ROOT_NEWCHILD, *      from left to right getting and releasing latches.  Note that *      fixChildrensParents() must not latch the leftchild as ROOT_OLDCHILD *      is already latched. *    RESOLVE: (mikem) does order of release matter. *    o ROOT         : released. *    o ROOT_NEWCHILD: released. *    o ROOT_OLDCHILD: released. Is the current page the leftmost leaf of tree? <p> Is the current page the rightmost leaf of tree? <p> Recursively print the tree starting at current node in tree. * Perform a recursive search, ultimately returning the latched * leaf page and row slot after which the given key belongs. * The slot is returned in the result structure.  If the key * exists on the page, the result.exact will be true.  Otherwise, * result.exact will be false, and the row slot returned will be * the one immediately preceding the position at which the key * belongs. * * @exception StandardException Standard exception policy. Search and return the left most leaf page. <p> Perform a recursive search, ultimately returning the leftmost leaf page which is the first leaf page in the leaf sibling chain.  (This method might better be called getFirstLeafPage()). Search and return the right most leaf page. <p> Perform a recursive search, ultimately returning the rightmost leaf page which is the last leaf page in the leaf sibling chain.  (This method might better be called getLastLeafPage()). *	Perform a recursive shrink operation for the key. * If this method returns true, the caller should * remove the corresponding entry for the page. * This routine is not guaranteed to successfully * shrink anything.  The page lead to by the key might * turn out not to be empty by the time shrink gets * there, and shrinks will give up if there is a deadlock. * <P> * The receiver page must be latched on entry and is * returned latched. * * @exception StandardException Standard exception policy. Perform a top down split pass making room for the the key in "row". <p> Perform a split such that a subsequent call to insert given the argument index row will likely find room for it.  Since latches are released the client must code for the case where another user has grabbed the space made available by the split pass and be ready to do another split. <p> Latches: o PARENT    : is latched on entry (unless the split is the root then there is no parent. o THISBRANCH: the current page is latched on entry. o CHILD     : latch the child page which will be pointed at by the left child pointer of the new page. RESOLVE (mikem) -see comments below o NEWPAGE   : Allocate and latch new page. o CHILD     : release. (RESOLVE) o fixparents: latch pages and reset their parent pointers. Conditionally fix up the parent links on the pages pointed at by the newly allocated page.  First get latch and release on the left child page and then loop through slots on NEWPAGE, from left to right getting and releasing latches. * The standard toString.
Create a new branch row, given a old branch row and a new child page. Used by BranchControlRow to manufacture new branch rows when splitting or growing the tree. There is no way to "copy" values of a template row, so this class just stores a reference to each of the columns of the Indexable row passed in.  This is ok as all usages of this class when instantiated this way, have an old branch row from which they are creating a new branch row with the same key values, and a different child page number. WARNING - this branch row is only valid while the old branch row is valid, as it contains references to the columns of the old branch row. So use of the row should only provide read-only access to the objects of the old branch row which are referenced. Create a new branch row, given a old leaf row and a new child page. Used by LeafControlRow to manufacture new branch rows when splitting or growing the tree. There is no way to "copy" values of a template row, so this class just stores a referece to the Indexable row passed in.  This is ok as all usages of this class when instantiated this way, have an old leaf row from which they are creating a new branch row with the same key values, and a different child page number. WARNING - this branch row is only valid while the old leaf row is valid, as it contains references to the columns of the old leaf row. So use of the row should only provide read-only access to the objects of the old leaf row which are referenced. * The following methods implement the BranchRow Public interface. Create an empty branch row template suitable for reading branch rows in from disk. This routine will create newly allocated "empty" objects for every column in the template row. * The following methods implement the BranchRow Private interface. Accessor for the child page field of the branch row. Return the branch row. <p> Return the DataValueDescriptor array that represents the branch row, for use in raw store calls to fetch, insert, and update. <p> Set the page number field of the branch row to a new value.
Create a duplicate CalableStatement to this, including state, from the passed in Connection.  * Control methods Access the underlying CallableStatement. This method is package protected to restrict access to the underlying object to the brokered objects. Allowing the application to access the underlying object thtough a public method would JDBC 4.0 methods //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Access the underlying PreparedStatement. This method is package protected to restrict access to the underlying object to the brokered objects. Allowing the application to access the underlying object thtough a public method would JDBC 2.0 methods JDBC 3.0 methods

Add a SQLWarning to this Connection object. JDBC 4.0 methods Constructs an object that implements the {@code Blob} interface. The object returned initially contains no data. The {@code setBinaryStream} and {@code setBytes} methods of the {@code Blob} interface may be used to add data to the {@code Blob}. Constructs an object that implements the {@code Clob} interface. The object returned initially contains no data. The {@code setAsciiStream}, {@code setCharacterStream} and {@code setString} methods of the {@code Clob} interface may be used to add data to the {@code Clob}. JDBC 3.0 methods {@code getClientInfo} forwards to the real connection. {@code getClientInfo} forwards to the real connection. Obtain the name of the current schema. Not part of the java.sql.Connection interface, but is accessible through the EngineConnection interface, so that the NetworkServer can get at the current schema for piggy-backing Get the holdability for statements created by this connection when holdability is not passed in. Isolation level state in BrokeredConnection can get out of sync if the isolation is set using SQL rather than JDBC. In order to ensure correct state level information, this method is called at the start and end of a global transaction. Get the LOB reference corresponding to the locator. get the isolation level that is currently being used to prepare statements (used for network server) A little indirection for getting the real connection. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Get the name of the current schema.  Checks if the connection has not been closed and is still valid. The validity is checked by running a simple query against the database. Returns false unless {@code iface} is implemented. ///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// Generate an exception reporting that there is no current connection. JDBC 3.0 methods. Prepare statement with explicit holdability.  JDBC 2.0 methods {@code setClientInfo} forwards to the real connection. {@code setClientInfo} forwards to the real connection. If the call to {@code getRealConnection} fails the resulting {@code SQLException} is wrapped in a {@code SQLClientInfoException} to satisfy the specified signature. set the DrdaId for this connection. The drdaID prints with the statement text to the errror log Set the internal isolation level to use for preparing statements. Subsequent prepares will use this isoalation level Set the default schema for the Connection. Set the state of the underlying connection according to the state of this connection's view of state. * Methods private to the class. Check the result set holdability when creating a statement object. Section 16.1.3.1 of JDBC 4.0 (proposed final draft) says the driver may change the holdabilty and add a SQLWarning to the Connection object. This work-in-progress implementation throws an exception to match the old behaviour just as part of incremental development. Sync up the state of the underlying connection with the state of this new handle. Get the string representation for this connection.  Return the class name/hash code and various debug information. Returns {@code this} if this class implements the interface.

Allow control over setting auto commit mode. Check if the brokered connection can be closed. Allow control over calling commit. Can cursors be held across commits. Allow control over calling rollback. Allow control over creating a Savepoint (JDBC 3.0) Close called on BrokeredConnection. If this call returns true then getRealConnection().close() will be called. Return the real JDBC connection for the brokered connection. Is this a global transaction Returns true if isolation level has been set using JDBC/SQL. Notify the control class that a SQLException was thrown during a call on one of the brokered connection's methods. Close called on the associated PreparedStatement object Error occurred on associated PreparedStatement object Reset the isolation level flag used to keep state in BrokeredConnection. It will get set to true when isolation level is set using JDBC/SQL. It will get reset to false at the start and the end of a global transaction. Optionally wrap a CallableStatement with an CallableStatement. Optionally wrap a PreparedStatement with another PreparedStatement. Optionally wrap a Statement with another Statement.
JDBC 2.0 Add a set of parameters to the batch. <P>In general, parameter values remain in force for repeated use of a Statement. Setting a parameter value automatically clears its previous value.  However, in some cases it is useful to immediately release the resources used by the current parameter values; this can be done by calling clearParameters. Create a duplicate PreparedStatement to this, including state, from the passed in Connection.  JDBC 4.1 methods exposed via the EnginePreparedStatement interface A prepared SQL query is executed and its ResultSet is returned. Execute a SQL INSERT, UPDATE or DELETE statement. In addition, SQL statements that return nothing such as SQL DDL statements can be executed. JDBC 2.0 The number, types and properties of a ResultSet's columns are provided by the getMetaData method. * Control methods. Access the underlying PreparedStatement. This method is package protected to restrict access to the underlying object to the brokered objects. Allowing the application to access the underlying object thtough a public method would Override the BrokeredStatement's getStatement() to always return a PreparedStatement. Sets the designated parameter to the given input stream. We do this inefficiently and read it all in here. The target type is assumed to be a String. Sets the designated parameter to the given input stream, which will have the specified number of bytes. Set a parameter to a java.math.BigDecimal value. The driver converts this to a SQL NUMERIC value when it sends it to the database.  Sets the designated parameter to the given input stream, which will have the specified number of bytes. Sets the designated parameter to a {@code InputStream} object. This method differs from the {@code setBinaryStream(int, InputStream)} method because it informs the driver that the parameter value should be sent to the server as a {@code BLOB}. Set a parameter to a Java boolean value.  According to the JDBC API spec, the driver converts this to a SQL BIT value when it sends it to the database. But we don't have to do this, since the database engine supports a boolean type. Set a parameter to a Java byte value.  The driver converts this to a SQL TINYINT value when it sends it to the database. Set a parameter to a Java array of bytes.  The driver converts this to a SQL VARBINARY or LONGVARBINARY (depending on the argument's size relative to the driver's limits on VARBINARYs) when it sends it to the database. Sets the designated parameter to the given Reader, which will have the specified number of bytes. Sets the designated parameter to a {@code Reader} object. This method differs from the {@code setCharacterStream(int,Reader)} method because it informs the driver that the parameter value should be sent to the server as a {@code CLOB}. Set a parameter to a java.sql.Date value.  The driver converts this to a SQL DATE value when it sends it to the database. Set a parameter to a Java double value.  The driver converts this to a SQL DOUBLE value when it sends it to the database. Set a parameter to a Java float value.  The driver converts this to a SQL FLOAT value when it sends it to the database. Set a parameter to a Java int value.  The driver converts this to a SQL INTEGER value when it sends it to the database. Set a parameter to a Java long value.  The driver converts this to a SQL BIGINT value when it sends it to the database. Set a parameter to SQL NULL. <P><B>Note:</B> You must specify the parameter's SQL type. Set a parameter to SQL NULL. <P><B>Note:</B> You must specify the parameter's SQL type. <p>Set the value of a parameter using an object; use the java.lang equivalent objects for integral values. <p>The JDBC specification specifies a standard mapping from Java Object types to SQL types.  The given argument java object will be converted to the corresponding SQL type before being sent to the database. <p>Note that this method may be used to pass datatabase specific abstract data types, by using a Driver specific Java type. This method is like setObject above, but assumes a scale of zero. The interface says that the type of the Object parameter must be compatible with the type of the targetSqlType. We check that, and if it flies, we expect the underlying engine to do the required conversion once we pass in the value using its type. So, an Integer converting to a CHAR is done via setInteger() support on the underlying CHAR type. <p>If x is null, it won't tell us its type, so we pass it on to setNull JDBC 4.0 methods Set a parameter to a Java short value.  The driver converts this to a SQL SMALLINT value when it sends it to the database. Set a parameter to a Java String value.  The driver converts this to a SQL VARCHAR or LONGVARCHAR value (depending on the arguments size relative to the driver's limits on VARCHARs) when it sends it to the database. Set a parameter to a java.sql.Time value.  The driver converts this to a SQL TIME value when it sends it to the database. Set a parameter to a java.sql.Timestamp value.  The driver converts this to a SQL TIMESTAMP value when it sends it to the database. JDBC 3.0 methods We do this inefficiently and read it all in here. The target type is assumed to be a String. The unicode source is assumed to be in char[].  RESOLVE: might it be in UTF, instead? that'd be faster!

Checks if the statement is closed and throws an exception if it is. After this call getWarnings returns null until a new warning is reported for this Statement. In many cases, it is desirable to immediately release a Statements's database and JDBC resources instead of waiting for this to happen when it is automatically closed; the close method provides this immediate release. <P><B>Note:</B> A Statement is automatically closed when it is garbage collected. When a Statement is closed its current ResultSet, if one exists, is also closed. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Get the BrokeredStatementControl in order to perform a check. Obtained indirectly to ensure that the correct exception is thrown if the Statement has been closed. * Control methods * JDBC 3.0 methods //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.2 IN JAVA 8  ////////////////////////////////////////////////////////////////// JDBC 3.0 Retrieves any auto-generated keys created as a result of executing this Statement object. If this Statement object did not generate any keys, an empty ResultSet object is returned. If this Statement is a non-insert statement, an exception will be thrown. getMoreResults moves to a Statement's next result.  It returns true if this result is a ResultSet.  getMoreResults also implicitly closes any current ResultSet obtained with getResultSet. There are no more results when (!getMoreResults() &amp;&amp; (getUpdateCount() == -1) JDBC 3.0 Moves to this Statement obect's next result, deals with any current ResultSet object(s) according to the instructions specified by the given flag, and returns true if the next result is a ResultSet object getResultSet returns the current result as a ResultSet.  It should only be called once per result. Return the holdability of ResultSets created by this Statement. If this Statement is active in a global transaction the CLOSE_CURSORS_ON_COMMIT will be returned regardless of the holdability it was created with. In a local transaction the original create holdabilty will be returned. JDBC 2.0 Determine the result set type. getUpdateCount returns the current result as an update count; if the result is a ResultSet or there are no more results -1 is returned.  It should only be called once per result. <P>The only way to tell for sure that the result is an update count is to first test to see if it is a ResultSet. If it is not a ResultSet it is either an update count or there are no more results. The first warning reported by calls on this Statement is returned.  A Statment's execute methods clear its SQLWarning chain. Subsequent Statement warnings will be chained to this SQLWarning. <p>The warning chain is automatically cleared each time a statement is (re)executed. <P><B>Note:</B> If you are processing a ResultSet then any warnings associated with ResultSet reads will be chained on the ResultSet object. Checks if the statement is closed. Forwards to the real Statement. JDBC 4.0 java.sql.Wrapper interface methods Returns false unless <code>iface</code> is implemented setCursorName defines the SQL cursor name that will be used by subsequent Statement execute methods. This name can then be used in SQL positioned update/delete statements to identify the current row in the ResultSet generated by this getStatement().  If the database doesn't support positioned update/delete, this method is a noop. <P><B>Note:</B> By definition, positioned update/delete execution must be done by a different Statement than the one which generated the ResultSet being used for positioning. Also, cursor names must be unique within a Connection. If escape scanning is on (the default) the driver will do escape substitution before sending the SQL to the database. JDBC 2.0 Give a hint as to the direction in which the rows in a result set will be processed. The hint applies only to result sets created using this Statement object.  The default value is ResultSet.FETCH_FORWARD. JDBC 2.0 Give the JDBC driver a hint as to the number of rows that should be fetched from the database when more rows are needed.  The number of rows specified only affects result sets created using this getStatement(). If the value specified is zero, then the hint is ignored. The default value is zero. The maxFieldSize limit (in bytes) is set to limit the size of data that can be returned for any column value; it only applies to BINARY, VARBINARY, LONGVARBINARY, CHAR, VARCHAR, and LONGVARCHAR fields.  If the limit is exceeded, the excess data is silently discarded. The maxRows limit is set to limit the number of rows that any ResultSet can contain.  If the limit is exceeded, the excess rows are silently dropped. Forwards to the real Statement. Return an exception that reports that an unwrap operation has failed because the object couldn't be cast to the specified interface. Returns {@code this} if this class implements the specified interface. Provide the control access to every ResultSet we return. If required the control can wrap the ResultSet, but it (the control) must ensure a underlying ResultSet is only wrapped once, if say java.sql.Statement.getResultSet is returned twice.
Can cursors be held across commits. Returns the holdability that should be used which may be different from the passed in holdabilty. Close the real JDBC CallableStatement when this is controlling a CallableStatement. Close the real JDBC CallableStatement when this is controlling a PreparedStatement. Close the real JDBC Statement when this is controlling a Statement. Return the real JDBC CallableStatement for the brokered statement when this is controlling a CallableStatement. Return the real JDBC PreparedStatement for the brokered statement when this is controlling a PreparedStatement. Return the real JDBC statement for the brokered statement when this is controlling a Statement. Optionally wrap a returned ResultSet in another ResultSet.


<p> Get the next sequence number for bulk-insert. </p> <p> Get the current value of the sequence generator without advancing it. May return null if the generator is exhausted. </p>
Adjust the bulk fetch size according to the parameters. Bulk fetch may be disabled by returning 1 from this method. Disabling of bulk fetch may happen if the cursor is holdable and it contains LOB columns (DERBY-1511) because Can we get instantaneous locks when getting share row locks at READ COMMITTED. If the result set has been opened, close the open scan.  Delegate most of the work to TableScanResultSet. Return the next row (if any) from the scan (if open). Reload the rowArray as necessary. Get a blank row by cloning the candidate row and lopping off the trailing RowLocation column for scans done on behalf of MERGE statements. Open up the result set.  Delegate most work to TableScanResultSet.openCore(). Create a new array with <rowsPerRead> rows for use in fetchNextGroup(). Open the scan controller * Load up rowArray with a batch of * rows. Reopen the result set.  Delegate most work to TableScanResultSet.reopenCore(). Reuse the array of rows.
Returns entries in this bundle and its attached fragments. This bundle's classloader is not used to search for entries. Only the contents of this bundle and its attached fragments are searched for the specified entries. If this bundle's state is <code>INSTALLED</code>, this method must attempt to resolve this bundle before attempting to find entries. <p> This method is intended to be used to obtain configuration, setup, localization and other information from this bundle. This method takes into account that the &quot;contents&quot; of this bundle can be extended with fragments. This &quot;bundle space&quot; is not a namespace with unique members; the same entry name can be present multiple times. This method therefore returns an enumeration of URL objects. These URLs can come from different JARs but have the same path name. This method can either return only entries in the specified path or recurse into subdirectories returning entries in the directory tree beginning at the specified path. Fragments can be attached after this bundle is resolved, possibly changing the set of URLs returned by this method. If this bundle is not resolved, only the entries in the JAR file of this bundle are returned. <p> Examples: <pre> // List all XML files in the OSGI-INF directory and below Enumeration e = b.findEntries(&quot;OSGI-INF&quot;, &quot;*.xml&quot;, true); // Find a specific localization file Enumeration e = b.findEntries(&quot;OSGI-INF/l10n&quot;, &quot;bundle_nl_DU.properties&quot;, false); if (e.hasMoreElements()) return (URL) e.nextElement(); </pre> Returns this bundle's {@link BundleContext}. The returned <code>BundleContext</code> can be used by the caller to act on behalf of this bundle. <p> If this bundle is not in the {@link #STARTING}, {@link #ACTIVE}, or {@link #STOPPING} states or this bundle is a fragment bundle, then this bundle has no valid <code>BundleContext</code>. This method will return <code>null</code> if this bundle has no valid <code>BundleContext</code>. Returns this bundle's unique identifier. This bundle is assigned a unique identifier by the Framework when it was installed in the OSGi environment. <p> A bundle's unique identifier has the following attributes: <ul> <li>Is unique and persistent. <li>Is a <code>long</code>. <li>Its value is not reused for another bundle, even after a bundle is uninstalled. <li>Does not change while a bundle remains installed. <li>Does not change when a bundle is updated. </ul> <p> This method must continue to return this bundle's unique identifier while this bundle is in the <code>UNINSTALLED</code> state. Returns a URL to the entry at the specified path in this bundle. This bundle's classloader is not used to search for the entry. Only the contents of this bundle are searched for the entry. <p> The specified path is always relative to the root of this bundle and may begin with &quot;/&quot;. A path value of &quot;/&quot; indicates the root of this bundle. Returns an Enumeration of all the paths (<code>String</code> objects) to entries within this bundle whose longest sub-path matches the specified path. This bundle's classloader is not used to search for entries. Only the contents of this bundle are searched. <p> The specified path is always relative to the root of this bundle and may begin with a &quot;/&quot;. A path value of &quot;/&quot; indicates the root of this bundle. <p> Returned paths indicating subdirectory paths end with a &quot;/&quot;. The returned paths are all relative to the root of this bundle and must not begin with &quot;/&quot;. Returns this bundle's Manifest headers and values. This method returns all the Manifest headers and values from the main section of this bundle's Manifest file; that is, all lines prior to the first blank line. <p> Manifest header names are case-insensitive. The methods of the returned <code>Dictionary</code> object must operate on header names in a case-insensitive manner. If a Manifest header value starts with &quot;%&quot;, it must be localized according to the default locale. <p> For example, the following Manifest headers and values are included if they are present in the Manifest file: <pre> Bundle-Name Bundle-Vendor Bundle-Version Bundle-Description Bundle-DocURL Bundle-ContactAddress </pre> <p> This method must continue to return Manifest header information while this bundle is in the <code>UNINSTALLED</code> state. Returns this bundle's Manifest headers and values localized to the specified locale. <p> This method performs the same function as <code>Bundle.getHeaders()</code> except the manifest header values are localized to the specified locale. <p> If a Manifest header value starts with &quot;%&quot;, it must be localized according to the specified locale. If a locale is specified and cannot be found, then the header values must be returned using the default locale. Localizations are searched for in the following order: <pre> bn + "_" + Ls + "_" + Cs + "_" + Vs bn + "_" + Ls + "_" + Cs bn + "_" + Ls bn + "_" + Ld + "_" + Cd + "_" + Vd bn + "_" + Ld + "_" + Cd bn + "_" + Ld bn </pre> Where <code>bn</code> is this bundle's localization basename, <code>Ls</code>, <code>Cs</code> and <code>Vs</code> are the specified locale (language, country, variant) and <code>Ld</code>, <code>Cd</code> and <code>Vd</code> are the default locale (language, country, variant). If <code>null</code> is specified as the locale string, the header values must be localized using the default locale. If the empty string (&quot;&quot;) is specified as the locale string, the header values must not be localized and the raw (unlocalized) header values, including any leading &quot;%&quot;, must be returned. <p> This method must continue to return Manifest header information while this bundle is in the <code>UNINSTALLED</code> state, however the header values must only be available in the raw and default locale values. Returns the time when this bundle was last modified. A bundle is considered to be modified when it is installed, updated or uninstalled. <p> The time value is the number of milliseconds since January 1, 1970, 00:00:00 GMT. Returns this bundle's location identifier. <p> The location identifier is the location passed to <code>BundleContext.installBundle</code> when a bundle is installed. The location identifier does not change while this bundle remains installed, even if this bundle is updated. <p> This method must continue to return this bundle's location identifier while this bundle is in the <code>UNINSTALLED</code> state. Returns this bundle's <code>ServiceReference</code> list for all services it has registered or <code>null</code> if this bundle has no registered services. <p> If the Java runtime supports permissions, a <code>ServiceReference</code> object to a service is included in the returned list only if the caller has the <code>ServicePermission</code> to get the service using at least one of the named classes the service was registered under. <p> The list is valid at the time of the call to this method, however, as the Framework is a very dynamic environment, services can be modified or unregistered at anytime. Find the specified resource from this bundle. This bundle's class loader is called to search for the specified resource. If this bundle's state is <code>INSTALLED</code>, this method must attempt to resolve this bundle before attempting to get the specified resource. If this bundle cannot be resolved, then only this bundle must be searched for the specified resource. Imported packages cannot be searched when this bundle has not been resolved. If this bundle is a fragment bundle then <code>null</code> is returned. Find the specified resources from this bundle. This bundle's class loader is called to search for the specified resources. If this bundle's state is <code>INSTALLED</code>, this method must attempt to resolve this bundle before attempting to get the specified resources. If this bundle cannot be resolved, then only this bundle must be searched for the specified resources. Imported packages cannot be searched when a bundle has not been resolved. If this bundle is a fragment bundle then <code>null</code> is returned. Returns this bundle's <code>ServiceReference</code> list for all services it is using or returns <code>null</code> if this bundle is not using any services. A bundle is considered to be using a service if its use count for that service is greater than zero. <p> If the Java Runtime Environment supports permissions, a <code>ServiceReference</code> object to a service is included in the returned list only if the caller has the <code>ServicePermission</code> to get the service using at least one of the named classes the service was registered under. <p> The list is valid at the time of the call to this method, however, as the Framework is a very dynamic environment, services can be modified or unregistered at anytime. Returns this bundle's current state. <p> A bundle can be in only one state at any time. Returns the symbolic name of this bundle as specified by its <code>Bundle-SymbolicName</code> manifest header. The name must be unique, it is recommended to use a reverse domain name naming convention like that used for java packages. If this bundle does not have a specified symbolic name then <code>null</code> is returned. <p> This method must continue to return this bundle's symbolic name while this bundle is in the <code>UNINSTALLED</code> state. Determines if this bundle has the specified permissions. <p> If the Java Runtime Environment does not support permissions, this method always returns <code>true</code>. <p> <code>permission</code> is of type <code>Object</code> to avoid referencing the <code>java.security.Permission</code> class directly. This is to allow the Framework to be implemented in Java environments which do not support permissions. <p> If the Java Runtime Environment does support permissions, this bundle and all its resources including embedded JAR files, belong to the same <code>java.security.ProtectionDomain</code>; that is, they must share the same set of permissions. Loads the specified class using this bundle's classloader. <p> If this bundle is a fragment bundle then this method must throw a <code>ClassNotFoundException</code>. <p> If this bundle's state is <code>INSTALLED</code>, this method must attempt to resolve this bundle before attempting to load the class. <p> If this bundle cannot be resolved, a Framework event of type {@link FrameworkEvent#ERROR} is fired containing a <code>BundleException</code> with details of the reason this bundle could not be resolved. This method must then throw a <code>ClassNotFoundException</code>. <p> If this bundle's state is <code>UNINSTALLED</code>, then an <code>IllegalStateException</code> is thrown. Starts this bundle with no options. <p> This method calls <code>start(0)</code>. Starts this bundle. <p> If this bundle's state is <code>UNINSTALLED</code> then an <code>IllegalStateException</code> is thrown. <p> If the Framework implements the optional Start Level service and the current start level is less than this bundle's start level: <ul> <li>If the {@link #START_TRANSIENT} option is set, then a <code>BundleException</code> is thrown indicating this bundle cannot be started due to the Framework's current start level. <li>Otherwise, the Framework must set this bundle's persistent autostart setting to <em>Started with declared activation</em> if the {@link #START_ACTIVATION_POLICY} option is set or <em>Started with eager activation</em> if not set. </ul> <p> When the Framework's current start level becomes equal to or more than this bundle's start level, this bundle will be started. <p> Otherwise, the following steps are required to start this bundle: <ol> <li>If this bundle is in the process of being activated or deactivated then this method must wait for activation or deactivation to complete before continuing. If this does not occur in a reasonable time, a <code>BundleException</code> is thrown to indicate this bundle was unable to be started. <li>If this bundle's state is <code>ACTIVE</code> then this method returns immediately. <li>If the {@link #START_TRANSIENT} option is not set then set this bundle's autostart setting to <em>Started with declared activation</em> if the {@link #START_ACTIVATION_POLICY} option is set or <em>Started with eager activation</em> if not set. When the Framework is restarted and this bundle's autostart setting is not <em>Stopped</em>, this bundle must be automatically started. <li>If this bundle's state is not <code>RESOLVED</code>, an attempt is made to resolve this bundle. If the Framework cannot resolve this bundle, a <code>BundleException</code> is thrown. <li>If the {@link #START_ACTIVATION_POLICY} option is set and this bundle's declared activation policy is {@link Constants#ACTIVATION_LAZY lazy} then: <ul> <li>If this bundle's state is <code>STARTING</code> then this method returns immediately. <li>This bundle's state is set to <code>STARTING</code>. <li>A bundle event of type {@link BundleEvent#LAZY_ACTIVATION} is fired. <li>This method returns immediately and the remaining steps will be followed when this bundle's activation is later triggered. </ul> <i></i> <li>This bundle's state is set to <code>STARTING</code>. <li>A bundle event of type {@link BundleEvent#STARTING} is fired. <li>The {@link BundleActivator#start} method of this bundle's <code>BundleActivator</code>, if one is specified, is called. If the <code>BundleActivator</code> is invalid or throws an exception then: <ul> <li>This bundle's state is set to <code>STOPPING</code>. <li>A bundle event of type {@link BundleEvent#STOPPING} is fired. <li>Any services registered by this bundle must be unregistered. <li>Any services used by this bundle must be released. <li>Any listeners registered by this bundle must be removed. <li>This bundle's state is set to <code>RESOLVED</code>. <li>A bundle event of type {@link BundleEvent#STOPPED} is fired. <li>A <code>BundleException</code> is then thrown. </ul> <i></i> <li>If this bundle's state is <code>UNINSTALLED</code>, because this bundle was uninstalled while the <code>BundleActivator.start</code> method was running, a <code>BundleException</code> is thrown. <li>This bundle's state is set to <code>ACTIVE</code>. <li>A bundle event of type {@link BundleEvent#STARTED} is fired. </ol> <b>Preconditions </b> <ul> <li><code>getState()</code> in {<code>INSTALLED</code>, <code>RESOLVED</code>} or {<code>INSTALLED</code>, <code>RESOLVED</code>, <code>STARTING</code>} if this bundle has a lazy activation policy. </ul> <b>Postconditions, no exceptions thrown </b> <ul> <li>Bundle autostart setting is modified unless the {@link #START_TRANSIENT} option was set. <li><code>getState()</code> in {<code>ACTIVE</code>} unless the lazy activation policy was used. <li><code>BundleActivator.start()</code> has been called and did not throw an exception unless the lazy activation policy was used. </ul> <b>Postconditions, when an exception is thrown </b> <ul> <li>Depending on when the exception occurred, bundle autostart setting is modified unless the {@link #START_TRANSIENT} option was set. <li><code>getState()</code> not in {<code>STARTING</code>, <code>ACTIVE</code>}. </ul> Stops this bundle with no options. <p> This method calls <code>stop(0)</code>. Stops this bundle. <p> The following steps are required to stop a bundle: <ol> <li>If this bundle's state is <code>UNINSTALLED</code> then an <code>IllegalStateException</code> is thrown. <li>If this bundle is in the process of being activated or deactivated then this method must wait for activation or deactivation to complete before continuing. If this does not occur in a reasonable time, a <code>BundleException</code> is thrown to indicate this bundle was unable to be stopped. <li>If the {@link #STOP_TRANSIENT} option is not set then then set this bundle's persistent autostart setting to to <em>Stopped</em>. When the Framework is restarted and this bundle's autostart setting is <em>Stopped</em>, this bundle must not be automatically started. <li>If this bundle's state is not <code>ACTIVE</code> then this method returns immediately. <li>This bundle's state is set to <code>STOPPING</code>. <li>A bundle event of type {@link BundleEvent#STOPPING} is fired. <li>The {@link BundleActivator#stop} method of this bundle's <code>BundleActivator</code>, if one is specified, is called. If that method throws an exception, this method must continue to stop this bundle. A <code>BundleException</code> must be thrown after completion of the remaining steps. <li>Any services registered by this bundle must be unregistered. <li>Any services used by this bundle must be released. <li>Any listeners registered by this bundle must be removed. <li>If this bundle's state is <code>UNINSTALLED</code>, because this bundle was uninstalled while the <code>BundleActivator.stop</code> method was running, a <code>BundleException</code> must be thrown. <li>This bundle's state is set to <code>RESOLVED</code>. <li>A bundle event of type {@link BundleEvent#STOPPED} is fired. </ol> <b>Preconditions </b> <ul> <li><code>getState()</code> in {<code>ACTIVE</code>}. </ul> <b>Postconditions, no exceptions thrown </b> <ul> <li>Bundle autostart setting is modified unless the {@link #STOP_TRANSIENT} option was set. <li><code>getState()</code> not in {<code>ACTIVE</code>, <code>STOPPING</code>}. <li><code>BundleActivator.stop</code> has been called and did not throw an exception. </ul> <b>Postconditions, when an exception is thrown </b> <ul> <li>Bundle autostart setting is modified unless the {@link #STOP_TRANSIENT} option was set. </ul> Uninstalls this bundle. <p> This method causes the Framework to notify other bundles that this bundle is being uninstalled, and then puts this bundle into the <code>UNINSTALLED</code> state. The Framework must remove any resources related to this bundle that it is able to remove. <p> If this bundle has exported any packages, the Framework must continue to make these packages available to their importing bundles until the <code>PackageAdmin.refreshPackages</code> method has been called or the Framework is relaunched. <p> The following steps are required to uninstall a bundle: <ol> <li>If this bundle's state is <code>UNINSTALLED</code> then an <code>IllegalStateException</code> is thrown. <li>If this bundle's state is <code>ACTIVE</code>, <code>STARTING</code> or <code>STOPPING</code>, this bundle is stopped as described in the <code>Bundle.stop</code> method. If <code>Bundle.stop</code> throws an exception, a Framework event of type {@link FrameworkEvent#ERROR} is fired containing the exception. <li>This bundle's state is set to <code>UNINSTALLED</code>. <li>A bundle event of type {@link BundleEvent#UNINSTALLED} is fired. <li>This bundle and any persistent storage area provided for this bundle by the Framework are removed. </ol> <b>Preconditions </b> <ul> <li><code>getState()</code> not in {<code>UNINSTALLED</code>}. </ul> <b>Postconditions, no exceptions thrown </b> <ul> <li><code>getState()</code> in {<code>UNINSTALLED</code>}. <li>This bundle has been uninstalled. </ul> <b>Postconditions, when an exception is thrown </b> <ul> <li><code>getState()</code> not in {<code>UNINSTALLED</code>}. <li>This Bundle has not been uninstalled. </ul> Updates this bundle. <p> If this bundle's state is <code>ACTIVE</code>, it must be stopped before the update and started after the update successfully completes. <p> If this bundle has exported any packages, these packages must not be updated. Instead, the previous package version must remain exported until the <code>PackageAdmin.refreshPackages</code> method has been has been called or the Framework is relaunched. <p> The following steps are required to update a bundle: <ol> <li>If this bundle's state is <code>UNINSTALLED</code> then an <code>IllegalStateException</code> is thrown. <li>If this bundle's state is <code>ACTIVE</code>, <code>STARTING</code> or <code>STOPPING</code>, this bundle is stopped as described in the <code>Bundle.stop</code> method. If <code>Bundle.stop</code> throws an exception, the exception is rethrown terminating the update. <li>The download location of the new version of this bundle is determined from either this bundle's {@link Constants#BUNDLE_UPDATELOCATION} Manifest header (if available) or this bundle's original location. <li>The location is interpreted in an implementation dependent manner, typically as a URL, and the new version of this bundle is obtained from this location. <li>The new version of this bundle is installed. If the Framework is unable to install the new version of this bundle, the original version of this bundle must be restored and a <code>BundleException</code> must be thrown after completion of the remaining steps. <li>If this bundle has declared an Bundle-RequiredExecutionEnvironment header, then the listed execution environments must be verified against the installed execution environments. If they do not all match, the original version of this bundle must be restored and a <code>BundleException</code> must be thrown after completion of the remaining steps. <li>This bundle's state is set to <code>INSTALLED</code>. <li>If the new version of this bundle was successfully installed, a bundle event of type {@link BundleEvent#UPDATED} is fired. <li>If this bundle's state was originally <code>ACTIVE</code>, the updated bundle is started as described in the <code>Bundle.start</code> method. If <code>Bundle.start</code> throws an exception, a Framework event of type {@link FrameworkEvent#ERROR} is fired containing the exception. </ol> <b>Preconditions </b> <ul> <li><code>getState()</code> not in {<code>UNINSTALLED</code>}. </ul> <b>Postconditions, no exceptions thrown </b> <ul> <li><code>getState()</code> in {<code>INSTALLED</code>, <code>RESOLVED</code>,<code>ACTIVE</code>}. <li>This bundle has been updated. </ul> <b>Postconditions, when an exception is thrown </b> <ul> <li><code>getState()</code> in {<code>INSTALLED</code>, <code>RESOLVED</code>,<code>ACTIVE</code>}. <li>Original bundle is still used; no update occurred. </ul> Updates this bundle from an <code>InputStream</code>. <p> This method performs all the steps listed in <code>Bundle.update()</code>, except the new version of this bundle must be read from the supplied <code>InputStream</code>, rather than a <code>URL</code>. <p> This method must always close the <code>InputStream</code> when it is done, even if an exception is thrown.
Called when this bundle is started so the Framework can perform the bundle-specific activities necessary to start this bundle. This method can be used to register services or to allocate any resources that this bundle needs. <p> This method must complete and return to its caller in a timely manner. Called when this bundle is stopped so the Framework can perform the bundle-specific activities necessary to stop the bundle. In general, this method should undo the work that the <code>BundleActivator.start</code> method started. There should be no active threads that were started by this bundle when this bundle returns. A stopped bundle must not call any Framework objects. <p> This method must complete and return to its caller in a timely manner.
Adds the specified <code>BundleListener</code> object to the context bundle's list of listeners if not already present. BundleListener objects are notified when a bundle has a lifecycle state change. <p> If the context bundle's list of listeners already contains a listener <code>l</code> such that <code>(l==listener)</code>, this method does nothing. Adds the specified <code>FrameworkListener</code> object to the context bundle's list of listeners if not already present. FrameworkListeners are notified of general Framework events. <p> If the context bundle's list of listeners already contains a listener <code>l</code> such that <code>(l==listener)</code>, this method does nothing. Adds the specified <code>ServiceListener</code> object to the context bundle's list of listeners. <p> This method is the same as calling <code>BundleContext.addServiceListener(ServiceListener listener, String filter)</code> with <code>filter</code> set to <code>null</code>. Adds the specified <code>ServiceListener</code> object with the specified <code>filter</code> to the context bundle's list of listeners. See {@link Filter} for a description of the filter syntax. <code>ServiceListener</code> objects are notified when a service has a lifecycle state change. <p> If the context bundle's list of listeners already contains a listener <code>l</code> such that <code>(l==listener)</code>, then this method replaces that listener's filter (which may be <code>null</code>) with the specified one (which may be <code>null</code>). <p> The listener is called if the filter criteria is met. To filter based upon the class of the service, the filter should reference the {@link Constants#OBJECTCLASS} property. If <code>filter</code> is <code>null</code>, all services are considered to match the filter. <p> When using a <code>filter</code>, it is possible that the <code>ServiceEvent</code>s for the complete lifecycle of a service will not be delivered to the listener. For example, if the <code>filter</code> only matches when the property <code>x</code> has the value <code>1</code>, the listener will not be called if the service is registered with the property <code>x</code> not set to the value <code>1</code>. Subsequently, when the service is modified setting property <code>x</code> to the value <code>1</code>, the filter will match and the listener will be called with a <code>ServiceEvent</code> of type <code>MODIFIED</code>. Thus, the listener will not be called with a <code>ServiceEvent</code> of type <code>REGISTERED</code>. <p> If the Java Runtime Environment supports permissions, the <code>ServiceListener</code> object will be notified of a service event only if the bundle that is registering it has the <code>ServicePermission</code> to get the service using at least one of the named classes the service was registered under. Creates a <code>Filter</code> object. This <code>Filter</code> object may be used to match a <code>ServiceReference</code> object or a <code>Dictionary</code> object. <p> If the filter cannot be parsed, an {@link InvalidSyntaxException} will be thrown with a human readable message where the filter became unparsable. Returns an array of <code>ServiceReference</code> objects. The returned array of <code>ServiceReference</code> objects contains services that were registered under the specified class and match the specified filter criteria. <p> The list is valid at the time of the call to this method, however since the Framework is a very dynamic environment, services can be modified or unregistered at anytime. <p> <code>filter</code> is used to select the registered service whose properties objects contain keys and values which satisfy the filter. See {@link Filter} for a description of the filter string syntax. <p> If <code>filter</code> is <code>null</code>, all registered services are considered to match the filter. If <code>filter</code> cannot be parsed, an {@link InvalidSyntaxException} will be thrown with a human readable message where the filter became unparsable. <p> The following steps are required to select a set of <code>ServiceReference</code> objects: <ol> <li>If the filter string is not <code>null</code>, the filter string is parsed and the set <code>ServiceReference</code> objects of registered services that satisfy the filter is produced. If the filter string is <code>null</code>, then all registered services are considered to satisfy the filter. <li>If the Java Runtime Environment supports permissions, the set of <code>ServiceReference</code> objects produced by the previous step is reduced by checking that the caller has the <code>ServicePermission</code> to get at least one of the class names under which the service was registered. If the caller does not have the correct permission for a particular <code>ServiceReference</code> object, then it is removed from the set. <li>If <code>clazz</code> is not <code>null</code>, the set is further reduced to those services that are an <code>instanceof</code> and were registered under the specified class. The complete list of classes of which a service is an instance and which were specified when the service was registered is available from the service's {@link Constants#OBJECTCLASS} property. <li>An array of the remaining <code>ServiceReference</code> objects is returned. </ol> Returns the <code>Bundle</code> object associated with this <code>BundleContext</code>. This bundle is called the context bundle. Returns the bundle with the specified identifier. Returns a list of all installed bundles. <p> This method returns a list of all bundles installed in the OSGi environment at the time of the call to this method. However, since the Framework is a very dynamic environment, bundles can be installed or uninstalled at anytime. Creates a <code>File</code> object for a file in the persistent storage area provided for the bundle by the Framework. This method will return <code>null</code> if the platform does not have file system support. <p> A <code>File</code> object for the base directory of the persistent storage area provided for the context bundle by the Framework can be obtained by calling this method with an empty string as <code>filename</code>. <p> If the Java Runtime Environment supports permissions, the Framework will ensure that the bundle has the <code>java.io.FilePermission</code> with actions <code>read</code>,<code>write</code>,<code>delete</code> for all files (recursively) in the persistent storage area provided for the context bundle. Returns the value of the specified property. If the key is not found in the Framework properties, the system properties are then searched. The method returns <code>null</code> if the property is not found. <p> The Framework defines the following standard property keys: </p> <ul> <li>{@link Constants#FRAMEWORK_VERSION} - The OSGi Framework version. </li> <li>{@link Constants#FRAMEWORK_VENDOR} - The Framework implementation vendor.</li> <li>{@link Constants#FRAMEWORK_LANGUAGE} - The language being used. See ISO 639 for possible values.</li> <li>{@link Constants#FRAMEWORK_OS_NAME} - The host computer operating system.</li> <li>{@link Constants#FRAMEWORK_OS_VERSION} - The host computer operating system version number.</li> <li>{@link Constants#FRAMEWORK_PROCESSOR} - The host computer processor name.</li> </ul> <p> All bundles must have permission to read these properties. <p> Note: The last four standard properties are used by the {@link Constants#BUNDLE_NATIVECODE} <code>Manifest</code> header's matching algorithm for selecting native language code. Returns the specified service object for a service. <p> A bundle's use of a service is tracked by the bundle's use count of that service. Each time a service's service object is returned by {@link #getService(ServiceReference)} the context bundle's use count for that service is incremented by one. Each time the service is released by {@link #ungetService(ServiceReference)} the context bundle's use count for that service is decremented by one. <p> When a bundle's use count for a service drops to zero, the bundle should no longer use that service. <p> This method will always return <code>null</code> when the service associated with this <code>reference</code> has been unregistered. <p> The following steps are required to get the service object: <ol> <li>If the service has been unregistered, <code>null</code> is returned. <li>The context bundle's use count for this service is incremented by one. <li>If the context bundle's use count for the service is currently one and the service was registered with an object implementing the <code>ServiceFactory</code> interface, the {@link ServiceFactory#getService(Bundle, ServiceRegistration)} method is called to create a service object for the context bundle. This service object is cached by the Framework. While the context bundle's use count for the service is greater than zero, subsequent calls to get the services's service object for the context bundle will return the cached service object. <br> If the service object returned by the <code>ServiceFactory</code> object is not an <code>instanceof</code> all the classes named when the service was registered or the <code>ServiceFactory</code> object throws an exception, <code>null</code> is returned and a Framework event of type {@link FrameworkEvent#ERROR} is fired. <li>The service object for the service is returned. </ol> Returns a <code>ServiceReference</code> object for a service that implements and was registered under the specified class. <p> This <code>ServiceReference</code> object is valid at the time of the call to this method, however as the Framework is a very dynamic environment, services can be modified or unregistered at anytime. <p> This method is the same as calling {@link BundleContext#getServiceReferences(String, String)} with a <code>null</code> filter string. It is provided as a convenience for when the caller is interested in any service that implements the specified class. <p> If multiple such services exist, the service with the highest ranking (as specified in its {@link Constants#SERVICE_RANKING} property) is returned. <p> If there is a tie in ranking, the service with the lowest service ID (as specified in its {@link Constants#SERVICE_ID} property); that is, the service that was registered first is returned. Returns an array of <code>ServiceReference</code> objects. The returned array of <code>ServiceReference</code> objects contains services that were registered under the specified class, match the specified filter criteria, and the packages for the class names under which the services were registered match the context bundle's packages as defined in {@link ServiceReference#isAssignableTo(Bundle, String)}. <p> The list is valid at the time of the call to this method, however since the Framework is a very dynamic environment, services can be modified or unregistered at anytime. <p> <code>filter</code> is used to select the registered service whose properties objects contain keys and values which satisfy the filter. See {@link Filter} for a description of the filter string syntax. <p> If <code>filter</code> is <code>null</code>, all registered services are considered to match the filter. If <code>filter</code> cannot be parsed, an {@link InvalidSyntaxException} will be thrown with a human readable message where the filter became unparsable. <p> The following steps are required to select a set of <code>ServiceReference</code> objects: <ol> <li>If the filter string is not <code>null</code>, the filter string is parsed and the set <code>ServiceReference</code> objects of registered services that satisfy the filter is produced. If the filter string is <code>null</code>, then all registered services are considered to satisfy the filter. <li>If the Java Runtime Environment supports permissions, the set of <code>ServiceReference</code> objects produced by the previous step is reduced by checking that the caller has the <code>ServicePermission</code> to get at least one of the class names under which the service was registered. If the caller does not have the correct permission for a particular <code>ServiceReference</code> object, then it is removed from the set. <li>If <code>clazz</code> is not <code>null</code>, the set is further reduced to those services that are an <code>instanceof</code> and were registered under the specified class. The complete list of classes of which a service is an instance and which were specified when the service was registered is available from the service's {@link Constants#OBJECTCLASS} property. <li>The set is reduced one final time by cycling through each <code>ServiceReference</code> object and calling {@link ServiceReference#isAssignableTo(Bundle, String)} with the context bundle and each class name under which the <code>ServiceReference</code> object was registered. For any given <code>ServiceReference</code> object, if any call to {@link ServiceReference#isAssignableTo(Bundle, String)} returns <code>false</code>, then it is removed from the set of <code>ServiceReference</code> objects. <li>An array of the remaining <code>ServiceReference</code> objects is returned. </ol> Installs a bundle from the specified location string. A bundle is obtained from <code>location</code> as interpreted by the Framework in an implementation dependent manner. <p> Every installed bundle is uniquely identified by its location string, typically in the form of a URL. <p> The following steps are required to install a bundle: <ol> <li>If a bundle containing the same location string is already installed, the <code>Bundle</code> object for that bundle is returned. <li>The bundle's content is read from the location string. If this fails, a {@link BundleException} is thrown. <li>The bundle's <code>Bundle-NativeCode</code> dependencies are resolved. If this fails, a <code>BundleException</code> is thrown. <li>The bundle's associated resources are allocated. The associated resources minimally consist of a unique identifier and a persistent storage area if the platform has file system support. If this step fails, a <code>BundleException</code> is thrown. <li>If the bundle has declared an Bundle-RequiredExecutionEnvironment header, then the listed execution environments must be verified against the installed execution environments. If none of the listed execution environments match an installed execution environment, a <code>BundleException</code> must be thrown. <li>The bundle's state is set to <code>INSTALLED</code>. <li>A bundle event of type {@link BundleEvent#INSTALLED} is fired. <li>The <code>Bundle</code> object for the newly or previously installed bundle is returned. </ol> <b>Postconditions, no exceptions thrown </b> <ul> <li><code>getState()</code> in {<code>INSTALLED</code>,<code>RESOLVED</code>}. <li>Bundle has a unique ID. </ul> <b>Postconditions, when an exception is thrown </b> <ul> <li>Bundle is not installed and no trace of the bundle exists. </ul> Installs a bundle from the specified <code>InputStream</code> object. <p> This method performs all of the steps listed in <code>BundleContext.installBundle(String location)</code>, except that the bundle's content will be read from the <code>InputStream</code> object. The location identifier string specified will be used as the identity of the bundle. <p> This method must always close the <code>InputStream</code> object, even if an exception is thrown. Registers the specified service object with the specified properties under the specified class name with the Framework. <p> This method is otherwise identical to {@link #registerService(java.lang.String[], java.lang.Object, java.util.Dictionary)} and is provided as a convenience when <code>service</code> will only be registered under a single class name. Note that even in this case the value of the service's {@link Constants#OBJECTCLASS} property will be an array of strings, rather than just a single string. Registers the specified service object with the specified properties under the specified class names into the Framework. A <code>ServiceRegistration</code> object is returned. The <code>ServiceRegistration</code> object is for the private use of the bundle registering the service and should not be shared with other bundles. The registering bundle is defined to be the context bundle. Other bundles can locate the service by using either the {@link #getServiceReferences} or {@link #getServiceReference} method. <p> A bundle can register a service object that implements the {@link ServiceFactory} interface to have more flexibility in providing service objects to other bundles. <p> The following steps are required to register a service: <ol> <li>If <code>service</code> is not a <code>ServiceFactory</code>, an <code>IllegalArgumentException</code> is thrown if <code>service</code> is not an <code>instanceof</code> all the classes named. <li>The Framework adds these service properties to the specified <code>Dictionary</code> (which may be <code>null</code>): a property named {@link Constants#SERVICE_ID} identifying the registration number of the service and a property named {@link Constants#OBJECTCLASS} containing all the specified classes. If any of these properties have already been specified by the registering bundle, their values will be overwritten by the Framework. <li>The service is added to the Framework service registry and may now be used by other bundles. <li>A service event of type {@link ServiceEvent#REGISTERED} is fired. <li>A <code>ServiceRegistration</code> object for this registration is returned. </ol> Removes the specified <code>BundleListener</code> object from the context bundle's list of listeners. <p> If <code>listener</code> is not contained in the context bundle's list of listeners, this method does nothing. Removes the specified <code>FrameworkListener</code> object from the context bundle's list of listeners. <p> If <code>listener</code> is not contained in the context bundle's list of listeners, this method does nothing. Removes the specified <code>ServiceListener</code> object from the context bundle's list of listeners. <p> If <code>listener</code> is not contained in this context bundle's list of listeners, this method does nothing. Releases the service object referenced by the specified <code>ServiceReference</code> object. If the context bundle's use count for the service is zero, this method returns <code>false</code>. Otherwise, the context bundle's use count for the service is decremented by one. <p> The service's service object should no longer be used and all references to it should be destroyed when a bundle's use count for the service drops to zero. <p> The following steps are required to unget the service object: <ol> <li>If the context bundle's use count for the service is zero or the service has been unregistered, <code>false</code> is returned. <li>The context bundle's use count for this service is decremented by one. <li>If the context bundle's use count for the service is currently zero and the service was registered with a <code>ServiceFactory</code> object, the {@link ServiceFactory#ungetService(Bundle, ServiceRegistration, Object)} method is called to release the service object for the context bundle. <li><code>true</code> is returned. </ol>
Returns the bundle which had a lifecycle change. This bundle is the source of the event. Returns the type of lifecyle event. The type values are: <ul> <li>{@link #INSTALLED} <li>{@link #RESOLVED} <li>{@link #LAZY_ACTIVATION} <li>{@link #STARTING} <li>{@link #STARTED} <li>{@link #STOPPING} <li>{@link #STOPPED} <li>{@link #UPDATED} <li>{@link #UNRESOLVED} <li>{@link #UNINSTALLED} </ul>
Returns the cause of this exception or <code>null</code> if no cause was specified when this exception was created. Returns any nested exceptions included in this exception. <p> This method predates the general purpose exception chaining mechanism. The {@link #getCause()} method is now the preferred means of obtaining this information. The cause of this exception can only be set when constructed.
Return the bundle to be used. The msgIdf is passed in to allow it to be a factor in determing the resource name of the messages file.
Receives notification that a bundle has had a lifecycle change.
Determines the equality of two <code>BundlePermission</code> objects. This method checks that specified bundle has the same bundle symbolic name and <code>BundlePermission</code> actions as this <code>BundlePermission</code> object. Returns the canonical string representation of the <code>BundlePermission</code> actions. <p> Always returns present <code>BundlePermission</code> actions in the following order: <code>PROVIDE</code>,<code>REQUIRE</code>, <code>HOST</code>,<code>FRAGMENT. Returns the current action mask. <p> Used by the BundlePermissionCollection class. Parse action string into action mask. Returns the hash code value for this object. Determines if the specified permission is implied by this object. <p> This method checks that the symbolic name of the target is implied by the symbolic name of this object. The list of <code>BundlePermission</code> actions must either match or allow for the list of the target object to imply the target <code>BundlePermission</code> action. <p> The permission to provide a bundle implies the permission to require the named symbolic name. <pre> x.y.*,&quot;provide&quot; -&gt; x.y.z,&quot;provide&quot; is true *,&quot;require&quot; -&gt; x.y, &quot;require&quot;      is true *,&quot;provide&quot; -&gt; x.y, &quot;require&quot;      is true x.y,&quot;provide&quot; -&gt; x.y.z, &quot;provide&quot;  is false </pre> Called by constructors and when deserialized. Returns a new <code>PermissionCollection</code> object suitable for storing <code>BundlePermission</code> objects. readObject is called to restore the state of the BundlePermission from a stream. WriteObject is called to save the state of the <code>BundlePermission</code> object to a stream. The actions are serialized, and the superclass takes care of the name. Adds a permission to the <code>BundlePermission</code> objects. The key for the hash is the symbolic name. Returns an enumeration of all <code>BundlePermission</code> objects in the container. Determines if the specified permissions implies the permissions expressed in <code>permission</code>.
Return the number of bytes in the alphabet. The number of bytes in the alphabet is noramlly different from the number of characters in the alphabet, but it depends on the characters in the alphabet and encoding used to represent them as bytes. Return the number of characters in the alphabet. Create an alphabet returning bytes representing a subset of the CJK alphabet in the UTF-16BE encoding. Create an alphabet returning bytes representing a subset of the CJK alphabet in the UTF-8 encoding. Return the encoding used to represent characters as bytes. Return the name of the alphabet. Create an alphabet returning bytes representing the lowercase letters a-z in the "US-ASCII" encoding. Return the next byte in the alphabet. Compute the next byte to read after reading the specified number of bytes. Besides from returning the index, the internal state of the alphabet is updated. Create an alphabet returning bytes representing the 29 lowercase letters in the Norwegian/Danish alphabet in the "ISO-8859-1" encoding. Reset the alphabet, the next byte returned is the first byte in the alphabet, which might not be a complete character. Create an alphabet that consists of a single byte. Create an alphabet returning bytes representing a subset of the Tamil alphabet in the UTF-16BE encoding. Create an alphabet returning bytes representing a subset of the Tamil alphabet in the UTF-8 encoding.
Compare two byte arrays using value equality. Two byte arrays are equal if their length is identical and their contents are identical. Value equality for byte arrays.  Read this object from a stream of stored objects. Write the byte array out w/o compression
Return the number of available bytes. The method assumes the specified length of the stream is correct. Fetch the next array to read data from. The reference in the <code>ArrayList</code> is cleared when the array is "taken out". Read a single byte. Reads up to len bytes of data from the input stream into an array of bytes. An attempt is made to read as many as <code>len</code> bytes, but a smaller number may be read. The number of bytes actually read is returned as an integer.
Return the number of bytes that can be read from this ByteHolder without blocking on an IO. Clear the bytes from the ByteHolder and place it in writing mode. This may not free the memory the ByteHolder uses to store data. Return a byte holder matching existing type and size of current ByteHolder, but don't bother to fill the bytes. Normal usage is expected to reset the holding stream to the beginning, so the copy of current state would be wasted. Return the number of bytes that have been saved to this byte holder. This result is different from available() as it is unaffected by the current read position on the ByteHolder. Read a byte from this ByteHolder. <P>The ByteHolder must be in reading mode to call this. Read up to 'len' bytes from this ByteHolder and store them in an array at offset 'off'. <P>The ByteHolder must be in reading mode to call this. Read from the ByteHolder. <p> Read up to 'len' bytes from this ByteHolder and write them to the OutputStream <P>The ByteHolder must be in reading mode to call this. shift the remaining unread bytes to the beginning of the byte holder Skip over the specified number of bytes in a ByteHolder. Place a ByteHolder in reading mode. After this call, reads scan bytes sequentially in the order they were written to the ByteHolder starting from the first byte. When the ByteHolder is already in readmode this simply arranges for reads to start at the beginning of the sequence of saved bytes. Write len bytes of data starting at 'offset' to this ByteHolder. <P>The ByteHolder must be in writing mode to call this. Write a byte to this ByteHolder. <P>The ByteHolder must be in writing mode to call this. Write up to count bytes from an input stream to this ByteHolder. This may write fewer bytes if it encounters an end of file on the input stream. Return true if this is in writing mode.

Tell whether this type (CLOB) is compatible with the given type. Tell whether this type (LOB) can be converted to the given type.    Push the collation type if it is not COLLATION_TYPE_UCS_BASIC. Tell whether this type (LOB) can be stored into from the given type.


Used when searching


We assume here that the String is ASCII, thus this might return a size smaller than actual size. if this returns 0 then the caller must put another CONSTANT_Utf8_info into the constant pool with no hash table entry and then call setAlternative() with its index.
Tests whether the named file exists. end of exists Privileged wrapper for {@code Thread.getContextClassLoader()}. Creates an input stream from a file name. end of getInputStream Get the parent of this file. Privileged wrapper for {@code ClassLoader.getResource(String)}. Privileged wrapper for {@code ClassLoader.getResourceAsStream(String)}. Privileged wrapper for {@code ClassLoader.getSystemResource(String)}. Privileged wrapper for {@code ClassLoader.getSystemResourceAsStream(String)}. Return a URL for this file (resource).
end of doInit Construct a persistent StorageFile from a path name. Construct a StorageFile from a directory and file name. Construct a StorageFile from a directory and file name.


Clear this entry and notify the replacement algorithm that the <code>Cacheable</code> can be reused. Return the cached object held by this entry. Check whether or not this entry is kept. Check whether this entry holds a valid object. That is, it must hold a non-null <code>Cacheable</code> and have completed setting its identity. Increase the keep count for this entry. An entry which is kept cannot be removed from the cache. Block until the current thread is granted exclusive access to the entry. Set the cached object held by this entry. Set the callback object used to notify the replacement algorithm about actions performed on the cached object. Notify this entry that the initialization of its cacheable has been completed. This method should be called after <code>Cacheable.setIdentity()</code> or <code>Cacheable.createIdentity()</code> has been called. Decrement the keep count for this entry. An entry cannot be removed from the cache until its keep count is zero. Unkeep the entry and wait until no other thread is keeping it. This method is used when a thread requests the removal of the entry. As defined by the contract of <code>CacheManager.remove()</code>, it is the responsibility of the caller to ensure that only a single thread executes this method on an object. Give up exclusive access. Block until this entry's cacheable has been initialized (that is, until {@code settingIdentityComplete()} has been called on this object). If the cacheable has been initialized before this method is called, it will return immediately. The entry must have been locked for exclusive access before this method is called. If the method needs to wait, it will release the lock and reobtain it when it wakes up again.
Create a cache that uses the class represented by holderClass as the holder class. This holderClass must implement Cacheable.
Age as many objects as possible out of the cache. This call is guaranteed not to block. It is not guaranteed to leave the cache empty. <BR> It is guaranteed that all unkept, clean objects will be removed from the cache. Clean all objects that match the partialKey (or exact key). Any cached object that results in the partialKey.equals(Object) method returning true when passed the cached object will be cleaned. <P> In order to clean more than one object the Cacheable equals method must be able to handle a partial key, e.g. a page has PageKey but a clean may pass a ContainerKey which will discard all pages in that container. Place all objects in their clean state by calling their clean method if they are dirty. This method guarantees that all objects that existed in the cache at the time of the call are placed in the clean state sometime during this call. Objects that are added to the cache during this call or objects that are dirtied during this call (by other callers) are not guaranteed to be clean once this call returns. Create an object in the cache. The resulting object will match the key provided using the equals() method, i.e. the return Cacheable will have getIdentifier.equals(key) true. If an object that matches the key already exists in the cache then an exception is thrown. <BR> The object will be added by one of: <UL> <LI>creating a new holder object and calling its initParameter() method and then its createIdentity() method with key as the parameter. <LI>Calling clearIdentity() on an holder object in the clean state and then calling its createIdentity() method with key as the parameter. <LI>Calling clean() on a dirty holder object and then calling clearIdentity() on an holder object in the clean state and then calling its createIdentity() method with key as the parameter. </UL> In all cases the setIdentity() method is called with the createParameter as the second argument. If the object cannot be created then an exception is thrown by createIdentity. <BR> The returned object is kept, i.e. its identity will not change, until the release() method is called. The release() method must be called after the caller is finished with the object and throw away the reference to it, e.g. <PRE> Page p = (Page) pageCache.create(pageKey, createType); // do stuff with p // release p pageCache.release(p); p = null; </PRE> Deregister the MBean that monitors this cache. If there is no MBean for this instance, this is a no-op. Discard all objects that match the partialKey (or exact key). Any cached object that results in the partialKey.equals(Object) method returning true when passed the cached object will be thrown out of the cache if and only if it is not in use. The Cacheable will be discarded without its clean method being called. <P> If partialKey is null, it matches all objects.  This is a way to discard all objects from the cache in case of emergency shutdown. <P> In order to discard more than one object the Cacheable equals method must be able to handle a partial key, e.g. a page has PageKey but a discard may pass a ContainerKey which will discard all pages in that container. <P> Find an object in the cache. <p> Find an object in the cache that matches the key provided using the equals() method, i.e. the return Cacheable will have getIdentifier.equals(key) true. If the object does not exist in the cache it will be added by one of: <UL> <LI>creating a new holder object and calling its initParameter() method and then its setIdentity() method with key as the parameter. <LI>Calling clearIdentity() on an holder object in the clean state and then calling its setIdentity() method with key as the parameter. <LI>Calling clean() on a dirty holder object and then calling clearIdentity() on an holder object in the clean state and then calling its setIdentity() method with key as the parameter. </UL> In all cases the setIdentity() method is called with forCreate set to false. <BR> The returned object is kept, i.e. its identity will not change, until the release() method is called. The release() method must be called after the caller is finished with the object and throw away the reference to it, e.g. <PRE> Page p = (Page) pageCache.find(pageKey); // do stuff with p // release p pageCache.release(p); p = null; </PRE> Find an object in the cache. <p> Find an object in the cache that matches the key provided using the equals() method, i.e. the return Cacheable will have getIdentifier.equals(key) true. If a matching object does not exist in the cache, null is returned. <BR> The returned object is kept, i.e. its identity will not change, until the release() method is called. The release() method must be called after the caller is finished with the object and throw away the reference to it, e.g. <PRE> Page p = (Page) pageCache.findCached(pageKey); if (p != null) { // do stuff with p // release p pageCache.release(p); p = null; } </PRE> <p> Register an MBean that allows user to monitor this cache instance. This is a no-op if the platform does not support Java Management Extensions (JMX). </p> <p> The MBean will be automatically deregistered when {@link #shutdown()} is called, or it can be manually deregistered by calling {@link #deregisterMBean()}. </p> Release a <code>Cacheable</code> object previously found with <code>find()</code> or <code>findCached()</code>, or created with <code>create()</code>, and which is still kept by the caller. After this call the caller must throw away the reference to item. Delete and remove an object from the cache. It is up to the user of the cache to provide synchronization of some form that ensures that only one caller executes remove() on a cached object. <BR> The object must previously have been found with <code>find()</code> or <code>findCached()</code>, or created with <code>create()</code>, and it must still be kept by the caller. The item will be placed into the NoIdentity state through clean(true) (if required) and clearIdentity(). The removal of the object will be delayed until it is not kept by anyone. Objects that are in the to be removed state can still be found through find() and findCached() until their keep count drops to zero. This call waits until the object has been removed. <BR> After this call the caller must throw away the reference to item. Shutdown the cache. This call stops the cache returning any more valid references on a <code>find()</code>, <code>findCached()</code> or <code>create()</code> call, and then cleanAll() and ageOut() are called. The cache remains in existence until the last kept object has been unkept. This cache can use this DaemonService if it needs some work to be done in the background. The caller must ensure that it has exclusive access to the cache when this method is called. No synchronization is required in the implementations of this method. Return a Collection of the Cacheables currently in the cache. The Collection should be a copy so that external synchronization isn't required. <p> This method should only be used for diagnostic purposes.
Get the number of entries currently allocated in the cache. This number includes entries for objects that have been removed from the cache, whose entries have not yet been reused for other objects. Check if collection of cache access counts is enabled. Get the number of cached objects that have been evicted from the cache in order to make room for other objects. Get the number of cache accesses where the requested object was already in the cache. Get the maximum number of entries that could be held by this cache. Get the number of cache accesses where the requested object was not already in the cache. Get the number of objects that are currently in the cache. Enable or disable collection of cache access counts. That is, whether or not each hit, miss and eviction should be counted. Enabling it might impose a small overhead on cache accesses, and might reduce the system performance. Access counts are disabled by default.
Clean the object. It is up to the object to ensure synchronization of the isDirty() and clean() method calls. <BR> If forRemove is true then the object is being removed due to an explict remove request, in this case the cache manager will have called this method regardless of the state of the isDirty() <BR> If an exception is thrown the object must be left in the clean state. <BR> MT - thread safe - Can be called at any time by the cache manager, it is the responsibility of the object implementing Cacheable to ensure any users of the object do not conflict with the clean call. Put the object into the No Identity state. <BR> MT - single thread required - Method must only be called be cache manager and the cache manager will guarantee only one thread can be calling it. Create a new item. <p> Create a new item and set the identity of the object to represent it. The object will be in the No Identity state, ie. it will have just been created or clearIdentity() was just called. <BR> The object must copy the information out of key, not just store a reference to key if the key is not immutable. After this call the expression getIdentity().equals(key) must return true. <BR> <BR> If the class of the object needs to change (e.g. to support a different format) then the object should create a new object, call its initParameter() with the parameters the original object was called with, set its identity and return a reference to it. The cache manager will discard the reference to the old object. <BR> If an exception is thrown the object must be left in the no-identity state. <BR> MT - single thread required - Method must only be called be cache manager and the cache manager will guarantee only one thread can be calling it. Get the identity of this object. <BR> MT - thread safe. Returns true of the object is dirty. May be called when the object is kept or unkept. <BR> MT - thread safe Set the identity of the object. <p> Set the identity of the object to represent an item that already exists, e.g. an existing container. The object will be in the No Identity state, ie. it will have just been created or clearIdentity() was just called. <BR> The object must copy the information out of key, not just store a reference to key. After this call the expression getIdentity().equals(key) must return true. <BR> If the class of the object needs to change (e.g. to support a different format) then the object should create a new object, call its initParameter() with the parameters the original object was called with, set its identity and return a reference to it. The cache manager will discard the reference to the old object. <BR> If an exception is thrown the object must be left in the no-identity state. <BR> MT - single thread required - Method must only be called be cache manager and the cache manager will guarantee only one thread can be calling it.
Clean the object. It is up to the object to ensure synchronization of the isDirty() and clean() method calls. <BR> If forRemove is true then the object is being removed due to an explicit remove request, in this case the cache manager will have called this method regardless of the state of the isDirty() <BR> If an exception is thrown the object must be left in the clean state. <BR> MT - thread safe - Can be called at any time by the cache manager, it is the responsibility of the object implementing Cacheable to ensure any users of the object do not conflict with the clean call. Put the object into the No Identity state. <BR> MT - single thread required - Method must only be called be cache manager and the cache manager will guarantee only one thread can be calling it. Create a new item and set the identity of the object to represent it. The object will be in the No Identity state, ie. it will have just been created or clearIdentity() was just called. <BR> The object must copy the information out of key, not just store a reference to key.  After this call the expression getIdentity().equals(key) must return true. <BR> If the class of the object needs to change (e.g. to support a different format) then the object should create a new object, call its initParameter() with the parameters the original object was called with, set its identity and return a reference to it. The cache manager will discard the reference to the old object. <BR> If an exception is thrown the object must be left in the no-identity state. <BR> MT - single thread required - Method must only be called be cache manager and the cache manager will guarantee only one thread can be calling it. * protected Methods of CacheableConglomerate: Get the identity of this object. <BR> MT - thread safe. Returns true if the object is dirty. Will only be called when the object is unkept. <BR> MT - thread safe * Methods of Cacheable: Set the identity of the object to represent an item that already exists, e.g. an existing container. The object will be in the No Identity state, ie. it will have just been created or clearIdentity() was just called. <BR> The object must copy the information out of key, not just store a reference to key. After this call the expression getIdentity().equals(key) must return true. <BR> If the class of the object needs to change (e.g. to support a different format) then the object should create a new object, call its initParameter() with the parameters the original object was called with, set its identity and return a reference to it. The cache manager will discard the reference to the old object. <BR> If an exception is thrown the object must be left in the no-identity state. <BR> MT - single thread required - Method must only be called by cache manager and the cache manager will guarantee only one thread can be calling it.

Convert this page to requested type, as defined by input format id. <p> The current cache entry is a different format id than the requested type, change it.  This object is instantiated to the wrong subtype of cachedPage, this routine will create an object with the correct subtype, and transfer all pertinent information from this to the new correct object. <p> Write the page to disk. <p> MP - In a simple world we would just not allow clean until it held the latch on the page.  But in order to fit into the cache system, we don't have enough state around to just make clean() latch the page while doing the I/O - but we still need someway to insure that no changes happen to the page while the I/O is taking place. Also someday it would be fine to allow reads of this page while the I/O was taking place. Find the container and then create the page in that container. <p> This is the process of creating a new page in a container, in that case no need to read the page from disk - just need to initialize it in the cache. <p> create the page Returns the page data array used to write on disk version. <p> returns the page data array, that is actually written to the disk, when the page is cleaned from the page cache.  Takes care of flushing in-memory information to the array (like page header and format id info). <p> initialize in memory structure using the read in buffer in pageData Initialize a CachedPage. <p> Initialize the object, ie. perform work normally perfomed in constructor.  Called by setIdentity() and createIdentity(). Has the page or its header been modified. <p> See comment on class header on meaning of isDirty and preDirty bits. <p> Is the page dirty? <p> The isDirty flag indicates if the pageData or pageHeader has been modified.  The preDirty flag indicates that the pageData or the pageHeader is about to be modified.  The reason for these 2 flags instead of just one is to accomodate checkpoint.  After a clean (latched) page sends a log record to the log stream but before that page is dirtied by the log operation, a checkpoint could be taken.  If so, then the redoLWM will be after the log record but, without preDirty, the cache cleaning will not have waited for the change.  So the preDirty bit is to stop the cache cleaning from skipping over this (latched) page even though it has not really been modified yet. Set state to indicate the page or its header is about to be modified. <p> See comment on class header on meaning of isDirty and preDirty bits. read the page from disk into this CachedPage object. <p> A page is read in from disk into the pageData array of this object, and then put in the cache. <p> exclusive latch on page is being released. <p> The only work done in CachedPage is to update the row count on the container if it is too out of sync. Set state to indicate the page or its header has been modified. <p> See comment on class header on meaning of isDirty and preDirty bits. <p> * Methods of Cacheable Find the container and then read the page from that container. <p> This is the way new pages enter the page cache. <p> * if the page size is different from the page buffer, then make a * new page buffer and make subclass use the new page buffer methods for subclass of cached page use a new pageData buffer, initialize in memory structure that depend on the pageData's size.  The actual disk data may not have not been read in yet so don't look at the content of the buffer write out the formatId to the pageData page is about to be written, write everything to pageData array write the page from this CachedPage object to disk. <p>
Cacheable interface     Get the PreparedStatement that is associated with this Cacheable
Overrides for various ValueNode methods. Simply forward the calls to the wrapped ValueNode. Generate code that clears the field that holds the cached value, so that it can be garbage collected. Generate code that returns the value that this expression evaluates to. For the first occurrence of this node in the abstract syntax tree, this method generates the code needed to evaluate the expression. Additionally, it stores the returned value in a field in the {@code Activation} class. For subsequent occurrences of this node, it will simply generate code that reads the value of that field, so that reevaluation is not performed.

Accept the visitor for all visitable children of this node. Bind this UpdateNode.  This means looking up tables and columns and getting their types, and figuring out the result types of all expressions, as well as doing view resolution, permissions checking, etc. <p> Binding an update will also massage the tree so that the ResultSetNode has a single column, the RID. This method checks if the called procedure allows modification of SQL data. If yes, it cannot be compiled if the reliability is <code>CompilerContext.MODIFIES_SQL_DATA_PROCEDURE_ILLEGAL</code>. This reliability is set for BEFORE triggers in the create trigger node. This check thus disallows creation of BEFORE triggers which contain calls to procedures that modify SQL data in the trigger action statement. Code generation for CallStatementNode. The generated code will contain: o  A generated void method for the user's method call. Set default privilege of EXECUTE for this node. This method checks the SQL allowed by the called procedure. This method should be called only after the procedure has been resolved. Optimize a DML statement (which is the only type of statement that should need optimizing, I think). This method over-rides the one in QueryTreeNode. This method takes a bound tree, and returns an optimized tree. It annotates the bound tree rather than creating an entirely new tree. Throws an exception if the tree is not bound, or if the binding is out of date. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.
Need to explicitly close any dynamic result sets. <BR> If the dynamic results are not accessible then they need to be destroyed (ie. closed) according the the SQL Standard. <BR> An execution of a CALL statement through JDBC makes the dynamic results accessible, in this case the closing of the dynamic result sets is handled by the JDBC statement object (EmbedStatement) that executed the CALL. We cannot unify the closing of dynamic result sets to this close, as in accessible case it is called during the Statement.execute call, thus it would close the dynamic results before the application has a change to use them. <BR> With an execution of a CALL statement as a trigger's action statement the dynamic result sets are not accessible. In this case this close method is called after the execution of the trigger's action statement. <BR> <BR> Section 4.27.5 of the TECHNICAL CORRIGENDUM 1 to the SQL 2003 Standard details what happens to dynamic result sets in detail, the SQL 2003 foundation document is missing these details. Just invoke the method.
Allocates an empty BLOB on server and returns its locator.  Any subsequent operations on this BLOB value will be stored in temporary space on the server. Retrieves all or part of the <code>BLOB</code> value that is identified by <code>sourceLocator</code>, as an array of bytes.  This <code>byte</code> array contains up to <code>forLength</code> consecutive bytes starting at position <code>fromPosition</code>. <p> If <code>forLength</code> is larger than the maximum length of a VARCHAR FOR BIT DATA, the reading of the BLOB will be split into repeated procedure calls. Returns the number of bytes in the <code>BLOB</code> value designated by this <code>sourceLocator</code>. Retrieves the byte position at which the specified byte array <code>searchLiteral</code> begins within the <code>BLOB</code> value identified by <code>locator</code>.  The search for <code>searchLiteral</code> begins at position <code>fromPosition</code>. <p> If <code>searchLiteral</code> is longer than the maximum length of a VARCHAR FOR BIT DATA, it will be split into smaller fragments, and repeated procedure calls will be made to perform the entire search Retrieves the byte position at which the specified part of the byte array <code>searchLiteral</code> begins within the <code>BLOB</code> value identified by <code>locator</code>.  The search for <code>searchLiteral</code> begins at position <code>fromPosition</code>. <p> This is a helper function used by blobGetPositionFromBytes(int, byte[], long) for each call to the BLOBGETPOSITIONFROMBYTES procedure. Retrieves the byte position in the BLOB value designated by this <code>locator</code> at which pattern given by <code>searchLocator</code> begins. The search begins at position <code>fromPosition</code>. This method frees the BLOB and releases the resources that it holds. (E.g., temporary space used to store this BLOB on the server.) Writes all or part of the given <code>byte</code> array to the <code>BLOB</code> value designated by <code>sourceLocator</code>. Writing starts at position <code>fromPosition</code> in the <code>BLOB</code> value; <code>forLength</code> bytes from the given byte array are written. If the end of the <code>Blob</code> value is reached while writing the array of bytes, then the length of the <code>Blob</code> value will be increased to accomodate the extra bytes. <p> If <code>forLength</code> is larger than the maximum length of a VARCHAR FOR BIT DATA, the writing to the BLOB value will be split into repeated procedure calls. Truncates the <code>BLOB</code> value identified by <code>sourceLocator</code> to be <code>length</code> bytes. <p> <b>Note:</b> If the value specified for <code>length</code> is greater than the length+1 of the <code>BLOB</code> value then an <code>SqlException</code> will be thrown. Allocates an empty CLOB on server and returns its locator. Any subsequent operations on this CLOB value will be stored in temporary space on the server. Returns the number of character in the <code>CLOB</code> value designated by this <code>sourceLocator</code>. Retrieves the character position in the CLOB value designated by this <code>locator</code> at which substring given by <code>searchLocator</code> begins. The search begins at position <code>fromPosition</code>. Retrieves the character position at which the specified substring <code>searchLiteral</code> begins within the <code>CLOB</code> value identified by <code>locator</code>.  The search for <code>searchLiteral</code> begins at position <code>fromPosition</code>. <p> If <code>searchLiteral</code> is longer than the maximum length of a VARCHAR, it will be split into smaller fragments, and repeated procedure calls will be made to perform the entire search Retrieves the character position at which the specified part of the substring <code>searchLiteral</code> begins within the <code>CLOB</code> value identified by <code>locator</code>.  The search for <code>searchLiteral</code> begins at position <code>fromPosition</code>. <p> This is a helper function used by clobGetPositionFromString(int, String, long) for each call to the CLOBGETPOSITIONFROMSTRING procedure. Retrieves all or part of the <code>CLOB</code> value that is identified by <code>sourceLocator</code>, as a <code>String</code>.  This <code>String</code> contains up to <code>forLength</code> consecutive characters starting at position <code>fromPosition</code>. <p> If <code>forLength</code> is larger than the maximum length of a VARCHAR, the reading of the CLOB will be split into repeated procedure calls. This method frees the CLOB and releases the resources that it holds. (E.g., temporary space used to store this CLOB on the server.) Writes all or part of the given <code>String</code> to the <code>CLOB</code> value designated by <code>sourceLocator</code>. Writing starts at position <code>fromPosition</code> in the <code>CLOB</code> value; <code>forLength</code> characters from the given string are written. If the end of the <code>CLOB</code> value is reached while writing the string, then the length of the <code>CLOB</code> value will be increased to accomodate the extra characters. <p> If <code>forLength</code> is larger than the maximum length of a VARCHAR, the writing to the CLOB value will be split into repeated procedure calls. Truncates the <code>CLOB</code> value identified by <code>sourceLocator</code> to be <code>length</code> characters. <p> <b>Note:</b> If the value specified for <code>length</code> is greater than the length+1 of the <code>CLOB</code> value then an <code>SqlException</code> will be thrown. If the given exception indicates that locator was not valid, we assume the locator has been garbage-collected due to transaction commit, and wrap the exception in an exception with SQL state <code>LOB_OBJECT_INVALID</code>.
return the array of cardinalities that are kept internally. One value for each leading key; i.e c1, (c1,c2), (c1,c2,c3) etc. Gets next row from the row source and update the count of unique values that are returned. get the number of rows seen in the row source thus far.
Accept the visitor for all visitable children of this node. Bind this node but not its child.  Caller has already bound the child. This is useful for when we generate a CastNode during binding after having already bound the child. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views.  Do code generation for this unary operator. Get a constant representing the cast from a CHAR to another type.  If this is not an "easy" cast to perform, then just return this cast node. Here's what we think is "easy": source			destination ------			----------- char			boolean char			date/time/ts char			non-decimal numeric Get a constant representing the cast from an integral type to another type.  If this is not an "easy" cast to perform, then just return this cast node. Here's what we think is "easy": source				destination ------				----------- integral type		 non-decimal numeric integral type		 char Get a constant representing the cast from a non-integral type to another type.  If this is not an "easy" cast to perform, then just return this cast node. Here's what we think is "easy": source				destination ------				----------- non-integral type	 non-decimal numeric non-integral type	 char Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null. This is useful for bind time resolution of VTIs. RESOLVE: What do we do for primitives? Return whether or not this expression tree represents a constant expression. {@inheritDoc } Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set assignmentSemantics to true. Used by method calls for casting actual arguments set this to be a dataTypeScalarFunction This method gets called by the parser to indiciate that this CAST node has been generated by the parser. This means that we should use the collation info of the current compilation schmea for this node's collation setting. If this method does not get called, then it means that this CAST node has been an internally generated node and we should not touch the collation of this CAST node because it has been already set correctly by the class that generated this CAST node. Set the target type name if this is a cast to a UDT. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
builds a column list for the catalog abstract classes that should be implemented by subclasses. builds a tuple descriptor from a row Generate an index name based on the index number. Get the name for the heap conglomerate underlying this catalog. See getCanonicalTableUUID() for a description of canonical uuids. Get the UUID of the heap underlying this catalog. See getCanonicalTableUUID() for a description of canonical uuids. Get the UUID of the numbered index. See getCanonicalTableUUID() for a description of canonical uuids. Override the following methods in sub-classes if they have any indexes. Get the UUID of this catalog. This is the hard-coded uuid for this catalog that is generated for releases starting with Plato (1.3). Earlier releases generated their own UUIDs for system objectss on the fly. get the name of the catalog Get the Properties associated with creating the heap. Get the Properties associated with creating the specified index. Gets the DataValueFactory for this connection. Gets a ExecutionFactory Get the number of columns in the heap. Get the number of columns in the index for the specified index number. Return the column positions for a given index number Get the name for the specified index number. get the number of indexes on this catalog Get the index number for the primary key index on this catalog. Get the UUID factory Initialize info, including array of index names and array of index column counts. Called at constructor time. Return whether or not the specified index is unique. Return an empty row for this conglomerate. <p> Create an empty row for this conglomerate, in the format that would be used in a database that was created with, or hard upgraded to, the currently running version. That is, even if the database is only soft-upgraded, this method should return a row in the new format. </p> <p> This method is for use in code that creates the catalogs, or that upgrades the format of the catalogs to the newest version. Other code should call {@link #makeEmptyRow()}, which returns a row in the format used in the old database version if the database is soft-upgraded. </p> most subclasses should provide this method. One or two oddball cases in Replication and SysSTATEMENTSRowFactory don't. For those we call makeRow with the additional arguments.
Convert a Java String into bytes for a particular ccsid.  @param sourceString A Java String to convert. @return A new byte array representing the String in a particular ccsid. Convert a Java String into bytes for a particular ccsid. The String is converted into a buffer provided by the caller. Convert a byte array representing characters in a particular ccsid into a Java String.  @param sourceBytes An array of bytes to be converted. @return String A new Java String Object created after conversion. Convert a byte array representing characters in a particular ccsid into a Java String. Mind the fact that for certain encodings (e.g. UTF8), the offset and numToConvert actually represent characters and 1 character does not always equal to 1 byte. Returns the length in bytes for the String str using a particular ccsid.
Loggable methods Link the next alloc page into the page chain Return my format identifier.  method to support BeforeImageLogging debug Undoable methods Unlink the next alloc page from the page chain
Return the new configuration to use at setUp time. Most likely based upon the old configuration passed in.


Check that a procedure with a signature can not be added if the on-disk database version is 10.0. Test added by 10.1.
For pre-10.10 database, fileShouldExist will be false. For hard upgraded databases to 10.10, fileShouldExist will be true /////////////////////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Return the suite of tests to test the changes made in 10.10. Verify upgrade behavior DERBY-3398: removing DB2 float limits /////////////////////////////////////////////////////////////////////////////////  TESTS  ///////////////////////////////////////////////////////////////////////////////// Make sure that the following procedure(s) which are new to 10.10 are only available after hard-upgrade 1)invalidate stored statements SYCS_UTIL.SYSCS_INVALIDATE_STORED_STATEMENTS DERBY-5996(Create readme files (cautioning users against modifying database files) at database hard upgrade time) Simple test to make sure readme files are getting created Verify upgrade behavior for user-defined aggregates. Verify upgrade behavior for vararg routines. Check that the old DB2 limits are (still) enforced.
Assert that the statement text, when executed, raises a warning. Assert that the statement text, when executed, raises a warning. <p> Fill an ArrayList from an array. </p> Return the boolean value of a system property ////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  //////////////////////////////////////////////////////////////// Return the suite of tests to test the changes made in 10.11. Test how deferrable constraints work across upgrade and downgrade. Regression test for DERBY-532. Create a trigger in each upgrade phase and verify that they fire in the order in which they were created. DERBY-5866 changed how the trigger creation timestamp was stored (from local time zone to UTC), and we want to test that this change doesn't affect the trigger execution order when the triggers have been created with different versions. Test how dropping trigger dependencies works across upgrade and downgrade. Regression test for DERBY-2041. Test the addition of sequence generators to back identity columns Test the Lucene plugin Test the MERGE statement introduced by 10.11 ////////////////////////////////////////////////////////////////  TESTS  //////////////////////////////////////////////////////////////// Test that identity columns handle self-deadlock in soft-upgrade mode
////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  //////////////////////////////////////////////////////////////// Return the suite of tests to test the changes made in 10.12. Test the addition of support for adding identity columns with an ALTER TABLE statement. DERBY-3888. ////////////////////////////////////////////////////////////////  TESTS  //////////////////////////////////////////////////////////////// DERBY-6414(Incorrect handling when using an UPDATE to SET an identity column to DEFAULT) Starting Derby 10.12, we support updating an identity column using the keyword DEFAULT on 10.11 and higher dbs. A 10.11 database in soft upgrade mode can use this feature to update identity columns. Database versions earlier than that will not be able to use this feature. The reason for restricting the functionality to 10.11 and higher dbs is that starting 10.11, we started using sequence generator to create unique values for identity columns. Prior to that, we had really old code to generate unique values. In order to keep the code clean in 10.12, DERBY-6414 is fixed only for identity columns using sequence generator to create the new ids.
////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  //////////////////////////////////////////////////////////////// Return the suite of tests to test the changes made in 10.13. ////////////////////////////////////////////////////////////////  TESTS  //////////////////////////////////////////////////////////////// Test the addition of support for changing identity columns from ALWAYS to BY DEFAULT and vice versa via an ALTER TABLE statement. DERBY-6882. Test newly added system procedure to import table with header lines. DERBY-6892. Test newly added system procedure to import data with header lines. DERBY-6893.
////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  //////////////////////////////////////////////////////////////// Return the suite of tests to test the changes made in 10.13. ////////////////////////////////////////////////////////////////  TESTS  //////////////////////////////////////////////////////////////// Test the addition of support for changing the cycling behavior of identity columns. DERBY-6904.
Run the change encryption test against a encrypted database. Test that changing the encryption is only allowed if the database has been hard-upgraded. This test assumes it has its own single use database, which will not be booted by the general upgrade test setup. Run the change encryption test against a non-encrypted database. Test that changing the encryption is only allowed if the database has been hard-upgraded. This test assumes it has its own single use database, which will not be booted by the general upgrade test setup. This method lists the schema names and authorization ids in SYS.SCHEMAS table. This is to test that the owner of system schemas is changed from pseudo user "DBA" to the user invoking upgrade. Simple test of if GRANT/REVOKE statements are handled correctly in terms of being allowed in soft upgrade. In 10.2: We will write a ReusableRecordIdSequenceNumber in the header of a FileContaienr. Verify here that a 10.1 Database does not malfunction from this. 10.1 Databases should ignore the field. This method checks that some system routines are granted public access after a full upgrade. Triger (internal) VTI 10.2 - Check that a statement trigger created in 10.0 or 10.1 can be executed in 10.2 and that a statement trigger created in soft upgrade in 10.2 can be used in older releases. The VTI implementing statement triggers changed in 10.2 from implementations of ResultSet to implementations of PreparedStatement. See DERBY-438. The internal api for the re-written action statement remains the same. The re-compile of the trigger on version changes should automatically switch between the two implementations.
Verify whether the policy-reloading procedure exists. Verify that the policy-reloading procedure exists. Check if we can open the heap. <p> This test just does a simple select to verify that 10.3 heap conglomerate format id's are working right for all the various upgrade scenarios. Return the suite of tests to test the changes made in 10.3. Verify the compilation schema is nullable after upgrade to 10.3 or later. (See DERBY-630) Simple test to ensure new import/export procedures added in 10.3 are working on hard upgrade to 10.3 from previous derby versions. In 10.3: We will write a LogRecord with a different format that can also write negative values. Verify here that a 10.2 Database does not malfunction from this and 10.2 Databases will work with the old LogRecord format. Test that new format id for Heap is not used in soft upgrade. Ensure that the new policy-file-reloading procedure works after hard upgrade to 10.3 from previous derby versions. Call the policy reloading procedure.
Creates tables to test indexes during and after soft and hard upgrades Generates error messages and stores in a table. Return the suite of tests to test the changes made in 10.4. Tests if alter column works for a column in unique constraint. check if error message generated during soft upgrade is same as privious version. Test index created before upgrades to insure their behaviour remains same after soft and hard upgrades.  This is an index test and does not apply to constraint behavior. Tests whether or not indexes are exibiting their expected behaviour. Check that even though we have set schema to a user schema, the metadata queries get run with compilation schema as SYS. DERBY-2946 Test added for 10.4. Test that routine parameters and return types are handled correctly with 10.4 creating a procedure in soft-upgrade. 10.4 simplified the stored format of the types by ensuring the catalog type was written. See DERBY-2917 for details. Check that you must be hard-upgraded to 10.4 or later in order to declare table functions. Tests Unique constraint in soft and hard upgrade enviornment. Under soft upgrade environment creation of unique constrant over nullable columns and setting columns from unique constraint to null should fail. Also the constraint created during soft upgrade run should work fine when running under previous version. Verifies error messages priviously generated.
<p> Run a statement. If the sqlstate is not null, then we expect that error. </p> Return the suite of tests to test the changes made in 10.5. Test that the DETERMINISTIC keyword is not allowed until you hard-upgrade to 10.5. Test that generation clauses are not allowed until you hard-upgrade to 10.5. Check that when hard-upgraded to 10.5 or later SQL roles can be declared if DB has sqlAuthorization. Check that you must be hard-upgraded to 10.5 or later in order to use SQL roles Make sure that SYSCS_UTIL.SYSCS_UPDATE_STATISTICS can only be run in Derby 10.5 and higher. DERBY-269 Test added for 10.5.
We would like to just cast the alias descriptor to RoutineAliasDescriptor. However, this doesn't work if we are running on an old version because the descriptor comes from a different class loader. We use reflection to get the information we need. Return the suite of tests to test the changes made in 10.6. Verify that we don't enable the configurable hash authentication scheme when we upgrade a database. See DERBY-4483. Make sure that SYSIBM.CLOBGETSUBSTRING has the correct return value. See https://issues.apache.org/jira/browse/DERBY-4214 Make sure that SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE  has the correct permissons granted to it. See https://issues.apache.org/jira/browse/DERBY-4215 Make sure that SYSCS_UTIL.SYSCS_SET_XPLAIN_STYLE can only be run in Derby 10.5 and higher. DERBY-2487 Test added for 10.5. Make sure that you can only create UDTs in a hard-upgraded database. See https://issues.apache.org/jira/browse/DERBY-651 Vet the permissions on SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE. There should be only one permissions tuple for this system procedure and the grantor should be APP.
Create the table and trigger necessary for ALTER TABLE DROP COLUMN test Get the names of all supported types, as reported by {@code DatabaseMetaData.getTypeInfo()}. ALTER TABLE DROP COLUMN in not detected the trigger column dependency for columns being used through the REFERENCING clause for triggers created prior to 10.7 release /////////////////////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Return the suite of tests to test the changes made in 10.7. This test creates 2 kinds of triggers in old release for each of the three phase of upgrade. The triggers are of following 2 types 1)trigger action using columns available through the REFERENCING clause. 2)trigger action using columns without the REFERENCING clause. For both kinds of triggers, there is test case which drops the column being used in the trigger action column. In all three modes of upgrade, soft upgrade, post soft upgrade, and hard upgrade, ALTER TABLE DROP COLUMN will detect the trigger dependency. /////////////////////////////////////////////////////////////////////////////////  TESTS  ///////////////////////////////////////////////////////////////////////////////// Make sure that that database is at level 10.7 in order to enjoy extended support for the BOOLEAN datatype. Make sure that that database is at level 10.7 in order to enjoy routines with specified EXTERNAL SECURITY INVOKER or DEFINER. Make sure that DERBY-1482 changes do not break backward compatibility
assert that fName has the expected shape of a jar file in the database (version &gt;= 10.9). Initialize a pattern corresponding to: <p/> &lt;Derby uuid string&gt;[.]jar[.]G[0-9]+ <p/> where: <p/> &lt;Derby uuid string&gt; has the form hhhhhhhh-hhhh-hhhh-hhhh-hhhhhhhhhhhh <p/> where <em>h</em> id a lower case hex digit. Poor man's regexp matcher: can match patterns of type below, where start "^" and end "$" is implied: must match whole string. <p/> reg.exp: ( '[' &lt;fromchar&gt;-&lt;tochar&gt; ] '+'? ']' | &lt;char&gt; '+'? )* Set the passwords for all users specified in {@code USERS}. Return the suite of tests to test the changes made in 10.7. Make sure builtin authentication doesn't use a hash scheme that's not supported by the old version until the database has been hard upgraded. See DERBY-4483 and DERBY-5539. Verifies the behavior of the update statistics code when faced with "disposable statistics entries". <p> A disposable statistics entry is a row in SYS.SYSSTATISTICS that has been orphaned (see DERBY-5681) or it is on longer needed by the Derby optimizer (due to internal changes/improvements). <p> This test expects different things based on the phase: <dl> <dt>create</dt> <dd>- run statements that will cause disposable statistics entries to be created</dd> <dt>soft upgrade</dt> <dd>- run the new update statistics code, expecting it to leave the disposable statistics intact</dd> <dt>downgrade</dt> <dd>- verify that the relevant statistics are present</dd> <dt>hard upgrade</dt> <dd>- run the new update statistics code, expecting it to get rid of the disposable statistics</dd> </dl> Verifies that an orphaned statistics entry can be dropped by running the {@code SYSCS_DROP_STATISTICS} system procedure. <p> Relevant JIRAs: <ul> <li>DERBY-4115: Provide a way to drop statistics information</li> <li>DERBY-5681: When a foreign key constraint on a table is dropped, the associated statistics row for the conglomerate</li> </ul> <p> DERBY-5702(Creating a foreign key constraint does not automatically create a statistics row if foreign key constraint will share a backing index created for a primay key) is causing a problem for us to test the hanging statistics row with 10.4 and prior releases. Following test relies on having hanging statistics rows which should have been dropped when the constraint owing it was dropped. The test then goes ahead and uses the new drop statisitcs procedure to drop the hanging statistics rows. But because of DERBY-5702, when a constraint is added which will reuse an existing backing index, no statistics row is created for that constraint unless a user were to say use an update statistics stored procedure to create the statistics for that constraint. And later when that constraint is dropped, we will find that because of DERBY-5681, the statistics row never gets dropped. But update statistics stored procedure was not introduced up until 10.5 and because of that, we can't really test for hanging index created through constraints sharing the same backing index prior to 10.5 /////////////////////////////////////////////////////////////////////////////////  TESTS  ///////////////////////////////////////////////////////////////////////////////// Make sure that the drop statistics procedure only appears after hard-upgrade. For 10.9 and later storage of jar files changed. DERBY-5357. Make sure that the catalogs and procedures for NATIVE authentication only appear after hard-upgrade. Make sure that NATIVE LOCAL authentication can't be turned on before hard-upgrade. Test the changes introduced to fix correctness problems with sequences. Verify that all users specified in {@code USERS} can connect to the database. Verify that all passwords for the users in {@code USERS} are stored as expected. Raise an assert failure on mismatch.
Return the number of characters in the alphabet. Get a CJK subset alphabet. Returns a clone of the alphabet. Return the name of the alphabet. Get a modern latin lowercase alphabet. Return the next char. Return the next char as an <code>integer</code>. Compute the next character to read after reading the specified number of characters. Besides from returning the index, the internal state of the alphabet is updated. Reset the alphabet, the next character returned will be the first character in the alphabet. Get an alphabet consisting of a single character. Get a Tamil alphabet Returns a friendlier textual representation of the alphabet.
This generates the proper constant.  It is implemented by every specific constant node (e.g. IntConstantNode). Return the length public int	getLength() throws StandardException { return value.getLength(); } Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null. This is useful for bind time resolution of VTIs. RESOLVE: What do we do for primitives? Return the value from this CharConstantNode
Returns the next character that marks the beginning of the next token. All characters must remain in the buffer between two successive calls to this method to implement backup correctly. The lexer calls this function to indicate that it is done with the stream and hence implementations can free any resources held by this class. Again, the body of this function can be just empty and it will not affect the lexer's operation. Returns a string made up of characters from the marked token beginning to the current buffer position. Implementations have the choice of returning anything that they want to. For example, for efficiency, one might decide to just return null, which is a valid implementation. Returns an array of characters that make up the suffix of length 'len' for the currently matched token. This is used to build up the matched string for use in actions in the case of MORE. A simple and inefficient implementation of this is as follows : { String t = GetImage(); return t.substring(t.length() - len, t.length()).toCharArray(); } These methods were added to support re-initialization of CharStreams Backs up the input stream by amount steps. Lexer calls this method if it had already read some characters, but could not use them to match a (longer) token. So, they will be used again as the prefix of the next token and it is the implemetation's responsibility to do this right. Returns the column number of the first character for current token (being matched after the last call to BeginTOken). Returns the line number of the first character for current token (being matched after the last call to BeginTOken). This method was added to support ability to get the input between two tokens. Returns the column position of the character last read. Returns the column number of the last character for current token (being matched after the last call to BeginTOken). Returns the line number of the last character for current token (being matched after the last call to BeginTOken). This method was added to support ability to get the input between two tokens. Returns the line number of the character last read. Returns the next character from the selected input.  The method of selecting the input is the responsibility of the class implementing this interface.  Can throw any java.io.IOException.
A byte count is expected. Generates the header for the specified length and writes it into the provided buffer, starting at the specified offset. Generates the header for the specified length. Returns the maximum header length. Writes a Derby-specific end-of-stream marker to the buffer for a stream of the specified byte length, if required. Writes a Derby-specific end-of-stream marker to the destination stream for the specified byte length, if required. Write the EOF marker to a byte array and return the EOF marker's length Write the EOF marker to an Object stream  and return the EOF marker's length
Tell whether this type (char) is compatible with the given type. Tell whether this type (char) can be converted to the given type.    Push the collation type if it is not COLLATION_TYPE_UCS_BASIC. Tell whether this type (char) can be stored into from the given type.

Returns the current character position. Returns the first index of the described stream that contains real data. <p> The information is typically used to filter out meta data at the head of the stream, and to correctly reset the stream. Returns the imposed maximum character length on the described stream. <p> The default value is {@code Long.MAX_VALUE}. Returns the associated positioned stream, if the stream is position aware. Returns the associated stream. Tells if the described stream should be buffered or not. <p> Some of the reasons a stream should not be buffered at this level, are the stream is already buffered, or it serves bytes directly from a byte array in memory. Tells if the described stream is aware of its own position, and that it can reposition itself on request.
Get the UUID of the backing index, if one exists. Get the text of the constraint. (Only non-null/meaningful for check constraints.) Gets an identifier telling what type of descriptor it is (UNIQUE, PRIMARY KEY, FOREIGN KEY, CHECK). Get the referenced columns as an int[] of column ids. Get the ReferencedColumns. Does this constraint have a backing index? Does this constraint need to fire on this type of DML?  For a check constraint, all inserts, and appropriate updates Set the ReferencedColumns; used in drop column Convert the CheckConstraintDescriptor to a String.
public static void testIllegalDBCreate() throws Exception { System.out.println("Security Manager Test Starts"); // Initialize JavaCommonClient Driver. Class.forName("com.ibm.db2.jcc.DB2Driver"); Connection conn = null; // This tries to create a database that is not allowed. // To repro bug 6021 change to some disallowed file system. // There are two problems with this test. // 1) if set to a different file system than the test runs, //    (e.g. D:/wombat), a null pointer is thrown. // 2) If just set to a disallowed directory on the same file system. //    We seem to be able to create the database. // Ideally this test should attempt to create the database // ../wombat;create=true and get the security exception. String hostName = TestUtil.getHostName(); String databaseURL; if (hostName.equals("localhost")) { databaseURL = TestUtil.getJdbcUrlPrefix() + hostName + "/\"D:/wombat;create=true\""; } else { databaseURL = TestUtil.getJdbcUrlPrefix() + hostName + "wombat"; } //System.out.println(databaseURL); java.util.Properties properties = new java.util.Properties(); properties.put ("user", "cs"); properties.put ("password", "cs"); try { conn = DriverManager.getConnection(databaseURL, properties); System.out.println("FAILED: Expected Security Exception"); } catch (SQLException se) { System.out.println("Expected Security Exception"); JDBCTestDisplayUtil.ShowCommonSQLException(System.out, se); } } Try to set a property in a stored procedure for which there is not adequate permissions in the policy file
Loggable methods Nothing to do unless we are rollforward recovery; Redoing of checkpoints during rollforward recovery allows us to restart the  roll-forward recovery from the last checkpoint redone during rollforward recovery, if we happen to crash during the roll-forward recovery process. Another reason why we need to do this is dropped table stub files removed at checkpoint because the containerids may have been reused after a checkpoint if the system was rebooted. the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. Checkpoint has no optional data to write out Return my format identifier. Checkpoint is a raw store operation Checkpoint does not need to be redone unless we are doing rollforward recovery. Access attributes of the checkpoint record Checkpoint has not resource to release DEBUG: Print self.
Return suite of tests that checks the row counts for all the tables in the Order Entry bechmark. Consistency checks per Section 3.3.2 of TPC-C spec  Section 3.3.2.1 of TPC-C specification. Entries in the WAREHOUSE and DISTRICT tables must satisfy the relationship: W_YTD = sum(D_YTD) for each warehouse defined by (W_ID = D_W_ID). Section 3.3.2.2 Consistency Condition 2 (TPC-C spec) Entries in the DISTRICT, ORDER, and NEW-ORDER tables must satisfy the relationship: D_NEXT_O_ID - 1 = max(O_ID) = max(NO_O_ID) for each district defined by (D_W_ID = O_W_ID = NO_W_ID) and (D_ID = O_D_ID = NO_D_ID). This condition does not apply to the NEW-ORDER table for any districts which have no outstanding new orders (i.e., the number of rows is zero). 3.3.2.3 Consistency Condition 3 Entries in the NEW-ORDER table must satisfy the relationship: max(NO_O_ID) - min(NO_O_ID) + 1 = [number of rows in the NEW-ORDER table for this district] for each district defined by NO_W_ID and NO_D_ID. This condition does not apply to any districts which have no outstanding new orders (i.e., the number of rows is zero). 3.3.2.4 Consistency Condition 4 Entries in the ORDER and ORDER-LINE tables must satisfy the relationship: sum(O_OL_CNT) = [number of rows in the ORDER-LINE table for this district] for each district defined by (O_W_ID = OL_W_ID) and (O_D_ID = OL_D_ID). Test cardinality of CUSTOMER table Test cardinality of DISTRICT table Test cardinality of HISTORY table Test cardinality of ITEM table Test cardinality of NEWORDERS table Test cardinality of ORDERLINE table Test cardinality of ORDERS table Test cardinality of STOCK table Test cardinality of WAREHOUSE table
Loggable methods Nothing to do for the checksum log record because it does need to be applied during redo. Access attributes of the checksum log record the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. Checksum has no optional data to write out Return my format identifier. Checksum is a raw store operation Checksum does not need to be redone, it is used to just verify that log records are written completely. Checksum has no resources to release reset the checksum DEBUG: Print self. update the checksum
Returns a CipherProvider which is the encryption or decryption engine. Verify the external encryption key. Throws exception if unable to verify that the encryption key is the same as that used during database creation or if there are any problems when trying to do the verification process.
Create an instance of the cipher factory.
Decrypt data - use only with Cipher that has been initialized with CipherFactory.DECRYPT. Encrypt data - use only with Cipher that has been initialized with CipherFactory.ENCRYPT. Returns the encryption block size used during creation of the encrypted database
add a field to this class. Fields cannot be initialized here, they must be initialized in the static initializer code (static fields) or in the constructors. <p> Methods are added when they are created with the JavaFactory. At the time the class is completed and bytecode generated, if there are no constructors then the default no-arg constructor will be defined. the class's qualified name Fully create the bytecode and load the class using the ClassBuilder's ClassFactory. the class's unqualified name a constructor. Once it is created, parameters, thrown exceptions, statements, and local variable declarations must be added to it. It is put into its defining class when it is created. <verbatim> Java: #modifiers #className() {} // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. // className is taken from definingClass.name() </verbatim> <p> This is used to start a constructor as well; pass in null for the returnType when used in that manner. <p> a method. Once it is created, parameters, thrown exceptions, statements, and local variable declarations must be added to it. It is put into its defining class when it is created. <verbatim> Java: #modifiers #returnType #methodName() {} // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. </verbatim> <p> This is used to start a constructor as well; pass in null for the returnType when used in that manner. a method with parameters. Once it is created, thrown exceptions, statements, and local variable declarations must be added to it. It is put into its defining class when it is created. <verbatim> Java: #modifiers #returnType #methodName() {} // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. </verbatim> <p> This is used to start a constructor as well; pass in null for the returnType when used in that manner.
uses cpt and inner
Return a ClassInspector object Return the in-memory "version" of the class manager. The version is bumped everytime the classes are re-loaded. Was the passed in class loaded by a ClassManager. Load an application class, or a class that is potentially an application class. Load an application class, or a class that is potentially an application class. Add a generated class to the class manager's class repository. Notify the class manager that the classpath has been modified. Notify the class manager that a jar file has been modified.
Handle any errors. Only work here is to pop myself on a session or greater severity error. Get the mechanism to rad jar files. The ClassFactory may keep the JarReader reference from the first class load. Get the lock compatibility space to use for the transactional nature of the class loading lock. Used when the classpath changes or a database jar file is installed, removed or replaced. Get the set of properties stored with this service.
Get a reference to the data array the class data is being built in. No copy is made. Throw an ClassFormatError if a limit of the Java class file format is reached.
Add a class entry to the pool.      Add an entry, but only if it doesn't exist. public ClassMember getMemberReference(String fullyQualifiedClassName, String simpleName, String descriptor) { int classIndex; if (fullyQualifiedClassName == null) classIndex = this_class; else classIndex = constantPool.findClass(fullyQualifiedClassName); if (classIndex < 0) return null; int nameAndTypeIndex = constantPool.findNameAndType(simpleName, descriptor); if (nameAndTypeIndex < 0) return null; return constantPool.findReference(classIndex, nameAndTypeIndex); } * Public methods from ClassRead * Implementation specific methods. * Methods related to Constant Pool Table Generic add entry to constant pool. Includes the logic for an entry to occupy more than one slot (e.g. long). Add an extra UTF8 into the pool  Add an index reference. *	Public methods from ClassHolder   Add a name and type entry Add a string entry  Add a UTF8 into the pool and return the index to it. Return the class name for an index to a CONSTANT_Class_info. * Workhorse method.  Convert to internal format. @param descriptor True if converting to descriptor format, false if converting to class name format. * * Lifted from BCClass.java. * * Returns the result string. Convert a class name to the internal VM class name format. See sections 4.3.2, 4.4.1 of the vm spec. The normal leading 'L' and trailing ';' are left off of objects.  This is intended primarily for the class manager. <p> An example of a conversion would be java.lang.Double[] to "[Ljava/lang/Double;". <BR> java.lang.Double would be converted to "java/lang/Double" <BR> Note that for array types the result of convertToInternalClassName() and convertToInternalDescriptor() are identical. @param the external name (cannot be null) @return the internal string Convert a class name to internal JVM descriptor format. See sections 4.3.2 of the vm spec. <p> An example of a conversion would be "java.lang.Double[]" to "[Ljava/lang/Double;". <BR> java.lang.Double would be converted to "Ljava/lang/Double;" <BR> Note that for array types the result of convertToInternalClassName() and convertToInternalDescriptor() are identical. @param the external name (cannot be null) @return the internal string Find a class descriptor (section 4.4.1) and return its index, returns -1 if not found. public ClassMember findReference(int classIndex, int nameAndTypeIndex) { CONSTANT_Index_info item = findIndexEntry(VMDescriptor.CONSTANT_Methodref, classIndex, nameAndTypeIndex); if (item == null) { item = findIndexEntry(VMDescriptor.CONSTANT_InterfaceMethodref, classIndex, nameAndTypeIndex); if (item == null) { item = findIndexEntry(VMDescriptor.CONSTANT_Fieldref, classIndex, nameAndTypeIndex); if (item == null) return null; } } return new ReferenceMember(this, item); } Find a name and type descriptor (section 4.4.6) and return it's index. <p> returns -1 if not found. * Methods to find specific types of constant pool entries. In these methods we try to avoid using the ConstantPoolEntry.matchValue() as that requires creating a new object for the search. The matchValue() call is really intended for when objects are being added to the constant pool. Return the index of a UTF entry or -1 if it doesn't exist. get the class name of a Class given the index of its CONSTANT_Class_info entry in the Constant Pool.  * Methods to convert indexes to constant pool entries and vice-versa. *	Public methods from ClassHolder. Convert the object representation of the class into its class file format. * Public methods from ClassMember   Determine whether the class descriptor string is in external format or not.  Assumes that to be in external format means it must have a '.' or end in an ']'. @param className	the name of the class to check @return true/false A helper to build a type description based on a built-in type and an array arity. A helper to build a type description based on a Java class and an array arity. If descriptor is true create a descriptor according to section 4.3.2 of the vm spec. If false create a class name according to sections 4.3.2 and 4.4.1 of the vm spec. get a string (UTF) given a name_index into the constant pool
Return the name of this class. Return the class object for this class. Create an instance of this class. Assumes that clazz has already been initialized. Optimizes Class.newInstance() by caching and using the no-arg Constructor directly. Class.newInstance() looks up the constructor each time.

Does the named class exist, and is it accessible? Given a generic type, add its parameter types to an evolving map of resolved types. Some of the resolved types may be generic type variables which will need further resolution from other generic types. Is one named class assignable to another named class or interface? Get the bounds for a single type variable. Can we convert a fromClass to toClass. "mixTypes" is a flag to show if object/primitive type conversion is possible; this is used for comparing two candidate methods in the second pass of the two pass method resolution. Find a public constructor that implements a given signature. The signature is given using the full Java class names of the types. <BR> A untyped null parameter is indicated by passing in an empty string ("") as its class name. Find a public field  for a class. This follows the semantics of the java compiler for locating a field. This means if a field fieldName exists in the class with package, private or protected then an error is raised. Even if the field hides a field fieldName in a super-class/super--interface. See the JVM spec on fields. Find a public method that implements a given signature. The signature is given using the full Java class names of the types. <BR> A untyped null parameter is indicated by passing in an empty string ("") as its class name. <BR> If receiverType represents an interface then the methods of java.lang.Object are included in the candidate list. <BR> If the caller is simply checking to see that a public method with the specified name exists, regardless of the signature, exists, then the caller should pass in a null for parmTypes.  (This is useful for checking the validity of a method alias when creating one.) <BR> We use a two-pass algorithm to resolve methods.  In the first pass, we use all "object" types to try to match a method.  If this fails, in the second pass, an array of "primitive" types (if the parameter has one, otherwise the same object type is used) is passed in, as well as the "object" type array.  For each parameter of a method, we try to match it against either the "object" type, or the "primitive" type.  Of all the qualified candidate methods found, we choose the closest one to the input parameter types.  This involves comparing methods whose parameters are mixed "object" and "primitive" types in the second pass.  This is eventually handled in classConvertableFromTo. ///////////////////////////////////////////////////////////////////////  MINIONS FOR getTypeBounds()  /////////////////////////////////////////////////////////////////////// Get the type bounds for all of the type variables of the given parameterized type. Get (load) the class for the given class name. This method converts any java language class name into a Class object. This includes cases like String[] and primitive types. This will attempt to load the class from the application set. Get the declaring class for a method. Given an implementation of a parameterized interface, return the actual types of the interface type variables. May return null or an array of nulls if type resolution fails. Given a map of resolved types, compose them together in order to resolve the actual concrete types that are plugged into the parameterized type. Get the parameter types for a method described by a Member as a String[]. Get the raw type of a type bound. Given an inheritance chain of types, stretching from a superclass down to a terminal concrete class, construct a map of generic types to their resolved types. Get the Java name of the return type from a Member representing a method or the type of a Member representing a field. Given an implementation of a parameterized interface, return the bounds on the type variables. May return null if type resolution fails. ///////////////////////////////////////////////////////////////////////  MINIONS FOR getGenericParameterTypes()  /////////////////////////////////////////////////////////////////////// Construct an inheritance chain of types stretching from a supertype down to a concrete implementation. Is the given object an instance of the named class? Return true if the method or constructor supports varargs. Determine whether a type is a Java primitive, like int or boolean Translate a JVM-style type descriptor to a Java-language-style type name. Tricky function to resolve a method.  If primParamClasses is null we know it's first pass.  First pass try to match as all "object" types, second pass try to match any combination of "object" and "primitive" types.  Find the closest match among all the qualified candidates.  If there's a tie, it's ambiguous. The preceding paragraph is a bit misleading. As of release 10.4, the second pass did not consider arbitrary combinations of primitive and wrapper types. This is because the first pass removed from consideration candidates which would be allowed under ANSI rules. As a fix for bug DERBY-3652, we now allow primitive and wrapper type matches during the first pass. The ANSI rules are documented in DERBY-3652. Can we convert a signature from fromTypes(primFromTypes) to toTypes. "mixTypes" is a flag to show if object/primitive type conversion is possible; this is used for comparing two candidate methods in the second pass of the two pass method resolution.
Return an Enumeration of all referenced classes * Methods to investigate this class Return an Enumeration of all Member References Enumeration getMemberReferences() { return new ReferenceEnumeration(this, elements()); } * Methods to modify the class. remove all atttributes that are not essential
* Public methods from ClassMember *	 ----

Estimate the static space taken up by a class instance. Save the coefficients in a catalog. end of estimateAndCatalogBase  Estimate the static space taken up by the fields of a class. This includes the space taken up by by references (the pointer) but not by the referenced object. So the estimated size of an array field does not depend on the size of the array. Similarly the size of an object (reference) field does not depend on the object. End of estimateBase Estimate the static space taken up by a class instance from cataloged coefficients. end of estimateBaseFromCatalog Estimate the static space taken up by a class instance given the coefficients returned by getSizeCoefficients. end of estimateBaseFromCoefficients Estimate the size of a Hashtable entry. In Java 1.2 we can use Map.entry, but this is not available in earlier versions of Java. Estimate the size of a string. Tries to determine the reference size in bytes by checking whether the VM we're running in is 32 or 64 bit by looking at the system properties.  Get the estimate of the size of an object reference. The estimate of the size of a class instance depends on whether the JVM uses 32 or 64 bit addresses, that is it depends on the size of an object reference. It is a linear function of the size of a reference, e.g. 24 + 5*r where r is the size of a reference (usually 4 or 8 bytes). This method returns the coefficients of the linear function, e.g. {24, 5} in the above example. end of getSizeCoefficients Attempts to read the specified system property. do not try to use the catalog.
Get the singleton {@code ClassSizeCatalog} instance.
end of addClass end of crawl end of main
/////////////////////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Check whether this platform supports closing a {@code URLClassLoader}.
Perform the work for a single iteration of the test (typically a single transaction). Initialize this client (typically prepare the statements needed in the {@code doWork()} method). Print a report from the test run.
-------------------------- JDBC 4.0 ----------------------------------- This method frees the <code>Blob</code> object and releases the resources that it holds. The object is invalid once the <code>free</code> method is called. If <code>free</code> is called multiple times, the subsequent calls to <code>free</code> are treated as a no-op. Returns an <code>InputStream</code> object that contains a partial <code> Blob</code> value, starting  with the byte specified by pos, which is length bytes in length. Returns as an array of bytes part or all of the <code>BLOB</code> value that this <code>Blob</code> object designates.  The byte array contains up to <code>length</code> consecutive bytes starting at position <code>pos</code>. The starting position must be between 1 and the length of the BLOB plus 1. This allows for zero-length BLOB values, from which only zero-length byte arrays can be returned. If a larger length is requested than there are bytes available, characters from the start position to the end of the BLOB are returned. Get the length in bytes of the <code>Blob</code> value represented by this locator based <Blob> object. A stored procedure call will be made to get it from the server. ------------------ Material layer event callback methods ------------------- ---------------------------- helper methods -------------------------------- precondition: binaryString_ is long enough for the comparison ---------------------------jdbc 2------------------------------------------ Materialize the stream used for input to the database. -------------------------- JDBC 3.0 -----------------------------------
---------------------------entry points------------------------------------- ----------------------------overrides---------------------------------- no result sets returned  --------------------------------getter methods------------------------------ -------------------------- JDBC 4.0 methods -------------------------------- also used by SQLCA ----------------------------helper methods---------------------------------- Returns the name of the java.sql interface implemented by this class. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// also used by SQLCA ---------------------constructors/finalizer--------------------------------- Derby ignores the typeName argument because UDTs don't need it --------------------------JDBC 3.0------------------------------------------ also used by Sqlca

---------------------------- jdbc 4.0 ------------------------------------- This method frees the <code>Clob</code> object and releases the resources the resources that it holds.  The object is invalid once the <code>free</code> method is called. If <code>free</code> is called multiple times, the subsequent calls to <code>free</code> are treated as a no-op. Returns a <code>Reader</code> object that contains a partial <code>Clob</code> value, starting with the character specified by pos, which is length characters in length. --------------------------------------------------------------------- Methods used in the locator implementation. ---------------------------------------------------------------------- Get the length in bytes of the <code>Clob</code> value represented by this locator based <code>Clob</code> object. A stored procedure call will be made to get it from the server. Returns a copy of the specified substring in the <code>CLOB</code> value designated by this <code>ClientClob</code> object. The substring begins at position <code>pos</code> and has up to <code>length</code> consecutive characters. The starting position must be between 1 and the length of the CLOB plus 1. This allows for zero-length CLOB values, from which only zero-length substrings can be returned. If a larger length is requested than there are characters available, characters to the end of the CLOB are returned. Return the length of the equivalent UTF-8 string precondition: string_ is not null and dataType_ includes STRING ----------------------------helper methods---------------------------------- ---------------------------jdbc 2------------------------------------------ Create another method lengthX for internal calls Materialize the stream used for input to the database. Reinitialize the value of this CLOB. This is legacy code, only used when talking to servers that don't support locators. The StringBufferInputStream class is deprecated, but we don't care too much since this code is only for talking to very old servers. Suppress the deprecation warnings for now. ---------------------------- jdbc 3.0 -----------------------------------
Returns the path of the JUnit classes. Runs the client compatibility test suite with the client driver in a separate JVM. <p> The server is expected to be running already.

--------------------------------------------------------------------------- Driver-specific determination if local COMMIT/ROLLBACK is allowed; primary usage is distinction between local and global trans. envs.; Begin aborting the connection -------------------------------helper methods------------------------------- Check if the transaction is in progress and the connection cannot be closed. An untraced version of clearWarnings() Just like closeX except the socket is not pulled. Physical resources are not closed. Close physical socket or attachment even if connection is marked close. Used by ClientPooledConnection.close(). This is a no-op if the connection is already closed. Occurs autonomously Rollback the UnitOfWorkListener specifically. Called by Connection.close(), NetConnection.errorRollbackDisconnect(). The Agent's client-side resources associated with database connection are reclaimed (eg. socket). And this connection and all associated statements and result sets are marked closed. This is a client-side only operation. This method will only throw an exception if the agent cannot be closed. ----------------------- abstract box car and callback methods --------------------- All callbacks must be client-side only operations. Sets the default isolation level of the connection upon connection initialization. <p> Note that depending on the server version, the default isolation value may not be piggy-backed on the initialization flow. In that case, the default is assumed / hardcoded to be READ_COMMITTED. Sets the current schema upon connection initialization. A callback for certain non-fatal exceptions that occur when parsing error replies. This is a client-side only operation. This method will only throw an exception on bug check. <br>NOTE:</br>The following comments are valid for the changes done as part of implementing statement caching only (see DERBY-3313 and linked issues). <p> We don't reset the isolation level to unknown unconditionally, as this forces us to go to the server all the time. Since the value should now be valid (DERBY-3192), we check if it has been changed from the default. Rollback the specific UnitOfWorkListener. Constructs an object that implements the <code>Blob</code> interface. The object returned initially contains no data. Constructs an object that implements the <code>Clob</code> interface. The object returned initially contains no data. ---------------------------jdbc 1------------------------------------------ --------------------------JDBC 2.0----------------------------- Per jdbc spec, when a result set type is unsupported, we downgrade and issue a warning rather than to throw an exception. Users are advised to call the method close() on Statement and Connection objects when they are done with them. However, some users will forget, and some code may get killed before it can close these objects. Therefore, if JDBC drivers have state associated with JDBC objects that need to get explicitly cleared up, they should provide finalize methods to take care of them. The garbage collector will call these finalize methods when the objects are found to be garbage, and this will give the driver a chance to close (or otherwise clean up) the objects. Note, however, that there is no guarantee that the garbage collector will ever run. If that is the case, the finalizers will not be called.  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  precondition: autoCommit_ is true Even if we're not in a transaction, all open result sets will be closed. So we could probably just return if we're not in a transaction using the following code: if (!this.inUnitOfWork) return; But we'll just play it safe, and blindly flow the rollback. We won't try to be "too smart", if the user requests a rollback, we'll flow a rollback, regardless of whether or not we're in a unit of work or in auto-commit mode.  Per JDBC specification (see javadoc for Connection.rollback()): "This method should be used only when auto-commit mode has been disabled." However, rather than trying to be too smart, we'll just flow the rollback anyway before throwing an exception. As a side-effect of invoking rollback() in auto-commit mode, we'll close all open result sets on this connection in the rollbackEvent().  Returns the current schema (the schema that would be used for compilation. This is not part of the java.sql.Connection interface, and is only intended for use with statement caching. ====================================================================== Advanced features: //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Get the name of the current schema. get the server version Returns the ID of the active transaction for this connection. Return the holdabilty for the Connection. Matches the embedded driver in the restriction that while in a global (XA) transaction the holdability is CLOSE_CURSORS_AT_COMMIT. Otherwise return the holdability set by the user. For jdbc 2 connections Return true if the connection is aborting Return true if the physical connection is still open. Might be logically closed but available for reuse. Get handle to the object that contains prepared statements for calling locator procedures for this connection.  The object will be created on the first invocation. An example of how to call a stored procedure via this method: <pre> <code> connection.locatorProcedureCall().blobReleaseLocator(locator); </code> </pre> --------------------Abstract material factory methods----------------- For internal use only.  Use by updatable result set code. used by DBMD precondition: autoCommit_ is true can this only be called by the PooledConnection can this be called on a closed connection can this be called in a unit of work can this be called from within a stored procedure  This is a callback method, called by subsystem - NetConnection not sure if holding on to cursorAttributesToSendOnPrepare and restoring it is the right thing to do here... because if property on the dataSource changes, we may have to send different attributes, i.e. SENSITIVE DYNAMIC, instead of SENSITIVE STATIC. not sure if holding on to cursorAttributesToSendOnPrepare and restoring it is the right thing to do here... because if property on the dataSource changes, we may have to send different attributes, i.e. SENSITIVE DYNAMIC, instead of SENSITIVE STATIC. Checks whether the server supports locators for large objects. Note that even though the server supports LOB locators, the database that is accessed through the server may not have the necessary stored procedures to support LOB locators (e.g., because the database is soft upgraded from an earlier version). Return true if the server supports nanoseconds in timestamps --------------------------JDBC 3.0----------------------------- Set the default schema for the Connection. Set the transaction isolation level as specified. <p> If this method is called during a transaction, the result is implementation-defined. <p> Information about Derby specific isolation level handling: <ul> <li>REPEATABLE_READ = JDBC: TRANSACTION_SERIALIZABLE, DERBY: RR, PROTOCOL: repeatable read</li> <li>READ_STABILITY = JDBC: TRANSACTION_REPEATABLE_READ, DERBY: RS, PROTOCOL: All</li> <li>CURSOR_STABILITY = JDBC: TRANSACTION_READ_COMMITTED, DERBY: CS, PROTOCOL: Cursor stability</li> <li>UNCOMMITTED_READ = JDBC: TRANSACTION_READ_UNCOMMITTED, DERBY: UR, PROTOCOL: Change</li> <li>NO_COMMIT = JDBC: TRANSACTION_NONE, DERBY: NC, PROTOCOL: No commit</li> </ul> @GuardedBy("this") Finds out if the underlaying database connection supports session data caching. Check if there are uncommitted operations in the current transaction that prevent us from closing the connection. Translates the isolation level from a SQL string to the JDBC int value precondition: autoCommit_ is true
Returns the maximum number of JDBC prepared statements a connection is allowed to cache. ---------------------------interface methods------------------------------- Attempt to establish a physical database connection that can be used as a pooled connection. Standard method that establishes the initial physical connection using CPDS properties. Internally used method. Read an object from the ObjectInputStream. <p> This implementation differs from the default one by initiating state validation of the object created. Specifies the maximum size of the statement cache. Make sure the state of the de-serialized object is valid.

Returns the maximum number of JDBC prepared statements a connection is allowed to cache. Specifies the maximum size of the statement cache.
Add Java Bean properties to the reference using StringRefAddr for each property. List of bean properties is defined from the public getXXX() methods on this object that take no arguments and return short, int, boolean or String. The StringRefAddr has a key of the Java bean property name, converted from the method name. E.g. traceDirectory for traceDirectory. ------------------------ Referenceable interface methods -----------------------------

Reconstructs a Derby client-driver data source object from a JNDI data source reference. <p> The {@code getObjectInstance} method is passed a reference that corresponds to the object being retrieved as its first parameter. The other parameters are optional in the case of JDBC data source objects. The object factory should use the information contained in the reference to reconstruct the data source. If for some reason, a data source object cannot be reconstructed from the reference, a value of {@code null} may be returned. This allows other object factories that may be registered in JNDI to be tried. If an exception is thrown then no other object factories are tried. Set the Java bean properties for an object from its Reference. The Reference contains a set of StringRefAddr values with the key being the bean name and the value a String representation of the bean's value. This code looks for setXXX() method where the set method corresponds to the standard bean naming scheme and has a single parameter of type String, int, boolean or short.

Retrieves whether an <code>SQLException</code> will cause all open <code>ResultSet</code>s to be closed when auto-commit is <code>true</code>. A "public" version of checkForClosedConnection() that throws SQLException instead of SqlException.  In particular this is used by all the DatabaseMetadata methods Checks whether the server supports a JDBC version. If the server does not support the JDBC version, an exception is thrown. -----------------------------helper methods--------------------------------- Set flags describing the level of support for this connection. Flags will be set based on manager level and/or specific product identifiers. Support for a specific server version can be set as follows. For example if (productLevel_.greaterThanOrEqualTo(11,1,0)) supportsTheBestThingEver = true  WARNING WARNING WARNING !!!!  If you define an instance variable of NetDatabaseMetaData that you want computeFeatureSet_() to compute, DO NOT assign an initial value to the variable in the declaration. NetDatabaseMetaData's constructor will invoke DatabaseMetaData's constructor, which then invokes computeFeatureSet_(). Initialization of instance variables in NetDatabaseMetaData will happen *after* the invocation of computeFeatureSet_() and will therefore overwrite the computed values. So, LEAVE INSTANCE VARIABLES UNINITIALIZED!  END OF WARNING Derby uses a PreparedStatement argument rather than a callable statement ------------------- JDBC 4.1 ------------------------- See DatabaseMetaData javadoc call stored procedure SYSIBM.SQLSPECIALCOLUMNS ( IN COLTYPE SMALLINT, IN CATALOG_NAME VARCHAR(128), IN SCHEMA_NAME  VARCHAR(128), IN TABLE_NAME   VARCHAR(128), IN SCOPE        SMALLINT, IN NULLABLE     SMALLINT, IN OPTIONS      VARCHAR(4000) )  DERBY does not have the notion of a catalog, so we return a result set with no rows. Returns a list of the client info properties supported by the driver. The result set contains the following columns: <p> <ol> <li>NAME String=&gt; The name of the client info property.</li> <li>MAX_LEN int=&gt; The maximum length of the value for the property.</li> <li>DEFAULT_VALUE String=&gt; The default value of the property.</li> <li>DESCRIPTION String=&gt; A description of the property.</li> </ol> <p>The <code>ResultSet</code> is sorted by the NAME column. Untraced version of <code>getClientInfoProperties()</code>. Returns an empty <code>ResultSet</code> with the correct column names. call stored procedure SQLColumnPrivileges SYSIBM.SQLColPrivileges( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), ColumnName  varchar(128), Options     varchar(4000))  call stored procedure SQLColumns SYSIBM.SQLColumns( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), ColumnName  varchar(128), Options     varchar(4000))  call stored procedure SQLForeignKeys SYSIBM.SQLForeignKeys( PKCatalogName varchar(128), PKSchemaName  varchar(128), PKTableName   varchar(128), FKCatalogName varchar(128), FKSchemaName  varchar(128), FKTableName   varchar(128), Options       varchar(4000))  JDBC signature also does not throw SqlException, so we don't check for closed connection. JDBC signature also does not throw SqlException, so we don't check for closed connection. call stored procedure SQLForeignKeys SYSIBM.SQLForeignKeys( PKCatalogName varchar(128), PKSchemaName  varchar(128), PKTableName   varchar(128), FKCatalogName varchar(128), FKSchemaName  varchar(128), FKTableName   varchar(128), Options       varchar(4000))  Get the function names available in the database.  Calls stored procedure <code>SYSIBM.SQLFunctionParams(CatalogName varchar(128), SchemaName varchar(128), FuncName varchar(128), ParamName varchar(128), Options varchar(4000))</code> on the server. This procedure will in turn call <code>EmbedDatabaseMetaData.getFunctionColumns(String,String, String,String)</code><p> Compatibility: Only available if both server and client version &gt; 10.1, and JDK version &gt;= 1.6. Older clients will not have this method available. Newer clients will be able to call this method when connected to an older server, but this will be trigger an exception in <code>checkServerJdbcVersionX()</code>. <p>Upgrade: <code>SYSIBM.SQLFunctionParams</code> is added in <code>DataDictionaryImpl.create_10_2_system_procedures (TransactionController,UUID)</code> so it will become available in newly created databases and after <b>hard</b> upgrade. Untraced version of <code>getFunctionColumns(String, String, String, String)</code>. Get the function names available in the database.  Calls stored procedure <code>SYSIBM.SQLFunctions(CatalogName varchar(128), SchemaName varchar(128), FuncName varchar(128), Options varchar(4000))</code> on the server. This procedure will in turn call <code>EmbedDatabaseMetaData.getFunctions(String,String,String)</code><p> Compatibility: Only available if both server and client version &lt; 10.1, and JDK version &gt;= 1.6. Older clients will not have this method available. Newer clients will be able to call this method when connected to an older server, but this will be trigger an exception in <code>checkServerJdbcVersionX()</code>. <p>Upgrade: <code>SYSIBM.SQLFunctions</code> is added in <code>DataDictionaryImpl.create_10_2_system_procedures (TransactionController,UUID)</code> so it will become available in newly created databases and after <b>hard</b> upgrade. Untraced version of <code>getFunctions(String, String, String)</code>. call storlastGetPrimaryKeysResultSet_ed procedure SQLForeignKeys SYSIBM.SQLForeignKeys( PKCatalogName varchar(128), PKSchemaName  varchar(128), PKTableName   varchar(128), FKCatalogName varchar(128), FKSchemaName  varchar(128), FKTableName   varchar(128), Options       varchar(4000))  call stored procedure SQLStatistics SYSIBM.SQLStatistics( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), Unique      Smallint, Reserved    Smallint, Options     varchar(4000))  ------------------- JDBC 4.2 ------------------------- See DatabaseMetaData javadoc ------------helper methods for meta data info call methods------------------ Client's view of boolean metadata.  For values which depend on (added) functionality in *both* the client and the server, the client should have its own view of all such values here.  For other values, it can defer to the server. This is a prerequisite for negotiating down in a mixed client/Server context. Note that metadata negotiation should mirror the similar negotiation for use of the feature itself, for example, for scrollable updatable result sets of type insensitive, the server will downgrade to read-only if it is older than 10.2.  See also comments in getMetaDataInfoBooleanWithType and engine/org/apache/derby/impl/sql/catalog/metadata_net.properties.  helper method for the catalog queries only call stored procedure SQLPrimaryKeys SYSIBM.SQLPrimaryKeys( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), Options     varchar(4000))  call stored procedure SQLProcedureCols SYSIBM.SQLProcedureCols( CatalogName varchar(128), SchemaName  varchar(128), ProcName    varchar(128), ParamName   varchar(128), Options     varchar(4000))  ------------------------catalog query methods follow-------------------------------------------- call stored procedure SQLProcedures SYSIBM.SQLProcedures( CatalogName varchar(128), SchemaName  varchar(128), ProcName    varchar(128), Options     varchar(4000))  Indicates whether or not this data source supports the SQL <code>ROWID</code> type. Since Derby does not support the <code>ROWID</code> type, return <code>ROWID_UNSUPPORTED</code>. call stored procedure SQLTables SYSIBM.SQLTables( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), TaleType    varchar(4000), Options     varchar(4000))  Get the schema names available in this database. The results are ordered by schema name. <p>The schema columns are: <ol> <li><strong>TABLE_SCHEM</strong> String =&gt; schema name</li> <li><strong>TABLE_CATALOG</strong> String =&gt; catalog name (may be <code>null</code>)</li> </ol> Untraced version of <code>getSchemas(String, String)</code>. call stored procedure SQLTablePrivileges SYSIBM.SQLTablePrivileges( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), Options     varchar(4000))  call stored procedure SQLTables SYSIBM.SQLTables( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), TableType   varchar(4000), Options     varchar(4000)) call stored procedure SQLTables SYSIBM.SQLTables( CatalogName varchar(128), SchemaName  varchar(128), TableName   varchar(128), TaleType    varchar(4000), Options     varchar(4000))  call stored procedure SQLGetTypeInfo SYSIBM.SQLGetTypeInfo (IN DATATYPE SMALLINT, IN Options VARCHAR(4000))   --------------------------JDBC 2.0----------------------------- start tagging all abstract methods with an underscore like this !! Returns false unless <code>interfaces</code> is implemented We synchronize at this level so that we don't have to synchronize all the meta data info methods.  If we just return hardwired answers we don't need to synchronize at the higher level. ----------------------------helper methods---------------------------------- Check if the server accepts receiving booleans as parameter values. Check if server supports boolean values Check if server supports product specific EXTDTA abort protocol. Check whether the server has full support for the QRYCLSIMP parameter in OPNQRY. Check if server supports session data caching Check if server supports nanoseconds in timestamps Check if server supports UDTs All JDBC Drivers must return false for this method. For this reason we choose to return FALSE ------------------- JDBC 3.0 ------------------------- Derby does not support the Types.REF_CURSOR type. ------------------- JDBC 4.0 ------------------------- Retrieves whether this database supports invoking user-defined or vendor functions using the stored procedure escape syntax. Returns <code>this</code> if this class implements the interface
Append attributes to the database name except for user/password which are sent as part of the protocol, and SSL which is used locally in the client. Other attributes will  be sent to the server with the database name Assumes augmentedProperties is not null Returns an instance of the ClientJDBCObjectFactoryImpl class Returns an instance of the ClientJDBCObjectFactoryImpl40 class If a ClassNotFoundException occurs then it returns an instance of ClientJDBCObjectFactoryImpl If a future version of JDBC comes then a similar method would be added say createJDBCXXFactoryImpl in which if  the class is not found then it would return the lower version thus having a sort of cascading effect until it gets a valid instance Returns an instance of the ClientJDBCObjectFactoryImpl42 class If a ClassNotFoundException occurs then it returns an instance of the most refined ClientJDBCObjectFactoryImpl possible If a future version of JDBC comes then a similar method would be added say createJDBCXXFactoryImpl in which if  the class is not found then it would return the lower version thus having a sort of cascading effect until it gets a valid instance This method returns an Implementation of ClientJDBCObjectFactory depending on VM under use Currently it returns either ClientJDBCObjectFactoryImpl (or) ClientJDBCObjectFactoryImpl42 //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// return database name tokenize "[:portNumber]/" from URL jdbc:derby://server[:port]/ returns the portNumber or zero if portNumber is not specified. ----------------helper methods--------------------------------------------- Tokenize one of the following: "jdbc:derby:" and return 0 if the protcol is unrecognized return DERBY_PROTOCOL for "jdbc:derby" tokenize "/server" from URL jdbc:derby://server:port/ returns server name

Creates a BatchUpdateException depending on the JVM level. Returns an instance of a {@code CachingLogicalConnection}, which provides caching of prepared statements. Returns an instance of ClientCallableStatement, ClientCallableStatement40 or ClientCallableStatement42 which all implement java.sql.CallableStatement. This method is used to return an instance of the {@link org.apache.derby.client.ClientPooledConnection} class which implements {@code javax.sql.PooledConnection}. This method is used to return an instance of ClientXAConnection (or ClientXAConnection40) class which implements {@code javax.sql.XAConnection}. Returns an instanceof ColumnMetaData or ColumnMetaData40 depending on the jdk version under use Returns an instanceof ColumnMetaData or ColumnMetaData40 depending on the jdk version under use Returns a new logical callable statement object. Returns an instance of LogicalConnection. This method returns an instance of LogicalConnection (or LogicalConnection40) which implements {@code java.sql.Connection}. Returns a new logical prepared statement object. This method returns an instance of NetConnection (or NetConnection40) class which extends from ClientConnection this implements the java.sql.Connection interface This method returns an instance of NetConnection (or NetConnection40) class which extends from ClientConnection.  This implements the {@code java.sql.Connection} interface. This method returns an instance of NetConnection (or NetConnection40) class which extends Connection. This implements the {@code java.sql.Connection} interface. This method is used to pass the ClientPooledConnection object to the NetConnection object which can then be used to pass the statement events back to the user This method provides an instance of NetDatabaseMetaData (or NetDatabaseMetaData40) which extends from ClientDatabaseMetaData which implements {@code java.sql.DatabaseMetaData}. This method returns an instance of NetResultSet(or NetResultSet40) which extends from ClientResultSet which implements {@code java.sql.ResultSet}. returns an instance of ParameterMetaData or ParameterMetaData40 depending on the jdk version under use Returns an instance of PreparedStatement (or PreparedStatement40) which implements {@code java.sql.PreparedStatement}. It has the ClientPooledConnection as one of its parameters this is used to raise the Statement Events when the prepared statement is closed This method returns an instance of PreparedStatement (or PreparedStatement40) which implements {@code java.sql.PreparedStatement}. It has the {@link ClientPooledConnection} as one of its parameters this is used to raise the Statement Events when the prepared statement is closed This method provides an instance of Statement or Statement40 depending on the jdk version under use
This method is overridden on JVM 8 Creates a BatchUpdateException depending on the JVM level. Returns an instance of a {@link org.apache.derby.client.am.CachingLogicalConnection}, which provides caching of prepared statements. Returns an instance of ClientCallableStatement.   Returns an instance of ColumnMetaData Returns an instance of ColumnMetaData or ColumnMetaData40 depending on the JDK version under use Returns a new logical callable statement object.  Returns a new logical prepared statement object.   Returns an instance of NetConnection.   returns an instance of ParameterMetaData This method returns an instance of ClientPreparedStatement which implements {@code java.sql.PreparedStatement}. It has the {@link org.apache.derby.client.ClientPooledConnection} as one of its parameters this is used to raise the Statement Events when the prepared statement is closed. This method returns an instance of ClientPreparedStatement which implements java.sql.PreparedStatement. It has the ClientPooledConnection as one of its parameters this is used to raise the Statement Events when the prepared statement is closed. This method provides an instance of Statement
This method is overridden on JVM 8 to take advantage of long update counts. return a ClientCallableStatement42 object Returns a new logical callable statement object. Returns a new logical prepared statement object. returns an instance of org.apache.derby.client.net.NetResultSet This method returns an instance of PreparedStatement which implements java.sql.PreparedStatement. It has the ClientPooledConnection as one of its parameters this is used to raise the Statement Events when the prepared statement is closed. Returns a PreparedStatement.

JDBC 4.0 java.sql.Wrapper interface methods Check whether this instance wraps an object that implements the interface specified by {@code iface}. Returns {@code this} if this class implements the specified interface.
JDBC 4.0 methods Registers a StatementEventListener with this PooledConnection object. Components that wish to be informed of events associated with the PreparedStatement object created by this PooledConnection like the close or error occurred event can register a StatementEventListener with this PooledConnection object. Closes the physical connection to the data source and frees all associated resources. Creates a new logical connection by performing all the required steps to be able to reuse the physical connection. <p> @GuardedBy("this")  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  Fire all the {@code ConnectionEventListener}s registered. Callers must synchronize on {@code this} to prevent others from modifying the list of listeners. Creates a logical connection. <p> This is the standard API for getting a logical connection handle for a pooled connection. No "resettable" properties are passed, so user, password, and all other properties may not change. Inform listeners that an error has occured on the connection, if the error severity is high enough. <p> Not public, but needs to be visible to am.LogicalConnection Tells is statement pooling is enabled or not. Used by {@code LogicalConnection.close} in some circumstances when it disassociates itself from the pooled connection. Raise the statementClosed event for all the listeners when the corresponding events occurs. Raise the statementErrorOccurred event for all the listeners when the corresponding events occurs. Inform listeners that the logical connection has been closed and that the physical connection is ready for reuse. <p> Not public, but needs to be visible to am.LogicalConnection Removes the specified previously registered listener object from the list of components that would be informed of events with a PreparedStatement object.
--------------------------JDBC 2.0----------------------------- ------------------- Prohibited overrides from Statement -------------------- End of JDBC 4.2 methods Method calls onStatementError occurred on the BrokeredConnectionControl class after checking the SQLState of the SQLException thrown. @param sqle SqlException @throws java.sql.SQLException <p> Check for closed statement and extract the SQLException if it is raised. </p> Check the length passed in for the stream that is to be set. If length is larger than Integer.MAX_VALUE or smaller that 0, we fail by throwing an SQLException. Since parameters are cached as objects in parameters_[], java null may be used to represent SQL null. ------------------------- JDBC 3.0 ----------------------------------- Batch requires that input types are exact, we perform no input cross conversion for Batch. If so, this is an external semantic, and should go into the release notes End of JDBC 4.0 methods Beginning of JDBC 4.2 methods ---------------------------jdbc 1------------------------------------------ also called by some DBMD methods also used by SQLCA ----------------------------internal use only helper methods---------------- Returns the name of the java.sql interface implemented by this class. (non-Javadoc) @see org.apache.derby.client.am.Statement#markClosed(boolean) called immediately after the constructor by Connection prepare*() methods Resets the prepared statement for reuse in a statement pool. JDBC 4.0 methods Sets the designated parameter to the given input stream. When a very large ASCII value is input to a <code>LONGVARCHAR</code> parameter, it may be more practical to send it via a <code>java.io.InputStream</code>. Data will be read from the stream as needed until end-of-file is reached. The JDBC driver will do any necessary conversion from ASCII to the database char format. We do this inefficiently and read it all in here. The target type is assumed to be a String. We do this inefficiently and read it all in here. The target type is assumed to be a String. Sets the designated parameter to the given input stream. When a very large binary value is input to a <code>LONGVARBINARY</code> parameter, it may be more practical to send it via a <code>java.io.InputStream</code> object. The data will be read from the stream as needed until end-of-file is reached. sets the parameter to the  Binary Stream object sets the parameter to the  Binary Stream object Sets the designated parameter to a <code>InputStream</code> object. This method differs from the <code>setBinaryStream(int, InputStream) </code>  method because it informs the driver that the parameter value should be sent to the server as a <code>BLOB</code>. When the <code>setBinaryStream</code> method is used, the driver may have to do extra work to determine whether the parameter data should be sent to the server as a <code>LONGVARBINARY</code> or a <code>BLOB</code> Sets the designated parameter to a InputStream object. also used by CallableLocatorProcedures Sets the designated parameter to the given <code>Reader</code> object. When a very large UNICODE value is input to a LONGVARCHAR parameter, it may be more practical to send it via a <code>java.io.Reader</code> object. The data will be read from the stream as needed until end-of-file is reached. The JDBC driver will do any necessary conversion from UNICODE to the database char format. Sets the designated parameter to the given Reader, which will have the specified number of bytes. Sets the designated parameter to the given Reader, which will have the specified number of bytes. Sets the designated parameter to a <code>Reader</code> object. Sets the designated parameter to a Reader object. also used by DBMD methods also used by DBMD methods The Java compiler uses static binding, so we must use instanceof rather than to rely on separate setObject() methods for each of the Java Object instance types recognized below. also used by DBMD methods also used by DBMD methods Set a UDT parameter to an object value. Sets the specified parameter to the given input stream. Deprecated in JDBC 3.0 and this method will always just throw a feature not implemented exception. ------------------------ box car and callback methods --------------------------------

This method will not work if e is chained. It is assumed that e is a single warning and is not chained. Builds the insert statement that will be used well calling insertRow If no values have been supplied for a column, it will be set to the column's default value, if any. If no default value had been defined, the default value of a nullable column is set to NULL. Checks if a stream or a LOB object has already been created for the specified LOB column. <p> Accessing a LOB column more than once is not forbidden by the JDBC specification, but the Java API states that for maximum portability, result set columns within each row should be read in left-to-right order, and each column should be read only once. The restriction was implemented in Derby due to complexities with the positioning of store streams when the user was given multiple handles to the stream. Clear all warnings on this <code>ResultSet</code> and make subsequent calls to <code>getWarnings()</code> return <code>null</code> until a new warning is reported. An untraced version of clearWarnings() Closes the current stream, if there is one. <p> Note that streams are implicitly closed when the next value is fetched. Close Statement if it is set to closeOnCompletion TO DO: when parseEndqryrm() notifies common w/ endQueryCloseOnlyEvent() we need to mark something that we later check to drive a commit. An untraced version of close() Convert a date originally set using the default calendar to a value representing the same date in a different calendar. Convert a time originally set using the default calendar to a value representing the same time in a different calendar. Convert a timestamp originally set using the default calendar to a value representing the same timestamp in a different calendar. Create a calendar with default locale and time zone. Initializes the LOB state tracker. <p> The state tracker is used to free LOB locators on the server. If the server doesn't support locators, or there are no LOBs in the result set, a no-op tracker will be used. The query was ended at the server because all rows have been retrieved. An untraced version of findColumn() --------------------categorize the methods below ----------------- Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized   Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized ------------------- getters on column index -------------------------------- Live life on the edge and run unsynchronized ------------- Methods for accessing results by column name ---------------- Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized -------------------------- JDBC 4.0 -------------------------- Retrieves the holdability for this <code>ResultSet</code> object. Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized used by DBMD Live life on the edge and run unsynchronized //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Retrieve the column as an object of the desired type. Live life on the edge and run unsynchronized used by DBMD determines if a cursor is a: Return to Client - not to be read by the stored procedure only by client Return to Caller - only calling JSP can read it, not the client Live life on the edge and run unsynchronized ------------------------------------------------------------------------------ Push the getXXXRowset_() methods up from the material layers ------------------------------------------------------------------------------ This method is called to retrieve the total number of rows in the result set and then reposition the cursor to where it was before. The rowCount_ comes back in the SQLCA in fields SQLERRD1 and SQLERRD2 when sqlcode is +100. Live life on the edge and run unsynchronized Retrieves the <code>Statement</code> object that produced this object, or <code>null</code> if the <code>ResultSet</code> was not produced by a <code>Statement</code> object. Live life on the edge and run unsynchronized Go through all the columns in the select list to see if we can find a base table column and use that column's metadata to get the table name But, it is possible to have a sql of the form select 1,2 from t1 for update This sql will not be a good candidate for updateXXX calls(both in embedded and Network Server mode) since there is no updatable column in the select list. But a user can use a sql like that to issue deleteRow. In Network Server mode though, this sql will fail for deleteRow because none of the columns are from base table and w/o a base table column, there is no way to find the table name for delete Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized Live life on the edge and run unsynchronized -------------------------- JDBC 3.0 ---------------------------------------- Retrieve the value of the specified column as a stream of two-byte Unicode characters. Deprecated in JDBC 2.0 and this method will just throw a feature not implemented exception.  ----------------Advanced features ----------------------------------------- Returns the first <code>SQLWarning</code> reported on this <code>ResultSet</code> object, or <code>null</code> if there are no warnings. Subsequent warnings are chained on the returned object. --------------------------------------------------------------------------- -------------------------- Traversal/Positioning --------------------------- Checks whether this <code>ResultSet</code> object has been closed, either automatically or because <code>close()</code> has been called. Returns false unless <code>interfaces</code> is implemented Mark this ResultSet as closed. The ResultSet will not be removed from the commit and rollback listeners list in <code>Connection</code>. Mark this ResultSet as closed. Mark this ResultSet as closed on the server. Marks the LOB at the specified column as published. <p> When a LOB is marked as published, the release mechanism will not be invoked by the result set. It is expected that the code accessing the LOB releases the locator when it is done with the LOB, or that the commit/rollback handles the release. Positions the cursor at before the first row. Moves off the insert row if positioned there, and checks the current row for releasable LOB locators if positioned on a valid data row. ---------------------------jdbc 1------------------------------------------ used by DBMD Method that is invoked by <code>closeX()</code> before the result set is actually being closed. Subclasses may override this method if work needs to be done before closing. -------------------------------helper methods------------------------------- ----------------------------- Updates -------------------------------------- Set rowCount. Resets all rowset related flags. Called by getRowSet() from material layer. Clears the flags for used columns, typically invoked when changing the result set position. Returns <code>this</code> if this class implements the interface Updates the designated column with an ascii stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Update a column with an ascii stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with an ascii stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Update a column with an ascii stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a binary stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Update a column with a binary stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a binary stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Update a column with a binary stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column using the given input stream. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column using the given input stream, which will have the specified number of bytes. The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column with a <code>java.sql.Blob</code> value. The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column using the given input stream. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column using the given input stream, which will have the specified number of bytes. The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column with a <code>java.sql.Blob</code> value. The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column with a character stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Update a column with a character stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a character stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Update a column with a character stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column using the given <code>Reader</code> object. The data will be read from the stream as needed until end-of-stream is reached. The JDBC driver will do any necessary conversion from UNICODE to the database char format. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column using the given <code>Reader</code> object, which is the given number of characters long. When a very large UNICODE value is input to a <code>LONGVARCHAR</code> parameter, it may be more practical to send it via a <code>java.io.Reader</code> object. The JDBC driver will do any necessary conversion from UNICODE to the database char format. <p> The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column with a <code>java.sql.Clob</code> value. The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column using the given <code>Reader</code> object. The data will be read from the stream as needed until end-of-stream is reached. The JDBC driver will do any necessary conversion from UNICODE to the database char format. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column using the given <code>Reader</code> object, which is the given number of characters long. When a very large UNICODE value is input to a <code>LONGVARCHAR</code> parameter, it may be more practical to send it via a <code>java.io.Reader</code> object.  The JDBC driver will do any necessary conversion from UNICODE to the database char format. <p> The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column with a <code>java.sql.Clob</code> value. The updater methods are used to update column values in the current row or the insert row.  The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. Updates the designated column with a java.sql.NClob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. Updates the designated column with a java.sql.NClob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. Updates the designated column with a java.sql.NClob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. --------------------------- update column methods ------------------------- ---------------------- update on column name methods ---------------------- if no updateXXX were issued before this updateRow, then return false Mark a column as already having a stream or LOB accessed from it. If the column was already accessed, throw an exception. ---------------------------------------------------------------------------- This method only returns true if there is a new non-null updated value. If the resultset is updatable, sensitive, and updated, return the new non-null updated value. Otherwise this method will return false. If the column is updated to null, or if the column has not been update but is null, a null will be returned by isNull(), which first calls wasNullSensitiveUpdate() to check for a column that is updated to null, and columnUpdated_ is checked there. if updatedColumns_ entry is null, but columnUpdated_ entry indicates column has been updated, then column is updated to null. ------------------------------- abstract box car methods --------------------------------------

----------------- externals -----------------------------------------------
This method will not work if e is chained. It is assumed that e is a single warning and is not chained. Called by statement constructor only for jdbc 2 statements with scroll attributes. This method is not in the StatementRequest class because it is not building into the request buffer directly, it is building into a cache to be written into the request buffer at prepare-time. Two open result sets can not have the same cursor name. precondition: parseSqlAndSetSqlModes() must be called on the supplied sql string before invoking this method Checks that a stored procedure returns the correct number of result sets given its execute type. If the number is incorrect, make sure the transaction is rolled back when auto commit is enabled. Checks that the number of result sets returned by the statement is consistent with the executed type. <code>executeQuery()</code> should return exactly one result set and <code>executeUpdate()</code> none. Raises an exception if the result set count does not match the execute type. An untraced version of clearWarnings() The server holds statement resources until transaction end. Close all resources except for ResultSets. This code was factored out of markClosed() so that closeMeOnCompletion() could close the Statement without having to re-close the already closed ResultSets.  For tracking all of the ResultSets produced by this Statement so that the Statement can be cleaned up if closeOnCompletion() was invoked.  //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// An untraced version of <code>close</code>. This method cleans up client-side resources and also sends commands to network server to perform clean up. This should not be called in the finalizer. Difference between <code>finalize</code> and <code>close</code> is that close method does these things additionally (Changes done as part of DERBY-210): 1) Sends commands to the server to close the result sets. 2) Sends commands to the server to close the result sets of the generated keys query. 3) Sends a commit if autocommit is on and it is appropriate. 4) Explicitly removes the statement from connection_.openStatements_ and CommitAndRollbackListeners_ by passing true to markClosed. We may need to do 1) in finalizer too. This is being tracked in DERBY-1021 Callback for CALLS, and PreparedStatement updates. no result sets returned do nothing for now. do nothing for now. ----------------------- Multiple Results -------------------------- Added by JDBC 4.2 Added by JDBC 4.2 Added by JDBC 4.2 Added by JDBC 4.2 Added by JDBC 4.2 ---------------------------jdbc 1------------------------------------------ (non-Javadoc) @see java.lang.Object#finalize() This method cleans up client-side resources by calling markClosed(). It is different from close() method, which also does clean up on server. Changes done as part of DERBY-210.  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  -------------------------------helper methods------------------------------- Returns the name of the java.sql interface implemented by this class. Added by JDBC 4.2  --------------------------JDBC 3.0----------------------------- Returns the owner of this statement, if any. Provides public access for section_. This is necessary as the section concept is shared between client.am and net. Seems like it really belongs in net as it is tied to the particular protocol used. Accessor to state variable warnings_ Step past any initial non-significant characters and comments to find first significant SQL token so we can classify statement. Tell whether the statement has been closed or not. Returns the value of the poolable hint, indicating whether pooling is requested. Returns false unless <code>iface</code> is implemented Minion of getStatementToken. If the input string starts with an identifier consisting of letters only (like "select", "update"..),return it, else return supplied string. ------------------material layer event callbacks follow--------------------- All callbacks are client-side only operations do nothing for now. Mark all ResultSets associated with this statement as auto-committed. This method cleans up client-side resources held by this Statement. The Statement will not be removed from the open statements list and PreparedStatement will also not be removed from the commit and rollback listeners list in <code>org.apache.derby.client.am.Connection</code>. This method is called from: 1. finalize() - For the finalizer to be called, the Statement should not have any references and so it should have been already removed from the lists. 2. <code>org.apache.derby.client.am.Connection#markStatementsClosed</code> This method explicitly removes the Statement from open statements list. 3. To close positioned update statements - These statements are not added to the list of open statements. This method cleans up client-side resources held by this Statement. If removeListener is true, the Statement is removed from open statements list and PreparedStatement is also removed from commit and rollback listeners list. This is called from the close methods. Mark all ResultSets associated with this statement as closed. Should investigate if it can be optimized..  if we can avoid this parsing..  Helper method for S.flowCloseResultSets() and PS.flowExecute() If a dataSource is passed into resetClientConnection(), then we will assume properties on the dataSource may have changed, and we will need to go through the open-statement list on the connection and do a full reset on all statements, including preparedStatement's and callableStatement's.  This is because property change may influence the section we allocate for the preparedStatement, and also the cursor attributes, i.e. setCursorSensitivity(). If no dataSource is passed into resetClientConnection(), then we will do the minimum reset required for preparedStatement's and callableStatement's. This was being called only on positioned update statements. When working on DERBY-210, it was found that result sets of all statements (not just positioned update statements) get added to the table. So, this is called for all statements. Otherwise, this will cause memory leaks when statements are not explicitly closed in the application. Resets the statement for reuse in a statement pool. <p> Intended to be used only by prepared or callable statements, as {@link java.sql.Statement} objects aren't pooled. <p> The following actions are taken: <ul> <li>Batches are cleared.</li> <li>Warnings are cleared.</li> <li>Open result set are closed on the client and the server.</li> <li>Cached cursor names are cleared.</li> <li>Statement for fetching auto-generated keys is closed.</li> <li>Special registers are reset.</li> <li>User controllable attributes are reset to defaults, for instance query timeout and max rows to fetch.</li> </ul> Resets attributes that can be modified by the user through the {@link java.sql.Statement} interface to default values. Convenience method for resultSetCommitting(ClientResultSet, boolean) Method that checks to see if any other ResultSets are open. If not proceeds with the autocommit. Dnc statements are already associated with a unique cursor name as defined by our canned dnc package set. ResultSet.getCursorName() should be used to obtain the for update cursor name to use when executing a positioned update statement. See Jdbc 3 spec section 14.2.4.4. --------------------------JDBC 2.0----------------------------- Debug method used to test the setLargeMaxRows() method added by JDBC 4.2. This method is a NOP on a production (insane) build of Derby. Added by JDBC 4.2  Designates the owner of this statement, typically a logical statement. Requests that a Statement be pooled or not. Assigns a new value (even null) to section_. The existing section_ (if any) is freed. Set up information to be able to handle cursor names: canned or user named (via setCursorName). Substitute the client cursor name in the SQL string with the server's cursor name. Only called on positioned update statements. Returns {@code this} if this class implements the specified interface. The connection close processing passes allowAutoCommits=false because if we drove an auto-commits after each statement close, then when we issue close requests on non-held cursors the server would complain that the non-held cursor was already closed from the previous statement's auto-commit. So the solution is to never auto-commit statements during connection close processing. <p/> Here's the operative explanation: <p/> Given a sequence of open statements S1, S2, .... a logic problem is occurring after S1 close-query drives an auto-commit, and S2 close-query is driven against a non-held cursor. <p/> The first auto-commit driven by S1 triggers a callback that closes S2's non-held cursor, and so the subsequent S2 close-query request generates an error from the server saying that the cursor is already closed. <p/> This is fixed by passing a flag to our statement close processing that prevents driving additional auto-commits after each statement close.  Connection close drives its own final auto-commit. Used for re-prepares across commit only
Perform a server socket accept. Allow three attempts with a one second wait between each end run()





Returns the branch qualifier for this Xid.  @return the branch qualifier   return fields of Xid   Obtain the format identifier part of the Xid.  @return Format identifier. -1 indicates a null Xid   Returns the global transaction identifier for this Xid.  @return the global transaction identifier   Set the branch qualifier for this Xid.  @param qual a Byte array containing the branch qualifier to be set. If the size of the array exceeds MAXBQUALSIZE, only the first MAXBQUALSIZE elements of qual will be used.   Set the format identifier part of the Xid.  @param Format identifier. -1 indicates a null Xid.   Return a string representing this Xid for debugging  @return the string representation of this Xid
Writes <code>len</code> bytes from the specified byte array starting at offset <code>off</code> to this output stream. <p> The general contract for <code>write(b, off, len)</code> is that some of the bytes in the array <code>b</code> are written to the output stream in order; element <code>b[off]</code> is the first byte written and <code>b[off+len-1]</code> is the last byte written by this operation. <p> The <code>write</code> method of <code>OutputStream</code> calls the write method of one argument on each of the bytes to be written out. Subclasses are encouraged to override this method and provide a more efficient implementation. <p> If <code>b</code> is <code>null</code>, a <code>NullPointerException</code> is thrown. <p> If <code>off</code> is negative, or <code>len</code> is negative, or <code>off+len</code> is greater than the length of the array <code>b</code>, then an <tt>IndexOutOfBoundsException</tt> is thrown. Writes the specified byte to this output stream. <p> The general contract for <code>write</code> is that one byte is written to the output stream. The byte to be written is the eight low-order bits of the argument <code>b</code>. The 24 high-order bits of <code>b</code> are ignored.
Returns a <code>Byte</code> array from the <code>String</code> passed as Input.   Read the next <code>len</code> bytes of the <code>Clob</code> value from the server.
Write the <code>byte[]</code> to the <code>Clob</code> value on the server; starting from the current position of this stream.
Check to see if this <code>Reader</code> is closed. If it is closed throw an <code>IOException</code> that states that the stream is closed.    Read the next <code>len</code> characters of the <code>Clob</code> value from the server.
Check to see if this {@code Writer} is closed. If it is closed throw an {@code IOException} that states that the stream is closed.      Write the {@code char[]} to the {@code Clob} value on the server; starting from the current position of this stream.

Determines which header format to use. <p> <em>Implementation note:</em> The header format is determined by consulting the data dictionary throught the context service. If there is no context, the operation will fail. Tells if the header encodes a character or byte count. <p> Currently the header expects a character count if the header format is 10.5 (or newer), and a byte count if we are accessing a database created by a version prior to 10.5. Generates the header for the specified length and writes it into the provided buffer, starting at the specified offset. Generates the header for the specified length. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Returns the maximum header length. Writes a Derby-specific end-of-stream marker to the buffer for a stream of the specified character length, if required. Writes a Derby-specific end-of-stream marker to the destination stream for the specified character length, if required.
Closes this reader. <p> An {@code IOException} will be thrown if any of the read or skip methods are called after the reader has been closed. Updates the reader if the underlying data has been modified. <p> There are two cases to deal with: <ol> <li>The underlying data of the internal Clob representation has been modified.</li> <li>The internal Clob representation has changed.</li> </ol> The latter case happens when a read-only Clob, represented as a stream into store, is modified by the user and a new temporary internal representation is created.
Closes the stream. <p> Once the stream has been closed, further <code>write</code> or {@link #flush} invocations will cause an <code>IOException</code> to be thrown. Closing a previously closed stream has no effect. Flushes the stream. <p> Flushing the stream after {@link #close} has been called will cause an exception to be thrown. <p> <i>Implementation note:</i> In the current implementation, this is a no-op. Flushing is left to the underlying stream(s). Note that when programming against/with this class, always follow good practice and call <code>flush</code>. Writes a portion of an array of characters to the CLOB value.

Try to shrink the clock if it's larger than its maximum size. Insert an entry into the cache. If the maximum size is exceeded, evict a <em>not recently used</em> object from the cache. If there are no entries available for reuse, increase the size of the cache. Check if an entry can be evicted. Only entries that still are present in the cache, are not kept and not recently used, can be evicted. This method does not check whether the {@code Cacheable} contained in the entry is dirty, so it may be necessary to clean it before an eviction can take place even if the method returns {@code true}. The caller must hold the lock on the entry before calling this method. Get the holder under the clock hand, and move the hand to the next holder. Remove the holder at the given clock position. Rotate the clock in order to find a free space for a new entry. If <code>allowEvictions</code> is <code>true</code>, an not recently used object might be evicted to make room for the new entry. Otherwise, only unused entries are searched for. When evictions are allowed, entries are marked as not recently used when the clock hand sweeps over them. The search stops when a reusable entry is found, or when more than a certain percentage of the entries have been visited. If there are free (unused) entries, the search will continue until a reusable entry is found, regardless of how many entries that need to be checked. Perform the shrinking of the clock. This method should only be called by a single thread at a time.
Clone the stream. <p> To be used when a "deep" clone of a stream is required rather than multiple references to the same stream. <p> The resulting clone should support reads, resets, closes which do not affect the original stream source of the clone.

Compute/recover the parameters of a mixture-of-Gaussian distribution from given independent samples. Compute cluster weights. (Re-)compute cluster centers while holding cluster brackets fixed. Initialize the EM (expectation-maximization) iterations. (Re-)compute cluster brackets while holding cluster centers fixed. Compute a measure of total quantization error. Print out the clustering configuration. Compute an initial configuration of cluster centers uniformly distributed over the range of the input samples, for subsequent iterative refinement. Initialize cluster centers for EM iterations. Specify the input samples to work with. Compute an initial configuration of cluster centers uniformly spaced over the range of the input samples, for subsequent iterative refinement.
Clean up working tables after use. Compute/recover the parameters of a mixture-of-Gaussian distribution from given independent samples, using SQL. Download the computed cluster configuration. (Re-)compute cluster centers while holding sample-to-cluster assignment fixed. Initialize the EM (expectation-maximization) iterations. (Re-)compute sample-to-cluster assignment while holding cluster centers fixed. Set unique ID for this object.
Accept the visitor for all visitable children of this node. Binding this expression means setting the result DataTypeServices. In this case, the result type is based on the rules in the table listed earlier. Categorize this predicate. Do code generation for coalesce/value {@inheritDoc } Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all the {@code ColumnReference}s in this tree to be clones of the underlying expression. print the non-node subfields
Add an instruction that has no operand. All opcodes are 1 byte large. This takes an instruction that has a narrow and a wide form for CPE access, and generates accordingly the right one. We assume the narrow instruction is what we were given, and that the wide form is the next possible instruction. Add an instruction that has an 8 bit operand. Add an instruction that has a 16 bit operand. For adding an instruction with 3 operands, a U2 and two U1's. So far, this is used by VMOpcode.INVOKEINTERFACE. Add an instruction that has a 32 bit operand. This takes an instruction that can be wrapped in a wide for large variable #s and does so. wrap up the entry and stuff it in the class, now that it holds all of the instructions and the exception table. Find the limits of a conditional block starting at the instruction with the given opcode at the program counter pc. <P> Returns a six element integer array of program counters and lengths. <code> [0] - program counter of the IF opcode (passed in as pc) [1] - program counter of the start of the then block [2] - length of the then block [3] - program counter of the else block, -1 if no else block exists. [4] - length of of the else block, -1 if no else block exists. [5] - program counter of the common end point. </code> Looks for and handles conditionals that are written by the Conditional class. * Methods related to splitting the byte code chunks into sections that fit in the JVM's limits for a single method. For a block of byte code starting at program counter pc for codeLength bytes return the maximum stack value, assuming a initial stack depth of zero. now that we have codeBytes, fix the lengths fields in it to reflect what was stored. Limits checked here are from these sections of the JVM spec. <UL> <LI> 4.7.3 The Code Attribute <LI> 4.10 Limitations of the Java Virtual Machine </UL> Get the word count for a type descriptor in the format of the virual machine. For a method this returns the the word count for the return type. Return the opcode at the given pc. Get the current program counter Get the type descriptor in the virtual machine format for the type defined by the constant pool index for the instruction at pc. Get the unsigned short value for the opcode at the program counter pc. Get the unsigned 32 bit value for the opcode at the program counter pc. Get the number of words pushed (positive) or popped (negative) by this instruction. The instruction is a get/put field or a method call, thus the size of the words is defined by the field or method being access. Insert room for byteCount bytes after the instruction at pc and prepare to replace the instruction at pc. The instruction at pc is not modified by this call, space is allocated after it. The newly inserted space will be filled with NOP instructions. Returns a CodeChunk positioned at pc and available to write instructions upto (byteCode + length(existing instruction at pc) bytes. This chunk is left correctly positioned at the end of the code stream, ready to accept more code. Its pc will have increased by additionalBytes. It is the responsibility of the caller to patch up any branches or gotos. Return the complete instruction length for the passed in opcode. This will include the space for the opcode and its operand. See if the opcode is a return instruction. Assume an IOException means some limit of the class file format was hit Calculate the number of stack words in the arguments pushed for this method descriptor. Remove a block of code from this method that was pushed into a sub-method and call the sub-method. Returns the pc of this method just after the call to the sub-method. Split a block of code from this method into a sub-method and call it. Returns the pc of this method just after the call to the sub-method. Split an expression out of a large method into its own sub-method. <P> Method call expressions are of the form: <UL> <LI> expr.method(args) -- instance method call <LI> method(args) -- static method call </UL> Two special cases of instance method calls will be handled by the first incarnation of splitExpressionOut. three categories: <UL> <LI>this.method(args) <LI>this.getter().method(args) </UL> These calls are choosen as they are easier sub-cases and map to the code generated for SQL statements. Future coders can expand the method to cover more cases. <P> This method will split out such expressions in sub-methods and replace the original code with a call to that submethod. <UL> <LI>this.method(args) -&gt;&gt; this.sub1([parameters]) <LI>this.getter().method(args) -&gt;&gt; this.sub1([parameters]) </UL> The assumption is of course that the call to the sub-method is much smaller than the code it replaces. <P> Looking at the byte code for such calls they would look like (for an example three argument method): <code> this arg1 arg2 arg3 INVOKE // this.method(args) this INVOKE arg1 arg2 arg3 INVOKE // this.getter().metod(args) </code> The bytecode for the arguments can be arbitary long and consist of expressions, typical Derby code for generated queries is deeply nested method calls. <BR> If none of the arguments requred the parameters passed into the method, then in both cases the replacement bytecode would look like: <code> this.sub1(); </code> Parameter handling is just as in the method splitZeroStack(). <P> Because the VM is a stack machine the original byte code sequences are self contained. The stack at the start of is sequence is N and at the end (after the method call) will be: <UL> <LI> N - void method <LI> N + 1 - method returning a single word <LI> N + 2 - method returning a double word (java long or double) </UL> This code will handle the N+1 where the word is a reference, the typical case for generated code. <BR> The code is self contained because in general the byte code for the arguments will push and pop values but never drop below the stack value at the start of the byte code sequence. E.g. in the examples the stack before the first arg will be N+1 (the objectref for the method call) and at the end of the byte code for arg1 will be N+2 or N+3 depending on if arg1 is a single or double word argument. During the execution of the byte code the stack may have had many arguments pushed and popped, but will never have dropped below N+1. Thus the code for arg1 is independent of the stack's previous values and is self contained. This self-containment then extends to all the arguements, the method call itself and pushing the objectref for the method call, thus the complete sequence is self-contained. <BR> The self-containment breaks in a few cases, take the simple method call this.method(3), the byte code for this could be: <code> push3 this swap invoke </code> In this case the byte code for arg1 (swap) is not self-contained and relies on earlier stack values. <P> How to identify "self-contained blocks of code". <BR> We walk through the byte code and maintain a history of the program counter that indicates the start of the independent sequence each stack word depends on. Thus for a ALOAD_0 instruction which pushes 'this' the dependent pc is that of the this. If a DUP instruction followed then the top-word is now dependent on the previous word (this) and thus the dependence of it is equal to the dependence of the previous word. This information is kept in earliestIndepPC array as we process the instruction stream. <BR> When a INVOKE instruction is seen for an instance method that returns a single or double word, the dependence of the returned value is the dependence of the word in the stack that is the objectref for the call. This complete sequence from the pc the objectref depended on to the INVOKE instruction is then a self contained sequence and can be split into a sub-method. <BR> If the block is self-contained then it can be split, following similar logic to splitZeroStack(). <P> WORK IN PROGRESS - Incremental development <BR> Currently walks the method maintaining the earliestIndepPC array and identifies potential blocks to splt, performs splits as required. Called by BCMethod but commented out in submitted code. Tested with local changes from calls in BCMethod. Splits generally work, though largeCodeGen shows a problem that will be fixed before the code in enabled for real. Minimum split length for a sub-method. If the number of instructions to call the sub-method exceeds the length of the sub-method, then there's no point splitting. The number of bytes in the code stream to call a generated sub-method can take is based upon the number of method args. A method can have maximum of 255 words of arguments (section 4.10 JVM spec) which in the worst case would be 254 (one-word) parameters and this. For a sub-method the arguments will come from the parameters to the method, i.e. ALOAD, ILOAD etc. <BR> This leads to this number of instructions. <UL> <LI> 4 - 'this' and first 3 parameters have single byte instructions <LI> (N-4)*2 - Remaining parameters have two byte instructions <LI> 3 for the invoke instruction. </UL> Attempt to split the current method by pushing a chunk of its code into a sub-method. The starting point of the split (split_pc) must correspond to a stack depth of zero. It is the reponsibility of the caller to ensure this. Split is only made if there exists a chunk of code starting at pc=split_pc, whose stack depth upon termination is zero. The method will try to split a code section greater than optimalMinLength but may split earlier if no such block exists. <P> The method is aimed at splitting methods that contain many independent statements. <P> If a split is possible this method will perform the split and create a void sub method, and move the code into the sub-method and setup this method to call the sub-method before continuing. This method's max stack and current pc will be correctly set as though the method had just been created. Return the number of stack words pushed (positive) or popped (negative) by this instruction. Start a sub method that we will split the portion of our current code to, starting from start_pc and including codeLength bytes of code. Return a BCMethod obtained from BCMethod.getNewSubMethod with the passed in return type and same parameters as mb if the code block to be moved uses parameters. Does a section of code use parameters. Any load, exception ALOAD_0 in an instance method, is seen as using parameters, as this complete byte code implementation does not use local variables.

Given a manager codepoint find it's location in the managers array Check if a manager codepoint is a known manager

Get the provider's type.  ////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////// Return the name of this Provider.  (Useful for errors.) ----- getter functions for rowfactory ------
Check if this instance represents a value that has a single collation element.
DataValueDescriptor interface  Get the RuleBasedCollator for this instance of CollatorSQLChar. It will be used to do the collation.  We do not anticipate this method on collation sensitive DVD to be ever called in Derby 10.3 In future, when Derby will start supporting SQL standard COLLATE clause, this method might get called on the collation sensitive DVDs. Implementation of CollationElementsInterface interface Return a hash code that is consistent with {@link #stringCompare(SQLChar, SQLChar)}. This method implements the like function for char (with no escape value). The difference in this method and the same method in superclass is that here we use special Collator object to do the comparison rather than using the Collator object associated with the default jvm locale. This method implements the like function for char with an escape value. Set the RuleBasedCollator for this instance of CollatorSQLChar. It will be used to do the collation.
DataValueDescriptor interface  Get the RuleBasedCollator for this instance of CollatorSQLClob. It will be used to do the collation.  We do not anticipate this method on collation sensitive DVD to be ever called in Derby 10.3 In future, when Derby will start supporting SQL standard COLLATE clause, this method might get called on the collation sensitive DVDs. Implementation of CollationElementsInterface interface Return a hash code that is consistent with {@link #stringCompare(SQLChar, SQLChar)}. This method implements the like function for char (with no escape value). The difference in this method and the same method in superclass is that here we use special Collator object to do the comparison rather than using the Collator object associated with the default jvm locale. This method implements the like function for char with an escape value. Set the RuleBasedCollator for this instance of CollatorSQLClob. It will be used to do the collation.
DataValueDescriptor interface  Get the RuleBasedCollator for this instance of CollatorSQLLongvarchar. It will be used to do the collation.  We do not anticipate this method on collation sensitive DVD to be ever called in Derby 10.3 In future, when Derby will start supporting SQL standard COLLATE clause, this method might get called on the collation sensitive DVDs. Implementation of CollationElementsInterface interface Return a hash code that is consistent with {@link #stringCompare(SQLChar, SQLChar)}. This method implements the like function for char (with no escape value). The difference in this method and the same method in superclass is that here we use special Collator object to do the comparison rather than using the Collator object associated with the default jvm locale. This method implements the like function for char with an escape value. Set the RuleBasedCollator for this instance of CollatorSQLLongvarchar. It will be used to do the collation.
DataValueDescriptor interface  Get the RuleBasedCollator for this instance of CollatorSQLVarchar. It will be used to do the collation.  We do not anticipate this method on collation sensitive DVD to be ever called in Derby 10.3 In future, when Derby will start supporting SQL standard COLLATE clause, this method might get called on the collation sensitive DVDs. Implementation of CollationElementsInterface interface Return a hash code that is consistent with {@link #stringCompare(SQLChar, SQLChar)}. This method implements the like function for char (with no escape value). The difference in this method and the same method in superclass is that here we use special Collator object to do the comparison rather than using the Collator object associated with the default jvm locale. This method implements the like function for char with an escape value. Set the RuleBasedCollator for this instance of CollatorSQLVarchar. It will be used to do the collation.
//////////////////////////////////////////////  CLASS INTERFACE  ////////////////////////////////////////////// Return the list of matching nodes. The returned list may be empty, if there are no matching nodes. It is never {@code null}. Don't visit children under the skipOverClass node, if it isn't null. //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// If we have found the target node, we are done.
checks to see if autoincrementIncrement and autoincrementInitial are within the bounds of the type whose min and max values are passed into this routine. Check the validity of the default, if any, for this node. Check the validity of a user type.  Checks whether this column definition describes a user type that either doesn't exist or is inaccessible, or that doesn't implement Serializable. Check the validity of the default for this node Get the action associated with this node. Get the status of this autoincrement column Get the autoincrement cycle value Get the autoincrement increment value Get the autoincrement start value Returns the unqualified name of the column being defined. Return the DefaultInfo containing the default information for this column Return the DefaultNode, if any, associated with this node. Return the DataValueDescriptor containing the default value for this column Get the generation clause. Get the UUID of the old column default. Returns the data type of the column being defined. Return true if this column has a generation clause. Is this an autoincrement column? Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Set the collation type, note derivation is always implicit for any catalog item. Set the generation clause (Default) bound to this column. Set the nullability of the column definition node. Set the type of this column Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Check the validity of the autoincrement values for this node. The following errors are thrown by this routine. 1. 42z21 Invalid Increment; i.e 0. 2. 42z22 Invalid Type; autoincrement created on a non-exact-numeric type 3. 42995 The requested function does not apply to global temporary tables Check the validity of the default for this node.
Get the Increment value given by the user for an autoincrement column Get the start value of an autoincrement column Get the current value for an autoincrement column. One case in which this is used involves dropping a column from a table. When ALTER TABLE DROP COLUMN runs, it drops the column from SYSCOLUMNS, and then must adjust the column positions of the other subsequent columns in the table to account for the removal of the dropped columns. This involves deleting and re-adding the column descriptors to SYSCOLUMNS, but during that process we must be careful to preserve the current value of any autoincrement column. Get the name of the column. Get a DefaultDescriptor for the default, if any, associated with this column. Get the DefaultInfo for this ColumnDescriptor. Get the UUID for the column default, if any. Get the default value for the column. For columns with primitive types, the object returned will be of the corresponding object type. For example, for a float column, getDefaultValue() will return a Float.   Get the ordinal position of the column (1 based) Get the UUID of the object the column is a part of. Get the TableDescriptor of the column's table. Get the TypeDescriptor of the column's datatype. Is this column a generated column Return whether or not there is a non-null default on this column. Is this column to have autoincremented value always ? Is this column an autoincrement column? Sets the column name in case of rename column. Set the ordinal position of the column. Sets the table descriptor for the column. Convert the ColumnDescriptor to a String.
Add the column.  Currently, the table id is ignored. Return the nth (0-based) element in the list. Get the column descriptor Get the column descriptor Get an array of strings for all the columns in this CDL.
Accessors Get the formatID which corresponds to this class. Formatable methods Read this object from a stream of stored objects. Object methods. Write this object to a stream of stored objects.
assign ordinal position as the column name if null. ----------------------------helper methods---------------------------------- Cache the hashtable in ColumnMetaData. What's a column's table's catalog name? --------------------------jdbc 2.0----------------------------------- --------------------Abstract material layer call-down methods----------------- ------------------material layer event callback methods----------------------- ---------------------------jdbc 1------------------------------------------ all searchable except distinct JDBC 4.0 java.sql.Wrapper interface methods Check whether this instance wraps an object that implements the interface specified by {@code iface}. Returns {@code this} if this class implements the specified interface.
Indicate whether NULL values should be ordered below non-NULL. This function returns TRUE if the user has specified, via the <null ordering> clause in the ORDER BY clause, that NULL values of this column should sort lower than non-NULL values.
Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. NOTE: We must explicitly check for a null FromList here, column reference without a FROM list, as the grammar allows the following: insert into t1 values(c1) Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. Also, don't allow a predicate to be pushed down if it contains a ColumnReference that replaces an aggregate.  This can happen if the aggregate is in the HAVING clause.  In this case, we would be pushing the predicate into the SelectNode that evaluates the aggregate, which doesn't make sense, since the having clause is supposed to be applied to the result of the SelectNode. This also goes for column references that replaces a window function. RESOLVE - revisit this issue once we have views.  Copy all of the "appropriate fields" for a shallow copy. ColumnReference's are to the current row in the system. This lets us generate a faster get that simply returns the column from the current row, rather than getting the value out and returning that, only to have the caller (in the situations needed) stuffing it back into a new column holder object. We will assume the general generate() path is for getting the value out, and use generateColumn() when we want to keep the column wrapped. Return a clone of this node. Get the name of this column Get the column number for this ColumnReference. Return whether or not this CR is correlated. Determine whether or not this node was generated to replace an aggregate in the user's SELECT. Determine whether or not this node was generated to replace a window function call in the user's SELECT. Get the MERGE table (SOURCE or TARGET) associated with this column Get the nesting level for this CR. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Return the table name as the node it is. Get the column name for purposes of error messages or debugging. This returns the column name as used in the SQL statement. Thus if it was qualified with a table, alias name that will be included. Get the user-supplied schema name of this column.  This will be null if the user did not supply a name (for example, select t.a from t). Another example for null return value (for example, select b.a from t as b). But for following query select app.t.a from t, this will return APP Code generation of aggregate functions relies on this method Get the source this columnReference Get the source level for this CR. Get the ResultColumn that the source points to.  This is useful for getting what the source will be after this ColumnReference is remapped. Find the source result set for this ColumnReference and return it.  Also, when the source result set is found, return the position (within the source result set's RCL) of the column referenced by this ColumnReference.  The position is returned vai the colNum parameter. Get the name of the schema for the Column's base table, if any. Following example queries will all return APP (assuming user is in schema APP) select t.a from t select b.a from t as b select app.t.a from t Get the name of the underlying(base) table this column comes from, if any. Following example queries will all return T select a from t select b.a from t as b select t.a from t Get the user-supplied table name of this column.  This will be null if the user did not supply a name (for example, select a from t). The method will return B for this example, select b.a from t as b The method will return T for this example, select t.a from t Get the table number for this ColumnReference. Update the table map to reflect the source of this CR. The type of a ColumnReference is the type of its source unless the source is null then it is the type that has been set on this node. Returns true if this ColumnReference has been remapped; false otherwise. Return whether or not this expression tree is cloneable. Return whether or not this ColumnReference is scoped. Mark this column reference as "scoped", which means that it was created (as a clone of another ColumnReference) to serve as the left or right operand of a scoped predicate. Mark this node as being generated to replace an aggregate. (Useful for replacing aggregates in the HAVING clause with column references to the matching aggregate in the user's SELECT. Mark this node as being generated to replace a window function call. Return whether or not the source of this ColumnReference is itself a ColumnReference. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Do the 1st step in putting an expression into conjunctive normal form.  This step ensures that the top level of the expression is a chain of AndNodes. Remap all of the ColumnReferences in this expression tree to point to the ResultColumn that is 1 level under their current source ResultColumn. This is useful for pushing down single table predicates. RESOLVE: Once we start pushing join clauses, we will need to walk the ResultColumn/VirtualColumnNode chain for them to remap the references. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set the column number for this ColumnReference.  This is used when scoping predicates for pushdown. Associate this column with a SOURCE or TARGET table of a MERGE statement Set the nesting level for this CR.  (The nesting level at which the CR appears.) Set the source this columnReference Set the source level for this CR.  (The nesting level of the source of the CR.) Set this ColumnReference to refer to the given table number. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Is the column wirtable by the cursor or not. (ie, is it in the list of FOR UPDATE columns list)

Gets an object representing the owner of the compatibility space.
Set up the undoable operation during recovery redo.
Add a filter for determining which QueryTreeNodes give rise to privilege checks at run time. The null filter (the default) says that all QueryTreeNodes potentially give rise to privilege checks. Add a sequence descriptor to the list of referenced sequences. Add a column privilege to the list of used column privileges. Add a required role privilege to the list of privileges. Add a routine execute privilege to the list of used routine privileges. Add a schema privilege to the list of used privileges. Add a table or view privilege to the list of used table privileges. Add a usage privilege to the list of required privileges. Add an object to the pool that is created at compile time and used at execution time.  Use the integer to reference it in execution constructs.  Execution code will have to generate: <pre> (#objectType) (this.getPreparedStatement().getSavedObject(#int)) </pre> Add a compile time warning. Record that the compiler is entering a named scope. Increment the depth counter for that scope. Add a dependency between two objects. Add a dependency for the current dependent. Record that the compiler is exiting a named scope. Decrement the depth counter for that scope. Mark this CompilerContext as the first on the stack, so we can avoid continually popping and pushing a CompilerContext. Return the class factory to use in this compilation. Get the compilation schema descriptor for this compilation context. Will be null if no default schema lookups have occured. Ie. the statement is independent of the current schema. Get the current auxiliary provider list from this CompilerContext. Get the cursor info stored in the context. Return the in use state for the compiler context. Get the JavaFactory from this CompilerContext. Get the current next column number (for generated column names) from this CompilerContext. Get the next equivalence class for equijoin clauses. Get the current next ResultSet number from this CompilerContext. Get the current next subquery number from this CompilerContext. Get the current next table number from this CompilerContext. Get the number of Results in the current statement from this CompilerContext. Get the number of subquerys in the current statement from this CompilerContext. Get the number of tables in the current statement from this CompilerContext. Get the OptimizerFactory from this CompilerContext. Get the parameter list. ///////////////////////////////////////////////////////////////////////////////////  BEHAVIOR  /////////////////////////////////////////////////////////////////////////////////// Get the Parser from this CompilerContext. * Return the reliability requirements of this clause. See setReliability() for a definition of clause reliability.  Is the callable statement uses ? for return parameter. Get the saved object pool (for putting into the prepared statement). This turns it into its storable form, an array of objects. Get the isolation level for the scans in this query. Get a SortCostController. Get a StoreCostController for the given conglomerate. Get the TypeCompilerFactory from this CompilerContext. Get a unique Class name from this CompilerContext. Ensures it is globally unique for this JVM. Get the chain of compile time warnings. Is this the first CompilerContext on the stack? Report whether the given sequence has been referenced already. Return true if a QueryTreeNode passes all of the filters which determine whether the QueryTreeNode gives rise to run time privilege checks. Pop the default schema to use when compiling. Push a default schema to use when compiling. <p> Sometimes, we need to temporarily change the default schema, for example when recompiling a view, since the execution time default schema may differ from the required default schema when the view was defined. Another case is when compiling generated columns which reference unqualified user functions. </p> Sets the current privilege type context and pushes the previous on onto a stack. Column and table nodes do not know how they are being used. Higher level nodes in the query tree do not know what is being referenced. Keeping the context allows the two to come together. Remove a filter for determining which QueryTreeNodes give rise to privilege checks at run time. Reset compiler context (as for instance, when we recycle a context for use by another compilation. Reset the next ResultSet number from this CompilerContext. Get the current depth for the named scope. For instance, if we are processing a WHERE clause inside a subquery which is invoked inside an outer WHERE clause, the depth of the whereScope would be 2. Returns 0 if the compiler isn't inside any such scope. Set the compilation schema descriptor for this compilation context. Set the current auxiliary provider list for this CompilerContext. Set the current dependent from this CompilerContext. This should be called at the start of a compile to register who has the dependencies needed for the compilation. Set params Set the in use state for the compiler context. Set the parameter list. Sets which kind of query fragments are NOT allowed. Basically, these are fragments which return unstable results. CHECK CONSTRAINTS and CREATE PUBLICATION want to forbid certain kinds of fragments. If callable statement uses ? = form Set the saved object pool (for putting into the prepared statement). Set the isolation level for the scans in this query. Set whether we should skip adding USAGE privileges for user-defined types. Returns the previous setting of this variable. Return whether we are skipping USAGE privileges for user-defined types
Add a Provider to the current AuxiliaryProviderList, if one exists. Add a column privilege to the list of used column privileges. end of addRequiredColumnPriv Add a required role privilege to the list privileges. Add a routine execute privilege to the list of used routine privileges. Add a required schema privilege to the list privileges. Add a table or view privilege to the list of used table privileges.  Add a compile time warning.  Context interface     Add a dependency between two objects.  Get the compilation schema descriptor for this compilation context. Will be null if no default schema lookups have occured. Ie. the statement is independent of the current schema. Get the current auxiliary provider list from this CompilerContext.  Return the in use state for the compiler context. Get the next equivalence class for equijoin clauses. Get the current next subquery number from this CompilerContext. Get the number of subquerys in the current statement from this CompilerContext. Get the OptimizerFactory for this context  Get an array of type descriptors for all the ? parameters.  CompilerContext interface  we might want these to refuse to return anything if they are in-use -- would require the interface provide a 'done' call, and we would mark them in-use whenever a get happened. Return the reliability requirements of this clause. See setReliability() for a definition of clause reliability.  end of getRequiredPermissionsList      Get the chain of compile time warnings. end of initRequiredPriv  Report whether the given sequence has been referenced already.   Sets the current privilege type context. Column and table nodes do not know how they are being used. Higher level nodes in the query tree do not know what is being referenced. Keeping the context allows the two to come together. Reset compiler context (as for instance, when we recycle a context for use by another compilation. Set the compilation schema descriptor for this compilation context. Set the current auxiliary provider list for this CompilerContext.  Set the in use state for the compiler context.  Sets which kind of query fragments are NOT allowed. Basically, these are fragments which return unstable results. CHECK CONSTRAINTS and CREATE PUBLICATION want to forbid certain kinds of fragments.    Set whether we should skip adding USAGE privileges for user-defined types Return whether we are skipping USAGE privileges for user-defined types
************************************************************************ Public Methods of Loggable interface. ************************************************************************* Compress space from container. <p> Compress the indicate space from the container, returning the free pages to the OS.  Update the allocation page to reflect the file change. Return my format identifier.  method to support BeforeImageLogging debug ************************************************************************ Public Methods of Undoable interface. ************************************************************************* Compress space undo. <p>
Return my format identifier.  ************************************************************************ Public Methods of Formatable interface. *************************************************************************

Read an integer previously written by writeInt(). Read an integer previously written by writeInt(). Read a long previously written by writeLong(). Read a long previously written by writeLong(). Skip an integer previously written by writeInt(). Return the number of bytes that would be written by a writeInt call Write a compressed integer only supporting signed values. Formats are (with x representing value bits): <PRE> 1 Byte - 00xxxxxx                              Represents the value &lt;= 63 (0x3f) 2 Byte - 01xxxxxx xxxxxxxx                     Represents the value &gt; 63 &amp;&amp; &lt;= 16383 (0x3fff) 4 byte - 1xxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx   Represents the value &gt; 16383 &amp;&amp; &lt;= MAX_INT </PRE> Write a compressed integer directly to an OutputStream. Write a compressed long only supporting signed values. Formats are (with x representing value bits): <PRE> 2 byte - 00xxxxxx xxxxxxxx                     Represents the value &lt;= 16383 (0x3fff) 4 byte - 01xxxxxx xxxxxxxx xxxxxxxx xxxxxxxx   Represents the value &gt; 16383  &amp;&amp; &lt;= 0x3fffffff 8 byte - 1xxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx   Represents the value &gt; 0x3fffffff &amp;&amp; &lt;= MAX_LONG </PRE> Write a compressed integer only supporting signed values.
The SQL char_length() function. substr() function matchs DB2 syntax and behaviour.
overrides BindOperatorNode.bindExpression because concatenation has special requirements for parameter binding. Check if this node always evaluates to the same value. If so, return a constant node representing the known result. Resolve a concatenation operator
Remove all objects that are not kept and not dirty. Clean all dirty objects matching a partial key. Clean all dirty objects in the cache. All objects that existed in the cache at the time of the call will be cleaned. Objects added later may or may not be cleaned. Clean an entry in the cache and decrement its keep count. The entry must be kept before this method is called, and it must contain the specified <code>Cacheable</code>. Clean all dirty objects matching a partial key. If no key is specified, clean all dirty objects in the cache. Clean an entry in the cache. Count an eviction from the cache. Count a cache hit. Count a cache miss. Create an object in the cache. The object is kept until <code>release()</code> is called. Discard all unused objects that match a partial key. Dirty objects will not be cleaned before their removal. Evict an entry to make room for a new entry that is being inserted into the cache. Clear the identity of its {@code Cacheable} and set it to {@code null}. When this method is called, the caller has already chosen the {@code Cacheable} for reuse. Therefore, this method won't call {@code CacheEntry.free()} as that would make the {@code Cacheable} free for reuse by other entries as well. <p> The caller must have locked the entry that is about to be evicted. Implementation of the CacheManager interface Find an object in the cache. If it is not present, add it to the cache. The returned object is kept until <code>release()</code> is called. Find an object in the cache. If it is not present, return <code>null</code>. The returned object is kept until <code>release()</code> is called. Get the number of allocated entries in the cache. Check if collection of hit/miss/eviction counts is enabled. Get the entry associated with the specified key from the cache. If the entry does not exist, insert an empty one and return it. The returned entry is always locked for exclusive access by the current thread, but not kept. If another thread is currently setting the identity of this entry, this method will block until the identity has been set. Get the number of evictions from the cache. Get the number of cache hits. Get the maximum number of entries in the cache. Get the number of cache misses. Return the <code>ReplacementPolicy</code> instance for this cache. Privileged module lookup. Must be private so that user code can't call this entry point. Get the number of cached objects. Insert a {@code CacheEntry} into a free slot in the {@code ReplacementPolicy}'s internal data structure, and return a {@code Cacheable} that the caller can reuse. The entry must have been locked before this method is called. Release an object that has been fetched from the cache with <code>find()</code>, <code>findCached()</code> or <code>create()</code>. Remove an object from the cache. The object must previously have been fetched from the cache with <code>find()</code>, <code>findCached()</code> or <code>create()</code>. The user of the cache must make sure that only one caller executes this method on a cached object. This method will wait until the object has been removed (its keep count must drop to zero before it can be removed). Remove an entry from the cache. Its <code>Cacheable</code> is cleared and made available for other entries. This method should only be called if the entry is present in the cache and locked by the current thread. Enable or disable collection of hit/miss/eviction counts. Complete the setting of the identity. This includes notifying the threads that are waiting for the setting of the identity to complete, so that they can wake up and continue. If setting the identity failed, the entry will be removed from the cache. Shut down the cache. Specify a daemon service that can be used to perform operations in the background. Callers must provide enough synchronization so that they have exclusive access to the cache when this method is called. Return a collection view of all the <code>Cacheable</code>s in the cache. There is no guarantee that the objects in the collection can be accessed in a thread-safe manner once this method has returned, so it should only be used for diagnostic purposes. (Currently, it is only used by the <code>StatementCache</code> VTI.)
Create a new <code>ConcurrentCache</code> instance.


Add all waiters in this lock table to a <code>Map</code> object. This method can only be called by the thread that is currently performing deadlock detection. All entries that are visited in the lock table will be locked when this method returns. The entries that have been seen and locked will be unlocked after the deadlock detection has finished. Check whether anyone is blocked. Check whether there is a deadlock. Make sure that only one thread enters deadlock detection at a time. Get an entry from the lock table. If no entry exists for the <code>Lockable</code>, insert an entry. The returned entry will be locked and is guaranteed to still be present in the table. Get the wait timeout in milliseconds. * Public Methods Lock an object within a specific compatibility space. Decrease blockCount by one. EXCLUDE-END-lockdiag- Increase blockCount by one. Set the deadlock timeout. * Non public methods EXCLUDE-START-lockdiag- Set the wait timeout. EXCLUDE-START-lockdiag- make a shallow clone of myself and my lock controls EXCLUDE-END-lockdiag- Unlock an object, previously locked by lockObject(). If unlockCOunt is not zero then the lock will be unlocked that many times, otherwise the unlock count is taken from item. Unlock an object, previously locked by lockObject(). Unlock an object once if it is present in the specified group. Also remove the object from the group. {@inheritDoc }
Create the <code>ConcurrentLockSet</code> object that keeps the locks.
Complete the conditional and patch up any jump instructions. Fill in the offsets for a conditional or goto instruction that were dummied up as zero during code generation. Handles modifying branch logic when the offset for the branch is greater than can fit in 16 bits. In this case a GOTO_W with a 32 bit offset will be used, see details within the method for how this is acheived in all situations. This method might insert instructions in the already generated byte code, thus increasing the program counter. Complete the 'then' block and start the 'else' block for this conditional
Accept the visitor for all visitable children of this node. <p> Bind the case operand, if there is one, and check that it doesn't contain anything that's illegal in a case operand (such as calls to routines that are non-deterministic or modify SQL). </p> <p> Also, if the type of the case operand needs to be inferred, insert dummy parameter nodes into {@link #testConditions} instead of the case operand, so that the type can be inferred individually for each test condition. Later, {@link #bindExpression} will find a common type for all of them, use that type for the case operand, and reinsert the case operand into the test conditions. </p> Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views.  Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. Do code generation for this conditional expression. Return whether or not this expression tree represents a constant expression. {@inheritDoc } Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. This method makes sure any SQL NULLs will be cast to the correct type. Remap all ColumnReferences in this tree to be clones of the underlying expression.
Returns this service's configuration object. <p> Services implementing <code>Configurable</code> should take care when returning a service configuration object since this object is probably sensitive. <p> If the Java Runtime Environment supports permissions, it is recommended that the caller is checked for some appropriate permission before returning the configuration object.
Create ProductVersionHolder in security block for Java 2 security. load product version information and accumulate exceptions Check to see if the jvm version is such that JDBC 4.2 is supported
Request set of properties associated with a table. <p> Returns a property object containing all properties that the store knows about, which are stored persistently by the store.  This set of properties may vary from implementation to implementation of the store. <p> This call is meant to be used only for internal query of the properties by jbms, for instance by language during bulk insert so that it can create a new conglomerate which exactly matches the properties that the original container was created with.  This call should not be used by the user interface to present properties to users as it may contain properties that are meant to be internal to jbms.  Some properties are meant only to be specified by jbms code and not by users on the command line. <p> Note that not all properties passed into createConglomerate() are stored persistently, and that set may vary by store implementation. Request the system properties associated with a table. <p> Request the value of properties that are associated with a table.  The following properties can be requested: derby.storage.pageSize derby.storage.pageReservedSpace derby.storage.minimumRecordSize derby.storage.initialPages <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(ConglomerateController cc) { Properties prop = new Properties(); prop.put("derby.storage.pageSize", ""); cc.getTableProperties(prop); System.out.println( "table's page size = " + prop.getProperty("derby.storage.pageSize"); }
Add a column to the conglomerate. <p> This routine update's the in-memory object version of the Conglomerate to have one more column of the type described by the input template column. Note that not all conglomerates may support this feature. Online compress table. Returns a ScanManager which can be used to move rows around in a table, creating a block of free pages at the end of the table.  The process of executing the scan will move rows from the end of the table toward the beginning.  The GroupFetchScanController will return the old row location, the new row location, and the actual data of any row moved.  Note that this scan only returns moved rows, not an entire set of rows, the scan is designed specifically to be used by either explicit user call of the SYSCS_ONLINE_COMPRESS_TABLE() procedure, or internal background calls to compress the table. The old and new row locations are returned so that the caller can update any indexes necessary. This scan always returns all collumns of the row. All inputs work exactly as in openScan().  The return is a GroupFetchScanController, which only allows fetches of groups of rows from the conglomerate. <p> Note that all Conglomerates may not implement openCompressScan(), currently only the Heap conglomerate implements this scan. Drop this conglomerate. Retrieve the maximum value row in an ordered conglomerate. <p> Returns true and fetches the rightmost row of an ordered conglomerate into "fetchRow" if there is at least one row in the conglomerate.  If there are no rows in the conglomerate it returns false. <p> Non-ordered conglomerates will not implement this interface, calls will generate a StandardException. <p> RESOLVE - this interface is temporary, long term equivalent (and more) functionality will be provided by the openBackwardScan() interface. Get the containerid of conglomerate. <p> Will have to change when a conglomerate could have more than one containerid. Return dynamic information about the conglomerate to be dynamically reused in repeated execution of a statement. <p> The dynamic info is a set of variables to be used in a given ScanController or ConglomerateController.  It can only be used in one controller at a time.  It is up to the caller to insure the correct thread access to this info.  The type of info in this is a scratch template for btree traversal, other scratch variables for qualifier evaluation, ... <p> Get the id of the container of the conglomerate. <p> Will have to change when a conglomerate could have more than one container.  The ContainerKey is a combination of the container id and segment id. Return static information about the conglomerate to be included in a a compiled plan. <p> The static info would be valid until any ddl was executed on the conglomid, and would be up to the caller to throw away when that happened.  This ties in with what language already does for other invalidation of static info.  The type of info in this would be containerid and array of format id's from which templates can be created. The info in this object is read only and can be shared among as many threads as necessary. <p> Is this conglomerate temporary? <p> Bulk load into the conglomerate. <p> Individual rows that are loaded into the conglomerate are not logged. After this operation, the underlying database must be backed up with a database backup rather than an transaction log backup (when we have them). This warning is put here for the benefit of future generation. <p> Open a conglomerate controller. <p> Open a scan controller. Return an open StoreCostController for the conglomerate. <p> Return an open StoreCostController which can be used to ask about the estimated row counts and costs of ScanController and ConglomerateController operations, on the given conglomerate. <p>
Check consistency of a conglomerate. Checks the consistency of the data within a given conglomerate, does not check consistency external to the conglomerate (ie. does not check that base table row pointed at by a secondary index actually exists). Raises a StandardException on first consistency problem. Close the conglomerate controller. <p> Close the conglomerate controller.  Callers must not use the conglomerate controller after calling close.  It is strongly recommended that callers clear out the reference after closing, e.g., <p> <blockquote><pre> ConglomerateController cc; cc.close; cc = null; </pre></blockquote> Close conglomerate controller as part of terminating a transaction. <p> Use this call to close the conglomerate controller resources as part of committing or aborting a transaction.  The normal close() routine may do some cleanup that is either unnecessary, or not correct due to the unknown condition of the controller following a transaction ending error. Use this call when closing all controllers as part of an abort of a transaction. <p> This call is meant to only be used internally by the Storage system, clients of the storage system should use the simple close() interface. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this. Dump debugging output to error log. <p> Dump information about the conglomerate to error log. This is only for debugging purposes, does nothing in a delivered system, currently. Delete a row from the conglomerate. Fetch the (partial) row at the given location. <p> Fetch the (partial) row at the given location. <p> Get information about space used by the conglomerate. Fetch the (partial) row at the given location. <p> RESOLVE - interface NOT SUPPORTED YET!!!!! boolean fetch( RowLocation             loc, DataValueDescriptor[]   destRow, FormatableBitSet                 validColumns, Qualifier[][]           qualifier) throws StandardException; Insert a row into the conglomerate. insert row and fetch it's row location in one operation. <p> Insert a row into the conglomerate, and store its location in the provided destination row location.  The row location must be of the correct type for this conglomerate (a new row location of the correct type can be obtained from newRowLocationTemplate()). Return whether this is a keyed conglomerate. Lock the given record id/page num pair. <p> Should only be called by access, to lock "special" locks formed from the Recordhandle.* reserved constants for page specific locks. <p> This call can be made on a ConglomerateController that was opened for locking only. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this. Lock the given row location. <p> Should only be called by access. <p> This call can be made on a ConglomerateController that was opened for locking only. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this. Return a row location object of the correct type to be used in calls to insertAndFetchLocation. Replace the (partial) row at the given location. UnLock the given row location. <p> Should only be called by access. <p> This call can be made on a ConglomerateController that was opened for locking only. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this.
This method searches the received array of conglom descriptors to find all descriptors that currently share a physical conglom with "this".  The method then searches within those sharing descriptors to find one that fully describes what a physical conglom would have to look like in order to support _all_ of the sharing descriptors in the array--esp. one that correctly enforces the uniqueness requirements for those descriptors. Drop this ConglomerateDescriptor when it represents an index. If this is the last desciptor for a physical index then the physical index (conglomerate) and its descriptor will be dropped. Get the provider's type. Get the column names for this conglomerate descriptor. This is useful for tracing the optimizer. Gets the name of the conglomerate.  For heaps, this is null.  For indexes, it is the index name. Gets the number for the conglomerate.  Provider interface     Gets the index row generator for this conglomerate, null if the conglomerate is not an index. Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Gets the UUID for the schema that the conglomerate belongs to. Gets the UUID for the table that the conglomerate belongs to. Gets the UUID String for the conglomerate. Tells whether the conglomerate is an index backing up a constraint. Tells whether the conglomerate can be used as an index. Set the column names for this conglomerate descriptor. This is useful for tracing the optimizer. Set the name of the conglomerate.  Used only by rename index. Set the conglomerate number. This is useful when swapping conglomerates, like for bulkInsert. Convert the conglomerate descriptor to a String
Remove the specified conglomerate descriptor from the conglomerate descriptor list.  If the descriptor is not found, no errors are issued. Remove the specified conglomerate descriptor from the conglomerate descriptor list.  If the descriptor is not found, no errors are issued. Get a conglomerate descriptor by its Name Get a conglomerate descriptor by its number Get a conglomerate descriptor by its UUID String Get an array of conglomerate descriptors with the given conglomerate number.  We get more than one descriptors if duplicate indexes share one conglomerate. Get an array of conglomerate descriptors by a UUID String.  We get more than one descriptors if duplicate indexes share one conglomerate.
Create the conglomerate and return a conglomerate object for it.  It is expected that the caller of this method will place the the resulting object in the conglomerate directory. Return the conglomerate factory id. <p> Return a number in the range of 0-15 which identifies this factory. Code which names conglomerates depends on this range currently, but could be easily changed to handle larger ranges.   One hex digit seemed reasonable for the number of conglomerate types currently implemented (heap, btree) and those that might be implemented in the future: gist, gist btree, gist rtree, hash, others? ). <p> Interface to be called when an undo of an insert is processed. <p> Implementer of this class provides interface to be called by the raw store when an undo of an insert is processed.  Initial implementation will be by Access layer to queue space reclaiming events if necessary when a rows is logically "deleted" as part of undo of the original insert.  This undo can happen a lot for many applications if they generate expected and handled duplicate key errors. <p> Caller may decide to call or not based on deleted row count of the page, or if overflow rows/columns are present. Return Conglomerate object for conglomerate with container_key. <p> Return the Conglomerate Object.  This is implementation specific. Examples of what will be done is using the key to find the file where the conglomerate is located, and then executing implementation specific code to instantiate an object from reading a "special" row from a known location in the file.  In the btree case the btree conglomerate is stored as a column in the control row on the root page. <p> This operation is costly so it is likely an implementation using this will cache the conglomerate row in memory so that subsequent accesses need not perform this operation.
Given an array of columnOrderings, return an array of collation ids. <p> If input array is null, produce a default collation_id array of all StringDataValue.COLLATION_TYPE_UCS_BASIC values. Given an array of objects, return an array of format id's. <p> Public Methods of This class: (arranged Alphabetically ) Create a list of all the properties that Access wants to export through the getInternalTablePropertySet() call. <p> This utility routine creates a list of properties that are shared by all conglomerates.  This list contains the following: derby.storage.initialPages derby.storage.minimumRecordSize derby.storage.pageReservedSpace derby.storage.pageSize derby.storage.reusableRecordId <p> Create a list of all the properties that Access wants to export through the getInternalTablePropertySet() call. <p> This utility routine creates a list of properties that are shared by all conglomerates.  This list contains the following: derby.storage.initialPages derby.storage.minimumRecordSize derby.storage.pageReservedSpace derby.storage.pageSize <p> * Format a page of data, as access see's it. Read "sparse" array of collation id's <p> The format to be read first has the number of entries as a compressed int.  And then for each non-COLLATION_TYPE_UCS_BASIC value there is pair of compressed ints: (array offset, array entry value) <p> reads the sparse array as written by writeCollationIdArray(). Read a format id array in from a stream. <p> Write array of collation id's as a sparse array. <p> The format only writes out those array entries which are not StringDataValue.COLLATION_TYPE_UCS_BASIC.  The sparse array first writes the number of entries as a compressed int.  And then for each non-COLLATION_TYPE_UCS_BASIC, it writes out a pair of compressed ints: (array offset, array entry value) Write a format id array to a stream. <p>
Rollback the UnitOfWorkListener specifically. Completes piggy-backing of the new current isolation level by updating the cached copy in am.Connection. Completes piggy-backing of the new current schema by updating the cached copy in am.Connection.
Perform a commit if autocommit is enabled. Perform a commit if one is needed. Get and save a unique calendar object for this JDBC object. No need to synchronize because multiple threads should not be using a single JDBC object. Even if they do there is only a small window where each would get its own Calendar for a single call. Return an object to be used for connection synchronization. Return a reference to the EmbedConnection Gets the LanguageConnectionContext for this connection. Get and cache the LanguageConnectionContext for this connection. Handle any exception. If Autocommit is on, note that a commit is needed. Setup the context stack (a.k.a. context manager) for this connection. Setup the context stack (a.k.a. context manager) for this connection.
Get a new connection object equivalent to the call <PRE> DriverManager.getConnection("jdbc:default:connection"); </PRE> Get a jdbc ResultSet based on the execution ResultSet. Process the resultSet as a dynamic result for closure. The result set will have been created in a Java procedure. If the ResultSet is a valid dynamic ResultSet for this connection, then it is set up as a dynamic result which includes: <UL> <LI> breaking its link with the JDBC connection that created it, since there is a good chance that connection was closed explicitly by the Java procedure. <LI> marking its activation as single use to ensure the close of the ResultSet will close the activation. </UL> <P> If the result set a valid dynamic result then false will be returned and no action made against it.
Making a new connection, add it to the pool, and make it current. returns a unique Connection# name by going through existing sessions separate from the constructor so that connection failure does not prevent object creation.

<p>This fixture tries a number of times to connect to a non-existent database, in order to test Derby's ability to handle this situation without running out of resources (for example Java heap space (memory)). See <a href="https://issues.apache.org/jira/browse/DERBY-2480">DERBY-2480</a> for details.</p> <p>This test fixture is currently not part of any large JUnit suite because <b>1)</b> the default number of connection attempts is rather large, and takes some time to complete (depending on hardware), and <b>2)</b> if the tested Derby version is still vulnerable to DERBY-2480 or similar issues the JVM will most likely run out of memory (depending on heap settings), causing subsequent tests to fail, hang or not run at all.</p> <p> <b>Note:</b> The JVM may slow down significantly (even appear to hang) before an OOME is thrown. Depending on the avaliable resources, the error may or may not be reported in the logs (derby.log, server console).</p> <p> This fixture requires java.sql.DriverManager. This is because simple and easy control of the connection handling and database creation is desired (see implementation comments). However, the test logic itself should also work with other connection mechanisms.</p> Returns a log writer that discards all the data written to it. Will check if the JDBC driver has been loaded and load it if that is not the case. Any other exception messages than "No suitable driver" on the first attempt to get the JDBC driver will result in an assertion failure. Creates a Test Suite to be run by a JUnit TestRunner. The elements of the test suite may depend on the environment in which the TestRunner is running (classpath, JVM, etc.).

Enable statement pooling on the specified data source. Get a connection from a single use ConnectionPoolDataSource configured from the configuration but with the passed in property set.


Implementations of this interface should not dereference common layer Connection state, as it is passed in, but may dereference material layer Connection state if necessary for performance.
Privileged lookup of a Context. Must be private so that user code can't call this entry point. Get the current LanguageConnectionContext. Used by public api code that needs to ensure it is in the context of a SQL connection.
get the physical databasename in use at this time Get the login timeout in seconds. Open a connection with the database, user and password defined by the configuration passed to setConfiguration. If the database does not exist then it should be created. Open a connection with the database, user and password defined by the configuration passed to setConfiguration. If the database does not exist then it should be created. Open a connection to the database defined by the configuration passed to setConfiguration. If the database does not exist then it should be created. Open a connection to the database defined by the configuration passed to setConfiguration. If the database does not exist then it should be created. Open a connection to the database defined by the configuration passed to setConfiguration. If the database does not exist then it should be created. Link this connector to the given configuration. Should be called once upon setup. Set the login timeout for getting connections. Timeout is measured in seconds. Shutdown the running default database using user and password defined by the configuration passed to setConfiguration. Return nothing, exception is expected to be thrown with SQLState 08006 Shutdown the running derby engine (not the network server). This method can only be called when the engine is running embedded in this JVM. Return nothing, exception is expected to be thrown with SQLState XJ015

This ConsInfo describes columns in a referenced table. What are their names? This ConsInfo describes columns in a referenced table. What is that table? Get the name of the table that these column live in. This ConsInfo describes columns in a referenced table. What is the schema that the referenced table lives in? Get the referential Action for a Delete. Get the referential Action for an Update.
Check the named table, ensuring that all of its indexes are consistent with the base table. Use this method only within an SQL-J statement; do not call it directly. <P>When tables are consistent, the method returns true. Otherwise, the method throws an exception. <p>To check the consistency of a single table: <p><code> VALUES ConsistencyChecker::checkTable(<i>SchemaName</i>, <i>TableName</i>)</code></p> <P>For example, to check the consistency of the table <i>APP.Flights</i>: <p><code> VALUES ConsistencyChecker::checkTable('APP', 'FLIGHTS')</code></p> <p>To check the consistency of all of the tables in the 'APP' schema, stopping at the first failure: <P><code>SELECT tablename, ConsistencyChecker::checkTable(<br> 'APP', tablename)<br> FROM sys.sysschemas s, sys.systables t WHERE s.schemaname = 'APP' AND s.schemaid = t.schemaid</code> <p> To check the consistency of an entire database, stopping at the first failure: <p><code>SELECT schemaname, tablename,<br> ConsistencyChecker::checkTable(schemaname, tablename)<br> FROM sys.sysschemas s, sys.systables t<br> WHERE s.schemaid = t.schemaid</code>
Check whether this object is equal to another object. Get the byte array representation of the consistency token. Calculate the hash code. Return a string representation of the consistency token by converting it to a <code>BigInteger</code> value. (For debugging only.)
Run the ConstantAction.
Always return false since constant actions don't need recompilation when the row counts change.
{@inheritDoc } {@inheritDoc } Visit the node and call {@code evaluateConstantExpressions()} if it is a {@code ValueNode}. {@inheritDoc }
Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. In this case, there are no sub-expressions, and the return type is already known, so this is just a stub.  This generates the proper constant.  It is implemented by every specific constant node (e.g. IntConstantNode). For a ConstantNode, we generate the equivalent literal value. A null is generated as a Null value cast to the type of the constant node. The subtypes of ConstantNode generate literal expressions for non-null values. Return a clone of this node. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query VARIANT				- immutable Get the value in this ConstantNode Return whether or not this expression tree is cloneable. Return whether or not this expression tree represents a constant expression. Return whether or not this node represents a typed null constant. Set the value in this ConstantNode. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Return an estimate of the size of the constant pool entry. Get the first index in a index type pool entry. This call is valid when getTag() returns one of <UL> <LI> CONSTANT_Class <LI> CONSTANT_Fieldref <LI> CONSTANT_Methodref <LI> CONSTANT_InterfaceMethodref <LI> CONSTANT_String <LI> CONSTANT_NameAndType </UL> Get the second index in a index type pool entry. This call is valid when getTag() returns one of <UL> <LI> CONSTANT_Fieldref <LI> CONSTANT_Methodref <LI> CONSTANT_InterfaceMethodref <LI> CONSTANT_NameAndType </UL> Return the key used to key this object in a hashtable * Public API methods Return the tag or type of the entry. Will be equal to one of the constants above, e.g. CONSTANT_Class.

Get the constraint id of the constraint Get the constraint name Class implementation Get the constraint type. Get the associated index constant action. Evaluate a check constraint or not null column constraint. Generate a query of the form SELECT COUNT(*) FROM t where NOT(<check constraint>) and run it by compiling and executing it.   Will work ok if the table is empty and query returns null. Make sure that the foreign key constraint is valid with the existing data in the target table.  Open the table, if there aren't any rows, ok.  If there are rows, open a scan on the referenced key with table locking at level 2.  Pass in the scans to the BulkRIChecker.  If any rows fail, barf.
Bind this constraint definition. Return the auxiliary provider list. Gets a unique name for the backing index for this constraint of the form SQLyymmddhhmmssxxn yy - year, mm - month, dd - day of month, hh - hour, mm - minute, ss - second, xx - the first 2 digits of millisec because we don't have enough space to keep the exact millisec value, n - number between 0-9 Allocates a UUID if one doesn't already exist for the index backing this constraint. This allows Replication logic to agree with the core compiler on what the UUIDs of indices are. Get the check condition from this table constraint. Get the column list from this node. Get the name of the constraint. If the user didn't provide one, we make one up. This allows Replication to agree with the core compiler on the names of constraints. Get the text of the constraint. (Only meaningful for check constraints.) Get the constraint type Return the behavior of this constraint. See {@link org.apache.derby.iapi.sql.StatementType#DROP_CASCADE} etc. To support dropping existing constraints that may have mismatched schema names we need to support ALTER TABLE S1.T DROP CONSTRAINT S2.C. If a constraint name was specified this returns it, otherwise it returns null. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Get the optional properties for the backing index to this constraint. Get the count of enabled fks that reference this constraint /////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////// Get the UUID factory  Does this element have a check constraint. Does this element have a constraint on it. Is this a foreign key constraint. Is this a primary key constraint. Is this a unique key constraint. Is this constraint enabled. Is this constraint referenced. Qualify all SQL object names in a CHECK constraint with schema name. Does this constraint require a backing index for its implementation? Is this a primary key or unique constraint? Set the auxiliary provider list. Set the check condition for this table constraint. Set the column list for this node.  This is useful for check constraints where the list of referenced columns is built at bind time. Set the optional properties for the backing index to this constraint. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Indicates whether the column descriptor list is type comparable with the constraints columns.  The types have to be identical AND in the same order to succeed. Does a column intersect with our referenced columns Returns TRUE if the constraint is deferrable Does a column in the input set intersect with our referenced columns? Drop the constraint.  Clears dependencies, drops the backing index and removes the constraint from the list on the table descriptor.  Does NOT do an dm.invalidateFor() Is this constraint enforced? Get the provider's type. Get the column descriptors for all the columns referenced by this constraint. Gets the name of the constraint. Get the text of the constraint. (Only non-null/meaningful for check constraints.) Gets an identifier telling what type of descriptor it is (UNIQUE, PRIMARY KEY, FOREIGN KEY, CHECK). //////////////////////////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////////////////////////////   RESOLVE: For now the ConstraintDescriptor code stores the array of key columns in the field 'otherColumns'. Jerry plans to re-organize things. For now to minimize his rototill I've implemented this function on the old structures. All new code should use getKeyColumns to get a constraint's key columns. Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Get the number of enforced fks that reference this key.  Overriden by ReferencedKeyConstraints. Returns an array of column ids (i.e. ordinal positions) for the columns referenced in this table for a primary key, unique key, referential, or check constraint. Get the SchemaDescriptor for the schema that this constraint belongs to. Get the table descriptor upon which this constraint is declared. Gets the UUID of the table the constraint is on. Gets the UUID of the constraint. Does this constraint have a backing index? Returns TRUE if the constraint is initially deferred Is this constraint referenced?  Return false.  Overridden by ReferencedKeyConstraints. ////////////////////////////////////////////////////  DEPENDENT INTERFACE  //////////////////////////////////////////////////// Check that all of the dependent's dependencies are valid. Mark the dependent as invalid (due to at least one of its dependencies being invalid).  Always an error for a constraint -- should never have gotten here. Does this constraint need to fire on this type of DML? Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). Convert the ColumnDescriptor to a String.
Drop the constraint with the given UUID. Return the nth (0-based) element in the list. Get the ConstraintDescriptor with the matching UUID String for the backing index. Get the ConstraintDescriptor with the matching constraint id. Get the ConstraintDescriptor with the matching constraint name. Return a list of constraints where enforced is as passed in. Get the ConstraintDescriptor with the matching constraint name. Return whether or not the underlying system table has been scanned. Return a ConstraintDescriptorList containing the ConstraintDescriptors of the specified type that are in this list. Mark whether or not the underlying system table has been scanned.  (If a table does not have any constraints then the size of its CDL will always be 0.  We used these get/set methods to determine when we need to scan the table.
This ConsInfo describes columns in a referenced table. What are their names? Get the name of the table that these column live in. Get the referential Action for a Delete. Get the referential Action for an Update. Get the formatID which corresponds to this class. Read this object from a stream of stored objects. ////////////////////////////////////////////////////////////  Misc  //////////////////////////////////////////////////////////// ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out

An equals method that returns true if the other obejct is a sub-class of this, and the container identities are equal *and* it is the same class as this. <BR> This allows mutiple additions of value equality obejcts to the observer list while only retaining one.
Methods specific to this class Open the container with this segmentId and containerId. This method should only be called if the container has already been created. Subclass (e.g., ContainerOperation) that wishes to do something abou missing container in load tran should override this method to return the recreated container Loggable methods the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. Space Operation has no optional data to write out A space operation is a RAWSTORE log record
Add an empty page to the container and obtain exclusive access to it. <P> Note that the added page may not be the last page in the Container. Once the Page is no longer required the Page's unlatch() method must be called. Add an empty page to the container and obtain exclusive access to it. <P> If flag == ADD_PAGE_DEFAULT, this call is identical to addPage(). <BR> If flag == ADD_PAGE_BULK, then this call signifies to the container that this addPage is part of a large number of additional pages and it is desirable to do whatever possible to facilitate adding many subsequent pages. The actual container implementation will decide whether or not to heed this hint and what to do about it. Backup the container to the specified path. Close me. After using this method the caller must throw away the reference to the Container object, e.g. <PRE> ref.close(); ref = null; </PRE> <BR> The container will be closed automatically at the commit or abort of the transaction if this method is not called explictly. <BR> Any pages that were obtained using me and have not been released using Page's unlatch method are released, and references to them must be thrown away. This record probably has shrunk considerably.  Free its reserved space or compact it. Release free space to the OS. <P> As is possible release any free space to the operating system.  This will usually mean releasing any free pages located at the end of the file using the java truncate() interface. Flush all dirty pages of the container to disk.  Used mainly for UNLOGGED or CREATE_UNLOGGED operation. Request the system properties associated with a container. <p> Request the value of properties that are associated with a table.  The following properties can be requested: derby.storage.pageSize derby.storage.pageReservedSpace derby.storage.minimumRecordSize <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(ConglomerateController cc) { Properties prop = new Properties(); prop.put("derby.storage.pageSize", ""); cc.getTableProperties(prop); System.out.println( "table's page size = " + prop.getProperty("derby.storage.pageSize"); } Get the total estimated number of allocated (not freed, not deallocated) user pages in the container, including overflow pages. this number is a rough estimate and may be grossly off. Cost estimation Get the total estimated number of rows in the container, not including overflow rows.  This number is a rough estimate and may be grossly off. Obtain exclusive access to the current first page of the container. Only a valid, non overflow page will be returned. Pages in the container are ordered in an internally defined ordering. <P> Note that once this method returns this page may no longer be the first page of the container.  I.e, other threads may allocate pages prior to this page number while this page is latched.  It is up to the caller of this routine to synchronize this call with addPage to assure that this is the first page. <BR> As long as the client provide the necessary lock to ensure that no addPage is called, then this page is guaranteed to be the first page of the container in some internally defined ordering of the pages. Return my identifier. Return the locking policy for this open container. Obtain exclusive access to the next valid page of the given page number in the container. Only a valid, non overflow page will be returned. Pages in the container are ordered in an internally defined ordering. <P> Note that once this method returns this page may no longer be the next page of the container.  I.e, other threads may allocate pages prior to this page number while this page is latched.  It is up to the caller of this routine to synchronize this call with addPage to assure that this is the first page. <BR> As long as the client provide the necessary lock to ensure that no addPage is called, then this page is guaranteed to be the next page of the container in some internally defined ordering of the pages. <BR> If no pages are added or removed, then an iteration such as: <PRE> for (Page p = containerHandle.getFirstPage(); p != null; p = containerHandle.getNextPage(p.getPageNumber())) <PRE> will guarentee to iterate thru and latched all the valid pages in the container Obtain exclusive access to the page with the given page number. Once the Page is no longer required the Page's unlatch() method must be called. <P> The Page object is guaranteed to remain in-memory and exclusive to the caller until its unlatch() method is called. Get a page for insert.  If RawStore thinks it knows where a potentially suitable page is for insert, it will return it.  If RawStore doesn't know where a suitable page for insert is, or if there are no allocated page, then null is returned.  If a page is returned, it will be a valid, non-overflow page.   A potentially suitable page is one which has enough space for a minium sized record. Identical to getPage but returns null immediately if the desired page is already latched by another Container. Get the reusable recordId sequence number. Get information about space used by the container. Return my unique identifier, this identifier will be unique to each instance of an open container handle.  This id is used by the locking system to group locks to an open container handle. Obtain exclusive access to the page with the given page number. Will only return a valid, non-overflow user page - so can be used by routines in post commit to get pages to attempt deleted row space reclamation.  If for some reason a request is made for an overflow page a null will be returned. Once the Page is no longer required the Page's unlatch() method must be called. <P> The Page object is guaranteed to remain in-memory and exclusive to the caller until its unlatch() method is called. Obtain exclusive access to the page with the given page number. Will only return a valid, non-overflow user page - so can be used by routines in post commit to get pages to attempt deleted row space reclamation.  If for some reason a request is made for an overflow page a null will be returned. Once the Page is no longer required the Page's unlatch() method must be called. <P> The Page object is guaranteed to remain in-memory and exclusive to the caller until its unlatch() method is called. Is the container opened for read only or update? Return true if this containerHandle refers to a temporary container. Return a record handle that is initialized to the given segment id, container id, page number and record id. Try to preallocate numPage new pages if possible. Remove this page from the container and unlatch the page.  <B>Caller should commit or abort this transaction ASAP because failure to do so will slow down page allocation of this container. </B> <BR>The page to be removed must be latched and gotten (or added) by this ContainerHandle.  The page should not be used again after this call as if it has been unlatched.  If the call to removePage is successful, this page is invalid should not be gotten again with getPage. <BR>RemovePage will guarantee to unlatch the page even if a StandardException is thrown. <P> <B>Locking Policy</B> <BR> The page will not be freed until the transaction that removed the page commits.  A special RecordHandle.DEALLOC_PROTECTION_HANDLE lock will be gotten for the transaction and which is used to prevent the page from being freed.  This lock will be held regardless of the default locking policy of the transaction that called removedPage. Set the total estimated number of rows in the container.  Often, after a scan, the client of RawStore has a much better estimate of the number of rows in the container then what RawStore has.  Use this better number for future reference. <BR> It is OK for a ReadOnly ContainerHandle to set the estimated row count. Set the locking policy for this open container
*	Methods of Observer Open the container and call the doIt method
* Methods of Object Return my identifier within the segment Return my segment identifier This lockable wants to participate in the Virtual Lock table. * Methods of Lockable This method will only be called if requestCompatible returned false. This results from two cases, some other compatabilty space has some lock that would conflict with the request, or this compatability space has a lock tha * methods of Matchable * Methods to read and write ContainerKeys.
Get an integer representation of the type of the lock. This method is guaranteed to return an integer &gt;= 0 and &t; C_NUMBER. No correlation between the value and one of the static variables (CIS etc.) is guaranteed, except that the values returned do not change.
Obtain a Container shared or exclusive lock	until the end of the nested transaction. Unlock read locks. <p> In Cursor stability release all read locks obtained.  unlockContainer() will be called when the container is closed. <p>
Obtain a Container shared or exclusive lock	until the end of the nested transaction.
override ContainerBasicOperation's findContainerForRedoRecovery Find container for load tran. <p> If we are in load tran, and the operation is a create, the container may not (should not?) exist yet.  We need to recreate it.  Return my format identifier.  debug Undo of create, drop or remove
Loggable methods Apply the undo operation, in this implementation of the RawStore, it can only call the undoMe method of undoOp Return my format identifier. Undo operation is a COMPENSATION log operation  make sure resource found in undoOp is released Compensation method Set up a Container undo operation during recovery redo.
Contexts will be passed errors that are caught by the outer system when they are serious enough to require corrective action. They will be told what the error is, so that they can react appropriately. Most of the time, the contexts will react by either doing nothing or by removing themselves from the context manager. If there are no other references to the context, removing itself from the manager equates to freeing it. <BR> On an exception that is session severity or greater the Context must push itself off the stack. This is to ensure that after a session has been closed there are no Contexts on the stack that potentially hold references to objects, thus delaying their garbage collection. <p> Contexts must release all their resources before removing themselves from their context manager. <p> The context manager will "unwind" the contexts during cleanup in the reverse order they were placed on its global stack. <P> If error is an instance of StandardException then an implementation of this method may throw a new exception if and only if the new exception is an instance of StandardException that is more severe than the original error or the new exception is a not an instance of StandardException (e.g java.lang.NullPointerException). Returns the context manager that has stored this context in its stack. Returns the current id name associated with this context. Contexts are placed into stacks by id, in a context manager. Null if the context is not assigned to an id. Contexts known by context managers are always assigned to an id. <p> A default Id name should be defined in each specific context interface as a static final field with the name CONTEXT_ID. For example, see org.apache.derby.iapi.sql.compile.CompilerContext.CONTEXT_ID. Return whether or not this context is the "last" handler for a the specified severity level.  Previously, the context manager would march through all of the contexts in cleanupOnError() and call each of their cleanupOnError() methods.  That did not work with server side JDBC, especially for a StatementException, because outer contexts could get cleaned up incorrectly.  This functionality is specific to the Language system.  Any non-language system contexts should return ExceptionSeverity.NOT_APPLICABLE_SEVERITY. NOTE: Both the LanguageConnectionContext and the JDBC Connection Context are interested in session level errors because they both have clean up to do. This method allows both of them to return false so that all such handlers under them can do their clean up. Pop myself of the context stack. Push myself onto my context stack.

Context interface
Check to see if we have been interrupted. If we have then a ShutdownException will be thrown. This will be either the one passed to interrupt or a generic one if some outside source interrupted the thread. clean up error and print it to derby.log. Extended diagnosis including thread dump to derby.log and javaDump if available, will print if the database is active and severity is greater than or equals to SESSTION_SEVERITY or as configured by derby.stream.error.extendedDiagSeverityLevel property Flush the built up error string to whereever it is supposed to go, and reset the error string Obtain the last pushed Context object of the type indicated by the contextId argument. Return an unmodifiable list reference to the ArrayList backing CtxStack object for this type of Contexts. This method allows fast traversal of all Contexts on that stack. The first element in the List corresponds to the bottom of the stack. The assumption is that the Stack will not be modified while it is being traversed. return the severity of the exception. Currently, this method does not determine a severity that is not StandardException or SQLException. Is the ContextManager empty containing no Contexts. Remove the last pushed Context object, regardless of type. If there are no Context objects, no action is taken. Removes the specified Context object. If the specified Context object does not exist, the call will fail. Add a Context object to the ContextManager. The object is added both to the holder list and to a stack for the specific type of Context. * Class methods Set the locale for this context.
The current thread (passed in a me) is setting associateCM to be its current context manager. Sets the thread local variable threadContextList to reflect associateCM being the current ContextManager. Find the context with the given name in the context service factory loaded for the system. Find the context with the given name in the context service factory loaded for the system. This version will not do any debug checking, but return null quietly if it runs into any problems. Get current Context Manager linked to the current Thread. See setCurrentContextManager for details. Note that this call can be expensive and is only intended to be used in "stateless" situations. Ideally code has a reference to the correct ContextManager from another Object, such as a pushed Context. It's up to the caller to track this context manager and set it in the context manager list using setCurrentContextManager. We don't keep track of it due to this call being made. Remove a ContextManager from the list of all active contexts managers. Break the link between the current Thread and the passed in ContextManager. Called in a pair with setCurrentContextManager, see that method for details. Link the current thread to the passed in Contextmanager so that a subsequent call to getCurrentContextManager by the current Thread will return cm. ContextManagers are tied to a Thread while the thread is executing Derby code. For example on most JDBC method calls the ContextManager backing the Connection object is tied to the current Thread at the start of the method and reset at the end of the method. Once the Thread has completed its Derby work the method resetCurrentContextManager must be called with the same ContextManager to break the link. Note that a subsquent use of the ContextManager may be on a separate Thread, the Thread is only linked to the ContextManager between the setCurrentContextManager and resetCurrentContextManager calls. <BR> ContextService supports nesting of calls by a single Thread, either with the same ContextManager or a different ContextManager. <UL> <LI>The same ContextManager would be pushed during a nested JDBC call in a procedure or function. <LI>A different ContextManager would be pushed during a call on a different embedded JDBC Connection in a procedure or function. </UL> So it can be given to us and taken away...
EXCLUDE-END-lockdiag- EXCLUDE-START-lockdiag- Clone this lock for the lock table information. Objects cloned will not be altered.
vjbms: when user types \n in vjbms, it comes as 2 characters \ and n and not just one character '\n' That's the reason for the following check. I look for "\n" and replace it with '\n'. Same thing for \t \r and \f for fixed format, get column definitions read the column widths property which is comma delimited. In case of fixed format, if column widths are missing, it will throw an exception read the control file properties into a local variable which is used later on In case there is no control file, read the default values for these properties get control file version.  default is DEFAULT_FIELD_SEPARATOR 2 possible formats: fixed and delimited. default is ASCII_DELIMITED to be used to cover cases where column delimeters are placed at the end of each column resulting in an extra delimeter at the end of a row. if at the time of export, the column has null into it, we will spit nullString in the output file. If at the time of import, we see nullString for a column, we will send null as part of resultSet interface read the value of a given property following are the default values for few of the properties Following set routines can be used to change the default properties for fixed format, set column definitions
* Methods of AuxObject Called when the page is being evicted from cache or when a rollback happened on the page and may possibly have changed the control row's value The following methods must be implemented by all control rows. Check consistency of the page and its children, returning the number of pages seen, and throwing errors if inconsistencies are found. <p> * Perform consistency checks which are common to all * pages that derive from ControlRow (both leaf and * branch pages).  The checks are: * <menu> * <li> This page thinks the parent argument is actually *      its parent. * <li> The level of this page is 1 less than the level of *      the parent. * <li> All the rows on the page are in order. * <li> Both left and right siblings, if they exist, are at *      the same level of this page. * <li> This page is the left sibling of its right sibling, *      and it's the right sibling of its left sibling. * <li> The last row on the left sibling is &lt; the first *      row on this page. * <li> The first row on the right sibling is &gt; than the *      the last row on this page. * </menu> * Note that these last two are really only true if there * are never duplicate keys. * Check that all rows on the page are in order.  This * means that each key is &gt; than the previous key. * Perform checks on the siblings of this page: make sure * that they're at the same level as this page, that they're * mutually linked together, and that the first/last keys * on sibling pages are in order. Compare two orderable rows, considering nCompareCols, and return -1, 0, or 1 depending on whether the first row (indexrow) is less than, equal to, or greater than the second (key).  The key may have fewer columns present than nCompareCols. In such a case, if all the columns of the partial key match all of the corresponding columns in the index row, then the value passed in in partialKeyOrder is returned.  The caller should pass in partialKeyOrder=1 if the index rows which match a partial key should be considered to be greater than the partial key, and -1 if they should be considered to be less. This routine only reads objects off the page if it needs them, so if a multi-part key differs in the first column the subsequent columns are not read. Perform page specific initialization. <p> * Debug toString() method's. Dump complete information about control row and rows on the page. <p> * Methods for getting control rows from pages. Get the control row from the given page in the b-tree. The returned control row will be of the correct type for the page (i.e., either a LeafControlRow or a BranchControlRow). Get format id information for row on page. <p> Returns the format id information for a row on the page. faulting it in from the page if necessary. Return the left child pointer for the page. <p> Leaf pages don't have children, so they override this and return null. Get the control row for this page's left sibling, or null if there is no left sibling (which probably means it's the leftmost page at its level). Since right-to-left traversal of an index level	is deadlock-prone, this method will only get get the left sibling if it can latch it without waiting. Get the control row for the given page if the latch on the page can be obtained without waiting, else return null. Get the number of columns in the control row. <p> Control rows all share the first columns as defined by this class and then add columns to the end of the control row.  For instance a branch control row add a child page pointer field. <p> Get the page number of the parent, if it's being maintained. Note that there is intentionally no way to get the control row for the parent page - the b-tree code NEVER traverses up the tree, even in consistency checks. Return the right child pointer for the page. <p> Leaf pages don't have children, so they override this and return null. Return the control row for this page's right sibling.  Unlike getting the left sibling, it's ok to wait for the right sibling latch since left-to-right is the deadlock-free ordering. Get the row. <p> Return the object array that represents the control row for use in raw store fetch, insert, and/or update. Return a new template for reading a data row from the current page. <p> Default implementation for rows which are the same as the conglomerates template, sub-classes can alter if underlying template is different (for instance branch rows add an extra field at the end). Private/Protected methods of ControlRow: Get version of the control row. <p> Returns the version of the control row, faulting it in from the page if necessary. Get the page number of the left sibling. Fault it's value in if it hasn't been yet. Get the page number of the right sibling. Fault it's value in if it hasn't been yet. Is the current page the leftmost leaf of tree? <p> Is the current page the rightmost leaf of tree? <p> * Link this page to the right of the target page. * <P> * Upon entry, this page and the target must be * latched.  Upon exit, this page and the target * remain latched. * <P> * This method carefully acquires pages from left * to right in order to avoid deadlocks. * Recursively print the tree starting at current node in tree. Release this control row's resources. * Perform a recursive search, ultimately returning the latched * leaf page and row slot after which the given key belongs. * The slot is returned in the result structure.  If the key * exists on the page, the resultExact field will be true.  Otherwise, * resultExact field will be false, and the row slot returned will be * the one immediately preceding the position at which the key * belongs. Search this index page. <P> This method is very performance sensitive.  It is the intention that no object allocations occur during the execution of this method. <P> This method performs a binary search on the page and finds the entry i on the page such that entry[i] &lt;= key &lt; entry[i+1].  The result of the search is filled into the passed in params structure. Search and return the left most leaf page. <p> Perform a recursive search, ultimately returning the leftmost leaf page which is the first leaf page in the leaf sibling chain.  (This method might better be called getFirstLeafPage()). Search and return the right most leaf page. <p> Perform a recursive search, ultimately returning the rightmost leaf page which is the last leaf page in the leaf sibling chain.  (This method might better be called getLastLeafPage()). This method will have to update the row. Set version of the control row. <p> Sets the version of the control row.  Updates both the in-memory control row and the disk copy. *	Perform a recursive shrink operation for the key. * If this method returns true, the caller should * remove the corresponding entry for the page. * This routine is not guaranteed to successfully * shrink anything.  The page lead to by the key might * turn out not to be empty by the time shrink gets * there, and shrinks will give up if there is a deadlock. * <P> * As currently implemented shrinkFor must be executed while holding * an exclusive container lock on the entire table.  It is expected that * this call is made within an internal transaction which has been called * by a post commit thread.  Latches are released by the code.  The raw * store guarantees that deallocated pages are not seen by other xacts * until the transaction has been committed. * <P> * Note that a non-table level lock implementation must hold latches on * pages affected until end transaction. * <p> * On entry, the current page is latched.  On exit, all pages will have * been unlatched. * * @exception StandardException Standard exception policy. Perform a top down split pass making room for the the key in "row". <p> Perform a split such that a subsequent call to insert given the argument index row will likely find room for it.  Since latches are released the client must code for the case where another user has grabbed the space made available by the split pass and be ready to do another split. <p> The standard toString(). <p> This is a concise print out of the info in the control row, does not include anything the page. <p> * Unlink this page from its siblings.  This method * will give up and return false rather than run the * risk of a deadlock. * <P> * On entry this page must be latched.  The siblings * are latched and unlatched during the operation.  Upon * exit, this page will remain latched, but unlinked from * its siblings and deallocated from the container. * <P> * The seemingly odd situation that this page will be * returned latched but deallocated is intentional. * The container will not be able to reuse this page * until the latch is released, and the caller may still * need to read information out of it.
Loggable methods  methods to support prepared log the following two methods should not be called during recover Return my format identifier. Read this in PageBasicOperation method to support BeforeImageLogging restore the before image of the page DEBUG: Print self. PhysicalPageOperation method to undo this operation, purge all records that were copied over. public CopyRowsOperation(BasePage destPage) { super(destPage); } Write the rows that are to be copied into this page


Create and returns a temporary file in temporary file system of database. Get the canonical name of the database. This is a name that uniquely identifies it. It is system dependent. The normal, disk based implementation uses method java.io.File.getCanonicalPath on the directory holding the database to construct the canonical name. get the  real storage factory Get the pathname separator character used by the StorageFile implementation. Get the abstract name of the directory that holds temporary files. Classes implementing the StorageFactory interface must have a null constructor.  This method is called when the database is booted up to initialize the class. It should perform all actions necessary to start the basic storage, such as creating a temporary file directory. The init method will be called once, before any other method is called, and will not be called again. end of init This method is used to determine whether the storage is fast (RAM based) or slow (disk based). It may be used by the database engine to determine the default size of the page cache. Construct a StorageFile from a path name. Construct a StorageFile from a directory and file name. Construct a StorageFile from a directory and file name. Set the canonicalName. May need adjustment due to DERBY-5096 Determine whether the storage supports random access. If random access is not supported then it will only be accessed using InputStreams and OutputStreams (if the database is writable). This method tests whether the "rws" and "rwd" modes are implemented. If the "rws" and "rwd" modes are supported then the database engine will conclude that the write methods of "rws" mode StorageRandomAccessFiles are slow but the sync method is fast and optimize accordingly. Force the data of an output stream out to the underlying storage. That is, ensure that it has been made persistent. If the database is to be transient, that is, if the database does not survive a restart, then the sync method implementation need not do anything.
returns the real storage factory to which all the call should be delegated from the proxy methods.
Determine whether the named file is writable. If the named file does not already exist then create it as an empty normal file. The implementation must synchronize with other threads accessing the same file (in the same or a different process). If two threads both attempt to create a file with the same name at the same time then at most one should succeed. Deletes the named file or empty directory. This method does not delete non-empty directories. Deletes the named file and, if it is a directory, all the files and directories it contains. Tests whether the named file exists. Converts this StorageFile into a canonical pathname string. The form of the canonical path is system dependent. Get an exclusive lock. This is used to ensure that two or more JVMs do not open the same database at the same time. end of getExclusiveFileLock Creates an input stream from realFile  Creates an output stream from realFile. Creates an output stream from a file name. Get the name of the parent directory if this name includes a parent. Converts this StorageFile into a pathname string. The character returned by StorageFactory.getSeparator() is used to separate the directory and file names in the sequence. <p> <b>The returned path may include the database directory. Therefore it cannot be directly used to make an StorageFile equivalent to this one.</b> Get a random access (read/write) file. retuns the real file handle that is used to delegate the calls Tests whether the named file is a directory, or not. This is only called in writable storage factories. Get the names of all files and sub-directories in the directory named by this path name. This method is only used in a writable database. Creates the named directory. Creates the named directory, and all nonexistent parent directories. Release the resource associated with an earlier acquired exclusive lock End of releaseExclusiveFileLock Rename the file denoted by this name. Note that StorageFile objects are immutable. This method renames the underlying file, it does not change this StorageFile object. The StorageFile object denotes the same name as before, however the exists() method will return false after the renameTo method executes successfully. <p> It is not specified whether this method will succeed if a file already exists under the new name. Make the named file or directory read-only. This interface does not specify whether this also makes the file undeletable.
Closes this file. Get the current offset in this file. Gets the length of this file. Reads up to <code>len</code> bytes of data from this file into an array of bytes. This method blocks until at least one byte of input is available. <p> Reads a  byte and returns true if the byte is not zero otherwise false. returns one input byte from the stream. returns a char value from the stream. returns a double from the stream. returns a float from the stream. * Following functions Implement DataInput interfaces *** Reads some bytes from an input  stream into the byte array. Reads the specified number of  bytes from an input stream. returns an Int from the stream. returns the next line of text from the input stream. returns a long from the stream. returns a short  value from the stream. returns a string that has been encoded using in the  UTF-8 format. Reads one input byte in the unsigned form. returns unsigned short. Set the file pointer. Sets the length of this file, either extending or truncating it. skip over <code>nBytes</code> bytes of data Force any changes out to the persistent store. Writes all the bytes in array to the stream. Writes specified number bytes from array to the stream. If the corruption flags are enabled, byte array is corrupted before doing the real write. Proxy Implementation of DataOutput interface Writes an int to the output stream . Writes a boolean value to this output stream. Writes to  the eight low-order bits of ant int. Writes a string as bytes to the stream. Writes a char value to the output stream. Writes  the string to the stream. Writes a a double value to the stream. Writes a float value to the output stream. Writes an int value to the output stream. Writes a long  value to the output stream. Writes a short value to the output stream Writes the string in the utf format.
Mark the module as corrupt. It is safe to call this multiple times.
corrupt the byte array at the specified bytes, currenly this metods just complemetns the bits at the specified offsets.
Add this cost estimate to another one.  This presumes that any row ordering is destroyed. Get a copy of this CostEstimate Compare this cost estimate with the given cost estimate. Divide this cost estimate by a scalar, non-dimensional number. Return whether or not this CostEstimate is uninitialized. Multiply this cost estimate by a scalar, non-dimensional number.  This presumes that any row ordering is destroyed. Get the estimated number of rows returned by the ResultSet that this CostEstimate models. Set the cost for this cost estimate. Copy the values from the given cost estimate into this one. Set the single scan row count. Get the estimated number of rows returned by a single scan of the ResultSet that this CostEstimate models.

Determines the result datatype. We can run count() on anything, and it always returns a INTEGER (java.lang.Integer).
Accumulate for count().  Toss out all nulls in this kind of count. Increment the count for count(*). Count even the null values. Return the result of the aggregation.  Just spit out the running count. ///////////////////////////////////////////////////////////  FORMATABLE INTERFACE  /////////////////////////////////////////////////////////// Get the formatID which corresponds to this class.     ///////////////////////////////////////////////////////////  EXTERNALIZABLE INTERFACE  /////////////////////////////////////////////////////////// Although we are not expected to be persistent per se, we may be written out by the sorter temporarily.  So we need to be able to write ourselves out and read ourselves back in.
Get count of bytes written to the stream since the last reset() call. Set a limit at which an exception will be thrown. This allows callers to count the number of bytes up to some point, without having to complete the count. E.g. a caller may only want to see if some object will write out over 4096 bytes, without waiting for all 200,000 bytes of the object to be written. <BR> If the passed in limit is 0 or negative then the stream will count bytes without throwing an exception. Add len to the count, discard the data. * Methods of OutputStream Add 1 to the count.
This test simply tests a covered index scan and retrieves an int column Scan starts from 1/4 into the data set and set to end 3/4 into the dataset Override initializeConnection to set the autocommit to false Do the necessary setup for the test ,prepare the statement  Cleanup - close resources opened in this test.
INTERFACE METHODS This is the guts of the Execution-time logic for CREATE FUNCTION, PROCEDURE, SYNONYM, and TYPE. <P> A function, procedure, or udt is represented as: <UL> <LI> AliasDescriptor </UL> Routine dependencies are created as: <UL> <LI> None </UL> <P> A synonym is represented as: <UL> <LI> AliasDescriptor <LI> TableDescriptor </UL> Synonym dependencies are created as: <UL> <LI> None </UL> In both cases a SchemaDescriptor will be created if needed. No dependency is created on the SchemaDescriptor. OBJECT SHADOWS Common checks to be performed for functions and procedures
Extra logic for binding user-defined aggregate definitions Bind the class names for UDTs We inherit the generate() method from DDLStatementNode. Bind this CreateAliasNode.  This means doing any static error checking that can be done before actually creating the table. For example, verifying that the column name list does not contain any duplicate column names. Construct an exception flagging an illegal aggregate name Create the Constant information that will drive the guts of Execution.
INTERFACE METHODS This is the guts of the Execution-time logic for CREATE CONSTRAINT. <P> A constraint is represented as: <UL> <LI> ConstraintDescriptor. </UL> If a backing index is required then the index will be created through an CreateIndexConstantAction setup by the compiler. <BR> Dependencies are created as: <UL> <LI> ConstraintDescriptor depends on all the providers collected at compile time and passed into the constructor. <LI> For a FOREIGN KEY constraint ConstraintDescriptor depends on the ConstraintDescriptor for the referenced constraints and the privileges required to create the constraint. </UL> Generate an array of column positions for the column list in the constraint. /////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////// Get the text defining this constraint. Is the constant action for a foreign key

Determines if a statistics entry is to be added for the index. <p> As an optimization, it may be better to not write a statistics entry to SYS.SYSSTATISTICS. If it isn't needed by Derby as part of query optimization there is no reason to spend resources keeping the statistics up to date. INTERFACE METHODS This is the guts of the Execution-time logic for creating an index. <P> A index is represented as: <UL> <LI> ConglomerateDescriptor. </UL> No dependencies are created. Get the conglomerate number for the conglomerate that was created by this constant action.  Will return -1L if the constant action has not yet been executed.  This is used for updating conglomerate descriptors which share a conglomerate that has been dropped, in which case those "sharing" descriptors need to point to the newly-created conglomerate (the newly-created conglomerate replaces the dropped one). Get the UUID for the conglomerate descriptor that was created (or re-used) by this constant action. CLASS METHODS /////////////////////////////////////////////////////////////////////  GETTERs called by CreateConstraint  ///////////////////////////////////////////////////////////////////// If the purpose of this constant action was to "replace" a dropped physical conglomerate, then this method returns the conglomerate number of the dropped conglomerate.  Otherwise this method will end up returning -1. Scan the base conglomerate and insert the keys into a sorter, returning a rowSource on the sorter. /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
We inherit the generate() method from DDLStatementNode. Bind this CreateIndexNode.  This means doing any static error checking that can be done before actually creating the table. For example, verifying that the column name list does not contain any duplicate column names. Create the Constant information that will drive the guts of Execution. Return true if the node references SESSION schema tables (temporary or permanent) Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Check the uniqueness of the column names within the derived column list.
INTERFACE METHODS This is the guts of the Execution-time logic for CREATE ROLE. PRIVATE METHODS Heuristically, try to determine is a proposed role identifier is already known to Derby as a user name. Method: If BUILTIN authentication is used, check if there is such a user. If external authentication is used, we lose.  If there turns out to be collision, and we can't detect it here, we should block such a user from connecting (FIXME), since there is now a role with that name. OBJECT SHADOWS
Bind this createRoleNode. Main work is to create a StatementPermission object to require CREATE_ROLE_PRIV at execution time. We inherit the generate() method from DDLStatementNode. Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
INTERFACE METHODS This is the guts of the Execution-time logic for CREATE SCHEMA. This is the guts of the Execution-time logic for CREATE SCHEMA. This is variant is used when we to pass in a tc other than the default used in executeConstantAction(Activation). /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
Bind this createSchemaNode. Main work is to create a StatementPermission object to require CREATE_SCHEMA_PRIV at execution time. We inherit the generate() method from DDLStatementNode. Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
INTERFACE METHODS This is the guts of the Execution-time logic for CREATE SEQUENCE. OBJECT SHADOWS
Bind this CreateSequenceNode. The main objectives of this method are to resolve the schema name, determine privilege checks, and vet the variables in the CREATE SEQUENCE statement. We inherit the generate() method from DDLStatementNode. Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
INTERFACE METHODS This is the guts of the Execution-time logic for CREATE TABLE. Create a sequence generator for an identity column OBJECT METHODS
Accept the visitor for all visitable children of this node. We inherit the generate() method from DDLStatementNode. Bind this CreateTableNode.  This means doing any static error checking that can be done before actually creating the base table or declaring the global temporary table. For eg, verifying that the TableElementList does not contain any duplicate column names. Create the Constant information that will drive the guts of Execution. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Return true if the node references SESSION schema tables (temporary or permanent) If no schema name specified for global temporary table, SESSION is the implicit schema. Otherwise, make sure the specified schema name for global temporary table is SESSION. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
* Create an sps that is used by the trigger. This is the guts of the Execution-time logic for CREATE TRIGGER. Construct the creation timestamp for the trigger. DERBY-5866: Also make sure the creation timestamp is higher than any timestamp on an existing trigger on the same table. Otherwise, the triggers may not fire in the correct order.
* BIND OLD/NEW TRANSITION TABLES/VARIABLES AND collect TRIGGER ACTION * COLUMNS referenced through REFERECING CLAUSE in CREATE TRIGGER statement * * 1) validate the referencing clause (if any) * * 2) convert trigger action text and WHEN clause text.  e.g. *	DELETE FROM t WHERE c = old.c * turns into *	DELETE FROM t WHERE c = org.apache.derby.iapi.db.Factory:: *		getTriggerExecutionContext().getOldRow(). *      getInt(columnNumberFor'C'inRuntimeResultset); * or *	DELETE FROM t WHERE c in (SELECT c FROM OLD) * turns into *	DELETE FROM t WHERE c in ( *      SELECT c FROM new TriggerOldTransitionTable OLD) * * 3) check all column references against new/old transition *	variables (since they are no longer 'normal' column references * 	that will be checked during bind) * * 4) collect all column references in trigger action through new/old * transition variables. Information about them will be saved in * SYSTRIGGERS table DERBY-1482(if we are dealing with pre-10.7 db, then we * will not put any information about trigger action columns in the system * table to ensure backward compatibility). This information along with the * trigger columns will decide what columns from the trigger table will be * fetched into memory during trigger execution. * * 5) reparse the new action text * * You might be wondering why we regenerate the text and reparse * instead of just reworking the tree to have the nodes we want. * Well, the primary reason is that if we screwed with the tree, * then we would have a major headache if this trigger action * was ever recompiled -- spses don't really know that they are * triggers so it would be quite arduous to figure out that an * sps is a trigger and munge up its query tree after figuring * out what its OLD/NEW tables are, etc.  Also, it is just plain * easier to just generate the sql and rebind. * accessors We inherit the generate() method from DDLStatementNode. Bind this CreateTriggerNode.  This means doing any static error checking that can be done before actually creating the table. * Check for illegal combinations here: insert & old or * delete and new Compare two strings. Forbid references to generated columns in the actions of BEFORE triggers. This is DERBY-3948, enforcing the following section of the SQL standard: part 2, section 11.39 (<trigger definition>), syntax rule 12c: <blockquote> 12) If BEFORE is specified, then: : c) The <triggered action> shall not contain a <field reference> that references a field in the new transition variable corresponding to a generated column of T. </blockquote> Translate a position from the transformed trigger text ({@link #actionText} or {@link #whenText}) to the corresponding position in the original trigger text ({@link #originalActionText} or {@link #originalWhenText}). Get all transition tables referenced by a given node, sorted in the order in which they appear in the SQL text. Check if a table represents one of the transition tables. The arrary passed will have either -1 or a column position as it's elements. If the array only has -1 as for all it's elements, then this method will return null. Otherwise, the method will create a new arrary with all -1 entries removed from the original arrary. Create the Constant information that will drive the guts of Execution. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Make sure all references to SQL schema objects (such as tables and functions) in the SQL fragments that will be stored in the SPS and in the trigger descriptor, are fully qualified with a schema name. Qualify all names SQL object names in original and transformed SQL text for an action or a WHEN clause. Return true if the node references SESSION schema tables (temporary or permanent) Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Transform the WHEN clause or the triggered SQL statement of a statement trigger from its original shape to internal syntax where references to transition tables are replaced with VTIs that return the before or after image of the changed rows. * Make sure that the referencing clause is legitimate. * While we are at it we set the new/oldTableName to * be whatever the user wants.
INTERFACE METHODS This is the guts of the Execution-time logic for CREATE VIEW. OBJECT METHODS
These methods are used by execution to get information for storing into the system catalogs. Accept the visitor for all visitable children of this node. We inherit the generate() method from DDLStatementNode. Bind this CreateViewNode.  This means doing any static error checking that can be done before actually creating the table. For example, verifying that the ResultColumnList does not contain any duplicate column names. Bind the query expression for a view definition. Fill in the ColumnInfo[] for this create view. class interface Get the parsed query expression (the SELECT statement). Create the Constant information that will drive the guts of Execution. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Return true if the node references SESSION schema tables (temporary or permanent) Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.

Convert a string to a date in the specified calendar. Accept the same format as {@code Date.valueOf()}. Convert a character to a digit. ---------------------------- getBigDecimal*() methods ---------------------- move all these to Cursor and rename to crossConvertFrom*To*() --------------------------------------------------------------------------- The following methods are used for output cross conversion. --------------------------------------------------------------------------- ---------------------------- getBoolean*() methods ------------------------- <p> Get a boolean value from a CHAR column. In order to match the embedded driver and JCC we return false iff the CHAR value is "0" or "false". </p> <p> Leading and trailing whitespace is removed from the input string before it's compared to "0" and "false". No other normalization is performed. Specifically, no case conversion is performed, so the comparison is case sensitive, and everything that doesn't exactly match "0" or "false" will be considered true. </p> ---------------------------- getByte*() methods ---------------------------- All Numeric, and Date/Time types use String.valueOf (source) ---------------------------- getDate*() methods ---------------------------- ---------------------------- getDouble*() methods -------------------------- ---------------------------- getFloat*() methods --------------------------- ------ method to convert to targetJdbcType ------ Convert the input targetJdbcType to the correct JdbcType used by CrossConverters. ---------------------------- getInt*() methods ----------------------------- ---------------------------- getLong*() methods ---------------------------- ---------------------------- getShort*() methods --------------------------- ---------------------------- getString*() methods -------------------------- ---------------------------- getTime*() methods ---------------------------- ---------------------------- getTimestamp*() methods ----------------------- Initialize the date components of a {@code java.util.Calendar} from a string on the format YYYY-MM-DD. All other components are left untouched. Initialize the time components of a {@code java.util.Calendar} from a string on the format HH:MM:SS. All other components are left untouched. Custom version of java.lang.parseInt() that allows for space padding of char fields. This method is used in lieu of setObject(targetType, sourceObject) because we don't support the BIT/BOOLEAN as underlying DERBY targetTypes. This method is used in lieu of setObject(targetType, sourceObject) because we don't support the BIT/BOOLEAN as underlying DERBY targetTypes. create a byte[] by reading all of the bytes from inputStream --------------------------------------------------------------------------- The following methods are used for input cross conversion. --------------------------------------------------------------------------- ---------------------------- setObject() methods --------------------------- Convert from boolean source to target type. In support of PS.setBoolean(). Convert from byte source to target type In support of PS.setByte() -- methods in support of setObject(String)/getString() on BINARY columns--- Convert from byte[] source to target type In support of PS.setBytes() Convert from double floating point source to target type In support of PS.setDouble() Convert from floating point source to target type In support of PS.setFloat() Convert from integer source to target type In support of PS.setInt() Convert from Reader source to target type In support of PS.setCharacterStream() The Java compiler uses static binding, so we can't rely on the strongly typed setObject() methods above for each of the Java Object instance types. setString() against BINARY columns cannot be implemented consistently because w/out metadata, we'll send char encoding bytes. So we refuse setString() requests altogether. Convert from string source to target type. In support of PS.setString() Convert from big decimal source to target type In support of PS.setBigDecimal() Convert from Blob source to target type In support of PS.setBlob() Convert from Clob source to target type In support of PS.setClob() Convert from date source to target type In support of PS.setDate() Convert from time source to target type In support of PS.setTime() Convert from date source to target type In support of PS.setTimestamp() Convert from long source to target type In support of PS.setLong() Convert from short source to target type In support of PS.setShort() Convert from InputStream source to target type In support of PS.setBinaryStream() Convert from InputStream source to target type. In support of PS.setAsciiStream, PS.setUnicodeStream Note: PS.setCharacterStream() is handled by setObject(Reader) create a String by reading all of the bytes from reader create a String by reading all of the bytes from inputStream, applying encoding Convert a string to a time in the specified calendar. Accept the same format as {@code java.sql.Time.valueOf()}. Convert a string to a timestamp in the specified calendar. Accept the same format as {@code java.sql.Timestamp.valueOf()}.
This is called prior to each execution of the statement, to ensure that it starts over with a new current datetime value. class interface class implementation
QueryTreeNode interface  Binding this expression means setting the result DataTypeServices. In this case, the result type is based on the operation requested. CurrentDatetimeOperatorNode is used in expressions. The expression generated for it invokes a static method on a special Derby type to get the system time and wrap it in the right java.sql type, and then wrap it into the right shape for an arbitrary value, i.e. a column holder. This is very similar to what constants do. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) {@inheritDoc } print the non-node subfields
Bind the expressions in this ResultSetNode.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression.  FromTable interface  Binding this FromTable means finding the prepared statement for the cursor and creating the result columns (the columns updatable on that cursor). We expect someone else to verify that the target table of the positioned update or delete is the table under this cursor. Optimizable interface  Generation on a CurrentOfNode creates a scan on the cursor, CurrentOfResultSet. <p> This routine will generate and return a call of the form: <pre><verbatim> ResultSetFactory.getCurrentOfResultSet(cursorName) </verbatim></pre> Return the CursorNode associated with a positioned update/delete.  class interface  Try to find a ResultColumn in the table represented by this CurrentOfNode that matches the name in the given ColumnReference. <p> Construct a dummy CurrentOfNode just for compiling the DELETE action of a MERGE statement. </p> Optimize this CurrentOfNode.  Nothing to do. Preprocess a CurrentOfNode.  For a CurrentOfNode, this simply means allocating a referenced table map to avoid downstream NullPointerExceptions. NOTE: There are no bits set in the referenced table map. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Get the lock mode for this table as the target of an update statement (a delete or update).  This is implemented only for base tables and CurrentOfNodes.
If the result set has been opened, close the open scan.   class implementation  Because the positioned operation only gets one location per execution, and the cursor could be completely different for each execution (closed and reopened, perhaps), we determine where caching the cursor could be applied. <p> When cached, we check if the cursor was closed'd, and if so, throw it out and see if there's one in the cache with our name. If open and not returned yet, returns the row. This result set has its row location from the last fetch done. If it is closed, a null is returned. Return a sparse heap row, based on a compact index row. Return the total amount of time spent in this ResultSet   ResultSet interface (leftover from NoPutResultSet)  open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
Binding this expression means setting the result DataTypeServices. In this case, the result type is always the same. CurrentRowLocationNode is used in updates and deletes.  See generate() in UpdateNode and DeleteNode to get the full overview of generate().  This class is responsible for generating the method that will return the RowLocation for the next row to be updated or deleted. This routine will generate a method of the form: private SQLRef	fieldx; ... protected DataValueDescriptor exprx() throws StandardException { return fieldx = <SQLRefConstructor>( "result set member".getRowLocation(), fieldx); } and return the generated code: exprx() ("result set member" is a member of the generated class added by UpdateNode or DeleteNode.) This exprx function is used within another exprx function, and so doesn't need a static field or to be public; but at present, it has both. fieldx is a generated field that is initialized to null when the activation is constructed.  getSQLRef will re-use fieldx on calls after the first call, rather than allocate a new SQLRef for each call.

Return {@code true} if all rows are received from the server. Calculate the column offsets for a row. Returns a {@code Blob} object. ------- the following getters perform any necessary cross-conversion _------ Build a Java String from a database CHAR field. Returns a {@code Clob} object. Build a JDBC Date object from the DERBY ISO DATE field. Build a JDBC Date object from the DERBY ISO TIMESTAMP field. Build a Java double from a fixed point decimal byte representation. Get updated status for this row. Minion of ResultSet#rowUpdated. Get deleted status for this row. Minion of ResultSet#rowDeleted. Returns a reference to the locator procedures. <p> These procedures are used to operate on large objects referenced on the server by locators. Build a Java long from a fixed point decimal byte representation. Instantiate an instance of Calendar that can be re-used for getting Time, Timestamp, and Date values from this cursor.  Assumption is that all users of the returned Calendar object will clear it as appropriate before using it. Build a string object from the DERBY byte TIME representation. Build a string object from the DERBY byte TIMESTAMP representation. Build a JDBC Time object from the DERBY ISO TIME field. Build a JDBC Timestamp object from the DERBY ISO TIMESTAMP field. Build a JDBC Time object from the DERBY ISO TIMESTAMP field. Build a JDBC Timestamp object from the DERBY ISO DATE field. Build a JDBC Timestamp object from the DERBY ISO TIME field. Build a Java String from a database VARCHAR or LONGVARCHAR field.  Depending on the ccsid, length is the number of chars or number of bytes. For 2-byte character ccsids, length is the number of characters, for all other cases length is the number of bytes. The length does not include the null terminator. Build a Java long from an 8-byte signed binary representation. ------- the following getters are called on known column types ------------- Direct conversions only, cross conversions are handled by another set of getters. Build a Java boolean from a 1-byte signed binary representation. Extract bytes from a database Types.BINARY field. This is the DERBY type CHAR(n) FOR BIT DATA. Build a java.math.BigDecimal from a fixed point decimal byte representation. Build a Java double from an 8-byte floating point representation. Build a Java float from a 4-byte floating point representation. Build a Java int from a 4-byte signed binary representation. Build a Java short from a 2-byte signed binary representation. Deserialize a UDT from a database Types.JAVA_OBJECT field. This is used for user defined types. Extract bytes from a database Types.VARBINARY or LONGVARBINARY field. This includes the DERBY types: VARCHAR(n) FOR BIT DATA LONG VARCHAR(n) FOR BIT DATA This tracks the total number of rows read into the client side buffer for this result set, irregardless of scrolling. Per jdbc semantics, this should never exceed statement.maxRows. This event should be generated in the materialized cursor's implementation of calculateColumnOffsetsForRow(). Returns the locator for the specified LOB column, or {@link Lob#INVALID_LOCATOR} if the LOB was not sent as a locator. The server may send the LOB value instead of a locator if it is running an old version which doesn't support locators, or if the database it accesses is soft upgraded from a version that doesn't have the necessary stored procedures for locator support. <p> Note that this method cannot be invoked on a LOB column that is NULL. ---------------------------cursor positioning------------------------------- Makes the next row the current row. Returns true if the current row position is a valid row position. reset the beginning and ending position in the data buffer to 0 reset the currentRowPosition and nextRowPosition to 0 reset lastRowReached and sqlcode100Received to false clear the column data offsets cache --------------------------reseting cursor state----------------------------- Set the value of value of allRowsReceivedFromServer_. Keep track of updated status for this row. Makes the next row the current row. Returns true if the current row position is a valid row position.
Returns the cursor result set for this activation, so that the current row can be re-qualified, and so that the current row location can be determined. Returns the target result set for this activation, so that the current base row can be determined.
Get the formatID which corresponds to this class. Read this object from a stream of stored objects. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out
Accept the visitor for all visitable children of this node. Returns the type of activation this class generates. Bind this CursorNode.  This means looking up tables and columns and getting their types, and figuring out the result types of all expressions, as well as doing view resolution, permissions checking, etc. It also includes determining whether an UNSPECIFIED cursor is updatable or not, and verifying that an UPDATE cursor actually is. Bind the update columns by their names to the target table of the cursor specification. Doesn't check for duplicates in the list, although it could... REVISIT: If the list is empty, should it expand it out? at present, it leaves it empty. Collects table descriptors for base tables whose index statistics we want to check for staleness (or to create). Take a cursor and determine if it is UPDATE or READ_ONLY based on the shape of the cursor specification. <p> The following conditions make a cursor read only: <UL> <LI>if it says FOR READ ONLY <LI>if it says ORDER BY <LI>if its query specification is not read only. At present this is explicitly tested here, with these conditions.  At some future point in time, this checking ought to be moved into the ResultSet nodes themselves.  The conditions for a query spec. not to be read only include: <UL> <LI>if it has a set operation such as UNION or INTERSECT, i.e. does not have a single outermost SELECT <LI>if it does not have exactly 1 table in its FROM list; 0 tables would occur if we ever support a SELECT without a FROM e.g., for generating a row without an underlying table (like what we do for an INSERT of a VALUES list); &gt;1 tables occurs when joins are in the tree. <LI>if the table in its FROM list is not a base table (REMIND when views/from subqueries are added, this should be relaxed to be that the table is not updatable) <LI>if it has a GROUP BY or HAVING (NOTE I am assuming that if and aggregate is detected in a SELECT w/o a GROUP BY, one has been added to show that the whole table is a group) <LI> NOTE that cursors are updatable even if none of the columns in the select are updatable -- what they care about is the updatability of the columns of the target table. </UL> </UL> Do code generation for this CursorNode Get information about this cursor.  For sps, this is info saved off of the original query tree (the one for the underlying query). Check if this cursor references any session schema tables. If so, pass those names to execution phase through savedObjects This list will be used to check if there are any holdable cursors referencing temporary tables at commit time. If yes, then the data in those temporary tables should be preserved even if they are declared with ON COMMIT DELETE ROWS option class interface Returns whether or not this Statement requires a set/clear savepoint around its execution.  The following statement "types" do not require them: Cursor	- unnecessary and won't work in a read only environment Xact	- savepoint will get blown away underneath us during commit/rollback Optimize a DML statement (which is the only type of statement that should need optimizing, I think). This method over-rides the one in QueryTreeNode. This method takes a bound tree, and returns an optimized tree. It annotates the bound tree rather than creating an entirely new tree. Throws an exception if the tree is not bound, or if the binding is out of date. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Return true if the node references SESSION schema tables (temporary or permanent) Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Returns a list of base tables for which the index statistics of the associated indexes should be updated. Support routine for translating an updateMode identifier to a String
Returns the current row of the result set. REMIND: eventually, this will only return the current row for result sets that need to return it; either some field in the activation or a parameter in the constructor will be used to signal that this needs to function. This will let us limit the number of live objects we are holding on to. <p> Returns the row location of the current base table row of the cursor. If this cursor's row is composed of multiple base tables' rows, i.e. due to a join, then a null is returned.
Return the base name of the table Return the exposed name of the table.  Exposed name is another term for correlation name.  If there is no correlation, this will return the base name. Return the schema for the table. Get the formatID which corresponds to this class. Read this object from a stream of stored objects. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out

Clear all information to allow object re-use.
Bind this operator Do code generation for this unary operator. end of generateExpression end of getConstantLength This is a length operator node.  Overrides this method in UnaryOperatorNode for code generation purposes.
Populate the database with the data needed by a test.
/////////////////////////////////////////////////////////////////////////////////  WRAPPER FUNCTIONS WHICH ARE REGISTERED WITH DERBY  ///////////////////////////////////////////////////////////////////////////////// <p>Append function arguments to an evolving ddl text buffer.</p> <p>Append return type to an evolving ddl text buffer</p> <p>Append the signature of a table function to an evolving ddl text buffer</p> <p>Execute a DDL statement.</p> /////////////////////////////////////////////////////////////////////////////////  GENERAL MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Get the current session's database metadata. </p> <p> Get the default connection, called from inside the database engine. </p> <p>Get a dummy value for an argument to a DBMD method.</p> Comment out this method because we don't support this datatype public  static  RowIdLifetime 	getRowIdLifetime() throws SQLException { return getDBMD().getRowIdLifetime(); } Comment out this method so that we don't drop the following method public  static  ResultSet 	getSchemas() throws SQLException { return getDBMD().getSchemas(); } Needs to cast String[] to something else Eliminated the final "int[] types" argument /////////////////////////////////////////////////////////////////////////////////  REGISTRATION MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p>Return true if the requested modifer is set</p> <p>Return true if the method describes a table function.</p> /////////////////////////////////////////////////////////////////////////////////  OptionalTool BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p>Get the SQL type which corresponds to a Java type.</p> <p> Prepare a statement and print out the text. </p> <p> Workhorse to register or unregister all public static methods as Derby routines. </p> <p>Register the method as a Derby function.</p> <p>Append the name of a SQL type to an evolving ddl text buffer</p> Comment this out in favor of the more general overload which follows. Derby only allows one function by a given name in a given schema. public  static  boolean 	supportsConvert() throws SQLException { return getDBMD().supportsConvert(); } <p>Drop the function with this method name.</p> <p> Wrap an exception in a SQLException. </p>
commits the trasnaction. Deletes the record with key value passed in constroctor. Returns the SQLException received while executing insert. Null if no transaction was received. Returns if any unexpected trasnaction was thrown during any of the operation. Inserts a record with key value passed in constroctor. Rollbacks the transaction.
************************************************ Generate DDL for a specific stored procedure or function. @param aliasName Name of the current procedure/function @param aliasInfo Info about the current procedure/function @param aliasType Indicator of whether we're generating a stored procedure or a function. @return DDL for the current stored procedure is returned, as a String. ** Prepared statements use throughout the DDL generation process. ************************************************ Generate the DDL for all stored procedures, functions, aggregates, and UDTs in a given database and write it to output via Logs.java. @param conn Connection to the source database. @param at10_6 True if the database is at 10.6 or higher ** ************************************************ Generate the DDL for all synonyms in a given database. On successul return, the DDL for the synonyms has been written to output via Logs.java. @param conn Connection to the source database. @return ** ************************************************ Generate the DDL for the stored procedures, functions, or UDTs in a given database, depending on the the received aliasType. @param rs Result set holding either stored procedures or functions. @param aliasType Indication of whether we're generating stored procedures or functions. ** ************************************************ Generate the DDL for the stored procedures, functions, or UDTs in a given database, depending on the the received aliasType. @param rs Result set holding either stored procedures or functions. @param aliasType Indication of whether we're generating stored procedures or functions. **
************************************************ Generate DDL for a specific check. @param tableName Name of the table on which the check exists. @param aCheck Information about the check in question. @return The DDL for the specified check has been generated returned as a StringBuffer. ** ************************************************ Generate the DDL for all checks in a given database. @param conn Connection to the source database. @return The DDL for the indexes has been written to output via Logs.java. **
************************************************ ************************************************ Generate Grant and Revoke statements if sqlAuthorization is on ************************************************ Generate aggregate privilege statements ************************************************ Generate column privilege statements ************************************************ Generate routine privilege statements ************************************************ Generate sequence privilege statements ************************************************ Generate table privilege statements ************************************************ Generate udt privilege statements ************************************************ Generate one column grant statement ************************************************** Generate table privilege statement for the current row
************************************************ Generate DDL for a specific index. @param ixName Name of the index. @param tableName Name of table on which the index exists. @param tableId Id of table on which the index exists. @param ixDescribe Column list for the index. @return The DDL for the specified index, as a string buffer. ** ************************************************ Generate the DDL for all indexes in a given database. @param conn Connection to the source database. @return The DDL for the indexes has been written to output via Logs.java. **
************************************************ Generate the DDL for all jars in a given database. @param dbName Name of the database (for locating the jar). @param conn Connection to the source database. @param at10_9 Dictionary is at 10.9 or higher @return The DDL for the jars has been written to output via Logs.java. **
************************************************ Generate DDL for a specific key. @param tableId Id of table on which the key exists. @param tableName Name of table on which the key exists. @param aKey Info on the key to generate. @return DDL for the specified key is returned as a string. ** ************************************************ Generate the DDL for the a set of keys in the source database. @param rs Info on keys to dump; either a set of non- foreign keys (primary and unique), or a set of foreign keys. @return DDL for the receive set of keys has been written to output via Logs.java. ** ************************************************ Generate the DDL for all keys in a given database. @param conn Connection to the source database. @return The DDL for the keys has been written to output via Logs.java. ** ************************************************ Takes a character representing a key type and returns the full type name (as it will appear in in the DDL). @param keyType Key type as a char. @return Key type as a full string. ** ************************************************ Generate the clauses for deferred constraints. @param buffer    Evolving buffer where we write additional clauses. @param aKey Info on the key to generate. @param stateColumn 1-based position of the STATE column in the result set @return DDL for the specified key is returned as a string. ** ************************************************ Generate the DDL for a foreign key's "REFERENCES" clause. @param constraintId Id of the foreign key constraint. @param deleteChar What action to take on delete. @param updateChar What action to take on update. @return The DDL for the references clause of the foreign key, returned as a string. ** ************************************************ Print a simple header to output. **
Generate role definition statements and role grant statements. Note that privileges granted to roles are handled by DB_GrantRevoke, similar to privileges granted to users. Generate role definition statements Generate a role definition statement for the current row Generate role grant statement for the current row
************************************************ Generate the DDL for all schemas in a given database. @param conn Connection to the source database. @param tablesOnly true if we're only generating objects specific to a particular table (in which case we don't generate schemas). @return The DDL for the schemas has been written to output via Logs.java. **
<p> Generate DDL for a specific sequence. </p> /////////////////////////////////////////////////////////////////////////////////  CONSTANTS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Generate the DDL for all sequences and output it via Logs.java. </p> Strip the trailing NOT NULL off of the string representation of a datatype
************************************************ Generate the DDL for a specific column of the the table corresponding to the received tableId. @param colName the name of the column to generate. @param tableId Which table the column belongs to. @param colNum the number of the column to generate (1 => 1st column, 2 => 2nd column, etc) @return The generated DDL, as a string. ** ************************************************ Generate the DDL for all user tables in a given database. @param conn Connection to the source database. @param tableIdToNameMap Mapping of table ids to table names, for quicker reference. @return The DDL for the tables has been written to output via Logs.java. ** ************************************************ Generate autoincrement DDL for a given column and write it to received StringBuffer @param colName: Name of column that is autoincrement. @param tableId: Id of table in which column exists. @param colDef: StringBuffer to which DDL will be added. @return True if autoincrement DDL has been generated. **
************************************************ Generate DDL for a specific trigger. ************************************************ Generate the DDL for all triggers in a given database.
************************************************ Generate the DDL for all views in a given database. @param conn Connection to the source database. @return The DDL for the views has been written to output via Logs.java. **
//////////////////////////////////////////////////////////////////////  DDColumnDependable METHODS  ////////////////////////////////////////////////////////////////////// Find a dependable object, which is essentially a table descriptor with referencedColumnMap field set. ////////////////////////////////////////////////////////////////  FORMATABLE METHODS  //////////////////////////////////////////////////////////////// Read this object from a stream of stored objects.  Just read the byte array, besides what the parent does. Write this object to a stream of stored objects.  Just write the byte array, besides what the parent does.
Add dependencies of a column on providers. These can arise if a generated column depends on a user created function. Add and drop dependencies of an object on UDTs. Add and drop dependencies of a routine on UDTs. Adjust dependencies of a table on ANSI UDTs. We only add one dependency between a table and a UDT. If the table already depends on the UDT, we don't add a redundant dependency. We have determined that the statement permission described by statPerm is not granted to the current user nor to PUBLIC, so it must be granted to the current role or one of the roles inherited by the current role. Find the relevant permission descriptor and return it. Get the schema descriptor for the schemaid. Get the schema descriptor in the creation of an object in the passed in schema. Lock the table in exclusive or share mode to prevent deadlocks. This method saves dependencies of constraints on privileges in the dependency system. It gets called by CreateConstraintConstantAction. Views and triggers and constraints run with definer's privileges. If one of the required privileges is revoked from the definer, the dependent view/trigger/constraint on that privilege will be dropped automatically. In order to implement this behavior, we need to save view/trigger/constraint dependencies on required privileges in the dependency system. Following method accomplishes that part of the equation for constraints only. The dependency collection for constraints is not same as for views and triggers and hence constraints are handled by this special method. Views and triggers can depend on many different kind of privileges where as constraints only depend on REFERENCES privilege on a table (FOREIGN KEY constraints) or EXECUTE privileges on one or more functions (CHECK constraints). Another difference is only one view or trigger can be defined by a sql statement and hence all the dependencies collected for the sql statement apply to the view or trigger in question. As for constraints, one sql statement can defined multiple constraints and hence the all the privileges required by the statement are not necessarily required by all the constraints defined by that sql statement. We need to identify right privileges for right constraints for a given sql statement. Because of these differences between constraints and views (and triggers), there are 2 different methods in this class to save their privileges in the dependency system. For each required privilege, we now register a dependency on a role if that role was required to find an applicable privilege. This method saves dependencies of views and triggers on privileges in the dependency system. It gets called by CreateViewConstantAction and CreateTriggerConstantAction. Views and triggers and constraints run with definer's privileges. If one of the required privileges is revoked from the definer, the dependent view/trigger/constraint on that privilege will be dropped automatically. In order to implement this behavior, we need to save view/trigger/constraint dependencies on required privileges in the dependency system. Following method accomplishes that part of the equation for views and triggers. The dependency collection for constraints is not same as for views and triggers and hence constraints are not covered by this method. Views and triggers can depend on many different kind of privileges where as constraints only depend on REFERENCES privilege on a table. Another difference is only one view or trigger can be defined by a sql statement and hence all the dependencies collected for the sql statement apply to the view or trigger in question. As for constraints, one sql statement can defined multiple constraints and hence the all the privileges required by the statement are not necessarily required by all the constraints defined by that sql statement. We need to identify right privileges for right constraints for a given sql statement. Because of these differences between constraints and views (and triggers), there are 2 different methods in this class to save their privileges in the dependency system. For each required privilege, we now register of a dependency on a role if that role was required to find an applicable privilege. The statement permission needed for dependent has been found to rely on the current role. If not already done, register the dependency so that if the current role (or any of the roles it inherits) is revoked (or dropped), we can invalidate dependent.
See "dropConglomerate(...)" above. Similar to dropConstraint(...) above, except this method drops a conglomerate directly instead of going through a ConstraintDescriptor. Drop the constraint corresponding to the received descriptor. If in doing so we also drop a backing conglomerate that is shared by other constraints/indexes, then we have to create a new conglomerate to fill the gap. This method exists here as a "utility" method for the various constant actions that may drop constraints in one way or another (there are several that do). See "dropConstraint(...") above. See "dropConstraint(...") above. Execute the received ConstantAction, which will create a new physical conglomerate (or find an existing physical conglomerate that is "sharable") to replace some dropped physical conglomerate.  Then find any conglomerate descriptors which still reference the dropped physical conglomerate and update them all to have a conglomerate number that points to the conglomerate created by the ConstantAction. This method is called as part of DROP processing to handle cases where a physical conglomerate that was shared by multiple descriptors is dropped--in which case a new physical conglomerate must be created to support the remaining descriptors. Create a ConstantAction which, when executed, will create a new conglomerate whose attributes match those of the received ConglomerateDescriptor. Get any table properties that exist for the received index descriptor. Recreate backing index of unique constraint. It first drops the existing index and creates it again with uniqueness set to false and uniqueWhenNotNull set to true. It reuses the uuid so there is no need to update ConstraintDescriptor.
Generic generate code for all DDL statements. Return the full dot expression name of the object being dropped. Return the name of the table being dropped. This is the unqualified table name. Get a schema descriptor for this DDL object. Uses this.objectName.  Always returns a schema, we lock in the schema name prior to execution. Checks if current authorizationID is owner of the schema. Get a schema descriptor for this DDL object. Uses this.objectName.  Always returns a schema, we lock in the schema name prior to execution. The most common call to this method is with 2nd parameter true which says that SchemaDescriptor should not be requested for system schema. The only time this method will get called with 2nd parameter set to false will be when user has requested for inplace compress using SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE Above inplace compress can be invoked on system tables. A call to SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE internally gets translated into ALTER TABLE sql. When ALTER TABLE is executed for SYSCS_INPLACE_COMPRESS_TABLE, we want to allow SchemaDescriptor request for system tables. DERBY-1062 Validate that the table is ok for DDL -- e.g. that it exists, it is not a view. It is ok for it to be a system table. Also check that its schema is ok. Currently, the only time this method is called is when user has asked for inplace compress. eg call SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE('SYS','SYSTABLES',1,1,1); Inplace compress is allowed on both system and user tables. Validate that the table is ok for DDL -- e.g. that it exists, it is not a view, and is not a system table, and that its schema is ok. Initialize the object name we will be performing the DDL on and check that we are not in the system schema and that DDL is allowed. A DDL statement is always atomic Just get the table descriptor. Don't worry if it belongs to a view, system table, synonym or a real table. Let the caller decide what to do. Make a from list for binding query fragments in a CREATE/ALTER TABLE statement. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Private methods Adjust remaining length Compress B Layer data if extended total length is used by removing the continuation headers Compute the int array of magnitude from input value segments. Convert EBCDIC byte array to unicode string *********************************************************************** Private methods /************************************************************************* Make sure a certain amount of Layer A data is in the buffer. The data will be in the buffer after this method is called. Make sure a certain amount of Layer B data is in the buffer. The data will be in the buffer after this method is called. This method makes sure there is enough room in the buffer for a certain number of bytes.  This method will allocate a new buffer if needed and shift the bytes in the current buffer to make ensure space is available for a fill.  Right now this method will shift bytes as needed to make sure there is as much room as possible in the buffer before trying to do the read.  The idea is to try to have space to get as much data as possible if we need to do a read on the socket's stream. This method will attempt to read a minimum number of bytes from the underlying stream.  This method will keep trying to read bytes until it has obtained at least the minimum number. Get the next CodePoint from a collection Get the next CodePoint from a collection and check that it matches the specified CodePoint Return chaining bit for current DSS. Length of current DDM object Creates an InputStream which can stream EXTDTA objects. The InputStream uses this DDMReader to read data from network. The DDMReader should not be used before all data in the stream has been read. Initialize values for this session, the reader is reused so we need to set null and 0 values Next DSS has different correlator than current DSS Next DSS has same correlator as current DSS Check for the command protocol checks the null EXTDTA byte returns true if null, false otherwise helper method for getEXTDTAData Push DDM Length on to collection stack Is there more data in the buffer Is there more in this DDM object Is there more in this DDS object Convert a range of packed nybbles (up to 9 digits without overflow) to an int. Note that for performance purpose, it does not do array-out-of-bound checking. Convert a range of packed nybbles (up to 18 digits without overflow) to a long. Note that for performance purpose, it does not do array-out-of-bound checking. Read a BigDecimal value The following routines read different types from the input stream Data can be in network order or platform order depending on whether the data is part of the protocol or data being received The platform is determined by EXCSAT protocol Read byte value Read byte string value Read byte string value Read string value ***************************************************************** NetworkServerControl  command protocol reading routines ***************************************************************** Read string value Read the CodePoint reads a DSS continuation header prereq: pos is positioned on the first byte of the two-byte header post:   dssIsContinued is set to true if the continuation bit is on, false otherwise dssLength is set to DssConstants.MAXDSS_LEN - 2 (don't count the header for the next read) helper method for getEXTDTAData Read platform double value Read DSS header DSS Header format is 2 bytes - length 1 byte  - 'D0'  - indicates DDM data 1 byte  - DSS format |---|---------|----------| | 0 |  flags  |  type    | |---|---------|----------| | 0 | 1  2  3 | 4 5 6 7  | |---|---------|----------| bit 0 - '0' bit 1 - '0' - unchained, '1' - chained bit 2 - '0' - do not continue on error, '1' - continue on error bit 3 - '0' - next DSS has different correlator, '1' - next DSS has same correlator type - 1 - Request DSS - 2 - Reply DSS - 3 - Object DSS - 4 - Communications DSS - 5 - Request DSS where no reply is expected 2 bytes - request correlation id Read encrypted string Read platform float value Read platform int value Read length delimited string value in DDM data with default encoding This method is used by EXTDTAReaderInputStream to read the next chunk of data. Furthermore, when Layer B streaming is carried out, calling this method may finish layer B streaming. This method is used by EXTDTAReaderInputStream to read the next chunk of data. Calling this method finishes layer B streaming if continuation of DSS segment was finished. This lengthless method must be called only when layer B streaming. This method is used by EXTDTAReaderInputStream to read the next chunk of data. Furthermore, when Layer B streaming is carried out, calling this method finishes layer B streaming if continuation of DSS segment was finished. This method is used by EXTDTAReaderInputStream to read the first chunk of data. This lengthless method must be called only when layer B streaming. This method is used by EXTDTAReaderInputStream to read the first chunk of data. Read the DDM Length and CodePoint Read platform long value Read network int value Read network long value Read network short value Read network six byte value and put it in a long v Read Reply DSS This is used in testing the protocol.  We shouldn't see a reply DSS when we are servicing DRDA commands Read platform short value Read signed network short value Read string value Read string value Strings in DRDA protocol are encoded in EBCDIC by default so we need to convert to UCS2 Read encoded string value Read string value into a <code>DRDAString</code> object. Read specified length of string value in DDM data with default encoding Read byte value and mask out high order bytes before returning Switch the ccsidManager to the EBCDIC instance Switch the ccsidManager to the UTF-8 instance Methods to manage the data buffer. Methods orginally from JCC RESOLVE: need to check if this is the best performing way of doing this This is a helper method which shifts the buffered bytes from wherever they are in the current buffer to the beginning of different buffer (note these buffers could be the same). State information is updated as needed after the shift. Skip byte string value Skip byte string value Skip remaining DSS Print a internal trace message
Begins a DSS stream (for writing LOB data). private methods Write DSS header DSS Header format is 2 bytes - length 1 byte  - 'D0'  - indicates DDM data 1 byte  - DSS format |---|---------|----------| | 0 |   flags | type     | |---|---------|----------| | 0 | 1 2   3 | 4 5 6 7  | |---|---------|----------| bit 0 - '0' bit 1 - '0' - unchained, '1' - chained bit 2 - '0' - do not continue on error, '1' - continue on error bit 3 - '0' - next DSS has different correlator, '1' - next DSS has same correlator type - 1 - Request DSS - 2 - Reply DSS - 3 - Object DSS - 4 - Communications DSS - 5 - Request DSS where no reply is expected Calculate extended length byte count which follows the DSS header for extended DDM. Clear the entire send buffer Does a logical "clear" of everything written to the buffer after the received mark.  It's assumed that this method will be used in error cases when we've started writing one or more DSSes, but then hit an error and need to back out.  After backing out, we'll always need to write _something_ back to the client to indicate an error (typically, we just write an SQLCARD) but what exactly gets written is handled in DRDAConnThread.  Here, we just do the necessary prep so that whatever comes next will succeed. Erase all writes for the current ddm and reset the top Copy Data to End Create a buffer and copy from the position given to the end of data Note that the position given is treated as relative to the current DSS, for there may be other DSS blocks (chained, presumably) which are sitting unwritten in the buffer. The caller doesn't know this, though, and works only with the current DSS. getDSSLength, copyDSSDataToEnd, and truncateDSS work together to provide a sub-protocol for DRDAConnThread to use in its implementation of the LMTBLKPRC protocol. They enable the caller to determine when it has written too much data into the current DSS, to reclaim the extra data that won't fit, and to truncate that extra data once it has been reclaimed and stored elsewhere. Note that this support only works for the current DSS. Earlier, chained DSS blocks cannot be accessed using these methods. For additional background information, the interested reader should investigate bugs DERBY-491 and 492 at: http://issues.apache.org/jira/browse/DERBY-491 and http://issues.apache.org/jira/browse/DERBY-492 Create DSS data object Create DSS reply object Create DSS request object NOTE: This is _ONLY_ used for testing the protocol (via the ProtocolTestAdapter.java file in this package)! We should never create a DSS request in normal DRDA processing (we should only create DSS replies and DSS objects). method to determine if any data is in the request. this indicates there is a dss object already in the buffer. End the current DDM End final DDM and DSS header by writing the length in the length location End DSS header by writing the length in the length location and setting the chain bit. End DSS header by writing the length in the length location and setting the chain bit. End DSS header by writing the length in the length location and setting the chain bit.  Unlike the other two endDss methods, this one overrides the default chaining byte (which is set in beginDss) with the chaining byte that is passed in.  NOTE: This method is only used in association with createDssRequest, and thus is for TESTING purposes only (via ProtocolTestAdpater.java).  No calls should be made to this method in normal DRDA processing (because for normal processing, chaining must be determined automatically based on DSS requests). Ensure that there is space in the buffer Finalize the current DSS chain and send it if needed. Updates the chaining state of the most recently-written- to-buffer DSS to correspond to the most recently-read- from-client request.  If that chaining state indicates we've reached the end of a chain, then we go ahead and send the buffer across the wire. Finish a DSS Layer A object. The length of dss object will be calculated based on the difference between the start of the dss, saved on the beginDss call, and the current offset into the buffer which marks the end of the data.  In the event the length requires the use of continuation Dss headers, one for each 32k chunk of data, the data will be shifted and the continuation headers will be inserted with the correct values as needed. Flush buffer to outputstream Flush buffer to specified stream Writes out a scalar stream DSS segment, along with DSS continuation headers if necessary. Get a copy of a subsequence of the output buffer, starting at the specified position and ending at the current buffer position. Get the current position in the output buffer. Looks at chaining info for previous DSS written, and use that to figure out what the correlation id for the current DSS should be.  Return that correlation id. Get the current ccsidManager Get the length of the current DSS block we're working on. This is used by the LMTBLKPRC protocol, which does its own conversational blocking protocol above the layer of the DRDA blocking. The LMTBLKPRC implementation (in DRDAConnThread) needs to be able to truncate a DSS block when splitting a QRYDTA response. Check if a byte value represents a continuation byte in a UTF-8 byte sequence. Continuation bytes in UTF-8 always match the bit pattern {@code 10xxxxxx}. Takes note of the location of the most recently completed DSS in the buffer, and then returns the current offset. This method is used in conjunction with "clearDSSesBackToMark" to allow for DRDAConnThread to "back-out" DSSes in the event of errors. Mark the DSS that we're currently writing as a continued DSS, which is done by setting the high-order bit to "1", per DDM spec. This means: 1. One or more continuation DSSes will immediately follow the current (continued) DSS. 2. All continuation DSSes will have a 2-byte continuation header, followed by data; in other words, chaining state, correlation id, dss format info, and code point will NOT be included.  All of that info is present ONLY in the FIRST DSS in the list of continued DSSes. NOTE: A DSS can be a "continuation" DSS _and_ a "continued" DSS at the same time.  However, the FIRST DSS to be continued canNOT be a continuation DSS. Find the maximum number of bytes needed to represent the string in the default encoding. Override the default chaining byte with the chaining byte that is passed in. Write pad bytes using spaceChar prepScalarStream does the following prep for writing stream data: 1.  Flushes an existing DSS segment, if necessary 2.  Determines if extended length bytes are needed 3.  Creates a new DSS/DDM header and a null byte indicator, if applicable If value of length was less than 0, this method processes streaming as Layer B Streaming. cf. page 315 of specification of DRDA, Version 3, Volume 3 reset values for sending next message Reset any chaining state that needs to be reset at time of the send Change the current position in the output buffer. set protocol to CMD protocol Switch the ccsidManager to the EBCDIC instance Switch the ccsidManager to the UTF-8 instance Collection methods Mark the location of the length bytes for the collection so they can be updated later Truncate the current DSS. Before making this call, you should ensure that you have copied the data to be truncated somewhere else, by calling copyDSSDataToEnd Write a Java <code>java.math.BigDecimal</code> to packed decimal bytes. Write platform boolean Write routines Write byte Write byte array Write byte array Write byte array Write code point and 4 bytes Write platform double Write platform float Write platform int Write length delimited string Write length delimited string insert a 4 byte length/codepoint pair into the buffer. total of 4 bytes inserted in buffer. Note: the length value inserted in the buffer is the same as the value passed in as an argument (this value is NOT incremented by 4 before being inserted). Write platform long Write network int Write network short Write scalar 1 byte object includes length, codepoint and value Write scalar 2 byte object includes length, codepoint and value Write scalar byte array object includes length, codepoint and value Write scalar object header includes length and codepoint Write padded scalar byte array object  value Write padded scalar byte array object includes length, codepoint and value Write padded scalar string object includes length, codepoint and value the string is converted into the appropriate codeset (EBCDIC) Write padded scalar <code>DRDAString</code> object value. The string is converted into the appropriate codeset. Write scalar string object includes length, codepoint and value the string is converted into the appropriate codeset (EBCDIC) Write boolean as short The following methods write data in the platform format The platform format was indicated during connection time as ASC since JCC doesn't read JVM platform (yet) Write platform short Write string with default encoding Write a value of a user defined type.
* Check whether the delete rule of FOREIGN KEY  must not be CASCADE because * the  new relationship would cause another table to be delete-connected to * the same table through multiple paths with different delete rules or with * delete rule equal to SET NULL. * * For example : *                      t1 *  		 CASCADE   /  \  CASCADE *                    /    \ *                  t2      t3 *                   \      / *          SET NULL  \    /  CASCADE (Can we add this one ? NO) *			          \   / \t4/ * *   existing links: *   t2 references t1   ON DELETE CASCADE  (fkey1) *   t3 references t1   ON DELETE CASCADE  (fkey2) *   t2 reference  t4   ON DELETE SET NULL (fkey3) *   Now if if try to add a new link i.e *   t4 references t3   ON DELETE SET NULL  (fkey4) *   Say if we add it,  then if we execute 'delete from t1' *   Because of referential actions , we will try to delete a row through *   one path and tries to update  through another path. *   Nothing in standard that say whether we are suppose to delete the row *   or update the row.  DB2UDB raises error when we try to create the *   foreign key fkey4, Derby also does the same. * *   How we catch the error case ? *   Point to note here is the table(t4) we are  adding the foreign key does *   not have a problem in this scenarion because we are adding a *   a CASACDE link , some other table(t2) that is referring *   can get multiple referential action paths. We can not *   this error case for self referencing links. *   Algorithm: *   -Gather the foreign keys that are *   referring(ReferencedKeyConstraintDescriptor) to the table we are adding *   foreign key, in our example case we get (fkey3 - table t2 -t4 link) *   for each ReferencedKeyConstraintDescriptor *   { *    1)find the delete connections of the referring table. *    [getCurrentDeleteConnections() will return this hash table] *	  2) we already have collected the Delete connections *       in validDeleteConnections() for the actual table we are adding the *       foreign key. *    3) Now check whether the referring table  is also *       referring  any table that the table we are adding *       foreign key has delete connection. * *     for each table referring table delete connection hash table *     { *      if it is there in the actual table delete connection hash table *      { *         //In our example case we find t1 in both the hash tables. *         make sure we are having valid referential action *         from the existing path and the new path we got from *         new foreign key relation ship. *        //In our example case t2 has CASCADE relations with t1 *        //Because of new foreign key added we also get *        //SET NULL relation ship with t1. This is not valid *        //so we should throw error. *      } *     } * } *Check whether the mulitple path case is valid or not following * cases are invalid: * case 1: The relationship causes the table to be delete-connected to * the indicated table through multiple relationships and the * delete rule of the existing relationship is SET NULL. * case 2: The relationship would cause the table to be * delete-connected to the same table through multiple * relationships and such relationships must have the same * delete rule (NO ACTION, RESTRICT or CASCADE). * case 3: The relationship would cause another table to be * delete-connected to the same table through multiple paths * with different delete rules or with delete rule equal to SET NULL. Finds the existing delete connection for the table and the referential actions that will occur and stores the information in the hash table. HashTable (key , value) = ( table name that this table is delete connected to, referential action that will occur if there is a delete on the table this table connected to [CASCADE, SET NULL, RESTRICT, NO ACTION] ) * For a foreign key, this is used to locate the referenced * key using the ConstraintInfo.  If it doesn't find the * correct constraint it will throw an error. The following function validates whether the new foreign key relationship violates any restriction on the referential actions. The current refAction implementation does not allow cases where we can possible land up having multiple action for the same row in a table. This could happen because the user can possibly define different actions through multiple foreign key paths.  The following function throws an error while creating foreign keys if the new releationship leads to any such conditions. NOTE: The SQL99 standard also does not clearly says what we are supposed to do in these cases.  Our implementation just follows what is done in DB2 and throws error messaged similar to DB2 (sql0632N, sql0633N, sql0634N). *checks whether the foreign key relation ships referential action *is violating the restrictions we have in the current system.
Apply changes that can safely be made in soft upgrade. Any changes must not prevent the database from being re-booted by the a Derby engine at the older version fromMajorVersionNumber. <BR> Examples are fixes to catalog meta data, e.g. fix nullability of a system column. <BR> <B>Upgrade items for 10.1</B> <UL> <LI> None. </UL> Check to see if a database has been upgraded to the required level in order to use a language feature. Do full upgrade.  Apply changes that can NOT be safely made in soft upgrade. <BR> <B>Upgrade items for every new release</B> <UL> <LI> Drop and recreate the stored versions of the JDBC database metadata queries </UL> <BR> <B>Upgrade items for 10.1</B> <UL> <LI> None. </UL> Drop a System catalog. Remove the description of a System table from the data dictionary. This does not delete the conglomerates that hold the catalog or its indexes. Get the minor version from the JBMS product minor version/maint version. Bumps it up by 1 if production, or 0 if beta to ensure minor upgrade across beta.  Starts at 2 because of an old convention. We use this starting at 2 to allow soft upgrade to write a version of 1 with the old major number to ensure a minor upgrade when reverting to an old version afer a soft upgrade. E.g run with 5.0.2, then 5.2.1.1, then 5.0.2. Want to ensure 5.0.2 does the minor upgrade. //////////////////////////////////////////////////////////////////////  FORMATABLE INTERFACE  ////////////////////////////////////////////////////////////////////// Get the formatID which corresponds to this class. Map to the 5.0 version identifier so that 5.0 will understand this object when we write it out in soft upgrade mode. CS 5.0 will de-serialize it correctly. When we are writing out a 5.1 version number we write out the 5.1 version just to ensure no problems. Do any work needed for a minor revision change. For the data dictionary this is always invalidating stored prepared statements.  When we are done with the upgrade, we always recompile all SPSes so the customer doesn't have to (and isn't going to get deadlocks because of the recomp). Privileged startup. Must be private so that user code can't call this entry point. Modifies the nullability of the system table corresponding to the received catalog number. Read this object from a stream of stored objects. Set the minor version.  Ignore the major version. //////////////////////////////////////////////////////////////////////  OVERRIDE OBJECT METHODS  ////////////////////////////////////////////////////////////////////// Stringify this Version. //////////////////////////////////////////////////////////////////////  DataDictionary SPECIFIC  ////////////////////////////////////////////////////////////////////// Upgrade the data dictionary catalogs to the version represented by this DD_Version. Write this object to a stream of stored objects. Write out the minor version which is bumped across minor release. Just to be safe, write out the major version too.  This will allow us to do versioning of a specific Version impl in the future.
Find the dependable for getDependable. Can return a null references, in which case getDependable() will thrown an exception. Get the dependable for the given UUID //////////////////////////////////////////////////////////////////////  DDdependable METHODS  //////////////////////////////////////////////////////////////////////  Get the formatID which corresponds to this class. ////////////////////////////////////////////////////////////////  VACUOUS FORMATABLE INTERFACE. ALL THAT A VACUOUSDEPENDABLEFINDER NEEDS TO DO IS STAMP ITS FORMAT ID ONTO THE OUTPUT STREAM.  //////////////////////////////////////////////////////////////// Read this object from a stream of stored objects. Nothing to do. Our persistent representation is just a 2-byte format id. ////////////////////////////////////////////////////////////////  OBJECT SUPPORT  //////////////////////////////////////////////////////////////// Write this object to a stream of stored objects. Again, nothing to do. We just stamp the output stream with our Format id.
This is the method that is invoked from the outer query
if this is 10.11 or higher and the table has an identity column, get the uuid of the sequence generator backing the identity column
Accept the visitor for all visitable children of this node. Gets and binds all the constraints for an INSERT/UPDATE/DELETE. First finds the constraints that are relevant to this node. This is done by calling getAllRelevantConstriants().  If getAllRelevantConstraints() has already been called, then this list is used.  Then it creates appropriate dependencies. Then binds check constraints.  It also generates the array of FKInfo items that are used in code generation. Note: we have a new flag here to see if defer processing is enabled or not, the only scenario that is disabled is when we reapply the reply message we get from the source Binds an already parsed expression that only involves columns in a single row. E.g., a check constraint or a generation clause. Get all of our dependents due to a constraint. Makes the calling object (usually a Statement) dependent on all the constraints. Makes the calling object (usually a Statement) dependent on all the constraints. * Simple little helper method Generate a method to evaluate a tree of CHECK CONSTRAINTS. Generate the code to evaluate a tree of CHECK CONSTRAINTS. Get the ANDing of all appropriate check constraints as 1 giant query tree. Makes the calling object (usually a Statement) dependent on all the constraints. If the DML is on a temporary table, generate the code to mark temporary table as modified in the current UOW. At rollback transaction (or savepoint), we will check if the temporary table was modified in that UOW. If yes, we will remove all the data from the temporary table Generate the FKInfo structures used during code generation. For each constraint that isn't a check constraint, add another one of these FKInfo structures and then package them up into a single array. Generate a method to compute all of the generation clauses in a row. Generate the code to evaluate all of the generation clauses. If there are generation clauses, this routine builds an Activation method which evaluates the generation clauses and fills in the computed columns. Generate the TriggerInfo structures used during code generation. Get the list of indexes that must be updated by this DML statement. WARNING: As a side effect, it creates dependencies on those indexes. Get all the constraints relevant to this DML operation Get all the triggers relevant to this DML operation Get the check constraints for this node Return the FKInfo structure.  Just  a little wrapper to make sure we don't try to access it until after binding. Get a map to efficiently find heap columns from a compressed set of read columns. The returns a map such that <PRE> map[heapColId (0 based)] -&gt; readCol id (0 based) </PRE> Get and bind the ResultColumnList representing the columns in the target table, given the table's name. Get and bind the ResultColumnList representing the columns in the target table, given a FromTable for the target table. Get and bind the ResultColumnList representing the columns in the target table, given the table's name. Get a integer based row map from a bit set. Get a schema descriptor for the given table. Uses this.targetTableName. Return the TriggerInfo structure.  Just  a little wrapper to make sure we don't try to access it until after binding. Marks which indexes are affected by an UPDATE of the desired shape. Is passed a list of updated columns. Does the following: 1)	finds all indices which overlap the updated columns 2)	adds the index columns to a bitmap of affected columns 3)	adds the index descriptors to a list of conglomerate descriptors. Determine whether or not there are check constraints on the specified table. Determine whether or not there are generated columns in the specified table. Returns true if this DMLModStatement a [ NOT ] MATCHED action of a MERGE statement INSERT/UPDATE/DELETE are always atomic. Normalize synonym column references to have the name of the base table. Generate an optimized QueryTree from a bound QueryTree.  Actually, it can annotate the tree in place rather than generate a new tree, but this interface allows the root node of the optimized QueryTree to be different from the root node of the bound QueryTree. For non-optimizable statements, this method is a no-op. Throws an exception if the tree is not bound, or if the binding is out of date. Parse and bind the generating expressions of computed columns. Parse a check constraint and turn it into a query tree. Parse the generation clause for a column. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap referenced columns in the cd to reflect the passed in row map. Does this DML Node require deferred processing? Set to true if we have triggers or referential constraints that need deferred processing. Verify the target table.  Get the TableDescriptor if the target table is not a VTI.
Accept the visitor for all visitable children of this node. Returns the type of activation this class generates. Bind this DMLStatementNode.  This means looking up tables and columns and getting their types, and figuring out the result types of all expressions, as well as doing view resolution, permissions checking, etc. Bind the expressions in this DML statement. Bind the expressions in the underlying ResultSets with tables. Bind only the underlying ResultSets with tables.  This is necessary for INSERT, where the binding order depends on the underlying ResultSets. This means looking up tables and columns and getting their types, and figuring out the result types of all expressions, as well as doing view resolution, permissions checking, etc. Bind the tables in this DML statement. Generate the code to create the ParameterValueSet, if necessary, when constructing the activation.  Also generate the code to call a method that will throw an exception if we try to execute without all the parameters being set. Return default privilege needed for this node. Other DML nodes can override this method to set their own default privilege. Get the ResultSetNode from this DML Statement. (Useful for view resolution after parsing the view definition.) A read statement is atomic (DMLMod overrides us) if there are no work units, and no SELECT nodes, or if its SELECT nodes are all arguments to a function.  This is admittedly a bit simplistic, what if someone has: <pre> VALUES myfunc(SELECT max(c.commitFunc()) FROM T) </pre> but we aren't going too far out of our way to catch every possible wierd case.  We basically want to be permissive w/o allowing someone to partially commit a write. Make a ResultDescription for use in a PreparedStatement. ResultDescriptions are visible to JDBC only for cursor statements. For other types of statements, they are only used internally to get descriptions of the base tables being affected.  For example, for an INSERT statement, the ResultDescription describes the rows in the table being inserted into, which is useful when the values being inserted are of a different type or length than the columns in the base table. Optimize a DML statement (which is the only type of statement that should need optimizing, I think). This method over-rides the one in QueryTreeNode. This method takes a bound tree, and returns an optimized tree. It annotates the bound tree rather than creating an entirely new tree. Throws an exception if the tree is not bound, or if the binding is out of date. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.
end of cleanUp end of finish  end of open()
If user didn't provide columns list for auto-generated columns, then only include columns with auto-generated values in the resultset. Those columns would be ones with default value defined. Take the input row and return a new compact ExecRow using the column positions provided in columnIndexes. Copies references, no cloning. getSetAutoincrementValue will get the autoincrement value of the columnPosition specified for the target table. If increment is non-zero we will also update the autoincrement value. Remove duplicate columns from the array. Then use this array to generate a sub-set of insert resultset to be returned for JDBC3.0 getGeneratedKeys() call. Check that the received ColumnDescriptor corresponds to a column for which it is possible to fetch auto-generated keys. Verify that the auto-generated columns list (by position) has valid column positions for the table. Verify that the auto-generated columns list (by name) has valid column names for the table. If all the column names are valid, convert column names array to corresponding column positions array Save that column positions array in activation. We do this to simplify the rest of the logic(it only has to deal with column positions here after). * verify the auto-generated key columns list(ie there are no invalid * column names or positions). This is done at execution time because * for a precompiled insert statement, user can specify different column * selections for auto-generated keys.
Decode the update lock mode. <p> The value for update lock mode is in the second most significant byte for TransactionControl.SERIALIZABLE_ISOLATION_LEVEL isolation level. Otherwise (REPEATABLE READ, READ COMMITTED, and READ UNCOMMITTED) the lock mode is located in the least significant byte. <p> This is done to override the optimizer choice to provide maximum concurrency of record level locking except in SERIALIZABLE where table level locking is required in heap scans for correctness. get the index name given the conglomerate id of the index. Get next row from the source result set. Returns the description of the inserted rows. REVISIT: Do we want this to return NULL instead? For deferred update, get a deferred sparse row based on the deferred non-sparse row. Share the underlying columns. If there is no column bit map, make them the same row. <p> Normalize a row as part of the INSERT/UPDATE action of a MERGE statement. This applies logic usually found in a NormalizeResultSet, which is missing for the MERGE statement. </p>
Agent error - something very bad happened Object length not allowed Build the SQLERRMC for a {@code java.sql.DataTruncation} warning. Serialize all the fields of the {@code DataTruncation} instance in the order in which they appear in the parameter list of the constructor. Build preformatted SQLException text for severe exceptions or SQLExceptions that are not Derby exceptions. Just send the message text localized to the server locale. Create error message or message argements to return to client. The SQLERRMC will normally be passed back  to the server in a call to the SYSIBM.SQLCAMESSAGE but for severe exceptions the stored procedure call cannot be made. So for Severe messages we will just send the message text. This method will also truncate the value according the client capacity. CCC can only handle 70 characters. Server sends the sqlerrmc using UTF8 encoding to the client. To get the message, client sends back information to the server calling SYSIBM.SQLCAMESSAGE (see Sqlca.getMessage).  Several parameters are sent to this procedure including the locale, the sqlerrmc that the client received from the server. On server side, the procedure SQLCAMESSAGE in SystemProcedures then calls the MessageService.getLocalizedMessage to retrieve the localized error message. In MessageService.getLocalizedMessage the sqlerrmc that is passed in, is parsed to retrieve the message id. The value it uses to parse the MessageId is char value of 20, otherwise it uses the entire sqlerrmc as the message id. This messageId is then used to retrieve the localized message if present, to the client. Build Tokenized SQLERRMC to just send the tokenized arguments to the client. for a Derby SQLException or an SQLException thrown by user code. Message argument tokens are separated by SQLERRMC_TOKEN_DELIMITER Multiple messages are separated by SystemProcedures.SQLERRMC_MESSAGE_DELIMITER ... Check that the length is equal to the required length for this codepoint Check whether we have seen all the required code points check that the given typdefnam is acceptable Check SQLWarning and write SQLCARD as needed. Cleans up and closes a result set if an exception is thrown when collecting QRYDTA in response to OPNQRY or CNTQRY. Close DRDA  connection thread Close the current session Close a stream. Test if DRDA connection thread is closed Don't support this code point convert byte array to a Hex string Methods to keep track of required codepoints Copy a list of required code points to template for checking Done data Send SQLCARD for the end of the data If there's a severe error in the DDM chain, and if the header indicates "terminate chain on error", we stop processing further commands in the chain nor do we send any reply for them.  In accordance to this, a SQLERRRM message indicating the severe error must have been sent! (otherwise application requestor, such as JCC, would not terminate the receiving of chain replies.) Each DRDA command is processed independently. DRDA defines no interdependencies across chained commands. A command is processed the same when received within a set of chained commands or received separately.  The chaining was originally defined as a way to save network costs. Exchange server attributes with application requester Finalize the current DSS chain and send it if needed. Privileged service lookup. Must be private so that user code can't call this entry point. Convert a {@code java.sql.Date} to a string with the format expected by the client. Convert a {@code java.sql.Time} to a string with the format expected by the client. Convert a {@code java.sql.Timestamp} to a string with the format expected by the client. Get connection from a database name Username and password is verified by making a connection to the database Get correlation id Get correlation token Get Database we are working on Get database name Translate from Derby exception severity to SVRCOD Get a {@code Calendar} instance with time zone set to GMT. The instance is cached for reuse by this thread. This calendar can be used to consistently read and write date and time values using the same calendar. Since the local default calendar may not be able to represent all times (for instance because the time would fall into a non-existing hour of the day when switching to daylight saving time, see DERBY-4582), we use the GMT time zone which doesn't observe daylight saving time. Get input stream Get whether connections are logged Privileged Monitor lookup. Must be private so that user code can't call this entry point. <p> Get the value of an output parameter of the specified type from a {@code CallableStatement}, in a form suitable for being writted by {@link #writeFdocaVal}. For most types, this means just calling {@code CallableStatement.getObject(int)}. </p> <p> This method should behave like the corresponding method for {@code ResultSet}, and changes made to one of these methods, must be reflected in the other method. See {@link #getObjectForWriteFdoca(java.sql.ResultSet, int, int)} for details. </p> <p> Get a column value of the specified type from a {@code ResultSet}, in a form suitable for being writted by {@link #writeFdocaVal}. For most types, this means just calling {@code ResultSet.getObject(int)}. </p> <p> The only exception currently is the data types representing dates and times, as they need to be fetched using the same {@code java.util.Calendar} as {@link #writeFdocaVal} uses when writing them (DERBY-4582). </p> <p> <b>Note:</b> Changes made in this method should also be made in the corresponding method for {@code CallableStatement}: {@link #getObjectForWriteFdoca(java.sql.CallableStatement, int, int)}. </p> Get output stream Get product id as bytes Check the database access exception and return the appropriate error codepoint. RDBNFNRM - Database not found RDBATHRM - Not Authorized RDBAFLRM - Access failure @return RDB Access codepoint get DDMReader Get server Get session we are working on <p> Get the SQLCODE to send for an exception or a warning. </p> <p> The client expects a negative SQLCODE for exceptions and a positive SQLCODE for warnings. SQLCODE 0 means there is no error or warning condition. SQLCODE is also used to encode the severity of the condition (as returned by {@code SQLException.getErrorCode()}). </p> <p> For warnings, the SQLCODE is 10000, which is identical to {@link ExceptionSeverity#WARNING_SEVERITY}. </p> <p> For exceptions, the SQLCODE is set to {@code -severity-1}, which allows all non-negative severity values to be encoded. (Derby only uses non-negative severity values in the first place.) </p> Get time slice value for length of time to work on a session get  DDMWriter Handle Exceptions - write error protocol if appropriate and close session or thread as appropriate ************************************************************************* Private methods ************************************************************************* Initialize class Create a new database and intialize the DRDAConnThread database. Initialize for a new session Invalid non-derby client tried to connect. thrown a required Value not found error and log a message to derby.log Invalid codepoint for this command Invalid value for this code point There are multiple reasons for not getting a connection, and all these should throw SQLExceptions with SQL state 08004 according to the SQL standard. Since only one of these SQL states indicate that an authentication error has occurred, it is not enough to check that the SQL state is 08004 and conclude that authentication caused the exception to be thrown. This method tries to get a StandardException from the SQLException and use getMessageId on that object to check for authentication error instead of the SQL state we get from SQLExceptions#getSQLState. getMessageId returns the entire id as defined in SQLState (e.g. 08004.C.1), while getSQLState only return the 5 first characters (i.e. 08004 instead of 08004.C.1) If the SQLException isn't linked to a StandardException, the assumption that SQL State 08004 is caused by an authentication failure is followed even though this is not correct. This was the pre DERBY-3060 way of solving the issue. Indicate a communications failure. Log to derby.log Indicate a communications failure Missing code point Insert an integer into a char array and pad it with leading zeros if its string representation is shorter than {@code length} characters. Parse access RDB Instance variables RDBACCCL - RDB Access Manager Class - required must be SQLAM CRRTKN - Correlation Token - required RDBNAM - Relational database name -required PRDID - Product specific identifier - required TYPDEFNAM   - Data Type Definition Name -required TYPDEFOVR   - Type definition overrides -required RDBALWUPD -  RDB Allow Updates optional PRDDTA - Product Specific Data - optional - ignorable STTDECDEL - Statement Decimal Delimiter - optional STTSTRDEL - Statement String Delimiter - optional TRGDFTRT - Target Default Value Return - optional Parse Access Security If the target server supports the SECMEC requested by the application requester then a single value is returned and it is identical to the SECMEC value in the ACCSEC command. If the target server does not support the SECMEC requested, then one or more values are returned and the application requester must choose one of these values for the security mechanism. We currently support - user id and password (default for JCC) - encrypted user id and password - strong password substitute (USRSSBPWD w/ Derby network client only) Instance variables SECMGRNM  - security manager name - optional SECMEC    - security mechanism - required RDBNAM    - relational database name - optional SECTKN    - security token - optional, (required if sec mech. needs it) Parse CLSQRY Instance Variables RDBNAM - relational database name - optional PKGNAMCSN - RDB Package Name, Consistency Token and Section Number - required QRYINSID - Query Instance Identifier - required - level 7 MONITOR - Monitor events - optional. Parse CNTQRY - Continue Query Instance Variables RDBNAM - Relational Database Name - optional PKGNAMCSN - RDB Package Name, Consistency Token, and Section Number - required QRYBLKSZ - Query Block Size - required QRYRELSCR - Query Relative Scrolling Action - optional QRYSCRORN - Query Scroll Orientation - optional - level 7 QRYROWNBR - Query Row Number - optional QRYROWSNS - Query Row Sensitivity - optional - level 7 QRYBLKRST - Query Block Reset - optional - level 7 QRYRTNDTA - Query Returns Data - optional - level 7 QRYROWSET - Query Rowset Size - optional - level 7 QRYRFRTBL - Query Refresh Answer Set Table - optional NBRROW - Number of Fetch or Insert Rows - optional MAXBLKEXT - Maximum number of extra blocks - optional RTNEXTDTA - Return of EXTDTA Option - optional MONITOR - Monitor events - optional. Parse CNTQRY objects Instance Variables OUTOVR - Output Override Descriptor - optional Parse mixed character string Parse single byte character string Parse DSCSQLSTT - Describe SQL Statement previously prepared Instance Variables TYPSQLDA - sqlda type expected (output or input) RDBNAM - relational database name - optional PKGNAMCSN - RDB Package Name, Consistency Token and Section Number - required MONITOR - Monitor events - optional. Parse a date string as it is received from the client. Parses EXCSAT (Exchange Server Attributes) Instance variables EXTNAM(External Name)   - optional MGRLVLLS(Manager Levels) - optional SPVNAM(Supervisor Name) - optional SRVCLSNM(Server Class Name) - optional SRVNAM(Server Name) - optional, ignorable SRVRLSLV(Server Product Release Level) - optional, ignorable Parses EXCSAT2 (Exchange Server Attributes) Instance variables EXTNAM(External Name)   - optional MGRLVLLS(Manager Levels) - optional SPVNAM(Supervisor Name) - optional SRVCLSNM(Server Class Name) - optional SRVNAM(Server Name) - optional, ignorable SRVRLSLV(Server Product Release Level) - optional, ignorable Parse EXCSQLIMM - Execute Immediate Statement Instance Variables RDBNAM - relational database name - optional PKGNAMCSN - RDB Package Name, Consistency Token and Section Number - required RDBCMTOK - RDB Commit Allowed - optional MONITOR - Monitor Events - optional Command Objects TYPDEFNAM - Data Type Definition Name - optional TYPDEFOVR - TYPDEF Overrides -optional SQLSTT - SQL Statement -required Parse EXCSQLSET - Execute Set SQL Environment Instance Variables RDBNAM - relational database name - optional PKGNAMCT - RDB Package Name, Consistency Token  - optional MONITOR - Monitor Events - optional Command Objects TYPDEFNAM - Data Type Definition Name - required TYPDEFOVR - TYPDEF Overrides - required SQLSTT - SQL Statement - required (at least one; may be more) Parse EXCSQLSET objects Objects TYPDEFNAM - Data type definition name - optional TYPDEFOVR - Type defintion overrides - optional SQLSTT - SQL Statement - required (a list of at least one) Objects may follow in one DSS or in several DSS chained together. Parse EXCSQLSTT - Execute non-cursor SQL Statement previously prepared Instance Variables RDBNAM - relational database name - optional PKGNAMCSN - RDB Package Name, Consistency Token and Section Number - required OUTEXP - Output expected NBRROW - Number of rows to be inserted if it's an insert PRCNAM - procedure name if specified by host variable, not needed for Derby QRYBLKSZ - query block size MAXRSLCNT - max resultset count MAXBLKEXT - Max number of extra blocks RSLSETFLG - resultset flag RDBCMTOK - RDB Commit Allowed - optional OUTOVROPT - output override option QRYROWSET - Query Rowset Size - Level 7 MONITOR - Monitor events - optional. Parse EXCSQLSTT command objects Command Objects TYPDEFNAM - Data Type Definition Name - optional TYPDEFOVR - TYPDEF Overrides -optional SQLDTA - optional, variable data, specified if prpared statement has input parameters EXTDTA - optional, externalized FD:OCA data OUTOVR - output override descriptor, not allowed for stored procedure calls If TYPDEFNAM and TYPDEFOVR are supplied, they apply to the objects sent with the statement.  Once the statement is over, the default values sent in the ACCRDB are once again in effect.  If no values are supplied, the values sent in the ACCRDB are used. Objects may follow in one DSS or in several DSS chained together. Parse EXCSQLIMM objects Objects TYPDEFNAM - Data type definition name - optional TYPDEFOVR - Type defintion overrides SQLSTT - SQL Statement required If TYPDEFNAM and TYPDEFOVR are supplied, they apply to the objects sent with the statement.  Once the statement is over, the default values sent in the ACCRDB are once again in effect.  If no values are supplied, the values sent in the ACCRDB are used. Objects may follow in one DSS or in several DSS chained together. Parse an encoded data string from the Application Requester Parse manager levels Instance variables MGRLVL - repeatable, required CODEPOINT CCSIDMGR - CCSID Manager CMNAPPC - LU 6.2 Conversational Communications Manager CMNSYNCPT - SNA LU 6.2 SyncPoint Conversational Communications Manager CMNTCPIP - TCP/IP Communication Manager DICTIONARY - Dictionary RDB - Relational Database RSYNCMGR - Resynchronization Manager SECMGR - Security Manager SQLAM - SQL Application Manager SUPERVISOR - Supervisor SYNCPTMGR - Sync Point Manager VALUE On the second appearance of this codepoint, it can only add managers Parse MONITOR DRDA spec says this is optional.  Since we don't currently support it, we just ignore. Parse nullable character mixed byte or nullable character single byte Format 1 byte - null indicator I4 - mixed character length N bytes - mixed character string 1 byte - null indicator I4 - single character length N bytes - single character length string Parse OPNQRY Instance Variables RDBNAM - relational database name - optional PKGNAMCSN - RDB Package Name, Consistency Token and Section Number - required QRYBLKSZ - Query Block Size - required QRYBLKCTL - Query Block Protocol Control - optional MAXBLKEXT - Maximum Number of Extra Blocks - optional - default value 0 OUTOVROPT - Output Override Option QRYROWSET - Query Rowset Size - optional - level 7 MONITOR - Monitor events - optional. Parse OPNQRY objects Objects TYPDEFNAM - Data type definition name - optional TYPDEFOVR - Type defintion overrides - optional SQLDTA- SQL Program Variable Data - optional If TYPDEFNAM and TYPDEFOVR are supplied, they apply to the objects sent with the statement.  Once the statement is over, the default values sent in the ACCRDB are once again in effect.  If no values are supplied, the values sent in the ACCRDB are used. Objects may follow in one DSS or in several DSS chained together. Parse OUTOVR - Output Override Descriptor This specifies the output format for data to be returned as output to a SQL statement or as output from a query. Parse OUTOVROPT - this indicates whether output description can be overridden on just the first CNTQRY or on any CNTQRY Parse PKGNAMCSN - RDB Package Name, Consistency Token, and Section Number Instance Variables NAMESYMDR - database name - not validated RDBCOLID - RDB Collection Identifier PKGID - RDB Package Identifier PKGCNSTKN - RDB Package Consistency Token PKGSN - RDB Package Section Number Parse PRPSQLSTT - Prepare SQL Statement Instance Variables RDBNAM - Relational Database Name - optional PKGNAMCSN - RDB Package Name, Consistency Token, and Section Number - required RTNSQLDA - Return SQL Descriptor Area - optional MONITOR - Monitor events - optional. Parse PRPSQLSTT objects Objects TYPDEFNAM - Data type definition name - optional TYPDEFOVR - Type defintion overrides - optional SQLSTT - SQL Statement required SQLATTR - Cursor attributes on prepare - optional - level 7 If TYPDEFNAM and TYPDEFOVR are supplied, they apply to the objects sent with the statement.  Once the statement is over, the default values sent in the ACCRDB are once again in effect.  If no values are supplied, the values sent in the ACCRDB are used. Objects may follow in one DSS or in several DSS chained together. Parse QRYBLSZ - this gives the maximum size of the query blocks that can be returned to the requester Parse a QRYCLSIMP - Implicitly close non-scrollable cursor after end of data. Parse QRYROWSET - this is the number of rows to return Parse RDBCMTOK - tells the database whether to allow commits or rollbacks to be executed as part of the command Since we don't have a SQL commit or rollback command, we will just ignore this for now Parse database name Parse security check Instance Variables SECMGRNM - security manager name - optional, ignorable SECMEC  - security mechanism - required SECTKN  - security token - optional, (required if encryption used) PASSWORD - password - optional, (required if security mechanism uses it) NEWPASSWORD - new password - optional, (required if sec mech. uses it) USRID   - user id - optional, (required if sec mec. uses it) RDBNAM  - database name - optional (required if databases can have own sec.) Parse SQLATTR - Cursor attributes on prepare This is an encoded string. Can have combination of following, eg INSENSITIVE SCROLL WITH HOLD Possible strings are SENSITIVE DYNAMIC SCROLL [FOR UPDATE] SENSITIVE STATIC SCROLL [FOR UPDATE] INSENSITIVE SCROLL FOR UPDATE WITH HOLD Parse SQLDTA - SQL program variable data and handle exception. Parse SQLDTA - SQL program variable data Instance Variables FDODSC - FD:OCA data descriptor - required FDODTA - FD:OCA data - optional Parse SQLSTT Dss Parse TYPDEFNAM Parse Type Defintion Overrides TYPDEF Overrides specifies the Coded Character SET Identifiers (CCSIDs) that are in a named TYPDEF. Instance Variables CCSIDSBC - CCSID for Single-Byte - optional CCSIDDBC - CCSID for Double-Byte - optional CCSIDMBC - CCSID for Mixed-byte characters -optional Parse TYPSQLDA - Type of the SQL Descriptor Area Parse a time string as it is received from the client. Parse a timestamp string as it is received from the client. Parse variable character mixed byte or variable character single byte Format I2 - VCM Length N bytes - VCM value I2 - VCS Length N bytes - VCS value Only 1 of VCM length or VCS length can be non-zero Position cursor for insensitive scrollable cursors Print a line to the DB2j log Process DRDA commands we can receive once server attributes have been exchanged. Process remainder data resulting from a split. This routine is called at the start of building each QRYDTA block. Normally, it observes that there is no remainder data from the previous QRYDTA block, and returns FALSE, indicating that there was nothing to do. However, if it discovers that the previous QRYDTA block was split, then it retrieves the remainder data from the result set, writes as much of it as will fit into the QRYDTA block (hopefully all of it will fit, but the row may be very long), and returns TRUE, indicating that this QRYDTA block has been filled with remainder data and should now be sent immediately. RDB not found Database name given under code point doesn't match previous database names Read different types of input parameters and set them in PreparedStatement Read different types of input parameters and set them in PreparedStatement Read and check a boolean value Read a UDT from the stream Remove codepoint from required list * Required value not found. Main routine for thread, loops until the thread is closed Gets a session, does work for the session Notice the client about a protocol error. Send unpexpected error to the client In initial state for a session, determine whether this is a command session or a DRDA protocol session.  A command session is for changing the configuration of the Net server, e.g., turning tracing on If it is a command session, process the command and close the session. If it is a DRDA session, exchange server attributes and change session state. Sets the specified binary EXTDTA parameter of the embedded statement. Sets the specified character EXTDTA parameter of the embedded statement. Set the current database Set logging of connections Set a statement or the database' byte order, depending on the arguments Set time slice value * Show runtime memory * Skip remainder of current DSS and all chained DSS'es Split QRYDTA into blksize chunks This routine is called if the QRYDTA data will not fit. It writes as much data as it can, then stores the remainder in the result set. At some later point, when the client returns with a CNTQRY, we will call processLeftoverQRYDTA to handle that data. The interaction between DRDAConnThread and DDMWriter is rather complicated here. This routine gets called because DRDAConnThread realizes that it has constructed a QRYDTA message which is too large. At that point, we need to reclaim the "extra" data and hold on to it. To aid us in that processing, DDMWriter provides the routines getDSSLength, copyDSSDataToEnd, and truncateDSS. For some additional detail on this complex sub-protocol, the interested reader should study bug DERBY-491 and 492 at: http://issues.apache.org/jira/browse/DERBY-491 and http://issues.apache.org/jira/browse/DERBY-492 Calculate SVRCOD value from SECCHKCD Switch the DDMWriter and DDMReader to EBCDIC Switch the DDMWriter and DDMReader to UTF8 IF supported Syntax error Object too big Error routines Seen too many of this code point Send string to console Sends a trace string to the console when reading an EXTDTA value (if tracing is enabled). Validate SECMEC_USRSSBPWD (Strong Password Substitute) can be used as DRDA security mechanism. Here we check that the target server can support SECMEC_USRSSBPWD security mechanism based on the environment, application requester's identity (PRDID) and connection URL. IMPORTANT NOTE: -------------- SECMEC_USRSSBPWD is ONLY supported by the target server if: - current authentication provider is Derby BUILTIN or NONE. (database / system level) (Phase I) - database-level password must have been encrypted with the SHA-1 based authentication scheme - Application requester is 'DNC' (Derby Network Client) (Phase I) Don't support this value Verify that the code point is in the right order Verify that the code point is the required code point Verify userId and password Username and password is verified by making a connection to the database Write ABNUOWRM - query process has terminated in an error condition such as deadlock or lock timeout. Severity code is always error * @exception DRDAProtocolException Write Access to RDB Completed Instance Variables SVRCOD - severity code - 0 info, 4 warning -required PRDID - product specific identifier -required TYPDEFNAM - type definition name -required TYPDEFOVR - type definition overrides - required RDBINTTKN - token which can be used to interrupt DDM commands - optional CRRTKN  - correlation token - only returned if we didn't get one from requester SRVDGN - server diagnostic information - optional PKGDFTCST - package default character subtype - optional USRID - User ID at the target system - optional SRVLST - Server List Write ACCSECRD If the security mechanism is known, we just send it back along with the security token if encryption is going to be used. If the security mechanism is not known, we send a list of the ones we know. Instance Variables SECMEC - security mechanism - required SECTKN - security token - optional (required if security mechanism uses encryption) SECCHKCD - security check code - error occurred in processing ACCSEC Write CMDCHKRM Instance Variables SVRCOD - Severity Code - required Write ENDQRYRM - query process has terminated in such a manner that the query or result set is now closed.  It cannot be resumed with the CNTQRY command or closed with the CLSQRY command Write ENDUOWRM Instance Variables SVCOD - severity code - WARNING - required UOWDSP - Unit of Work Disposition - required RDBNAM - Relational Database name - optional SRVDGN - Server Diagnostics information - optional Write reply to EXCSAT command Instance Variables EXTNAM - External Name (optional) MGRLVLLS - Manager Level List (optional) SRVCLSNM - Server Class Name (optional) - used by JCC SRVNAM - Server Name (optional) SRVRLSLV - Server Product Release Level (optional) This routine places some data into the current QRYDTA block using FDODTA (Formatted Data Object DaTA rules). There are 3 basic types of processing flow for this routine: - In normal non-rowset, non-scrollable cursor flow, this routine places a single row into the QRYDTA block and returns TRUE, indicating that the caller can call us back to place another row into the result set if he wishes. (The caller may need to send Externalized Data, which would be a reason for him NOT to place any more rows into the QRYDTA). - In ROWSET processing, this routine places an entire ROWSET of rows into the QRYDTA block and returns FALSE, indicating that the QRYDTA block is full and should now be sent. - In callable statement processing, this routine places the results from the output parameters of the called procedure into the QRYDTA block. This code path is really dramatically different from the other two paths and shares only a very small amount of common code in this routine. In all cases, it is possible that the data we wish to return may not fit into the QRYDTA block, in which case we call splitQRYDTA to split the data and remember the remainder data in the result set. Splitting the data is relatively rare in the normal cursor case, because our caller (writeQRYDTA) uses a coarse estimation technique to avoid calling us if he thinks a split is likely. The overall structure of this routine is implemented as two loops: - the outer "do ... while ... " loop processes a ROWSET, one row at a time. For non-ROWSET cursors, and for callable statements, this loop executes only once. - the inner "for ... i &lt; numCols ..." loop processes each column in the current row, or each output parmeter in the procedure. Most column data is written directly inline in the QRYDTA block. Some data, however, is written as Externalized Data. This is commonly used for Large Objects. In that case, an Externalized Data Pointer is written into the QRYDTA block, and the actual data flows in separate EXTDTA blocks which are returned after this QRYDTA block. Write Fdoca Value to client Write manager levels The target server must not provide information for any target managers unless the source explicitly requests it. For each manager class, if the target server's support level is greater than or equal to the source server's level, then the source server's level is returned for that class if the target server can operate at the source's level; otherwise a level 0 is returned.  If the target server's support level is less than the source server's level, the target server's level is returned for that class.  If the target server does not recognize the code point of a manager class or does not support that class, it returns a level of 0.  The target server then waits for the next command or for the source server to terminate communications. When the source server receives EXCSATRD, it must compare each of the entries in the mgrlvlls parameter it received to the corresponding entries in the mgrlvlls parameter it sent.  If any level mismatches, the source server must decide whether it can use or adjust to the lower level of target support for that manager class.  There are no architectural criteria for making this decision. The source server can terminate communications or continue at the target servers level of support.  It can also attempt to use whatever commands its user requests while receiving error reply messages for real functional mismatches. The manager levels the source server specifies or the target server returns must be compatible with the manager-level dependencies of the specified manangers.  Incompatible manager levels cannot be specified. Instance variables MGRLVL - repeatable, required CODEPOINT CCSIDMGR - CCSID Manager CMNAPPC - LU 6.2 Conversational Communications Manager CMNSYNCPT - SNA LU 6.2 SyncPoint Conversational Communications Manager CMNTCPIP - TCP/IP Communication Manager DICTIONARY - Dictionary RDB - Relational Database RSYNCMGR - Resynchronization Manager SECMGR - Security Manager SQLAM - SQL Application Manager SUPERVISOR - Supervisor SYNCPTMGR - Sync Point Manager XAMGR - XA manager VALUE Write a null SQLCARD as an object write nullability if this is a nullable drdatype and FDOCA null value if appropriate Write a OPNQFLRM - Open Query Failure Instance Variables SVRCOD - Severity Code - required - 8 ERROR RDBNAM - Relational Database Name - required Write OPNQRYRM - Open Query Complete Instance Variables SVRCOD - Severity Code - required QRYPRCTYP - Query Protocol Type - required SQLCSRHLD - Hold Cursor Position - optional QRYATTSCR - Query Attribute for Scrollability - optional - level 7 QRYATTSNS - Query Attribute for Sensitivity - optional - level 7 QRYATTUPD - Query Attribute for Updatability -optional - level 7 QRYINSID - Query Instance Identifier - required - level 7 SRVDGN - Server Diagnostic Information - optional Piggy-back any modified session attributes on the current message. Writes a PBSD conataining one or both of PBSD_ISO and PBSD_SCHEMA. PBSD_ISO is followed by the jdbc isolation level as an unsigned byte. PBSD_SCHEMA is followed by the name of the current schema as an UTF-8 String. Write PKGNAMCSN Instance Variables NAMESYMDR - database name - not validated RDBCOLID - RDB Collection Identifier PKGID - RDB Package Identifier PKGCNSTKN - RDB Package Consistency Token PKGSN - RDB Package Section Number There are two possible formats, fixed and extended which includes length information for the strings Write QRYDSC - Query Answer Set Description Write QRYDTA - Query Answer Set Data Contains some or all of the answer set data resulting from a query If the client is not using rowset processing, this routine attempts to pack as much data into the QRYDTA as it can. This may result in splitting the last row across the block, in which case when the client calls CNTQRY we will return the remainder of the row. Splitting a QRYDTA block is expensive, for several reasons: - extra logic must be run, on both client and server side - more network round-trips are involved - the QRYDTA block which contains the continuation of the split row is generally wasteful, since it contains the remainder of the split row but no additional rows. Since splitting is expensive, the server makes some attempt to avoid it. Currently, the server's algorithm for this is to compute the length of the current row, and to stop trying to pack more rows into this buffer if another row of that length would not fit. However, since rows can vary substantially in length, this algorithm is often ineffective at preventing splits. For example, if a short row near the end of the buffer is then followed by a long row, that long row will be split. It is possible to improve this algorithm substantially: - instead of just using the length of the previous row as a guide for whether to attempt packing another row in, use some sort of overall average row size computed over multiple rows (e.g., all the rows we've placed into this QRYDTA block, or all the rows we've process for this result set) - when we discover that the next row will not fit, rather than splitting the row across QRYDTA blocks, if it is relatively small, we could just hold the entire row in a buffer to place it entirely into the next QRYDTA block, or reset the result set cursor back one row to "unread" this row. - when splitting a row across QRYDTA blocks, we tend to copy data around multiple times. Careful coding could remove some of these copies. However, it is important not to over-complicate this code: it is better to be correct than to be efficient, and there have been several bugs in the split logic already. Instance Variables Byte string Write a QRYNOPRM - Query Not Opened Instance Variables SVRCOD - Severity Code - required -  4 Warning 8 ERROR RDBNAM - Relational Database Name - required PKGNAMCSN - RDB Package Name, Consistency Token, and Section Number - required Write a QRYPOPRM - Query Previously opened Instance Variables SVRCOD - Severity Code - required - 8 ERROR RDBNAM - Relational Database Name - required PKGNAMCSN - RDB Package Name, Consistency Token, and Section Number - required Write RDBNAM Write RDBUPDRM Instance variables SVRCOD - Severity code - Information only - required RDBNAM - Relational database name -required SRVDGN - Server Diagnostic Information -optional Write RDB Failure Instance Variables SVRCOD - Severity Code - required RDBNAM - Relational Database name - required SRVDGN - Server Diagnostics - optional (not sent for now) Write RSLSETRM Instance variables SVRCOD - Severity code - Information only - required PKGSNLST - list of PKGNAMCSN -required SRVDGN - Server Diagnostic Information -optional Write security check reply Instance variables SVRCOD - serverity code - required SECCHKCD    - security check code  - required SECTKN - security token - optional, ignorable SVCERRNO    - security service error number SRVDGN  - Server Diagnostic Information Write the ERR and WARN part of the SQLCA Same as writeSQLCAGRP, but optimized for the case when there is no real exception, i.e. the exception is null, or "End of data" SQLCAGRP : FDOCA EARLY GROUP SQL Communcations Area Group Description FORMAT FOR SQLAM &lt;= 6 SQLCODE; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; DRDA TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; DRDA TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; DRDA TYPE N-GDA; ENVLID 0x52; Length Override 0 FORMAT FOR SQLAM &gt;= 7 SQLCODE; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; DRDA TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; DRDA TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; DRDA TYPE N-GDA; ENVLID 0x52; Length Override 0 SQLDIAGGRP; DRDA TYPE N-GDA; ENVLID 0x56; Length Override 0 Write SQLCAGRP SQLCAGRP : FDOCA EARLY GROUP SQL Communcations Area Group Description FORMAT FOR SQLAM &lt;= 6 SQLCODE; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; DRDA TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; DRDA TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; DRDA TYPE N-GDA; ENVLID 0x52; Length Override 0 FORMAT FOR SQLAM &gt;= 7 SQLCODE; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; DRDA TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; DRDA TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; DRDA TYPE N-GDA; ENVLID 0x52; Length Override 0 SQLDIAGGRP; DRDA TYPE N-GDA; ENVLID 0x56; Length Override 0 Write SQLCAXGRP SQLCAXGRP : EARLY FDOCA GROUP SQL Communications Area Exceptions Group Description FORMAT FOR SQLAM &lt;= 6 SQLRDBNME; DRDA TYPE FCS; ENVLID 0x30; Length Override 18 SQLERRD1; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD2; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD3; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD4; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD5; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD6; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLWARN0; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN1; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN2; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN3; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN4; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN5; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN6; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN7; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN8; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN9; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARNA; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLERRMSG_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 70 SQLERRMSG_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 70 FORMAT FOR SQLAM &gt;= 7 SQLERRD1; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD2; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD3; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD4; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD5; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD6; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLWARN0; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN1; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN2; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN3; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN4; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN5; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN6; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN7; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN8; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN9; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARNA; DRDA TYPE FCS; ENVLID 0x30; Length Override 1 SQLRDBNAME; DRDA TYPE VCS; ENVLID 0x32; Length Override 1024 SQLERRMSG_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 70 SQLERRMSG_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 70 Write SQLCINRD - result set column information Write SQLDAGRP SQLDAGRP : EARLY FDOCA GROUP SQL Data Area Group Description FORMAT FOR SQLAM &lt;= 6 SQLPRECISION; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLSCALE; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLLENGTH; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLTYPE; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLCCSID; DRDA TYPE FB; ENVLID 0x26; Length Override 2 SQLNAME_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 30 SQLNAME_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 30 SQLLABEL_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 30 SQLLABEL_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 30 SQLCOMMENTS_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 254 SQLCOMMENTS_m; DRDA TYPE VCS; ENVLID 0x32; Length Override 254 FORMAT FOR SQLAM == 6 SQLPRECISION; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLSCALE; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLLENGTH; DRDA TYPE I8; ENVLID 0x16; Length Override 8 SQLTYPE; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLCCSID; DRDA TYPE FB; ENVLID 0x26; Length Override 2 SQLNAME_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 30 SQLNAME_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 30 SQLLABEL_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 30 SQLLABEL_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 30 SQLCOMMENTS_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 254 SQLCOMMENTS_m; DRDA TYPE VCS; ENVLID 0x32; Length Override 254 SQLUDTGRP; DRDA TYPE N-GDA; ENVLID 0x51; Length Override 0 FORMAT FOR SQLAM &gt;= 7 SQLPRECISION; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLSCALE; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLLENGTH; DRDA TYPE I8; ENVLID 0x16; Length Override 8 SQLTYPE; DRDA TYPE I2; ENVLID 0x04; Length Override 2 SQLCCSID; DRDA TYPE FB; ENVLID 0x26; Length Override 2 SQLDOPTGRP; DRDA TYPE N-GDA; ENVLID 0xD2; Length Override 0 Write SQLDARD SQLDARD : FDOCA EARLY ARRAY SQL Descriptor Area Row Description with SQL Communications Area FORMAT FOR SQLAM &lt;= 6 SQLCARD; ROW LID 0x64; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDAROW; ROW LID 0x60; ELEMENT TAKEN 0(all); REP FACTOR 0(all) FORMAT FOR SQLAM &gt;= 7 SQLCARD; ROW LID 0x64; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDHROW; ROW LID 0xE0; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 writeSQLDCGRP: SQL Diagnostics Condition Group Description SQLDCCODE; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCSTATE; DRDA TYPE FCS; ENVLID Ox30; Lengeh Override 5 SQLDCREASON; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCLINEN; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCROWN; DRDA TYPE FD; ENVLID 0x0E; Lengeh Override 31 SQLDCER01; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER02; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER03; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER04; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCPART; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCPPOP; DRDA TYPE I4; ENVLID 0x02; Length Override 4 SQLDCMSGID; DRDA TYPE FCS; ENVLID 0x30; Length Override 10 SQLDCMDE; DRDA TYPE FCS; ENVLID 0x30; Length Override 8 SQLDCPMOD; DRDA TYPE FCS; ENVLID 0x30; Length Override 5 SQLDCRDB; DRDA TYPE VCS; ENVLID 0x32; Length Override 255 SQLDCTOKS; DRDA TYPE N-RLO; ENVLID 0xF7; Length Override 0 SQLDCMSG_m; DRDA TYPE NVMC; ENVLID 0x3F; Length Override 32672 SQLDCMSG_S; DRDA TYPE NVCS; ENVLID 0x33; Length Override 32672 SQLDCCOLN_m; DRDA TYPE NVCM ; ENVLID 0x3F; Length Override 255 SQLDCCOLN_s; DRDA TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCCURN_m; DRDA TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCCURN_s; DRDA TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCPNAM_m; DRDA TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCPNAM_s; DRDA TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXGRP; DRDA TYPE N-GDA; ENVLID 0xD3; Length Override 1 writeSQLDCROW: SQL Diagnostics Condition Row - Identity 0xE5 SQLDCGRP; GROUP LID 0xD5; ELEMENT TAKEN 0(all); REP FACTOR 1 Holdability passed in as it can represent the holdability of the statement or a specific result set. writeSQLDIAGCI: SQL Diagnostics Condition Information Array - Identity 0xF5 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDCIROW; ROW LID 0xE5; ELEMENT TAKEN 0(all); REP FACTOR 0(all) writeSQLDIAGCN: Write NULLDATA for now Write SQLDIAGGRP: SQL Diagnostics Group Description - Identity 0xD1 Nullable Group SQLDIAGSTT; DRDA TYPE N-GDA; ENVLID 0xD3; Length Override 0 SQLDIAGCN;  DRFA TYPE N-RLO; ENVLID 0xF6; Length Override 0 SQLDIAGCI;  DRDA TYPE N-RLO; ENVLID 0xF5; Length Override 0 writeSQLDIAGSTT: Write NULLDATA for now Write SQLDTAGRP SQLDAGRP : Late FDOCA GROUP SQL Data Value Group Descriptor LENGTH - length of the SQLDTAGRP TRIPLET_TYPE - NGDA for first, CPT for following ID - SQLDTAGRP_LID for first, NULL_LID for following For each column DRDA TYPE LENGTH OVERRIDE For numeric/decimal types PRECISON SCALE otherwise LENGTH or DISPLAY_WIDTH Write SQLERRRM Instance Variables SVRCOD - Severity Code - required writeSQLNUMGRP: Writes SQLNUMGRP : FDOCA EARLY GROUP SQL Number of Elements Group Description FORMAT FOR ALL SQLAM LEVELS SQLNUM; DRDA TYPE I2; ENVLID 0x04; Length Override 2 writeSQLNUMROW: Writes SQLNUMROW : FDOCA EARLY ROW SQL Number of Elements Row Description FORMAT FOR SQLAM LEVELS SQLNUMGRP; GROUP LID 0x58; ELEMENT TAKEN 0(all); REP FACTOR 1 Write SQLRSLRD - result set reply data Write SQLUDTGRP (SQL Descriptor User-Defined Type Group Descriptor) This is the format from the DRDA spec, Volume 1, section 5.6.4.10. However, this format is not rich enough to carry the information needed by JDBC. This format does not have a subtype code for JAVA_OBJECT and this format does not convey the Java class name needed by ResultSetMetaData.getColumnClassName(). SQLUDXTYPE; DRDA TYPE I4; ENVLID 0x02; Length Override 4 Constants which map to java.sql.Types constants DISTINCT, STRUCT, and REF. But DRDA does not define a constant which maps to java.sql.Types.JAVA_OBJECT. SQLUDTRDB; DRDA TYPE VCS; ENVLID 0x32; Length Override 255 Database name. SQLUDTSCHEMA_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 255 SQLUDTSCHEMA_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 255 Schema name. One of the above. SQLUDTNAME_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 255 SQLUDTNAME_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 255 Unqualified UDT name. One of the above. Instead, we use the following format and only for communication between Derby servers and Derby clients which are both at version 10.6 or higher. For all other client/server combinations, we send null. SQLUDTNAME_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override 255 SQLUDTNAME_s; DRDA TYPE VCS; ENVLID 0x32; Length Override 255 Fully qualified UDT name. One of the above. SQLUDTCLASSNAME_m; DRDA TYPE VCM; ENVLID 0x3E; Length Override FdocaConstants.LONGVARCHAR_MAX_LEN SQLUDTCLASSNAME_s; DRDA TYPE VCS; ENVLID 0x32; Length Override FdocaConstants.LONGVARCHAR_MAX_LEN Name of the Java class bound to the UDT. One of the above. Write variable character mixed byte or single byte The preference is to write mixed byte if it is defined for the server, since that is our default and we don't allow it to be changed, we always write mixed byte.

Override getMessage() write will write the Error information to the buffer. Most errors will write only the codepoint and svrcod Where appropriate the codepoint specific error code and codePoint of origin will be written

Explicitly close the result set by CLSQRY needed to check for double close. Add extDtaObject Clear externalized lob objects in current result set This method closes the JDBC objects and frees up all references held by this object. Get the extData Objects  Get the cursor name for the ResultSet get  resultset/out parameter DRDAType  get  resultset  DRDALen get resultset /out parameter precision get resultset /out parameter scale ** Check to see if the result set for this statement has at least one column that is BLOB/CLOB. is ResultSet closed Is lob object nullable This method resets the state of this DRDAResultset object so that it can be re-used. This method should reset all variables of this class. Sets the OPNQRYOptions. For more information on the meaning of these values consult the DRDA Technical Standard document. set consistency token for this resultSet Set result set and initialize type array. set resultset/out parameter DRDAType set resultset/out parameter precision set resultset/out parameter scale Set state to SUSPENDED (result set is opened) @return whether CLSQRY has been called on the current result set.
end of boot Find the methods to start and shutdown the server. Perfomed through reflection so that the engine code is not dependent on the network server code. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Try to start the DRDA server. Log an error in error log and continue if it cannot be started. public static void start() { Sets configuration information for the network server to be started. end of stop
Explicitly close the result set by CLSQRY needed to check for double close. Add another parameter to this statement. Add extDtaObject Mark the pos'th parameter as external Add a new resultSet to this statement. Set as the current result set if  there is not an existing current resultset. Add a warning about data having been truncated. For a single result set, just echo the consistency token that the client sent us. For subsequent resultSets, just subtract the resultset number from the consistency token and that will differentiate the result sets. This seems to be what DB2 does Clears the parameter state (type, length and ext information) stored in this statement, but does not release any storage. This reduces the cost of re-executing the statement since no new storage needs to be allocated. Clear externalized lob objects in current result set Clear the chain of truncation warnings for this statement. This method closes the JDBC objects and frees up all references held by this object. Executes the prepared statement and populates the resultSetTable. Access to the various resultSets is then possible by using setCurrentDrdaResultSet(String pkgnamcsn)  to set the current resultSet and then calling getResultSet() or the other access methods to get resultset data. clear out type data for parameters. Unfortunately we currently overload the resultSet type info rsDRDATypes et al with parameter info. RESOLVE: Need to separate this Gets the current DRDA ResultSet Get the number of parameters, internal and external, that has been added to this statement. get DRDAResultSet by result set number get DRDAResultSet by consistency token Get the extData Objects Get the parameter position of the i'th external parameter Get the number of external parameters in this statement. External means parameters that are transmitted in a separate DSS in the DRDA protocol. get more results using reflection.  get the number of result set columns for the current resultSet get precision  for output parameter. get scale for output parameter. get type for output parameter. Given an object class  name get the paramameter type if the parameter mode is unknown. Arrays except for byte arrrays are assumed to be output parameters TINYINT output parameters are going to be broken because there is no way to differentiate them from binary input parameters. get parameter DRDAType returns drda length of parameter as sent by client. get parameter precision or DB2 max (31) get parameter scale or DB2 max (31) Retrieve the ParameterMetaData for the prepared statement. Get pkgnamcsn Get prepared statement Get result set   get  resultset/out parameter DRDAType  get resultset/out parameter DRDALen get resultset /out parameter precision get resultset /out parameter scale Use reflection to retrieve SQL Text for EmbedPreparedStatement or BrokeredPreparedStatement. Get the statement get the isolation level for a static package. Get the chain of truncation warnings added to this statement. This method is used to initialize the default statement of the database for re-use. It is different from reset() method since default statements get initialized differently. e.g: stmt variable used in default statement is created only once in Database.makeConnection. The default statement will be initialized to have the same byte order etc as the server. This may be changed when a TYPEDEFNAM is received from the client in DRDAConnThread.setStmtOrDbByteOrder() Is lob object nullable @param index - offset starting with 0 @return true if object is nullable is  parameter an ouput parameter Method to decide whether the ResultSet should be closed implicitly based on the QRYCLSIMP value sent from the client. Only forward-only result sets should be implicitly closed. Some clients do not expect result sets to be closed implicitly if the protocol is LMTBLKPRC. is this a scrollable cursor? return true if this is not a forward only cursor Create a prepared statement This method resets the state of this DRDAStatement object so that it can be re-used. This method should reset all variables of this class except the following: 1. database - This variable gets initialized in the constructor and by call to setDatabase. 2. members which get initialized in setPkgnamcsn (pkgnamcsn, pkgcnstkn, pkgid, pkgsn, isolationLevel, cursorName). pkgnamcsn is the key used to find if the DRDAStatement can be re-used. Hence its value will not change when the object is re-used. Close the current resultSet is Statement closed Set state to SUSPENDED (result set is opened) Set currentDrdaResultSet Set currentDrdaResultSet Set database Set query options sent on OPNQRY and pass options down to the current <code>DRDAResultSet</code> object. Set the pkgid sec num for this statement and the consistency token that will be used for the first resultSet. For dyamic packages The package name is encoded as follows SYS(S/L)(H/N)xyy where 'S' represents Small package and 'L' large (ignored by Derby) Where 'H' represents WITH HOLD, and 'N' represents NO WITH HOLD. (May be overridden by SQLATTR for WITH HOLD") Where 'www' is the package iteration (ignored by Derby) Where 'x' is the isolation level: 0=NC, 1=UR, 2=CS, 3=RS, 4=RR Where 'yy' is the package iteration 00 through FF Where 'zz' is unique for each platform Happilly, these values correspond precisely to the internal Derby isolation levels  in ExecutionContext.java x   Isolation Level --  --------------------- 0   NC  (java.sql.Connection.TRANSACTION_NONE) 1   UR  (java.sql.Connection.TRANACTION_READ_UNCOMMITTED) 2   CS  (java.sql.Connection.TRANSACTION_READ_COMMITTED) 3   RS  (java.sql.Connection.TRANSACTION_REPEATABLE_READ) 4   RR  (java.sql.Connection.TRANSACTION_SERIALIZABLE) static packages have preset isolation levels (see getStaticPackageIsolation) Set query options sent on CNTQRY set result  DRDAType Set resultSet defaults to match the statement defaults sent on EXCSQLSTT This might be overridden on OPNQRY or CNTQRY set resultset/out parameter precision set resultset/out parameter scale Set statement set TypDef values @return whether CLSQRY has been called on the current result set.
Check whether the internal buffer contains the same data as another byte buffer. Return the internal byte array. The returned array should not be modified, as it is used internally in <code>DRDAString</code>. The value of the array might be modified by subsequent calls to <code>DRDAString.setBytes()</code>. Return the length in bytes of the internal string representation. Modify the internal byte buffer. If the new data is equal to the old data, the cached values are not cleared. Convert the internal byte array to a string. The string value is cached. Check whether the contents of the <code>DRDAString</code> were modified in the previous call to <code>setBytes()</code>.
Commit local transaction. Send SYNCCRD response. Commit  the xa transaction. Send SYNCCRD response Commit  the xa transaction. Send SYNCCRD response. End  the xa transaction. Send SYNCRRD response Forget the xa transaction. Send SYNCCRD response.  get XAResource for the connection Parse SYNCCTL - Parse SYNCCTL command for XAMGR lvl 7 parse SYNCTYPE for XAMGR lvl 7 return synctype value CodePoint.SYNCTYPE_NEW_UOW -&gt; XAResource.start() CodePoint.SYNCTYPE_END_UOW -&gt; XAResource.end() CodePoint.SYNCTYPE_PREPARE -&gt; XAResource.prepare() CodePoint.SYNCTYPE_MIGRATE -&gt; not supported  //SYNCPT MGR LEVEL 5 CodePoint.SYNCTYPE_REQ_COMMIT -&gt; not supported //SYNCPT MGR LEVEL 5 CodePoint.SYNCTYPE_COMMITTED -&gt; XAResource.commit() or local commit for null XID CodePoint.SYNCTYPE_REQ_LOG -&gt;  not supported CodePoint.SYNCTYPE_REQ_FORGET -&gt; XAResource.forget() CodePoint.SYNCTYPE_ROLLBACK -&gt; XAResource.rollback() CodePoint.SYNCTYPE_MIGRATED -&gt; not supported CodePoint.SYNCTYPE_INDOUBT   -&gt; XAResource.recover(); parse XAFlags Parses a XA transaction timout value. Parse XID formatId -1 translates into a null XID and a local transaction Prepare the xa transaction. Send SYNCCRD response. return xa exception errorCode. print to console for debug output. JCC doesn't send xaflags but always wants TMSTARTRSCAN. So default to that if we got no xaflags Call recover. Send SYNCCRD response with indoubt list This function rollbacks the current global transaction associated with the XAResource or a local transaction. The function should be called only in exceptional cases - like client socket is closed. Rollback a local transaction. Optionally send SYNCCRD response. Rollback transaction. Optionally send SYNCCRD response. Rollback the xa transaction. Optionally send SYNCCRD response. Start the xa transaction. Send SYNCRRD response printable syncType for debug output write PRPHRCLST (indoubt list) Write SYNCCRD (SYNCCTL response) write XID printable xaflags
convert byte array to a Hex string Obtain the transaction branch qualifier part of the Xid in a byte array. <p> Obtain the format id part of the Xid. <p> Obtain the global transaction identifier part of XID as an array of bytes. <p>


Default implementation of diagnostic on the object. <p> This routine returns a string with whatever diagnostic information you would like to provide about this object. <p> This routine returns a summary table of information about pages in each level of the btree.  It tells the height of the tree, the average free and reserved bytes per level, and the page size. <p> Private/Protected methods of This class: * Methods of Diagnosticable
Return string identifying the underlying container. <p> Return a set of properties describing the the key used to lock container. <p> Used by debugging code to print the lock table on demand.
Return string identifying the underlying container. <p> Return a set of properties describing the the key used to lock container. <p> Used by debugging code to print the lock table on demand.
Return string describing id of container. <p>
Given a Database name and conglomid, return diagnositic string. <p> Return a string with diagnostic information about a particular conglomerate, can be called for any type of conglomerate (some types may not return any info though). <p> Can be called from ij to find out info about conglomid 19 in database 'msgdb' by using the following syntax: values com.ibm.db2j.protocol.BasicServices.Diagnostic.T_Diagnosticable:: diag_conglomid('msgdb', 19); maximumdisplaywidth 9000; CREATE FUNCTION DIAG_CONGLOMID(DBNAME VARCHAR(128), CONGLOMID INT) RETURNS VARCHAR(32000) RETURNS NULL ON NULL INPUT EXTERNAL NAME 'org.apache.derby.impl.store.raw.data.D_DiagnosticUtil.diag_conglomid' LANGUAGE JAVA PARAMETER STYLE JAVA; values DIAG_CONGLOMID('msgdb', 19); com.ibm.db2j.protocol.BasicServices.Diagnostic.T_Diagnosticable:: diag_conglomid_print('msgdb', 19); RESOLVE - An interface that takes a table name would be nice. Public Methods of This class: Given a Database name and conglomid print out diagnostic info. <p> Print diagnostic information about a particular conglomerate, can be called for either a btree or heap conglomerate.  This routine prints out the string to "System.out"; "ij", depending on it's configuration, will only print out a fixed length (default 128 bytes), so having ij print the string can be a problem. <p> Can be called from ij to find out info about conglomid 19 in database 'msgdb' by using the following syntax: maximumdisplaywidth 9000; CREATE FUNCTION D_CONGLOMID_PRINT(DBNAME VARCHAR(128), CONGLOMID INT) RETURNS VARCHAR(32000) RETURNS NULL ON NULL INPUT EXTERNAL NAME 'org.apache.derby.impl.store.raw.data.D_DiagnosticUtil.diag_conglomid_print' LANGUAGE JAVA PARAMETER STYLE JAVA; values D_CONGLOMID_PRINT('msgdb', 19); com.ibm.db2j.protocol.BasicServices.Diagnostic.T_Diagnosticable:: diag_conglomid_print('msgdb', 19); RESOLVE - An interface that takes a table name would be nice. Given a Database name and containerid, return conglomerate id. <p> Return the conglomerate id of a given conainer id. <p> Can be called from ij to find out info about conglomid 19 in database 'msgdb' by using the following syntax: values com.ibm.db2j.protocol.BasicServices.Diagnostic.T_Diagnosticable). diag_conglomid_to_containerid('msgdb', 19); RESOLVE - An interface that takes a table name would be nice. Given a Database name and conglomid, return container id. <p> Return the containerid of a given conglomerate id. <p> Can be called from ij to find out info about conglomid 19 in database 'msgdb' by using the following syntax: values com.ibm.db2j.protocol.BasicServices.Diagnostic.T_Diagnosticable). diag_containerid_to_conglomid('msgdb', 924300359390); RESOLVE - An interface that takes a table name would be nice. Dump raw contents of a page. <p> A utility routine that can be called from an ij session that will dump the raw contents of a page, in the raw store dump format. Privileged service lookup. Must be private so that user code can't call this entry point. Privileged startup. Must be private so that user code can't call this entry point. Private/Protected methods of This class: Given a database name come up with a module. <p> Privileged module lookup. Must be private so that user code can't call this entry point.

Default implementation of diagnostic on the object. <p> This routine returns a string with whatever diagnostic information you would like to provide about this object. <p> This routine should be overriden by a real implementation of the diagnostic information you would like to provide. <p> Private/Protected methods of This class: * Methods of Diagnosticable
* Static routines that were in SinglePool * Debugging routines  Private/Protected methods of This class: * Methods of Diagnosticable

Return string identifying the underlying container. <p> Return a set of properties describing the the key used to lock container. <p> Used by debugging code to print the lock table on demand.
Return the string for the qualifier.
Checks the slot table. <p> 1) checks the number of slot entries matches the record count 2) checks the slot table lengths match the field lengths Provide a string dump of the StoredPage. <p> RESOLVE - once the "Diagnostic" interface is accepted move the string dumping code into this routine from it's current place in the StoredPage code. <p> Provide detailed diagnostic information about a StoredPage. <p> Currently supports 3 types of information: Page.DIAG_PAGE_SIZE      - page size. Page.DIAG_BTYES_FREE     - # of free bytes on the page. Page.DIAG_BYTES_RESERVED - # of reserved bytes on the page. <p> Private/Protected methods of This class: * Methods of Diagnosticable
Default implementation of diagnostic on the object. <p> This routine returns a string with whatever diagnostic information you would like to provide about this object. <p> This routine should be overriden by a real implementation of the diagnostic information you would like to provide. <p>
Create a new DaemonService with the default daemon timer delay.
Clear all the queued up work from this daemon.  Subscriptions are not affected. Request a one time service from the Daemon.  Unless performWork returns REQUEUE (see Serviceable), the daemon will service this client once and then it will get rid of this client.  Since no client number is associated with this client, it cannot request to be serviced or be unsubscribed. The work is always added to the deamon, regardless of the state it returns. Pause.  No new service is performed until a resume is issued. Resume service after a pause Service this subscription ASAP. When this method is called, the subscriber's <code>performWork()</code> method is guaranteed to be invoked at some point in the future. However, there is no guarantee that a subscriber's <code>performWork()</code> is called the same number of times as the subscriber calls this method. More precisely, if a subscriber is waiting for this daemon service to invoke its <code>performWork()</code> method, the daemon service may, but is not required to, ignore requests from that subscriber until the <code>performWork()</code> method has been invoked. End this daemon service Add a new client that this daemon needs to service Get rid of a client from the daemon. If a client is being serviced when the call is made, the implementation may choose whether or not the call should block until the client has completed its work. If the call does not block, the client must be prepared to handle calls to its <code>performWork()</code> method even after <code>unsubscribe()</code> has returned. Wait until work in the high priorty queue is done.
Function to provided an updated C_DATA column for a customer account.
Privileged Monitor lookup. Must be package private so that user code can't call this entry point. get a UUIDFactory. This uses the Monitor to get one the first time and holds onto it for later.  Manufacture a new ColPermsDescriptor. Create a conglomerate descriptor for the given conglomerate id. Create  a new {@code FileInfoDescriptor} using the supplied arguments. id unique id to be used for the new file descriptor sd schema of the new file to be stored in the database SQLName the SQL name of the new schema object representing the file generationID version numberof the file the descriptor describes    Create a new role grant descriptor Create a new routine permissions descriptor Create a descriptor for the named schema with a null UUID. Create a new sequence descriptor Create a descriptor for the temporary table within the given schema. Create a descriptor for the named table within the given schema. If the schema parameter is NULL, it creates a schema descriptor using the current default schema. Create a new trigger descriptor.  Create a viewDescriptor for the view with the given UUID.
Reports whether an individual constraint must be enforced. For the Core product, this routine always returns true. However, during REFRESH we may have deferred some constraints until statement end. This method returns false if the constraint deferred Adds the given ConstraintDescriptor to the data dictionary, associated with the given table and constraint type. Adds a descriptor to a system catalog identified by the catalogNumber. array version of addDescriptor. Add or remove a permission to the permission database. Adds the given SPSDescriptor to the data dictionary, associated with the given table and constraint type. Check to see if a database has been upgraded to the required level in order to use a langauge feature that is. <P> This is used to ensure new functionality that would lead on disk information not understood by a previous release is not executed while in soft upgrade mode. Ideally this is called at compile time and the parser has a utility method to enable easy use at parse time. <P> To use this method, a feature implemented in a certain release (DataDictionary version) would call it with the constant matching the release. E.g. for a new feature added in 10.1, a call such as <PRE> // check and throw an exception if the database is not at 10.1 dd.checkVersion(DataDictionary.DD_VERSION_DERBY_10_1, "NEW FEATURE NAME"); </PRE> This call would occur during the compile time, usually indirectly through the parser utility method, but direct calls can be made during QueryNode initialization, or even at bind time. <BR> It is not expected that this method would be called at execution time. Clear all of the DataDictionary caches. Clear the DataDictionary caches, including the sequence caches if requested.. Clear all of the sequence number generators. returns an array of RowLocations corresponding to the autoincrement columns in the table. The RowLocation points to the row in SYSCOLUMNS for this particular ai column. The array has as many elements as there are columns in the table. If a column is not an ai column, the entry is NULL. Computes the RowLocation in SYSSEQUENCES for a particular sequence. Also constructs the sequence descriptor. Creates an index statistics refresher for this data dictionary. <p> The index statistics refresher is used to create and refresh index cardinality statistics, either automatically or on user demand (i.e. by invoking SYSCS_UTIL.SYSCS_UPDATE_STATISTICS). This method creates a new iterator over the closure of role grants starting or ending with a given role. This method will cause reading of dictionary, so should be called inside a transaction, after a {@code dd.startReading()} or {@code dd.startWriting()} call. Disables automatic refresh/creation of index statistics at runtime. <p> If the daemon is disabled, it can only be enabled again by rebooting the database. Note that this method concerns diabling the daemon at runtime, and only the automatic updates of statistics. If wanted, the user would disable the daemon at boot-time by setting a property (system-wide or database property). <p> <em>Usage note:</em> This method was added to allow the index refresher itself to notify the data dictionary that it should be disabled. This only happens if the refresher/daemon experiences severe errors, or a large amount of errors. It would then disable itself to avoid eating up system resources and potentially cause side-effects due to the errors. Tells if an index statistics refresher should be created for this database. <p> The only reason not to create an index statistics refresher is if one already exists. Inform this DataDictionary that we have finished reading it.  This typically happens at the end of compilation. Drop an AliasDescriptor from the DataDictionary Drops all column descriptors from the given table.  Useful for DROP TABLE. Drops all conglomerates associated with a table. Drops all ConstraintDescriptors from the data dictionary that are associated with the given table. NOTE: Caller is responsible for dropping any backing index Drops all permission descriptors for the given object Drop all permission descriptors corresponding to a grant to the named authentication identifier Drops all routine permission descriptors for the given routine. Drops all table and column permission descriptors for the given table. Given a column name and a table ID, drops the column descriptor from the table. Drops a conglomerate descriptor Drops the given ConstraintDescriptor from the data dictionary. NOTE: Caller is responsible for dropping any backing index Remove all of the stored dependencies for a given dependent's ID from the data dictionary. Remove all of the stored dependencies for a given dependent's ID from the data dictionary. Drop a FileDescriptor from the datadictionary. Drop a role grant Drop all role grants corresponding to a grant of (any) role to a named authentication identifier Drop all role grants corresponding to a grant of the named role to any authentication identifier Drops the given SPSDescriptor. Drops the given SPSDescriptor. Drop the descriptor for a schema, given the schema's name Drop a sequence descriptor. Drops all statistics descriptors for a given table/index column combination. If the index is not specified, then all statistics for the table are dropped. Drop a dependency from the data dictionary. Drop the table descriptor. Drops the given TriggerDescriptor that is associated with the given table and constraint type from the data dictionary. Drop a User from the DataDictionary Drops the view descriptor from the data dictionary. Check all dictionary tables and return true if there is any GRANT descriptor containing <code>authId</code> as its grantee. Return true of there exists a schema whose authorizationId equals authid, i.e.  SYSSCHEMAS contains a row whose column AUTHORIZATIONID equals authid. Flush the updated values of the BulkInsertCounter to disk and to the original, cached SequenceUpdater. This is used for the bulk-insert optimization in InsertResultSet. Convert a constraint descriptor list into a list of active constraints, that is, constraints which must be enforced. For the Core product, these are just the constraints on the original list. However, during REFRESH we may have deferred some constraints until statement end. This method returns the corresponding list of constraints which AREN'T deferred. Get a AliasDescriptor by alias name and name space. NOTE: caller responsible for handling no match. Get an AliasDescriptor given its UUID. Get the alias descriptor for an ANSI UDT. Build and return an List with DependencyDescriptors for all of the stored dependencies. This is useful for consistency checking. Get every statement in this database. Return the SPSDescriptors in an list. Get authorizationID of Database Owner Return the Java class to use for a builtin VTI to which the received table descriptor maps. Get the identity generator used to support the bulk-insert optimization in InsertResultSet. Returns the cache mode of the data dictionary. Return the collation type for SYSTEM schemas. In Derby 10.3, this will always be UCS_BASIC Return the collation type for user schemas. In Derby 10.3, this is either UCS_BASIC or TERRITORY_BASED. The exact value is decided by what has user asked for through JDBC url optional attribute COLLATION. If that atrribute is set to UCS_BASIC, the collation type for user schemas will be UCS_BASIC. If that attribute is set to TERRITORY_BASED, the collation type for user schemas will be TERRITORY_BASED. If the user has not provided COLLATION attribute value in the JDBC url at database create time, then collation type of user schemas will default to UCS_BASIC. Pre-10.3 databases after upgrade to Derby 10.3 will also use UCS_BASIC for collation type of user schemas. Get a {@code DependableFinder} instance for referenced columns in a table. Drop all table descriptors for a schema. public void dropAllTableDescriptors(SchemaDescriptor schema) throws StandardException; Get a ColumnDescriptor given its Default ID. Get one user's column privileges on a table using colPermsUUID Get one user's column privileges for a table. Get one user's column privileges for a table. This routine gets called during revoke privilege processing Gets a conglomerate descriptor for the named index in the given schema, getting an exclusive row lock on the matching row in sys.sysconglomerates (for DDL concurrency) if requested. Get a ConglomerateDescriptor given its conglomerate number.  If it is an index conglomerate shared by at least another duplicate index, this returns one of the ConglomerateDescriptors for those indexes. Get a ConglomerateDescriptor given its UUID.  If it is an index conglomerate shared by at least another duplicate index, this returns one of the ConglomerateDescriptors for those indexes. Get an array of conglomerate descriptors for the given conglomerate number.  If it is a heap conglomerate or an index conglomerate not shared by a duplicate index, the size of the return array is 1. Get an array of ConglomerateDescriptors given the UUID.  If it is a heap conglomerate or an index conglomerate not shared by a duplicate index, the size of the return array is 1. If the uuid argument is null, then this method retrieves descriptors for all of the conglomerates in the database. Get a ConstraintDescriptor given its name and schema ID. Get a ConstraintDescriptor given its UUID. Get the constraint descriptor given a table and the UUID String of the backing index. Get the constraint descriptor given a table and the UUID String of the constraint Get the constraint descriptor given a TableDescriptor and the constraint name. Load up the constraint descriptor list for this table descriptor (or all) and return it.  If the descriptor list is already loaded up, it is returned without further ado. Return a table descriptor corresponding to the TABLEID field in SYSCONSTRAINTS where CONSTRAINTID matches the constraintId passed in. Get the next number from an ANSI/ISO sequence generator which was created with the CREATE SEQUENCE statement. May raise an exception if the sequence was defined as NO CYCLE and the range of the sequence is exhausted. May allocate a range of sequence numbers and update the CURRENTVALUE column of the corresponding row in SYSSEQUENCES. This work is done in the execution transaction of the current session. Get a DataDescriptorGenerator, through which we can create objects to be stored in the DataDictionary. Get the DataValueFactory associated with this database. Get the descriptor for the declared global temporary table schema which is always named "SESSION". SQL92 allows a schema to specify a default character set - we will not support this. Get a {@code DependableFinder} instance. Returns the dependency manager for this DataDictionary. Associated with each DataDictionary object there is a DependencyManager object which keeps track of both persistent and stored dependencies. Gets a list of the dependency descriptors for the given dependent's id. Get the ExecutionFactory associated with this database. Get a FileInfoDescriptor given its id. Get a FileInfoDescriptor given its SQL name and schema name. Return a list of foreign keys constraints referencing this constraint.  Returns both enabled and disabled constraints. Get one user's privileges for an object using the permUUID Get permissions granted to one user for an object using the object's Id and the user's authorization Id. Returns the index statistics refresher. Gets a list of the dependency descriptors for the given provider's id. Get a role grant descriptor for a role definition. Get a descriptor for a role grant Get the role grant descriptor corresponding to the uuid provided Get the list of routines matching the schema and routine name. Get one user's privileges for a routine using routinePermsUUID Get one user's permissions for a routine (function or procedure). Returns a row location template for a table Get the stored prepared statement descriptor given a sps name. Get a SPSDescriptor given its UUID. Get all the parameter descriptors for an SPS. Look up the params in SYSCOLUMNS and turn them into parameter descriptors. Get the descriptor for the named schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this.  Will check default schema for a match before scanning a system table. Get the SchemaDescriptor for the given schema identifier. Get the SchemaDescriptor for the given schema identifier. get a descriptor for a Sequence by uuid get a descriptor for a Sequence by sequence name getSetAutoincrementValue fetches the autoincrement value from SYSCOLUMNS given a row location. If doUpdate is true it updates the autoincrement column with the new value. the value returned by this routine is the new value and *NOT* the value in the system catalogs. Gets all statistics Descriptors for a given table. Get a SubKeyConstraintDescriptor from syskeys or sysforeignkeys for the specified constraint id.  For primary foreign and and unique key constraints. Get the descriptor for the SYSIBM schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this. Returns a unique system generated name of the form SQLyymmddhhmmssxxn yy - year, mm - month, dd - day of month, hh - hour, mm - minute, ss - second, xx - the first 2 digits of millisec because we don't have enough space to keep the exact millisec value, n - number between 0-9 Get the descriptor for the system schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this. Get the descriptor for the named table within the given schema. If the schema parameter is NULL, it looks for the table in the current (default) schema. Table descriptors include object ids, object types (table, view, etc.) Get the descriptor for the table with the given UUID. NOTE: I'm assuming that the object store will define an UUID for persistent objects. I'm also assuming that UUIDs are unique across schemas, and that the object store will be able to do efficient lookups across schemas (i.e. that no schema descriptor parameter is needed). Get one user's privileges on a table using tablePermsUUID Get one user's privileges on a table using tableUUID and authorizationid This method does the job of transforming the trigger action plan text as shown below. DELETE FROM t WHERE c = old.c turns into DELETE FROM t WHERE c = org.apache.derby.iapi.db.Factory:: getTriggerExecutionContext().getOldRow(). getInt(columnNumberFor'C'inRuntimeResultset); In addition to that, for CREATE TRIGGER time, it does the job of collecting the column positions of columns referenced in trigger action plan through REFERENCEs clause. This information will get saved in SYSTRIGGERS table by the caller in CREATE TRIGGER case. It gets called either 1)at the trigger creation time for row level triggers or 2)if the trigger got invalidated by some other sql earlier and the current sql needs that trigger to fire. For such a trigger firing case, this method will get called only if it is row level trigger with REFERENCES clause. This work was done as part of DERBY-4874. Before DERBY-4874, once the stored prepared statement for trigger action plan was generated, it was never updated ever again. But, one case where the trigger action plan needs to be regenerated is say when the column length is changed for a column which is REFERENCEd as old or new column value. eg of such a case would be say the Alter table has changed the length of a varchar column from varchar(30) to varchar(64) but the stored prepared statement associated with the trigger action plan continued to use varchar(30). To fix varchar(30) in stored prepared statement for trigger action sql to varchar(64), we need to regenerate the trigger action sql. This new trigger action sql will then get updated into SYSSTATEMENTS table. If we are here for case 1) above, then we will collect all column references in trigger action through new/old transition variables. Information about them will be saved in SYSTRIGGERS table DERBY-1482 (if we are dealing with pre-10.7 db, then we will not put any information about trigger action columns in the system table to ensure backward compatibility). This information along with the trigger columns will decide what columns from the trigger table will be fetched into memory during trigger execution. If we are here for case 2) above, then all the information about column references in trigger action has already been collected during CREATE TRIGGER time and hence we can use that available information about column positions to do the transformation of OLD/NEW transient references. More information on case 1) above. DERBY-1482 One of the work done by this method for row level triggers is to find the columns which are referenced in the trigger action through the REFERENCES clause ie thro old/new transition variables. This information will be saved in SYSTRIGGERS so it can be retrieved during the trigger execution time. The purpose of this is to recognize what columns from the trigger table should be read in during trigger execution. Before these code changes, during trigger execution, Derby was opting to read all the columns from the trigger table even if they were not all referenced during the trigger execution. This caused Derby to run into OOM at times when it could really be avoided. We go through the trigger action text and collect the column positions of all the REFERENCEd columns through new/old transition variables. We keep that information in SYSTRIGGERS. At runtime, when the trigger is fired, we will look at this information along with trigger columns from the trigger definition and only fetch those columns into memory rather than all the columns from the trigger table. This is especially useful when the table has LOB columns and those columns are not referenced in the trigger action and are not recognized as trigger columns. For such cases, we can avoid reading large values of LOB columns into memory and thus avoiding possibly running into OOM errors. If there are no trigger columns defined on the trigger, we will read all the columns from the trigger table when the trigger fires because no specific columns were identified as trigger column by the user. The other case where we will opt to read all the columns are when trigger columns and REFERENCING clause is identified for the trigger but there is no trigger action column information in SYSTRIGGERS. This can happen for triggers created prior to 10.7 release and later that database got hard/soft-upgraded to 10.7 or higher release. Get the stored prepared statement descriptor given a sps name. Get a TriggerDescriptor given its UUID. Load up the trigger descriptor list for this table descriptor and return it.  If the descriptor list is already loaded up, it is returned without further ado. The descriptors are returned in the order in which the triggers were created, with the oldest first. Get the UUID Factory.  (No need to make the UUIDFactory a module.) Return the credentials descriptor for the named user. Return the Java class to use for the VTI to which the received table descriptor maps. There are two kinds of VTI mappings that we do: the first is for "table names", the second is for "table function names".  Table names can only be mapped to VTIs that do not accept any arguments; any VTI that has at least one constructor which accepts one or more arguments must be mapped from a table *function* name. An example of a VTI "table name" is the following: select * from SYSCS_DIAG.LOCK_TABLE In this case "SYSCS_DIAG.LOCK_TABLE" is the table name that we want to map.  Since the corresonding VTI does not accept any arguments, this VTI table name can be used anywhere a normal base table name can be used. An example of a VTI "table function name" is the following: select * from TABLE(SYSCS_DIAG.SPACE_TABLE(?)) x In this case "SYSCS_DIAG.SPACE_TABLE" is the table function name that we want to map.  Since the corresponding VTI can take either one or two arguments we have to use the TABLE constructor syntax to pass the argument(s) in as if we were making a function call.  Hence the term "table function". Gets the viewDescriptor for the view with the given UUID. Gets the viewDescriptor for the view given its TableDescriptor. Get all of the ConglomerateDescriptors in the database and hash them by conglomerate number. This is useful as a performance optimization for the locking VTIs. NOTE:  This method will scan SYS.SYSCONGLOMERATES at READ COMMITTED. It should really scan at READ UNCOMMITTED, but there is no such thing yet. Get all of the TableDescriptors in the database and hash them by TableId This is useful as a performance optimization for the locking VTIs. NOTE:  This method will scan SYS.SYSTABLES at READ COMMITTED. It should really scan at READ UNCOMMITTED, but there is no such thing yet. Invalidate all the stored plans in SYS.SYSSTATEMENTS. Invalidate all the stored plans in SYS.SYSSTATEMENTS for the given language connection context. Check if the database is read only and requires some form of upgrade that makes the stored prepared statements invalid. Indicate whether there is anything in the particular schema.  Checks for tables in the the schema, on the assumption that there cannot be any other objects in a schema w/o a table. Determine whether a string is the name of the system schema. Get the default password hasher for this database level. Returns null if the system is at rev level 10.5 or earlier. <p> Peek at the next value which will be returned by an identity generator. </p> <p> Peek at the next value which will be returned by a sequence generator. </p> sets a new value in SYSCOLUMNS for a particular autoincrement column. Inform this DataDictionary that we are about to start reading it.  This means using the various get methods in the DataDictionary. Generally, this is done during query compilation. Inform this DataDictionary that we are about to start writing to it. This means using the various add and drop methods in the DataDictionary. Generally, this is done during execution of DDL. Inform this DataDictionary that the transaction in which writes have been done (or may have been done) has been committed or rolled back. Update the conglomerateNumber for a ConglomerateDescriptor. This is useful, in 1.3, when doing a bulkInsert into an empty table where we insert into a new conglomerate. (This will go away in 1.4.) Update the conglomerateNumber for an array of ConglomerateDescriptors. In case of more than one ConglomerateDescriptor, they are for duplicate indexes sharing one conglomerate. This is useful, in 1.3, when doing a bulkInsert into an empty table where we insert into a new conglomerate. (This will go away in 1.4.) Update the constraint descriptor in question.  Updates every row in the base conglomerate. Set the current value of an ANSI/ISO sequence. This method does not perform any sanity checking but assumes that the caller knows what they are doing. If the old value on disk is not what we expect it to be, then we are in a race with another session. They won and we don't update the value on disk. However, if the old value is null, that is a signal to us that we should update the value on disk anyway. Update the lockGranularity for the specified table. Drop and recreate metadata stored prepared statements. Updates SYS.SYSSTATEMENTS with the info from the SPSD. Need to update SYSCOLPERMS for a given table because a new column has been added to that table. SYSCOLPERMS has a column called "COLUMNS" which is a bit map for all the columns in a given user table. Since ALTER TABLE .. ADD COLUMN .. has added one more column, we need to expand "COLUMNS" for that new column Currently, this code gets called during execution phase of ALTER TABLE .. ADD COLUMN .. Update SYSCOLPERMS to reflect the dropping of a column from a table. This method rewrites SYSCOLPERMS rows to update the COLUMNS bitmap to reflect the removal of a column from a table. Currently, this code gets called during execution phase of ALTER TABLE .. DROP COLUMN .. Update the trigger descriptor in question.  Updates every row in the base conglomerate. Update a user. Changes all columns in the corresponding SYSUSERS row except for the user name. Get authorization model in force, SqlStandard or legacy mode
Reports whether an individual constraint must be enforced. For the Core product, this routine always returns true. However, during REFRESH we may have deferred some constraints until statement end. This method returns false if the constraint deferred Adds the given ConstraintDescriptor to the data dictionary, associated with the given table and constraint type.  array version of addDescriptor. Add or remove a permission to/from the permission database. end of addPermissionsDescriptor Adds the given SPSDescriptor to the data dictionary, associated with the given table and constraint type. Add a column in SYS.SYSCOLUMNS for each parameter in the parameter list. Add the matching row to syskeys when adding a unique or primary key constraint Add a system schema to the database. <p> Add the required entries to the data dictionary for a System table. Add a table descriptor to the "other" cache. The other cache is determined by the type of the object c. Start-up method for this instance of the data dictionary. Privileged startup. Must be private so that user code can't call this entry point. ****************************************************************************** See RepBasicDataDictionary for sample code on how to create a system table. What follows here is special code for the core catalogs. These are catalogs which have to exist before any other system tables are created. Creating a core catalog consists of two steps: 1) creating all the infrastructure needed to make generic systemTableCreation work, 2) actually populating the Data Dictionary and core conglomerates with tuples. ****************************************************************************** Infrastructure work for indexes on catalogs. This is the data dictionary implementation for the standard database engine. * Check for illegal combinations here: insert & old or * delete and new Check to see if a database has been upgraded to the required level in order to use a language feature. Clear all of the DataDictionary caches. Clear the DataDictionary caches, including the sequence caches if requested.. Expected to be called only during boot time, so no synchronization. Mark all SPS plans in the data dictionary invalid. This does not invalidate cached plans. This function is for use by the boot-up code. Flush sequence caches to disk so that we don't leak unused, pre-allocated numbers.  Computes the RowLocation in SYSCOLUMNS for a particular autoincrement column. Computes the RowLocation in SYSSEQUENCES for a particular sequence. Also constructs the sequence descriptor. Create a conglomerate for a system table * Methods related to create Create all the required dictionary tables. Any classes that extend this class and need to create new tables should override this method, and then call this method as the first action in the new method, e.g. <PRE> protected Configuration createDictionaryTables(Configuration cfg, TransactionController tc, DataDescriptorGenerator ddg) throws StandardException { super.createDictionaryTables(params, tc, ddg); ... } </PRE> Create a sequence generator for an identity column on upgrade to 10.11. Create sequence generators for all identity columns on upgrade to 10.11. {@inheritDoc }  Create RoutinePermDescriptor to grant access to PUBLIC for this system routine using the grantor specified in authorizationID. Create a set of stored prepared statements from a properties file. Key is the statement name, value is the SQL statement. Generic create procedure routine. Takes the input procedure and inserts it into the appropriate catalog. Assumes all arguments are "IN" type. Generic create procedure routine. <p> Takes the input procedure and inserts it into the appropriate catalog. Assumes all arguments are "IN" type. * Create system built-in metadata stored prepared statements. <p> Create system procedures that are part of the SYSCS_UTIL schema, added in version 10.10. </p> <p> Create system procedures that are part of the SYSCS_UTIL schema, added in version 10.11. </p> <p> Create system procedures that are part of the SYSCS_UTIL schema, added in version 10.12. </p> <p> Create system procedures that are part of the SYSCS_UTIL schema, added in version 10.13. </p> Create system procedures added in version 10.1. <p> Create 10.1 system procedures, called by either code creating new database, or code doing hard upgrade from previous version. <p> Create system procedures added in version 10.2. <p> Create 10.2 system procedures, called by either code creating new database, or code doing hard upgrade from previous version. <p> Create the System procedures that are added in 10.3. Create system procedures that are part of the SYSCS_UTIL schema added in version 10.3. <p> Create 10.3 system procedures, called by either code creating new database, or code doing hard upgrade from previous version. <p> Create system procedures added in version 10.3. Create 10.3 system procedures related to the LOB Methods , called by either code creating new database, or code doing hard upgrade from previous version. Create the System procedures that are added to 10.5. Create the System procedures that are added to 10.6. <p> Create system procedures that are part of the SYSCS_UTIL schema added in version 10.9. These include the procedures for managing NATIVE credentials. See DERBY-866. </p> Create system procedures <p> Used to add the system procedures to the database when it is created.  System procedures are currently added to either SYSCS_UTIL or SQLJ schemas. <p> Create system procedures in SYSIBM <p> Used to add the system procedures to the database when it is created.  Full upgrade from version 5.1 or earlier also calls this method. <p> {@inheritDoc } {@inheritDoc } @see org.apache.derby.iapi.sql.dictionary.DataDictionary#doneReading Drop a AliasDescriptor from the DataDictionary Drops all column descriptors from the given table.  Useful for DROP TABLE. Drops all conglomerates associated with a table. Drops all ConstraintDescriptors from the data dictionary that are associated with the given table, Drops all permission descriptors for the object whose Id is given. Drop all permission descriptors corresponding to a grant to the named authentication identifier Drops all routine permission descriptors for the given routine. Drops all table and column permission descriptors for the given table. Given a column name and a table ID, drops the column descriptor from the table. Delete the appropriate rows from syscolumns when dropping 1 or more columns. Delete the appropriate rows from syscolperms when dropping a table Drops a conglomerate descriptor Drops the given ConstraintDescriptor from the data dictionary. Remove all of the stored dependencies for a given dependent's ID from the data dictionary.   Remove metadata stored prepared statements. Presently only used when dropping roles - user dropping is not under Derby control (well, built-in users are if properties are stored in database), any permissions granted to users remain in place even if the user is no more.    Drops the given SPSDescriptor. Drops the given SPSDescriptor. Drop the descriptor for a schema, given the schema's name Drops a sequence descriptor Drop a sequenceID from the ( schemaName, sequenceName ) map  Drop a single dependency from the data dictionary. Drop the matching row from syschecks when dropping a check constraint. Drop the matching row from syskeys when dropping a primary key or unique constraint. Drop the table descriptor. Delete the appropriate rows from systableperms when dropping a table Drops the given TriggerDescriptor.  WARNING: does not drop its SPSes!!! Drops the view descriptor from the data dictionary. Get the trigger action string associated with the trigger after the references to old/new transition tables/variables in trigger action sql provided by CREATE TRIGGER have been transformed eg DELETE FROM t WHERE c = old.c turns into DELETE FROM t WHERE c = org.apache.derby.iapi.db.Factory:: getTriggerExecutionContext().getOldRow(). getInt(columnNumberFor'C'inRuntimeResultset) or DELETE FROM t WHERE c in (SELECT c FROM OLD) turns into DELETE FROM t WHERE c in (SELECT c FROM new TriggerOldTransitionTable OLD) Check all dictionary tables and return true if there is any GRANT descriptor containing <code>authId</code> as its grantee. Return true if there exists a permission grant descriptor to this authorization id. Return true if there exists a role grant to authorization identifier. Return true of there exists a schema whose authorizationId equals authid, i.e.  SYS.SYSSCHEMAS contains a row whose column (AUTHORIZATIONID) equals authid. Finishes building a TabInfoImpl if it hasn't already been faulted in. NOP if TabInfoImpl has already been faulted in. Find the default message digest algorithm to use for BUILTIN authentication on this database. Privileged startup. Must be private so that user code can't call this entry point. Finish filling in the TableDescriptor. (Build the various lists that hang off the TD.) * Make sure the given column name is found in the trigger * target table.  Generate the appropriate SQL to get it. * * @return a string that is used to get the column using * getObject() on the desired result set and CAST it back * to the proper type in the SQL domain. * * @exception StandardException on invalid column name Generate an array of random bytes to use as salt when hashing credentials. Convert a constraint descriptor list into a list of active constraints, that is, constraints which must be enforced. For the Core product, these are just the constraints on the original list. However, during REFRESH we may have deferred some constraints until statement end. This method returns the corresponding list of constraints which AREN'T deferred. Get a AliasDescriptor by alias name and name space. NOTE: caller responsible for handling no match. Get a AliasDescriptor given its UUID. Get the alias descriptor for an ANSI UDT. Get every constraint in this database. Note that this list of ConstraintDescriptors is not going to be the same objects that are typically cached off of the table descriptors, so this will most likely instantiate some duplicate objects. Build and return an List with DependencyDescriptors for all of the stored dependencies. This is useful for consistency checking. Get every statement in this database. Return the SPSDescriptors in an list. The returned descriptors don't contain the compiled statement, so it it safe to call this method during upgrade when it isn't known if the saved statement can still be deserialized with the new version. Get authorizationID of Database Owner  Set up the builtin schema descriptors for system schemas.    Drop all table descriptors for a schema. public void dropAllTableDescriptors(SchemaDescriptor schema) throws StandardException { if (SanityManager.DEBUG) SanityManager.NOTREACHED(); } Get a ColumnDescriptor given its Default ID. Populate the ColumnDescriptorList for the specified TableDescriptor. MT synchronization: it is assumed that the caller has synchronized on the CDL in the given TD. Populate the ColumnDescriptorList for the specified TableDescriptor. MT synchronization: it is assumed that the caller has synchronized on the CDL in the given TD. @see org.apache.derby.iapi.sql.dictionary.DataDictionary#getColumnPermissions Get one user's column privileges for a table. end of getColumnPermissions Get one user's column privileges for a table. This routine gets called during revoke privilege processing Gets a conglomerate descriptor for the named index in the given schema, getting an exclusive row lock on the matching row in sys.sysconglomerates (for DDL concurrency) if requested. Get a ConglomerateDescriptor given its conglomerate number.  If it is an index conglomerate shared by at least another duplicate index, this returns one of the ConglomerateDescriptors for those indexes. Get a ConglomerateDescriptor given its UUID.  If it is an index conglomerate shared by at least another duplicate index, this returns one of the ConglomerateDescriptors for those indexes. Get an array of conglomerate descriptors for the given conglomerate number.  If it is a heap conglomerate or an index conglomerate not shared by a duplicate index, the size of the return array is 1. Get an array of ConglomerateDescriptors given the UUID.  If it is a heap conglomerate or an index conglomerate not shared by a duplicate index, the size of the return array is 1. If the uuid argument is null, then this method retrieves descriptors for all of the conglomerates in the database. Populate the ConglomerateDescriptorList for the specified TableDescriptor by scanning sysconglomerates. MT synchronization: it is assumed that the caller has synchronized on the CDL in the given TD. Get a ConstraintDescriptor given its name and schema ID. Please use getConstraintDescriptorByName() if you have the constraint's table descriptor, it is much faster. Get a ConstraintDescriptor given its UUID.  Please use getConstraintDescriptorById() is you have the constraints table descriptor, it is much faster. Get the constraint descriptor given a table and the UUID String of the backing index. Get the constraint descriptor given a table and the UUID String of the constraint Get the constraint descriptor given a TableDescriptor and the constraint name. Return a (single or list of) catalog row descriptor(s) from SYSCONSTRAINTS through a heap scan Return a (single or list of) ConstraintDescriptor(s) from SYSCONSTRAINTS where the access is from the index to the heap. Load up the constraint descriptor list for this table descriptor and return it.  If the descriptor list is already loaded up, it is retuned without further ado.  If no table descriptor is passed in, then all constraint descriptors are retrieved.  Note that in this case, the constraint descriptor objects may be duplicates of constraint descriptors that are hung off of the table descriptor cache. Populate the ConstraintDescriptorList for the specified TableDescriptor. MT synchronization: it is assumed that the caller has synchronized on the CDL in the given TD. Return a table descriptor corresponding to the TABLEID field in SYSCONSTRAINTS where CONSTRAINTID matches the constraintId passsed in. Return an List which of the relevant column matching the indexed criteria.  If nothing matches, returns an empty List (never returns null). Privileged lookup of a Context. Must be private so that user code can't call this entry point. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point.  Get a DataDescriptorGenerator, through which we can create objects to be stored in the DataDictionary. Get a DataValueFactory, through which we can create data value objects. Get the descriptor for the declared global temporary table schema which is always named "SESSION". returns the dependencymanager associated with this datadictionary. Gets a list of the dependency descriptors for the given dependent's id. Return a (single or list of) catalog row descriptor(s) from a system table where the access a heap scan Return a (single or list of) catalog row descriptor(s) from a system table where the access is from the index to the heap. Return a (single or list of) catalog row descriptor(s) from a system table where the access is from the index to the heap. This overload variant takes an explicit tc, in contrast to the normal one which uses the one returned by getTransactionCompile. Get ExecutionFactory associated with this database.   Scan sysfiles_index1 (schemaid,name) for a match. Scan sysfiles_index2 (id) for a match. Return a list of foreign keys constraints referencing this constraint.  Returns both enforced and not enforced foreign keys. Get one user's privileges for an object using the permUUID. Get permissions granted to one user for an object using the object's Id and the user's authorization Id. Converts a UUID to an DataValueDescriptor. Get an index row based on a row from the heap. {@inheritDoc } Get the value of an integer property. Privileged Monitor lookup. Must be package private so that user code can't call this entry point. Get a TabInfoImpl for a non-core table. (We fault in information about non-core tables as needed.) returns the tabinfo for a non core system catalog. Input is a catalogNumber (defined in DataDictionary). end of getPermissionsCache Gets a list of the dependency descriptors for the given provider's id. Get the target role definition by searching for a matching row in SYSROLES by rolename where isDef==true.  Read only scan. Uses index on (rolename, isDef) columns. Get the target role by searching for a matching row in SYSROLES by rolename, grantee and grantor.  Read only scan. Uses index on roleid, grantee and grantor columns.  Return an in-memory representation of the role grant graph (sans grant of roles to users, only role-role relation. Get the list of routines matching the schema and routine name. While we only support a single alias for a given name,namespace just return a list of zero or one item. If the schema is SYSFUN then do not use the system catalogs, but instead look up the routines from the in-memory table driven by the contents of SYSFUN_FUNCTIONS. @see org.apache.derby.iapi.sql.dictionary.DataDictionary#getRoutinePermissions Get one user's permissions for a routine (function or procedure). end of getRoutinePermissions Get a SPSDescriptor given its name. Currently no cacheing.  With caching we need to be very careful about invalidation. No caching means invalidations block on existing SPSD instances (since they were read in Get a SPSDescriptor given its UUID. Scan sysschemas_index1 (stmtname, schemaid) for a match. Scan sysstatements_index2 (stmtid) for a match. Note that we do not do a lookup of parameter info. Get all the parameter descriptors for an SPS. Look up the params in SYSCOLUMNS and turn them into parameter descriptors. Get the heap conglomerate number for SYS.SYSCOLUMNS. (Useful for adding new index to the table.) Get the descriptor for the named schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this.  Will check default schema for a match before scanning a system table. Get the SchemaDescriptor for the given schema identifier. Get the SchemaDescriptor for the given schema identifier. Get the sequence descriptor given a sequence name and a schema Id. <p> Get the uuid string of a sequence given its schema and sequence name. </p>  Returns all the statistics descriptors for the given table. <p> NOTE: As opposed to most other data dictionary lookups, this operation is performed with isolation level READ_UNCOMMITTED. The reason is to avoid deadlocks with inserts into the statistics system table. Get a SubCheckConstraintDescriptor from syschecks for the specified constraint id.  (Useful for check constraints.) Get a SubKeyConstraintDescriptor from syskeys or sysforeignkeys for the specified constraint id.  For primary foreign and and unique key constraints. Get the descriptor for the SYSIBM schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this. Returns a unique system generated name of the form SQLyymmddhhmmssxxn yy - year, mm - month, dd - day of month, hh - hour, mm - minute, ss - second, xx - the first 2 digits of millisec because we don't have enough space to keep the exact millisec value, n - number between 0-9 The number at the end is to handle more than one system generated name request came at the same time. In that case, the timestamp will remain the same, we will just increment n at the end of the name. Following is how we get around the problem of more than 10 system generated name requestes at the same time: When the database boots up, we start a counter with value -1 for the last digit in the generated name. We also keep the time in millisec to keep track of when the last system name was generated. At the boot time, it will be default to 0L. In addition, we have a calendar object for the time in millisec That calendar object is used to fetch yy, mm, dd, etc for the string SQLyymmddhhmmssxxn When the first request for the system generated name comes, time of last system generated name will be less than the current time. We initialize the counter to 0, set the time of last system generated name to the current time truncated off to lower 10ms time. The first name request is the only time we know for sure the time of last system generated name will be less than the current time. After this first request, the next request could be at any time. We go through the following algorithm for every generated name request. First check if the current time(truncated off to lower 10ms) is greater than the timestamp for last system generated name If yes, then we change the timestamp for system generated name to the current timestamp and reset the counter to 0 and generate the name using the current timestamp and 0 as the number at the end of the generated name. If no, then it means this request for generated name has come at the same time as last one. Or it may come at a time less than the last generated name request. This could be because of seasonal time change or somebody manually changing the time on the computer. In any case, if the counter is less than 10(meaning this is not yet our 11th request for generated name at a given time), we use that in the generated name. But if the counter has reached 10(which means, this is the 11th name request at the same time), then we increment the system generated name timestamp by 10ms and reset the counter to 0 (notice, at this point, the timestamp for system generated names is not in sync with the real current time, but we need to have this mechanism to get around the problem of more than 10 generated name requests at a same physical time). Get the descriptor for the system schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this. Get the descriptor for the SYSCS_UTIL system schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this. Get the descriptor for the named table within the given schema. If the schema parameter is NULL, it looks for the table in the current (default) schema. Table descriptors include object ids, object types (table, view, etc.) Get the descriptor for the table with the given UUID. NOTE: I'm assuming that the object store will define an UUID for persistent objects. I'm also assuming that UUIDs are unique across schemas, and that the object store will be able to do efficient lookups across schemas (i.e. that no schema descriptor parameter is needed). Scan systables_index1 (tablename, schemaid) for a match. Scan systables_index2 (tableid) for a match. @see org.apache.derby.iapi.sql.dictionary.DataDictionary#getTablePermissions Get one user's privileges on a table end of getTablePermissions Get the TransactionController to use, when not passed in as a parameter.  (This hides logic about whether or not we're at boot time in a single place.  NOTE:  There's no LCC at boot time.) NOTE: All <get> methods in the DD should call this method. Get the TransactionController to use, when not passed in as a parameter.  (This hides logic about whether or not we're at boot time in a single place.  NOTE:  There's no LCC at boot time.) NOTE: All <get> methods in the DD should call this method. Get all columns that reference transition variables in triggers. The columns should be returned in the same order as in the SQL text. Get the stored prepared statement descriptor given a sps name. Get a TriggerDescriptor given its UUID. Load up the trigger descriptor list for this table descriptor and return it.  If the descriptor list is already loaded up, it is retuned without further ado. Populate the TriggerDescriptorList for the specified TableDescriptor. MT synchronization: it is assumed that the caller has synchronized on the CDL in the given TD. Get the UUID Factory.  (No need to make the UUIDFactory a module.) Get the UUID for the specified system table.  Prior to Plato, system tables did not have canonical UUIDs, so we need to scan systables to get the UUID when we are updating the core tables. Get a column permissions descriptor from the system tables, without going through the cache. This method is called to fill the permissions cache. end of getUncachedColPermsDescriptor Get an object's permission descriptor from the system tables, without going through the cache. This method is called to fill the permissions cache. end of getUncachedGenericPermDescriptor end of getUncachedPermissionsDescriptor Get a routine permissions descriptor from the system tables, without going through the cache. This method is called to fill the permissions cache. end of getUncachedRoutinePermsDescriptor This method can get called from the DataDictionary cache. public SPSDescriptor getSPSBySQLText(String text) { return (SPSDescriptor) spsTextHash.get(text); } This method can get called from the DataDictionary cache. This method can get called from the DataDictionary cache. This method can get called from the DataDictionary cache. Get a table permissions descriptor from the system tables, without going through the cache. This method is called to fill the permissions cache. end of getUncachedTablePermsDescriptor  Gets the viewDescriptor for the view with the given UUID. Gets the viewDescriptor for the view given the TableDescriptor. Get the information for the view from sys.sysviews. Grant PUBLIC access to a system routine. This method should be used only for granting access to a system routine (other than routines in SYSFUN schema). It expects the routine to be present in SYSALIASES catalog. Grant PUBLIC access to specific system routines. Currently, this is done for some routines in SYSCS_UTIL schema. We grant access to routines which we have just added. Doing it this way lets us declare these routines in one place and re-use this logic during database creation and during upgrade. Get all of the ConglomerateDescriptors in the database and hash them by conglomerate number. This is useful as a performance optimization for the locking VTIs. NOTE:  This method will scan SYS.SYSCONGLOMERATES at READ UNCOMMITTED. Get all of the TableDescriptors in the database and hash them by TableId This is useful as a performance optimization for the locking VTIs.  NOTE: This method will scan SYS.SYSTABLES and SYS.SYSSCHEMAS at READ UNCOMMITTED. Initialize catalog information. This method is overridden by children. Initialized the core info array. Initialized the noncore info array.   Indicate whether there is anything in the particular schema.  Checks for tables in the the schema, on the assumption that there cannot be any other objects in a schema w/o a table. Is the schema id referenced by the system table in question? Currently assumes that the schema id is in an index. NOTE: could be generalized a bit, and possibly used elsewhere... Determine whether a string is the name of the system schema. Check if a table name is actually a transition variable. The arrary passed will have either -1 or a column position as it's elements. If the array only has -1 as for all it's elements, then this method will return null. Otherwise, the method will create a new arrary with all -1 entries removed from the original arrary. Initialize indices for an array of catalogs  class implementation  Initialize system catalogs. This is where we perform upgrade. It is our pious hope that we won't ever have to upgrade the core catalogs, other than to add fields inside Formatable columns in these catalogs. If we do have to upgrade the core catalogs, then we may need to move the loadCatalog calls into the upgrade machinery. It's do-able, just not pretty. Get the target schema by searching for a matching row in SYSSCHEMAS by schema name.  Read only scan. Get the target schema by searching for a matching row in SYSSCHEMAS by schemaId.  Read only scan. Get the target schema by searching for a matching row in SYSSCHEMAS by schemaId.  Read only scan. The dirty work of creating a catalog. Converts a SystemColumn to a ColumnDescriptor. returns null if database is at rev level 10.5 or earlier * CacheableFactory interface Populate SYSDUMMY1 table with a single row. Map ( schemaName, sequenceName ) to sequenceID Remove PermissionsDescriptor from permissions cache if present Reset the database owner according to what is stored in the catalogs. This can change at upgrade time so we have factored this logic into a separately callable method. Workhorse for ALTER TABLE-driven mods to SYSCOLPERMS This method finds all the SYSCOLPERMS rows for this table. Then it iterates through each row, either adding a new column to the end of the table, or dropping a column from the table, as appropriate. It updates each SYSCOLPERMS row to store the new COLUMNS value. sets a new value in SYSCOLUMNS for a particular autoincrement column. sets the dependencymanager associated with this dd. subclasses can override this to install their own funky dependency manager. Mark this database as a read only database whose stored prepared statements are invalid because some kind of upgrade is needed. Add an entry to the hashtables for lookup from the cache. * Methods related to ModuleControl  Privileged startup. Must be private so that user code can't call this entry point. @see org.apache.derby.iapi.sql.dictionary.DataDictionary#startWriting @exception StandardException		Thrown on error Stop this module.  In this case, nothing needs to be done. @see org.apache.derby.iapi.sql.dictionary.DataDictionary#transactionFinished Update the column descriptor in question.  Updates every row in the base conglomerate. Update the conglomerateNumber for a ConglomerateDescriptor. This is useful, in 1.3, when doing a bulkInsert into an empty table where we insert into a new conglomerate. (This will go away in 1.4.) Update the conglomerateNumber for an array of ConglomerateDescriptors. In case of more than one ConglomerateDescriptor, each descriptor should be updated separately, conglomerate id is not same for all the descriptors. Even when indexes are sharing the same conglomerate(conglomerate number), conglomerate ids are unique. This is useful, in 1.3, when doing a bulkInsert into an empty table where we insert into a new conglomerate. (This will go away in 1.4.) Update the constraint descriptor in question.  Updates every row in the base conglomerate. Set the current value of an ANSI/ISO sequence. This method does not perform any sanity checking but assumes that the caller knows what they are doing. If the old value on disk is not what we expect it to be, then we are in a race with another session. They won and we don't update the value on disk. However, if the old value is null, that is a signal to us that we should update the value on disk anyway. Update the lockGranularity for the specified table. Drop and recreate metadata stored prepared statements. Updates SYS.SYSSTATEMENTS with the info from the SPSD. Need to update SYSCOLPERMS for a given table because a new column has been added to that table. SYSCOLPERMS has a column called "COLUMNS" which is a bit map for all the columns in a given user table. Since ALTER TABLE .. ADD COLUMN .. has added one more column, we need to expand "COLUMNS" for that new column Currently, this code gets called during execution phase of ALTER TABLE .. ADD COLUMN .. Update SYSCOLPERMS due to dropping a column from a table. Since ALTER TABLE .. DROP COLUMN .. has removed a column from the table, we need to shrink COLUMNS by removing the corresponding bit position, and shifting all the subsequent bits "left" one position. Update authorizationId of specified schemaName Update all system schemas to have new authorizationId. This is needed while upgrading pre-10.2 databases to 10.2 or later versions. From 10.2, all system schemas would be owned by database owner's authorizationId. Update the trigger descriptor in question.  Updates every row in the base conglomerate that matches the uuid. 10.6 upgrade logic to update the return type of SYSIBM.CLOBGETSUBSTRING. The length of the return type was changed in 10.5 but old versions of the metadata were not upgraded at that time. See DERBY-4214. Upgrade an existing system catalog column's definition by setting it to the value it would have in a newly created database. This is only used to for a couple of columns that had incorrectly nullability. Other uses (e.g. changing column type) might require more work. Called by the upgrade code to upgrade the way we store jar files in the database.<p/> We now use UUID as part of the file name to avoid problems with path delimiters. Also, we henceforth use no schema subdirectories since there is no chance of name collision with the UUID. called by the upgrade code (dd_xena etc) to add a new system catalog. 10.6 upgrade logic to update the permissions granted to SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE. If a 10.0 database was upgraded to 10.2, 10.3, or 10.4, then there will be an extra permissions tuple in SYSROUTINEPERMS--that tuple will have a null grantor field. We must delete this tuple. See DERBY-4215. Add autoinccycle columns to an SYSCOLUMNS system catalog Upgrade an existing catalog by adding columns. Add invisible columns to an existing system catalog Initialize noncore columns to fixed values  Scan <code>indexNo</code> index on a permission table <code>catalog</code>, looking for match(es) for the grantee column (given by granteeColnoInIndex for the catalog in question). The action argument can be either <code>EXISTS</code> or <code>DROP</code> (to check for existence, or to drop that row). There is no index on grantee column only on on any of the permissions tables, so we use the index which contain grantee and scan that, setting up a scan qualifier to match the grantee, then fetch the base row. If this proves too slow, we should add an index on grantee only. Scan the {roleid, grantee, grantor} index on SYSROLES, locate rows containing authId in column columnNo. The action argument can be either <code>EXISTS</code> or <code>DROP</code> (to check for existence, or to drop that row). If the scan proves too slow, we should add more indexes.  only. Adds columns to the conglomerate underlying a system table.
Create and load a stream container. Add a container. Back up the data segment of the database. Return a record handle that is initialized to the given page number and record id. public RecordHandle makeRecordHandle(long segmentId, long containerId, long pageNumber, int recordId) throws	StandardException; Database creation finished Returns if data base is in encrypted mode. Decrypt cleartext from ciphertext. Decrypts all the containers in the data segment. Drop and remove a stream container. Encrypt cleartext into ciphertext. Encrypt all the containers in the data segment. Backup restore - stop writing dirty pages or container to disk Return the encryption block size used by the algorithm at time of encrypted database creation Get an object to handle non-transactional files. Return the identifier that uniquely identifies this raw store at runtime. This identifier is to be used as part of the lokcing key for objects locked in the raw store by value (e.g. Containers). Return an id which can be used to create a container. <p> Return an id number with is greater than any existing container in the current database.  Caller will use this to allocate future container numbers - most likely caching the value and then incrementing it as it is used. <p> <p> Get the root directory of the data storage area. It is always guaranteed to be an absolute path, and it is prefixed with the JDBC sub-sub-protocol if it is not a directory database. Examples: </p> <dl> <dt>{@code /path/to/database}</dt> <dd>in case of a directory database</dd> <dt>{@code memory:/path/to/database}</dt> <dd> in case of a memory database</dd> </dl>  Is the store read-only. Open a container that is not droped. Open a container that may have been dropped. Only internal raw store code should call this, e.g. recovery. Open a stream container. Called after recovery is performed. re-Create a container during redo recovery. Used if container is found to not exist during redo recovery of log records creating the container. Reclaim space used by this factory.  Called by post commit daemon. This function is called after a checkpoint to remove the stub files thar are not required during recovery. Crash recovery  uses these files to identify the dropped containers.   Stub files(d*.dat) gets creates  when a table/index(containers) dropped. Removes old versions of the containers after a cryptographic operation on the database. Tell the data factory it is OK to remove committed deleted containers when the data factory shuts down. Sets whether the database is encrypted. make data factory aware of which raw store factory it belongs to Also need to boot the LogFactory Register a handler class for insert undo events. <P> Register a class to be called when an undo of an insert is executed.  When an undo of an event is executed by the raw store UndoHandler.insertUndoNotify() will be called, allowing upper level callers to execute code as necessary.  The initial need is for the access layer to be able to queue post commit reclaim space in the case of inserts which are aborted (including the normal case of inserts failed for duplicate key violations) (see DERBY-4057) <p> Currently the handler is only called on abort of inserts on non-overflow pages that meet either of the following 2 requirements: 1) the row has either overflow columns (long columns) or the row columns span multiple pages (long rows). 2) after the action all user rows on the page are marked deleted. Set up the data factory's caches to use the specified daemon service for background cleaning. Backup restore - start writing dirty pages or container to disk Backup restore - write finished, if this is the last writer, allow the persistent store to proceed. Backup restore - don't allow the persistent store to be frozen - or if it is already frozen, block.   A write is about to commence.
//////////////////////////////////////////////////////////////////////  AUTHENTICATION  ////////////////////////////////////////////////////////////////////// Authenticate the user's permission to access the raw database. The following hurdles must be passed. Otherwise, an exception is raised. <ul> <li>The user must be the DBO of the raw database.</li> <li>If there are any tuples in the SYSUSERS catalog of the raw database, then the supplied credentials must match what's in SYSUSERS.</li> <li>If the database properties of the raw database specify an authentication scheme, then that scheme must be applied to the supplied credentials.</li> </ul> //////////////////////////////////////////////////////////////////////  TABLE FUNCTION  ////////////////////////////////////////////////////////////////////// Entry point declared in the external name clause of a CREATE FUNCTION statement. See FileContainer.decryptPage() Get a connection to a transient database which is only used to compile table signatures. Get a column value (1-based indexing) and check if it's null Get system property. Get the warnings //////////////////////////////////////////////////////////////////////  VTITemplate IMPLEMENTATION  ////////////////////////////////////////////////////////////////////// Read the properties conglomerate of the raw database. Read the next page of rows. Match credentials against the BUILTIN credentials stored in the properties conglomerate of the raw database. All of those properties have been read into the props object already. Validate credentials using a custom authenticator. Verify that the user is the DBO of the raw database. Match credentials using an LDAP server. If NATIVE authentication is on, then the user's credentials must be stored in SYSUSERS. Returns false if NATIVE authentication is not on. Raises an exception if NATIVE authentication is on and the credentials don't match. If an authentication scheme is specified by database properties in the raw database, then use that scheme to validate the credentials.
Skips requested number of bytes, throws EOFException if there is too few bytes in the DataInput.
Copy attributes from a {@code Properties} object to a {@code Map}. Get a connection from a single use DataSource configured from the configuration but with the passed in property set.
Deletes every child of the root path specified. <p> Note that the root itself must be removed outside of this method. @GuardedBy("LOCK") Creates all the parents of the specified path. Creates a new entry in the data store. <p> This method returns {@code null} if the path already exists, if one of the parent directories doesn't exist, or if one of the parents is a file instead of a directory. Deletes the specified entry and all its children. Deletes the specified entry. <p> If the specified entry is a directory, it is only deleted if it is empty. Read-only entries are deleted. Returns the database name, which is expected to equal the path of the service root. Returns the entry with the specified path. Returns the list of parents for the specified path. <p> The lowest level parent is listed first in the list, so all absolute paths will have the root listed as the last element. Returns an identifier for a temporary file. Lists the childen of the specified path. Moves / renames a file. Purges the database and releases all files associated with it. Tells if this data store is scheduled for deletion.
Checks if this entry has been released. Returns an input stream to read from this entry. Returns an output stream to write into this entry. Tells if this entry is a directory. Tells if this entry is read-only. Returns the length of this entry. Relases this entry. Sets the length of this entry. Makes this entry read-only.
Check the value to seem if it conforms to the restrictions imposed by DB2/JCC on host variables for this type. Default implementation of shallow cloning, which forwards to the deep clone method. <p> For many of the data types, a shallow clone will be the same as a deep clone. The data types requiring special handling of shallow clones have to override this method (for instance types whose value can be represented as a stream). DataValueDescriptor interface  Compare this Orderable with another, with configurable null ordering. The caller gets to determine how nulls should be treated - they can either be ordered values or unknown values. The caller also gets to decide, if they are ordered, whether they should be lower than non-NULL values, or higher Compare this Orderable with another, with configurable null ordering. This method treats nulls as ordered values, but allows the caller to specify whether they should be lower than all non-NULL values, or higher than all non-NULL values. Wrapper method for the "compare(DataValueDescriptor)" method of this class.  Allows sorting of an array of DataValueDescriptors using the JVMs own sorting algorithm.  Currently used for execution-time sorting of IN-list values to allow proper handling (i.e. elimination) of duplicates. Return an conversion exception from this type to another. equals The = operator as called from the language module, as opposed to the storage module. This default implementations uses compare(). Flip the operator used in a comparison (&lt; -&gt; &gt;). This is useful when flipping a comparison due to type precedence. DataValueDescriptor Interface Gets the value in the data value descriptor as a boolean. Throws an exception if the data value is not receivable as a boolean. Gets the value in the data value descriptor as a byte. Throws an exception if the data value is not receivable as a byte. Gets the value in the data value descriptor as a byte[]. Throws an exception if the data value is not receivable as a Binary or Varbinary. Gets the value in the data value descriptor as a java.sql.Date. Throws an exception if the data value is not receivable as a Date. Gets the value in the data value descriptor as a double. Throws an exception if the data value is not receivable as a double. Gets the value in the data value descriptor as a float. Throws an exception if the data value is not receivable as a float. Gets the value in the data value descriptor as a int. Throws an exception if the data value is not receivable as a int. Gets the value in the data value descriptor as a long. Throws an exception if the data value is not receivable as a long. Gets the value in the data value descriptor as a int. Throws an exception if the data value is not receivable as a int. Gets the value in the data value descriptor as a short. Throws an exception if the data value is not receivable as a short. Gets the value in the data stream descriptor as an InputStream. Throws an exception if the data value is not receivable as a stream. Gets the value in the data value descriptor as a java.sql.Time. Throws an exception if the data value is not receivable as a Time. Gets the value in the data value descriptor as a java.sql.Timestamp. Throws an exception if the data value is not receivable as a Timestamp. Gets the value in the data stream descriptor as a trace string. This default implementation simply forwards the call to <code>getString</code>. Get the type name of this value, possibly overriding with the passed in class name (for user/java types). The &gt;= operator as called from the language module, as opposed to the storage module. This default implementation uses compare(). The &gt; operator as called from the language module, as opposed to the storage module. This default implementations uses compare(). Tells that the value isn't represented as a stream, which is true for most Derby data types. <p> This method will be overridden by types able to use a stream as the source.  Return an out of range exception for this type. The is not null operator as called from the language module, as opposed to the storage module. Column interface The is null operator as called from the language module, as opposed to the storage module. The &lt;= operator as called from the language module, as opposed to the storage module. This default implementations uses compare(). The &lt; operator as called from the language module, as opposed to the storage module. Default normalization method. No information needed from DataTypeDescriptor. The &lt;&gt; operator as called from the language module, as opposed to the storage module. This default implementations uses compare(). Return an out of range exception for this type. Read the DataValueDescriptor from the stream. The default implementation calls {@code readExternal()}, which accesses the {@code ArrayInputStream} as a generic stream. If sub-classes can implement it more efficiently by accessing the array, they should override this method. Recycle this DataType object. Only to be called when the application sets a value using BigDecimal Set the value of this DataValueDescriptor based on the value of the specified DataValueDescriptor. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values. Set the value from an non-null object. Usually overridden. This implementation throws an exception. The object will have been correctly typed from the call to setObjectForCast.   Set the value. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor to the given byte value At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor to the given double value At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor to the given float value At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor to the given int value At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor. At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor to the given long value At DataType level just throws an error lower classes will override Set the value of this DataValueDescriptor to the given short value At DataType level just throws an error lower classes will override Each built-in type in JSQL has a precedence.  This precedence determines how to do type promotion when using binary operators.  For example, float has a higher precedence than int, so when adding an int to a float, the result type is float. The precedence for some types is arbitrary.  For example, it doesn't matter what the precedence of the boolean type is, since it can't be mixed with other types.  But the precedence for the number types is critical.  The SQL standard requires that exact numeric types be promoted to approximate numeric when one operator uses both.  Also, the precedence is arranged so that one will not lose precision when promoting a type.
Check if this type is comparable with the passed type. Compare the collation info on this DTD with the passed DTD. The rules for comparison are as follows (these are as per SQL standard 2003 Section 9.13) 1)If both the DTDs have collation derivation of NONE, then they can't be compared and we return false. 2)If both the DTDs have same collation derivation (which in Derby's case at this point will mean collation derivation of IMPLICIT), then check the collation types. If they match, then return true. If they do not match, then they can't be compared and hence return false. 3)If one DTD has collation derivation of IMPLICIT and other DTD has collation derivation of NONE, then 2 DTDs are comparable using the collation type of DTD with collation derivation of IMPLICIT. Derby does not implement this rule currently and it is being traked as DERBY-2678. Derby's current behavior is to throw an exception if both the DTDs involved in collation operation do not have collation derivation of IMPLICIT. This behavior is a subset of SQL standard. 4)Derby currently does not support collation derivation of EXPLICIT and hence we do not have the code to enforce rules as mentioned in Section 9.13 of SQL spec for collation derivation of EXPLICIT. When we implement collation derivation of EXPLICIT, we should make sure that we follow the rules as specified in the SQL spec for comparability. Compare if two DataTypeDescriptors are exactly the same Get the estimated memory usage for this type descriptor. * Static creators Get a descriptor that corresponds to a nullable builtin JDBC type. If a variable length type then the size information will be set to the maximum possible. Collation type will be UCS_BASIC and derivation IMPLICIT. For well known types code may also use the pre-defined runtime types that are fields of this class, such as INTEGER. Get a descriptor that corresponds to a builtin JDBC type. For well known types code may also use the pre-defined runtime types that are fields of this class, such as INTEGER. E.g. using DataTypeDescriptor.INTEGER is preferred to DataTypeDescriptor.getBuiltInDataTypeDescriptor(Types.INTEGER, true) (both will return the same immutable object). Get a descriptor that corresponds to a builtin JDBC type. Collation type will be UCS_BASIC and derivation IMPLICIT. Get a descriptor that corresponds to a nullable builtin variable length JDBC type. Collation type will be UCS_BASIC and derivation IMPLICIT. Get a DataTypeServices that corresponds to a nullable builtin SQL type. Collation type will be UCS_BASIC and derivation IMPLICIT. Get a DataTypeServices that corresponds to a builtin SQL type Collation type will be UCS_BASIC and derivation IMPLICIT. Get the simplified type descriptor that is intended to be stored in the system tables. Return a nullable catalog type for a fixed length JDBC builtin type. Return a nullable catalog type for a JDBC builtin type and length. Get a catlog type identical to the passed in type exception that the collationType is set to the passed in value. Return a type description identical to this type with the exception that its collation information is taken from the passed in information. If the type does not represent a string type then the collation will be unchanged and this is returned. Get the collation derivation for this type. This applies only for character string types. For the other types, this api should be ignored. SQL spec talks about character string types having collation type and collation derivation associated with them (SQL spec Section 4.2.2 Comparison of character strings). If collation derivation says explicit or implicit, then it means that there is a valid collation type associated with the charcter string type. If the collation derivation is none, then it means that collation type can't be established for the character string type. 1)Collation derivation will be explicit if SQL COLLATE clause has been used for character string type (this is not a possibility for Derby 10.3 because we are not planning to support SQL COLLATE clause in the 10.3 release). 2)Collation derivation will be implicit if the collation can be determined w/o the COLLATE clause eg CREATE TABLE t1(c11 char(4)) then c11 will have collation of USER character set. Another eg, TRIM(c11) then the result character string of TRIM operation will have collation of the operand, c11. 3)Collation derivation will be none if the aggregate methods are dealing with character strings with different collations (Section 9.3 Data types of results of aggregations Syntax Rule 3aii). Collation derivation will be initialized to COLLATION_DERIVATION_IMPLICIT if not explicitly set. Gets the name of the collation type in this descriptor if the collation derivation is not NONE. If the collation derivation is NONE, then this method will return "NONE". <p> This method is used for generating error messages which will use correct string describing collation type/derivation. Gets the name of the specified collation type. Obtain the collation type of the underlying catalog type. Shorthand method for getCatalogType().getCollationType(). Obtain the collation type from a collation property value. Get the dominant type (DataTypeDescriptor) of the 2. For variable length types, the resulting type will have the biggest max length of the 2. If either side is nullable, then the result will also be nullable. If dealing with character string types, then make sure to set the collation info on the dominant type. Following algorithm will be used for dominant DTD's collation determination. Each of the steps of the algorithm have been numbered in the comments below and those same numbers are used in the actual algorithm below so it is easier to understand and maintain. Step 1 If the DTD for "this" node has the same collation derivation as the otherDTS, then check if their collation types match too. If the collation types match too, then DTD for dominant type will get the same collation derivation and type. Step 2 If the collation derivation for DTD for "this" node and otherDTS do not match, then check if one of them has the collation derivation of NONE. If that is the case, then dominant DTD will get the collation type and derivation of DTD whose collation derivation is not NONE. Step 3 If the collation derivation for DTD for "this" node and otherDTS do not match, and none of them have the derivation of NONE then it means that we are dealing with collation derivation of IMPLICIT and EXPLICIT and hence the dominant DTD should get collation derivation of NONE. This is not a possibility in Derby 10.3 because the only 2 possible collation derivation supported are IMPLICIT and NONE. Step 4 If the collation derivation for DTD for "this" node and otherDTS match, then check if the collation types match too. If not, then the dominant DTD should get collation derivation of NONE. Return the SQL type name and, if applicable, scale/precision/length for this DataTypeDescriptor.  Note that we want the values from *this* object specifically, not the max values defined on this.typeId. Get the jdbc type id for this type.  JDBC type can be found in java.sql.Types. Shorthand method for getCatalogType().getJDBCTypeId(). Shorthand method for getCatalogType().getMaximumWidth(). Get a Null for this type. Return a type descriptor identical to the this type with the exception of its nullability. If the nullablity required matches the nullability of this then this is returned. Get the maximum and minimum value for a fixed numeric type. Throws an unimplemented feature exception for a non-numeric type. Returns the number of decimal digits for the datatype, if applicable. Shorthand method for getCatalogType().getPrecision(). Get a catalog type that corresponds to a SQL Row Multiset For a row multi set type return an identical type with the collation type changed. Note that since row types are only ever catalog types the derivation is not used (since derivation is a property of runtime types). <BR> Get a DataTypeServices that corresponds to a Java type Get a DataTypeServices that corresponds to a Java type Get a DataTypeDescriptor that corresponds to a Java type Return the typename with the collation name for String types. Converts this data type descriptor (including length/precision) to a string. E.g. VARCHAR(30) or java.util.Hashtable Returns the number of digits to the right of the decimal for the datatype, if applicable. Shorthand method for getCatalogType().getScale(). Return a runtime type for a catalog type. Get the formatID which corresponds to this class. Gets the TypeId for the datatype. Gets the name of this datatype. Determine if an ASCII stream can be inserted into a column or parameter of type <code>jdbcType</code>. Determine if a binary stream can be inserted into a column or parameter of type <code>jdbcType</code>. Check whether a JDBC type is compatible with the Java type <code>byte[]</code>. <p><strong>Note:</strong> <code>BLOB</code> is not compatible with <code>byte[]</code>. See tables B-4, B-5 and B-6 in the JDBC 3.0 Specification. Determine if a character stream can be inserted into a column or parameter of type <code>jdbcType</code>. Check whether a JDBC type is one of the character types that are compatible with the Java type <code>String</code>. <p><strong>Note:</strong> <code>CLOB</code> is not compatible with <code>String</code>. See tables B-4, B-5 and B-6 in the JDBC 3.0 Specification. <p> There are some non-character types that are compatible with <code>String</code> (examples: numeric types, binary types and time-related types), but they are not covered by this method. Check whether or not the 2 types (DataTypeDescriptor) have the same type and length. This is useful for UNION when trying to decide whether a NormalizeResultSet is required. Compare JdbcTypeIds to determine if they represent equivalent SQL types. For example Types.NUMERIC and Types.DECIMAL are equivalent Returns TRUE if the datatype can contain NULL, FALSE if not. JDBC supports a return value meaning "nullability unknown" - I assume we will never have columns where the nullability is unknown. Shorthand method for getCatalogType().isNullable(); Check to make sure that this type id is something a user can create him/herself directly through an SQL CREATE TABLE statement. This method is used for CREATE TABLE AS ... WITH [NO] DATA binding because it's possible for the query to return types which are not actually creatable for a user.  DERBY-2605. Three examples are: JAVA_OBJECT: A user can select columns of various java object types from system tables, but s/he is not allowed to create such a column him/herself. DECIMAL: A user can specify a VALUES clause with a constant that has a precision of greater than 31.  Derby can apparently handle such a value internally, but the user is not supposed to be able create such a column him/herself. DataTypeDescriptor Interface Formatable methods Read this object from a stream of stored objects. Write this object to a stream of stored objects.
Clone a Timestamp because they are mutable Compute the maximum width (column display width) of a decimal or numeric data value, given its precision and scale. Gets the display width of a column of a given type. Get the precision of the datatype, in decimal digits This is used by EmbedResultSetMetaData. Get the precision of the datatype. Is the data type case sensitive. Is the data type nullable. Is the data type signed.

Check the value to seem if it conforms to the restrictions imposed by DB2/JCC on host variables for this type. Get a shallow copy of this {@code codeDataValueDescriptor} (DVD). <p> The primary use of this method is to avoid materializing streams for data types like BLOB and CLOB. <p> In general the orginal DVD should be recycled or discarded when this method is invoked to ensure that changes to the original DVD don't affect the clone (or the other way around). Note that it is not safe to assume that a number of these clones can be used for read-only access to the same value. <p> <em>Implementation note:</em> The reason why the clones can't be guaranteed to work as "read clones" is that if the value is represented as a stream, the state of the stream will change on read operations. Since all the clones share the same stream, this may lead to wrong results, data corruption or crashes. Clone this DataValueDescriptor. Results in a new object that has the same value as this but can be modified independently. <p> Even though the objects can be modified independently regardless of the value of {@code forceMaterialization}, both the clone and the original may be dependent on the store state if {@code forceMaterialization} is set to {@code false}. An example is if you need to access the value you just read using {@code cloneValue} after the current transaction has ended, or after the source result set has been closed. The SQL language COALESCE/VALUE function.  This method is called from the language module. Compare this Orderable with a given Orderable for the purpose of qualification and sorting.  The caller gets to determine how nulls should be treated - they can either be ordered values or unknown values. Compare this Orderable with another, with configurable null ordering. The caller gets to determine how nulls should be treated - they can either be ordered values or unknown values. The caller also gets to decide, if they are ordered, whether they should be lower than non-NULL values, or higher Compare this Orderable with a given Orderable for the purpose of index positioning.  This method treats nulls as ordered values - that is, it treats SQL null as equal to null and greater than all other values. Compare this Orderable with another, with configurable null ordering. This method treats nulls as ordered values, but allows the caller to specify whether they should be lower than all non-NULL values, or higher than all non-NULL values. The SQL language = operator.  This method is called from the language module.  The storage module uses the compare method in Orderable. Estimate the memory usage in bytes of the data value and the overhead of the class. Gets the value in the data value descriptor as a boolean. Throws an exception if the data value is not a boolean. For DataValueDescriptor, this is the preferred interface for BIT, but for this no-casting interface, it isn't, because BIT is stored internally as a Bit, not as a Boolean. Gets the value in the data value descriptor as a byte. Throws an exception if the data value is not a byte. Gets the value in the data value descriptor as a byte array. Throws an exception if the data value is not a byte array. Gets the value in the data value descriptor as a java.sql.Date. Throws an exception if the data value is not a Date. Gets the value in the data value descriptor as a double. Throws an exception if the data value is not a double. Gets the value in the data value descriptor as a float. Throws an exception if the data value is not a float. Gets the value in the data value descriptor as an int. Throws an exception if the data value is not an int. Gets the length of the data value.  The meaning of this is implementation-dependent.  For string types, it is the number of characters in the string.  For numeric types, it is the number of bytes used to store the number.  This is the actual length of this value, not the length of the type it was defined as. For example, a VARCHAR value may be shorter than the declared VARCHAR (maximum) length. Gets the value in the data value descriptor as a long. Throws an exception if the data value is not a long. Get a new null value of the same type as this data value. Gets the value in the data value descriptor as a Java Object. The type of the Object will be the Java object type corresponding to the data value's SQL type. JDBC defines a mapping between Java object types and SQL types - we will allow that to be extended through user type definitions. Throws an exception if the data value is not an object (yeah, right). Gets the value in the data value descriptor as a short. Throws an exception if the data value is not a short. Gets the value in the data value descriptor as a stream of bytes. <p> Only data types that implement {@code StreamStorable} will have stream states, and the method {@code hasStream} should be called to determine if the value in question is, or will be, represented by a stream. Gets the value in the data value descriptor as a String. Throws an exception if the data value is not a string. Gets the value in the data value descriptor as a java.sql.Time. Throws an exception if the data value is not a Time. Gets the value in the data value descriptor as a java.sql.Timestamp. Throws an exception if the data value is not a Timestamp. Gets the value in the data value descriptor as a trace string. If the value itself is not suitable for tracing purposes, a more suitable representation is returned. For instance, data values represented as streams are not materialized. Instead, information about the associated stream is given. Get the SQL name of the datatype The SQL language &gt;= operator.  This method is called from the language module.  The storage module uses the compare method in Orderable. The SQL language &gt; operator.  This method is called from the language module.  The storage module uses the compare method in Orderable. Tells if this data value is, or will be, represented by a stream. <p> This method should be called to determine if the methods {@code getStream} or {@code DataValueDescriptor.getStreamWithDescriptor} can be invoked. The SQL language IN operator.  This method is called from the language module.  This method allows us to optimize and short circuit the search if the list is ordered. The SQL "IS NOT NULL" operator.  Returns true if this value is not null. The SQL "IS NULL" operator.  Returns true if this value is null. * The SQL language &lt;= operator.  This method is called from the language module.  The storage module uses the compare method in Orderable. The SQL language &lt; operator.  This method is called from the language module.  The storage module uses the compare method in Orderable. Normalize the source value to this type described by this class and the passed in DataTypeDescriptor. The type of the DataTypeDescriptor must match this class. The SQL language &lt;&gt; operator.  This method is called from the language module.  The storage module uses the compare method in Orderable. Read the DataValueDescriptor from the stream. <p> Initialize the data value by reading it's values from the ArrayInputStream.  This interface is provided as a way to achieve possible performance enhancement when reading an array can be optimized over reading from a generic stream from readExternal(). Recycle this DataValueDescriptor if possible. Create and return a new object if it cannot be recycled. Set this value from an application supplied java.math.BigDecimal. This is to support the PreparedStatement.setBigDecimal method and similar JDBC methods that allow an application to pass in a BigDecimal to any SQL type. Set this value into a PreparedStatement. This method must handle setting NULL into the PreparedStatement. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values. Set this value from an Object. Used from CAST of a Java type to another type, including SQL types. If the passed instanceOfResultType is false then the object is not an instance of the declared type resultTypeClassName. Usually an exception should be thrown. Set the value to SQL null. Set the value. Set the value of this DataValueDescriptor to the given byte value Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor to the given double value Set the value of this DataValueDescriptor to the given double value Set the value of this DataValueDescriptor to the given int value Set the value to be the contents of the stream. <p> The reading of the stream may be delayed until execution time, and the format of the stream is required to be the format of this type. <p> Note that the logical length excludes any header bytes and marker bytes (for instance the Derby specific EOF stream marker). Specifying the logical length may improve performance in some cases, but specifying that the length is unknown (<code>UNKNOWN_LOGICAL_LENGTH</code> should always leave the system in a functional state. Specifying an incorrect length will cause errors. Set the value of this DataValueDescriptor to the given Object value Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor from a Blob. Set the value of this DataValueDescriptor from a Clob. Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor. Set the value of this DataValueDescriptor to the given long value Set the value of this DataValueDescriptor from another. Set the value of this DataValueDescriptor to the given short value Set the value based on the value for the specified DataValueDescriptor from the specified ResultSet. Each built-in type in JSQL has a precedence.  This precedence determines how to do type promotion when using binary operators.  For example, float has a higher precedence than int, so when adding an int to a float, the result type is float. The precedence for some types is arbitrary.  For example, it doesn't matter what the precedence of the boolean type is, since it can't be mixed with other types.  But the precedence for the number types is critical.  The SQL standard requires that exact numeric types be promoted to approximate numeric when one operator uses both.  Also, the precedence is arranged so that one will not lose precision when promoting a type. How should this value be obtained so that it can be converted to a BigDecimal representation.
Get a SQL bit with the given value.  The second form re-uses the previous value, if non-null, as the data holder to return. ------ BLOB Get a SQL Blob with the given value.  A null argument means get a SQL null value.  Uses the previous value (if non-null) to hold the return value. Get a SQL Blob with the given value.  A null argument means get a SQL null value.  Uses the previous value (if non-null) to hold the return value. Get a new SQLChar object to represent a SQL CHAR (UCS_BASIC) with the given value. A null argument means get a SQL NULL value. Get a SQLChar object to represent a SQL CHAR  (UCS_BASIC with the given value. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new SQLChar will be created and set to the value. Get a StringDataValue to represent a SQL CHAR with the passed in collationType. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new StringDataValue will be created and set to the value. If collationType is equal to StringDataValue.COLLATION_TYPE_UCS_BASIC then the call is the equivalent of the overload without collationType. Return the RuleBasedCollator depending on the collation type. If the collation type is UCS_BASIC, then this method will return null. If the collation type is TERRITORY_BASED then the return value will be the Collator derived from the database's locale. If this is the first time Collator is being requested for a database with collation type of TERRITORY_BASED, then we will check to make sure that JVM supports the Collator for the database's locale. If not, we will throw an exception This method will be used when Store code is trying to create a DVD template row using the format ids and the collation types. First a DVD will be constructed just using format id. Then if the DVD is of type StringDataValue, then it will call this method to get the Collator object. If the Collator object returned from this method is null then we will continue to use the default DVDs for the character types, ie the DVDs which just use the JVM's default collation. (This is why, we want this method to return null if we are dealing with UCS_BASIC.) If the Collator object returned is not null, then we will construct collation sensitive DVD for the character types. So, the return value of this method determines if we are going to create a character DVD with default collation or with custom collation. Get a SQLClob object to represent a SQL CLOB  (UCS_BASIC) with the given value. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new SQLLongvarchar will be created and set to the value. Get a StringDataValue to represent a SQL LONG VARCHAR with the passed in collationType. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new StringDataValue will be created and set to the value. If collationType is equal to StringDataValue.COLLATION_TYPE_UCS_BASIC then the call is the equivalent of the overload without collationType. Get a SQLClob object to represent a SQL CLOB  (UCS_BASIC) with the given value. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new SQLLongvarchar will be created and set to the value. Get a StringDataValue to represent a SQL CLOB with the passed in collationType. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new StringDataValue will be created and set to the value. If collationType is equal to StringDataValue.COLLATION_TYPE_UCS_BASIC then the call is the equivalent of the overload without collationType. Get a SQL boolean with the given value.  The second arg re-uses the previous value, if non-null, as the data holder to return. Get a SQL TINYINT with the given value. Uses the previous value, if non-null, as the data holder to return. Get a SQL int with a char value.  A null argument means get a SQL null value.  Uses the previous value (if non-null) to hold the return value. Get a SQL double precision with the given value.  Uses the previous value, if non-null, as the data holder to return. Get a SQL real with the given value.  Uses the previous value, if non-null, as the data holder to return. Get a SQL int with the given value.  The second arg re-uses the previous value, if non-null, as the data holder to return. Get a SQL boolean with the given value.  A null argument means get a SQL null value.  The second arg  uses the previous value (if non-null) to hold the return value. Get a SQL TINYINT with the given value.  A null argument means get a SQL null value.  The second arg  uses the previous value (if non-null) to hold the return value. Get a SQL double precision with the given value.  A null argument means a SQL null value.  The second arg  uses the previous value (if non-null) to hold the return value. Get a SQL real with the given value.  A null argument means get a SQL null value.  The second arg  uses the previous value (if non-null) to hold the return value. Get a SQL int with the given value.  A null argument means get a SQL null value.  Uses the previous value (if non-null) to hold the return value. Get a SQL bigint with the given value.  A null argument means get a SQL null value.  The second arg  uses the previous value (if non-null) to hold the return value. Get a User-defined data value with the given value and type name. A null argument means get a SQL null value.  The second arg uses the previous value (if non-null) hold the return value. Get a SQL smallint with the given value.  A null argument means get a SQL null value.  The second arg  uses the previous value (if non-null) to hold the return value. Get a SQL date with the given value.  A null argument means get a SQL null value.  The second arg re-uses the previous value, if non-null, as the data holder to return. Get a SQL time with the given value.  A null argument means get a SQL null value.  The second arg re-uses the previous value, if non-null, as the data holder to return. Get a SQL timestamp with the given value.  A null argument means get a SQL null value.  The second arg re-uses the previous value, if non-null, as the data holder to return. Get a SQL bigint with the given value.  The second arg re-uses the previous value, if non-null, as the data holder to return. Get a RefDataValue with the given value.  A null argument means get a SQL null value.  Uses the previous value (if non-null) to hold the return value. Get a SQL SMALLINT with the given value.  Uses the previous value, if non-null, as the data holder to return. Implements the SQL date function  Get a SQL DECIMAL with the given value. Uses the previous value, if non-null, as the data holder to return. ------ LONGVARBIT Get a SQL Long Bit Varying with the given value.  A null argument means get a SQL null value.  Uses the previous value (if non-null) to hold the return value. Get a SQL long varchar with the given value.  A null argument means get a SQL null value. Get a SQLLongvarchar object to represent a SQL LONG VARCHAR  (UCS_BASIC) with the given value. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new SQLLongvarchar will be created and set to the value. Get a StringDataValue to represent a SQL LONG VARCHAR with the passed in collationType. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new StringDataValue will be created and set to the value. If collationType is equal to StringDataValue.COLLATION_TYPE_UCS_BASIC then the call is the equivalent of the overload without collationType. Return an object based on the format id and collation type. For format ids which do not correspond to character types, a format id is sufficient to get the right DVD. But for character types, Derby uses same format id for collation sensitive character types and for character types that use the default JVM collation. To get the correct DVD for character types, we need to know the collation type. Using collation type, we will determine if we need to construct collation sensitive DVD and associate the correct RuleBasedCollator with such DVDs. Get a SQL Bit with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. --- BLOB Get a SQL Blob with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL boolean with  a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL tinyint with  a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. ------ CHAR Get a SQL CHAR (UCS_BASIC) with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL CHAR set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL CLOB (UCS_BASIC) with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL CLOB set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL date with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL Decimal/Numeric with  a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL double with  a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL float with  a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL int with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL bigint with  a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. --- LONGVARBIT Get a SQL Long Bit Varying with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL LONG VARCHAR (UCS_BASIC) with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL LONG VARCHAR set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a User-defined data value with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a RefDataValue with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL smallint with  a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL time with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL timestamp with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL Bit Varying with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL VARCHAR (UCS_BASIC) with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL VARCHAR set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get an XML with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value.  Implement the timestamp SQL function: construct a SQL timestamp from a string, or timestamp. Construct a SQL timestamp from a date and time.  Get a SQL bit with the given value.  Uses the previous value, if non-null, as the data holder to return. Get a SQL varchar with the given value.  A null argument means get a SQL null value. Get a SQLVarhar object to represent a SQL VARCHAR  (UCS_BASIC) with the given value. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new SQLVarchar will be created and set to the value. Get a StringDataValue to represent a SQL VARCHAR with the passed in collationType. A null argument means get a SQL NULL value. If previous is not null (Java reference) then it will be set to the value passed in and returned, otherwise a new StringDataValue will be created and set to the value. If collationType is equal to StringDataValue.COLLATION_TYPE_UCS_BASIC then the call is the equivalent of the overload without collationType. Get a null XML value. Uses the previous value, if non-null, as the data holder to return.
* ModuleControl methods. (non-Javadoc) @see org.apache.derby.iapi.services.monitor.ModuleControl#boot(boolean, java.util.Properties) BLOB CHAR Return a StringDataValue to represent a SQL CHAR with the given collation re-using previous if not null.  Return a StringDataValue to represent a SQL CLOB with the given collation re-using previous if not null. Privileged lookup of a Context. Package protected so that user code can't call this entry point.  Implement the date SQL function: construct a SQL date from a string, number, or timestamp. end of getDateValue( String dateStr) RESOLVE: This is here to find the LocaleFinder (i.e. the Database) on first access. This is necessary because the Monitor can't find the Database at boot time, because the Database is not done booting. See LanguageConnectionFactory. LONGVARBIT Return a StringDataValue to represent a SQL LONG VARCHAR with the given collation re-using previous if not null. Privileged Monitor lookup. Must be private so that user code can't call this entry point.  / BLOB CHAR Get a SQL CHAR set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL CLOB set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. This method will return a DVD based on the formatId. It doesn't take into account the collation that should be associated with collation sensitive DVDs, which are all the character type DVDs. Such DVDs returned from this method have default UCS_BASIC collation associated with them. If collation associated should be territory based, then that needs to be handled by the caller of this method. An example of such code in the caller can be seen in DataValueFactory.getNull method. LONGVARBIT Get a SQL LONG VARCHAR set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. Get a SQL VARCHAR set to NULL with collation set to collationType. If the supplied value is null then get a new value, otherwise set it to null and return that value. getNullXML: Get an XML with a SQL null value. If the supplied value is null then get a new value, otherwise set it to null and return that value. end of getTimeValue( String timeStr) Implement the timestamp SQL function: construct a SQL timestamp from a string, or timestamp. end of getTimestampValue( String timestampStr) Return a StringDataValue to represent a SQL VARCHAR with the given collation re-using previous if not null. getXMLDataValue: Get a null XML  value.  If a non-null XMLDataValue is received then re-use that instance, otherwise create a new one. (non-Javadoc) @see org.apache.derby.iapi.services.monitor.ModuleControl#stop() Verify that JVM has support for the Collator for the database's locale.
Create string to pass to DataSource.setConnectionAttributes Database close does following cleanup tasks 1)Rollback any pending transaction on the Connection object (except for a global-XA Connection obejct) before closing the Connection. Without the rollback, the Connection close will result into an exception if there is a pending transaction on that Connection. 2)Clean up the statement table Get the connection Get current DRDA statement Get DRDA statement based on pkgnamcsn Get default statement for use in EXCIMM Get default statement for use in EXCIMM with specified pkgnamcsn The pkgnamcsn has the encoded isolation level Get a reference (handle) to the PiggyBackedSessionData object. Null will be returned either if Database.conn is not a valid connection, or if the create argument is false and no object has yet been created. Make a new connection using the database name and set the connection in the database This makes a dummy connection to the database in order to boot and/or create this last one. If database cannot be found or authentication does not succeed, this will throw a SQLException which we catch and do nothing. We don't pass a userid and password here as we don't need to for the purpose of this method - main goal is to cause the database to be booted via a dummy connection. Get a new DRDA statement and store it in the stmtTable if stortStmt is true. If possible recycle an existing statement. When the server gets a new statement with a previously used pkgnamcsn, it means that client-side statement associated with this pkgnamcsn has been closed. In this case, server can re-use the DRDAStatement by doing the following: 1) Retrieve the old DRDAStatement associated with this pkgnamcsn and close it. 2) Reset the DRDAStatement state for re-use. This method resets the state of this Database object so that it can be re-used. Note: currently this method resets the variables related to security mechanisms that have been investigated as needing a reset. TODO: Investigate what all variables in this class need to be reset when this database object is re-used on a connection pooling or transaction pooling. see DRDAConnThread.parseACCSEC (CodePoint.RDBNAM) where database object is re-used on a connection reset. Set connection and create the SQL statement for the default statement Make statement the current statement Take database name including attributes and set attrString and shortDbName accordingly. Set the internal isolation level to use for preparing statements. Subsequent prepares will use this isoalation level Store DRDA prepared statement Checks whether database can support locators.  This is done by checking whether one of the stored procedures needed for locators exists.  (If the database has been soft-upgraded from an earlier version, the procedures will not exist).

* Public methods of ModuleControl Privileged Monitor lookup. Must be private so that user code can't call this entry point. *	Public methods of ClassFactory Here we load the newly added class now, rather than waiting for the findGeneratedClass(). Thus we are assuming that the class is going to be used sometime soon. Delaying the load would mean storing the class data in a file, this wastes cycles and compilcates the cleanup. * Class specific methods * Keep track of loaded generated classes and their GeneratedClass objects. For creating the class inspector. Notify the class manager that the classpath has been modified.

Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Privileged Monitor lookup. Must be private so that user code can't call this entry point.
Return true if this DatabaseInstant equals DatabaseInstant from the same database. Return true if this DatabaseInstant is before another DatabaseInstant from the same database. Return the next higher DatabaseInstant. There is no requirement that a transaction with the next instant exist in the database. It is required that this.lessThan( this.next()), and that no instant can be between this and this.next(). If the DatabaseInstant is implemented using a integer then next() should return a new DatabaseInstant formed by adding one to the integer. Return the next lower DatabaseInstant. There is no requirement that a transaction with the next instant exist in the database. It is required that this.prior().lessThan( this), and that no instant can be between this and this.prior(). If the DatabaseInstant is implemented using a integer then prior() should return a new DatabaseInstant formed by subtracting one from the integer. Convert the database instant to a string. This is mainly used for debugging.
Checks two DatabasePermission objects for equality. <P> Checks that <i>obj</i> is a DatabasePermission and has the same canonizalized URL and actions as this object. <P> Returns the "canonical string representation" of the actions. Returns the hash code value for this object. Checks if this DatabasePermission implies a specified permission. <P> This method returns true if:<p> <ul> <li> <i>p</i> is an instanceof DatabasePermission and<p> <li> <i>p</i>'s directory pathname is implied by this object's pathname. For example, "/tmp/*" implies "/tmp/foo", since "/tmp/*" encompasses the "/tmp" directory and all files in that directory, including the one named "foo". </ul> Parses the list of database actions. Parses the database location URL. Called upon Deserialization for restoring the state of this DatabasePermission from a stream. Called upon Serialization for saving the state of this DatabasePermission to a stream.
pick quantity number of rows randomly and delete them get a random set of row ids
********************************************************* ********** Output converters (byte[] -> class) ********** ********************************************************* Expected character representation is DERBY string representation of a date, which is in JIS format: <code> yyyy-mm-dd </code> ********************************************************* ******* CROSS output converters (byte[] -> class) ******* ********************************************************* Expected character representation is DERBY string representation of a date which is in JIS format: <code> yyyy-mm-dd </code> ******************************************************** ********** Input converters (class -> byte[]) ********** ******************************************************** Date is converted to a char representation in JDBC date format: <code>yyyy-mm-dd</code> date format and then converted to bytes using UTF8 encoding Return a clean (i.e. all values cleared out) Calendar object that can be used for creating Time, Timestamp, and Date objects. If the received Calendar object is non-null, then just clear that and return it. ********************************************************* ******* CROSS input converters (class -> byte[]) ******** ********************************************************* Return the length of a timestamp depending on whether timestamps should have full nanosecond precision or be truncated to just microseconds. java.sql.Timestamp is converted to a character representation which is a DERBY string representation of a timestamp converted to bytes using UTF8 encoding. For Derby 10.6 and above, this is <code>yyyy-mm-dd-hh.mm.ss.fffffffff</code>. For Derby 10.5 and below, this is <code>yyyy-mm-dd-hh.mm.ss.ffffff</code>. See DERBY-2602. and then converted to bytes using UTF8 encoding Parse a String of the form <code>yyyy-mm-dd-hh.mm.ss.ffffff[fff]</code> and store the various fields into the received Calendar object. Expected character representation is DERBY string representation of time, which is in the format: <code> hh.mm.ss </code> Expected character representation is DERBY string representation of time which is in the format: <code> hh.mm.ss </code> java.sql.Time is converted to character representation which is in JDBC time escape format: <code>hh:mm:ss</code>, which is the same as JIS time format in DERBY string representation of a time.  The char representation is converted to bytes using UTF8 encoding. See getTimestampLength() for an explanation of how timestamps are formatted. See getTimestampLength() for an explanation of how timestamps are formatted. See getTimestampLength() for an explanation of how timestamps are formatted. See getTimestampLength() for an explanation of how timestamps are formatted.
Get the day of the month. Get the hour of the day out of a time or timestamp. Get the minute of the hour out of a time or timestamp. Get the month number out of a date. Get the second of the minute out of a time or timestamp. Get the year number out of a date. Add a number of intervals to a datetime value. Implements the JDBC escape TIMESTAMPADD function. Finds the difference between two datetime values as a number of intervals. Implements the JDBC TIMESTAMPDIFF escape function.
Check that we are at the end of the string: that the rest of the characters, if any, are blanks. end of checkEnd   Determine if the next characters are one of a choice of strings. end of parseChoice Parse the next integer. end of parseInt end of updateCurrentSeparator
Get day of month component. First day of the month is 1. Get hour of day component (24 hour clock). Get minute component. Get the month component. First month is 0 ({@code Calendar.JANUARY}). Get nanosecond component. Get second component. Get the year component. Set the time of a calendar.
Tell whether this type (date) is compatible with the given type. TypeCompiler methods User types are convertible to other user types only if (for now) they are the same type and are being used to implement some JDBC type.  This is sufficient for date/time types; it may be generalized later for e.g. comparison of any user type with one of its subtypes.    User types are storable into other user types that they are assignable to. The other type must be a subclass of this type, or implement this type as one of its interfaces. Built-in types are also storable into user types when the built-in type's corresponding Java type is assignable to the user type.
Get an InputStream for reading a resource. Get the URL for a resource. Read the current generation of a file stored in the database we are connected to and return a 1 line string representation of the file. Sample usage values org.apache.derbyTesting.functionTests.util.DbFile::readAsString('S1','J1'); CANT USE JarAccess - not a public API (actually it's gone!) public static String readAsString(String schemaName, String sqlName) throws Exception { InputStream is = JarAccess.getAsStream(schemaName, sqlName, FileResource.CURRENT_GENERATION_ID); return stringFromFile(is); } Create a string that contains a representation of the content of a file for testing.
The main database setup method end of method doIt() ** This method abstracts exception message printing for all exception messages. You may want to change ****it if more detailed exception messages are desired. ***Method is synchronized so that the output file will contain sensible stack traces that are not ****mixed but rather one exception printed at a time
Iterates through a stack of SQLExceptions Will give the information about the size of the database in regular intervals periodically compresses the table to get back the free spaces available after the deletion of some rows END errorPrint jdbcLoad - Create url, schema and set driver and database system property that will be use later in the test.
Add a row for each iteration  Delete one row from the table. The row to be deleted is chosen randomly using the pick_one method which randomly returns a number between the max of serialkey and the minimum serialkey value that is untouched (nstest.NUM_UNTOUCHED_ROWS)  end of method delete_one()  get a random serialkey value that matches the criteria: - should not be one of the "protected" rows (set by nstest.NUM_UNTOUCHED_ROWS) - should be less than the current value of the max(serialkey)  of method pick_one(...) ** This method abstracts exception message printing for all exception messages. You may want to change ****it if more detailed exception messages are desired. ***Method is synchronized so that the output file will contain sensible stack traces that are not ****mixed but rather one exception printed at a time Update a random row. This method is common to all the worker threads
Build an exception that describes a deadlock. Privileged lookup of a Context. Must be package protected so that user code can't call this entry point. Get all the waiters in a {@code LockTable}. The waiters are returned as pairs (space, lock) mapping waiting compatibility spaces to the lock request in which they are blocked, and (lock, prevLock) linking a lock request with the lock request that's behind it in the queue of waiters. Handle a deadlock when it has been detected. Find out if the waiter that started looking for the deadlock is involved in it. If it isn't, pick a victim among the waiters that are involved. <p> Look for a deadlock. </p> <p> Walk through the graph of all locks and search for cycles among the waiting lock requests which would indicate a deadlock. A simple deadlock cycle is where the granted locks of waiting compatibility space A is blocking compatibility space B and space B holds locks causing space A to wait. </p> <p> MT - if the <code>LockTable</code> is a <code>LockSet</code> object, the callers must be synchronized on the <code>LockSet</code> object in order to satisfy the synchronization requirements of <code>LockSet.addWaiters()</code>. If it is a <code>ConcurrentLockSet</code> object, the callers must not hold any of the <code>ReentrantLock</code>s guarding the entries in the lock table, and the callers must make sure that only a single thread calls <code>look()</code> at a time. </p> Backtrack in the depth-first search through the wait graph. Expect the top of the stack to hold the compatibility space we've just investigated. Pop the stack until the most recently examined granted lock has been removed.


--------------entry points for runtime representation----------------------- Write a Java <code>java.math.BigDecimal</code> to packed decimal bytes. Compute the int array of magnitude from input value segments. --------------entry points for runtime representation----------------------- Build a <code>java.math.BigDecimal</code> from a fixed point decimal byte representation. Build a Java <code>double</code> from a fixed point decimal byte representation. Build a Java <code>long</code> from a fixed point decimal byte representation. --------------------------private helper methods---------------------------- Convert a range of packed nybbles (up to 9 digits without overflow) to an int. Note that for performance purpose, it does not do array-out-of-bound checking. Convert a range of packed nybbles (up to 18 digits without overflow) to a long. Note that for performance purpose, it does not do array-out-of-bound checking.
Read this object from a stream of stored objects. Write this object to a stream of stored objects.
Decorate a test (or suite of tests) to use a single use database as the default database with a specified set connection attributes. Decorate a set of tests to use an encrypted single use database. This is to run tests using encryption as a general test and not specific tests of how encryption is handled. E.g. tests of setting various URL attributes would be handled in a specific test. <BR> The database will use the default encryption algorithm. <BR> A boot password (phrase) is used with a random set of characters and digits 16 characters long. Decorate a set of tests to use an encrypted single use database. This is to run tests using encryption as a general test and not specific tests of how encryption is handled. E.g. tests of setting various URL attributes would be handled in a specific test. <BR> The database will use the specified encryption algorithm. <BR> A boot password (phrase) is used with a random set of characters and digits 64 characters long. Decorate a set of tests to use an encrypted single use database. This is to run tests using encryption as a general test and not specific tests of how encryption is handled. E.g. tests of setting various URL attributes would be handled in a specific test. <BR> The database will use the default encryption algorithm. Decorate a set of tests to use an encrypted single use database. This is to run tests using encryption as a general test and not specific tests of how encryption is handled. E.g. tests of setting various URL attributes would be handled in a specific test. <BR> The database will use the specified encryption algorithm. Decorate a set of tests to use a single use database with logDevice pointing a log directory to non-default location Decorate a set of tests to use an single use database with TERRITORY_BASED:SECONDARY collation set to the passed in locale. Decorate a set of tests to use an single use database with TERRITORY_BASED collation set to the passed in locale.

This method is used to calculate the decryption token. DES encrypts the data using a token and the generated shared private key. The token used depends on the type of security mechanism being used: USRENCPWD - The userid is used as the token. The USRID is zero-padded to 8 bytes if less than 8 bytes or truncated to 8 bytes if greater than 8 bytes. EUSRIDPWD - The middle 8 bytes of the server's connection key is used as the token.  Decryption needs to use exactly the same token as encryption. This method generates a secret key using the application requester's public key, and decrypts the usreid/password with the middle 8 bytes of the generated secret key and a decryption token. Then it returns the decrypted data in a byte array. This method generates an 8-Byte random seed. Required for the SECMEC_USRSSBPWD security mechanism This method generates the public key and returns it. This shared public key is the application server's connection key and will be exchanged with the application requester's connection key. This connection key will be put in the sectkn in ACCSECRD command and send to the application requester. Convert a string into a byte array in hex format. <BR> For each character (b) two bytes are generated, the first byte represents the high nibble (4 bits) in hexidecimal (<code>b &amp; 0xf0</code>), the second byte represents the low nibble (<code>b &amp; 0x0f</code>). <BR> The character at <code>str.charAt(0)</code> is represented by the first two bytes in the returned String. Convert a byte array to a String with a hexidecimal format. The String may be converted back to a byte array using fromHexString. <BR> For each byte (b) two characaters are generated, the first character represents the high nibble (4 bits) in hexidecimal (<code>b &amp; 0xf0</code>), the second character represents the low nibble (<code>b &amp; 0x0f</code>). <BR> The byte at <code>data[offset]</code> is represented by the first two characters in the returned String.
Get the provider's type. Get the column number of the column. //////////////////////////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////////////////////////////  Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Get the UUID of the table. Get the UUID of the default. ////////////////////////////////////////////////////  DEPENDENT INTERFACE  //////////////////////////////////////////////////// Check that all of the dependent's dependencies are valid. Mark the dependent as invalid (due to at least one of its dependencies being invalid).  Always an error for a constraint -- should never have gotten here. Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). Convert the DefaultDescriptor to a String.
Get the text of a default. Return the name of the current schema when the default was created. This is filled in for generated columns. If this default is a generation clause, then return the names of other columns in the row which the generation clause references. Is default value generated by auto increment? ToCleanUp Additional definitive information of AutoIncremnt such as autoIncrementStart and autoInrementInc should be gotten from this interface. Return true if this is the generation clause for a generated column.
This function returns stored value for flags and so on.  Get the default value. (NOTE: This returns null if the default is not a constant.)   Get the formatID which corresponds to this class.   Formatable methods Read this object from a stream of stored objects. Set the default value. Write this object to a stream of stored objects.
used in urlLocale test and messageLocale test used in messageLocale test used in urlLocale test used in messageLocale test
Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. In this case, there are no sub-expressions, and the return type is already known, so this is just a stub.  Get the text of the default. Get the query tree for the default.  Parse a default and turn it into a query tree. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Append a string to the optimizer trace //////////////////////////////////////////////////////////////////////  REPORTING MINIONS  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  OptTrace BEHAVIOR  //////////////////////////////////////////////////////////////////////
end of columnRequiresDefer  end of subselectRequiresDefer( statementType, VTIClassName)  end of subselectRequiresDefer( statementType, schemaName, tableName)
This method is called during preparation of an insert, update, or delete statement with this VTI as the target. It indicates whether the statement should be deferred irregardless of the other clauses in the statement. If alwaysDefer returns <b>true</b> then the other methods in this interface are not called. (At least not for this statement type). This method is called during preparation of an update or delete statement on the virtual table if getResultSetType() returns ResultSet.TYPE_SCROLL_SENSITIVE or TYPE_SCROLL_SENSITIVE and alwaysDefer( statementType) returns <b>false</b>. ColumnRequiresDefer is called once for each column that is being updated, or each column in a DELETE where clause until it returns <b>true</b> or until all the columns have been exhausted. This VTI method is called by Derby when a VTI modification (insert, update, or delete) is executed. It is called after the VTI has been instantiated but before any rows are read, inserted, updated, or deleted. This method is called during preparation of an insert, update, or delete statement that has this virtual table as its target and that has a sub-select. It is invoked once for each virtual table in the sub-select, if it has not already been determined that the statement should be deferred or that the VTI does not support deferral. This method is called during preparation of an insert, update, or delete statement that has this virtual table as its target and that has a sub-select. It is invoked once for each regular table in a sub-select, if it has not already been determined that the statement should be deferred or that the VTI does not support deferral.
Save the row location of an offending row (one or more check constraints were violated) in a hash table (which may spill to disk) for later checking, typically on transaction commit, or upon request. The row locations are subject to invalidation, cf. {@code CheckInfo#setInvalidatedRowLocations}. Save the contents of an constraint supporting index row in a hash table (which may spill to disk) for later checking, typically on transaction commit, or upon request. Make note of a violated foreign key constraint, i.e. the referenced key is not present
check whether we have mutiple path delete scenario, if * find any retun true.	Multiple delete paths exist if we find more than * one parent source resultset for a table involved in the delete cascade if there is more than one node for the same table, copy the rows into one node , so that we don't fire trigger more than once.  *Incases where we have multiple paths we could get the same *rows to be deleted  mutiple time and also in case of cycles *there might be new rows getting added to the row holders through *multiple iterations. To handle these case we set the temporary row holders * to be  'uniqStream' type. Gathers the rows that needs to be deleted/updated and creates a temporary resulsets that will be passed as source to its  dependent result sets.
Get the formatID which corresponds to this class. INTERFACE METHODS Formatable methods
Bind this DeleteNode.  This means looking up tables and columns and getting their types, and figuring out the result types of all expressions, as well as doing view resolution, permissions checking, etc. <p> If any indexes need to be updated, we add all the columns in the base table to the result column list, so that we can use the column values as look-up keys for the index rows to be deleted.  Binding a delete will also massage the tree so that the ResultSetNode has column containing the RowLocation of the base row. end of bind Force column references (particularly those added by the compiler) to use the correlation name on the base table, if any. Code generation for delete. The generated code will contain: o  A static member for the (xxx)ResultSet with the RowLocations o  The static member will be assigned the appropriate ResultSet within the nested calls to get the ResultSets.  (The appropriate cast to the (xxx)ResultSet will be generated.) o  The CurrentRowLocation() in SelectNode's select list will generate a new method for returning the RowLocation as well as a call to that method which will be stuffed in the call to the ProjectRestrictResultSet. o In case of referential actions, this function generate an array of resultsets on its dependent tables. Builds a bitmap of all columns which should be read from the Store in order to satisfy an DELETE statement. 1)	finds all indices on this table 2)	adds the index columns to a bitmap of affected columns 3)	adds the index descriptors to a list of conglomerate descriptors. 4)	finds all DELETE triggers on the table 5)	if there are any DELETE triggers, then do one of the following a)If all of the triggers have MISSING referencing clause, then that means that the trigger actions do not have access to before and after values. In that case, there is no need to blanketly decide to include all the columns in the read map just because there are triggers defined on the table. b)Since one/more triggers have REFERENCING clause on them, get all the columns because we don't know what the user will ultimately reference. 6)	adds the triggers to an evolving list of triggers In case of referential actions, we require to perform DML (UPDATE or DELETE) on the dependent tables. Following function returns the DML Node for the dependent table. Gets the map of all columns which must be read out of the base table. These are the columns needed to: o	maintain indices o	maintain foreign keys The returned map is a FormatableBitSet with 1 bit for each column in the table plus an extra, unsued 0-bit. If a 1-based column id must be read from the base table, then the corresponding 1-based bit is turned ON in the returned FormatableBitSet. Return the type of statement, something from StatementType. Compile constants that Execution will use Return true if the node references SESSION schema tables (temporary or permanent)
Loggable methods Mark the record as deleted on the page. methods to support prepared log the following two methods should not be called during recover Return my format identifier. Read this in LogicalUndoable methods Restore the row stored in the optional data of the log record. PageBasicOperation method to support BeforeImageLogging restore the before image of the page Undoable methods Mark the record as not deleted, and then fix up the in-memory copy of the page. All logical undo logic has already been taken care of by generateUndo. Write this out. if logical undo, writes out the row that was deleted
create a source for the dependent table <P>Delete Cascade ResultSet class will override this method. delete the rows that in case deferred case and during cascade delete (All deletes are deferred during cascade action) execute the after triggers set on the table. execute the before triggers set on the table  Make sure foreign key constraints are not violated this routine open the source and find the dependent rows
end of openCore

Get the unique class id for the Dependable. Every Dependable belongs to a class of Dependables. Get an object which can be written to disk and which, when read from disk, will find or reconstruct this in-memory Dependable. Get the UUID of this Dependable OBJECT. Get the name of this Dependable OBJECT. This is useful for diagnostic messages. Return whether or not this Dependable is persistent. Persistent dependencies are stored in SYS.SYSDEPENDS.
Get the in-memory object associated with the passed-in object ID. The name of the class of Dependables as a "SQL Object" which this Finder can find. This is a value like "Table" or "View". Every DependableFinder can find some class of Dependables.
return the dependent for this dependency. return the provider for this dependency. return the provider's key for this dependency.
Get the dependent's type for the dependency. Get the provider's type for the dependency. Get the provider's ID for the dependency. DependencyDescriptor interface Get the dependent's ID for the dependency.
adds a dependency from the dependent on the provider. This will be considered to be the default type of dependency, when dependency types show up. <p> Implementations of addDependency should be fast -- performing alot of extra actions to add a dependency would be a detriment. Clear the in memory column bit map information in any table descriptor provider in a provider list.  This function needs to be called before the table descriptor is reused as provider in column dependency.  For example, this happens in "create publication" statement with target-only DDL where more than one views are defined and they all reference one table. Erases all of the dependencies the dependent has, be they valid or invalid, of any dependency type.  This action is usually performed as the first step in revalidating a dependent; it first erases all the old dependencies, then revalidates itself generating a list of new dependencies, and then marks itself valid if all its new dependencies are valid. <p> There might be a future want to clear all dependencies for a particular provider, e.g. when destroying the provider. However, at present, they are assumed to stick around and it is the responsibility of the dependent to erase them when revalidating against the new version of the provider. <p> clearDependencies will delete dependencies if they are stored; the delete is finalized at the next commit. Erases all of the dependencies the dependent has, be they valid or invalid, of any dependency type.  This action is usually performed as the first step in revalidating a dependent; it first erases all the old dependencies, then revalidates itself generating a list of new dependencies, and then marks itself valid if all its new dependencies are valid. <p> There might be a future want to clear all dependencies for a particular provider, e.g. when destroying the provider. However, at present, they are assumed to stick around and it is the responsibility of the dependent to erase them when revalidating against the new version of the provider. <p> clearDependencies will delete dependencies if they are stored; the delete is finalized at the next commit. Clear the specified in memory dependency. This is useful for clean-up when an exception occurs. (We clear all in-memory dependencies added in the current StatementContext.) This method will handle Dependency's that have already been removed from the DependencyManager. Copy dependencies from one dependent to another. Copy dependencies from one dependent to another. Count the number of active dependencies, both stored and in memory, in the system. Returns a string representation of the SQL action, hence no need to internationalize, which is causing the invokation of the Dependency Manager. Get a new array of ProviderInfos representing all the persistent providers for the given dependent. Get a new array of ProviderInfos representing all the persistent providers from the given list of providers. mark all dependencies on the named provider as invalid. When invalidation types show up, this will use the default invalidation type. The dependencies will still exist once they are marked invalid; clearDependencies should be used to remove dependencies that a dependent has or provider gives. <p> Implementations of this can take a little time, but are not really expected to recompile things against any changes made to the provider that caused the invalidation. The dependency system makes no guarantees about the state of the provider -- implementations can call this before or after actually changing the provider to its new state. <p> Implementations should throw DependencyStatementException if the invalidation should be disallowed.
Check that all of the dependent's dependencies are valid. Mark the dependent as invalid (due to at least one of its dependencies being invalid). Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid).
Can we get instantaneous locks when getting share row locks at READ COMMITTED. Close the all the opens we did in this result set. Fetch the base row corresponding to the current index row Fetch a row from the index scan. this function will return the rows from the parent result sets this function will return an index row on dependent table Cursor result set information. * Open the heap Conglomerate controller * * @param transaction controller will open one if null Get a scan controller positioned using searchRow as the start/stop position.  The assumption is that searchRow is of the same format as the index being opened. Return a start or stop positioner as a String. If we already generated the information, then use that.  Otherwise, invoke the activation to get it. Return an array of Qualifiers as a String reopen the scan with a differnt search row * Do reference copy for the qualifier row.  No cloning. * So we cannot get another row until we are done with * this one.
Returns true if the derby client is available to the tests. Returns true if the embedded engine is available to the tests. Returns true if the network server is available to the tests. Returns true if the tools are available to the tests.
<p> Trailing garbage after the credentials db name should produce a useful error message. </p> /////////////////////////////////////////////////////////////////////////////////  TESTS  ///////////////////////////////////////////////////////////////////////////////// <p> Trailing garbage after the credentials db name should produce a useful error message instead of an assertion failure. </p> /////////////////////////////////////////////////////////////////////////////////  JUnit BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Construct top level suite in this JUnit test
Test the original user report of this issue: <p> the issue can be reproduced 1. create table myTbl1 (name varchar(1000)); 2. create table myTbl2 (name varchar(1000)); 3. create view myView (name) as select t1.name from myTbl1 t1 union all select t2.name from myTbl2 t2; 4. select name from myView where upper(name) in ('AA', 'BB'); #4 failed with "org.apache.derby.impl.sql.compile.SimpleStringOperatorNode incompatible with org.apache.derby.impl.sql.compile.ColumnReference: java.lang.ClassCastException" If the view is created as "create myView (name) as select t1.name from myTbl1 t1", the query worked fine. <p> Test the original DERBY-6131 queries with some data to make sure results look right in addtion to not getting an exception. <p>
Construct top level suite in this JUnit test Make sure we get correct db name for different databases



Orders this distribution and the other distribution based on the version. Merges a list of JAR files into a classpath string. Returns the path to {@code derbyclient.jar}. Returns the path to {@code derby.jar}. Returns the path to {@code derbynet.jar}. Returns the path to {@code derbyrun.jar}. Returns a classpath with all production and testing JARs. Returns the absolute path to the JAR if it exists, otherwise null. Returns a classpath with all production JARs. Helper method extracting Derby production JARs from a directory. Returns a classpath with the network server production JARs. Returns a classpath with all testing JARs. Helper method extracting Derby testing JARs from a directory. Returns the version of this distribution. Tells if this distribution has a {@code derbyrun.jar}. Tells if the given file is a Derby testing JAR. <p> Returns a distribution with the specified version, based on the given library directory. </p> <p> It is the responsibility of the caller to ensure that the specified version matches the JARs in the given directory. </p> <p> Returns a distribution with the specified version, based on the given library and testing directories. </p> <p> It is the responsibility of the caller to ensure that the specified version matches the JARs in the given directories. </p>
For internal use only. createTmpFiles() create the temporary files with the build properties at the specified location. @exception	Exception if there is an error For internal use only. getProps() generates the required Properties from the DBMS.properties file. @exception	Exception if there is an error The public main() method to test the working of this class. A valid destination String is all that is needed as an argument for running this class. <p> example: java DerbyEclipsePlugin <destination> <p>

///////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////// Verify that we can read {@code length} bytes without hitting end of file (or end of the slice represented by this instance). Raise a Lucene error if this object has been closed ///////////////////////////////////////////////////////////////////  IndexInput METHODS  /////////////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////////////  DataInput METHODS  /////////////////////////////////////////////////////////////////// Set the constructor fields Wrap an exception in a Runtime exception
///////////////////////////////////////////////////////////////////  IndexOutput METHODS  /////////////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////// Wrap an exception in a Runtime exception ///////////////////////////////////////////////////////////////////  DataOutput METHODS  ///////////////////////////////////////////////////////////////////

/////////////////////////////////////////////////////////////////////////////////  FUNCTION ENTRY POINT (BOUND BY THE CREATE FUNCTION STATEMENT)  ///////////////////////////////////////////////////////////////////////////////// Create from an file name identifying the server log file /////////////////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> JIRA prepends "DERBY-" to the issue key. Strip off this prefix so that we can sort the key as an integer value. </p>
Raise an exception if this directory is closed Clear the lock <p> Close this directory and remove it from the map of open directories. </p> Create a new, empty file for writing Create the path if necessary. Create the path if necessary. <p> Get the Derby directory backing this Lucene directory. </p> ///////////////////////////////////////////////////////////////////  CONSTRUCTOR AND FACTORY METHODS  /////////////////////////////////////////////////////////////////// <p> Lookup a directory, creating its path as necessary. </p> ///////////////////////////////////////////////////////////////////  WRAPPERS FOR StorageFactory METHODS  /////////////////////////////////////////////////////////////////// <p> Get a file in this directory. </p> ///////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////// Get a DerbyIndexInput on the named file Get the key associated with a directory Get the lock factory used by this Directory. Turn a file name into a StorageFile handle Make a lock Make an IOException with the given SQLState and args <p> Remove a directory from the map. </p> ///////////////////////////////////////////////////////////////////  FOR USE WHEN CLOSING CHILD FILES  /////////////////////////////////////////////////////////////////// Remove the named file from the list of output files ///////////////////////////////////////////////////////////////////  Directory METHODS  /////////////////////////////////////////////////////////////////// Set the lock factory used by this Directory.

////////////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  //////////////////////////////////////////////////////////////////// Add another observer who wants to be told about changes to this object. Return the number of observers who are watching this object. Remove a specific observer from the list of watchers. Null is ignored. This method is equivalent to notifyObservers(null); If this object has changed, then notify all observers. Pass them this object and the extraInfo. This object is then marked as unchanged again. ////////////////////////////////////////////////////////////////////  PROTECTED BEHAVIOR TO BE CALLED BY SUBCLASSES  //////////////////////////////////////////////////////////////////// When the object state changes, the object calls this method in order to flag that fact. After this method has been called, then the notifyObservers() will wake up the observers which are watching this object.
This is the callback method which is invoked when a change happens to the object which is being observed.
Returns the shared instance. This method is called upon plug-in activation This method is called when the plug-in is stopped




another launch mechanism
Returns a simplified view of this version, where only the major and the minor versions are included. <p> Introduced for compatibility with existing/older test code. Checks if this version is at the same or higher level as the other version. Checks if this version is at the same major and minor level as the other version. Checks if this version is at the same major and minor level as the other version. Checks if this version is at the same or lower level as the other version. @Override Checks if this version is at a greater minor level than the other version. @Override Parses the given string as a Derby version. Checks if the major level of this version is the same as for the other version. @Override
Adds this class to the *existing server* suite. Test killing slave during replication.
Raise an exception if we are running with SQL authorization turned on but the current user isn't the database owner. This method is used to restrict access to VTIs which disclose sensitive information. See DERBY-5395. Privileged lookup of a Context. Must be private so that user code can't call this entry point.

Private/Protected methods of This class: Given an object return instance of the diagnostic object for this class. <p> Given an object this routine will determine the classname of the object and then try to instantiate a new instance of the diagnostic object for this class by prepending on "D_" to the last element of theclassname. If no matching class is found then the same lookup is made on the super-class of the object, looking all the way up the hierachy until a diagnostic class is found. <BR> This routine will call "init(ref)" on the new instance and then return the new instance. Return a diagnostic string associated with an object. <p> A utility interface to use if you just want to print a single string that represents the object in question.  In following order this routine will deliver the string to use: 1) find diagnostic help class, and use class.diag() 2) else just use class.toString() <p>
Default implementation of diagnostic on the object. <p> This routine returns a string with whatever diagnostic information you would like to provide about this associated object passed in the init() call. <p> This routine should be overriden by a real implementation of the diagnostic information you would like to provide. <p> Default implementation of detail diagnostic on the object. <p> This interface provides a way for an object to pass back pieces of information as requested by the caller.  The information is passed back and forth through the properties argument.  It is expected that the caller knows what kind of information to ask for, and correctly handles the situation when the diagnostic object can't provide the information. <p> As an example assume an object TABLE exists, and that we have created an object D_TABLE that knows how to return the number of pages in the TABLE object.  The code to get that information out would looks something like the following: <p> print_num_pages(Object table) { Properties prop = new Properties(); prop.put(Page.DIAG_NUM_PAGES,        ""); DiagnosticUtil.findDiagnostic(table).diag_detail(prop); System.out.println( "number of pages = " + prop.getProperty(Page.DIAG_NUM_PAGES)); } <p> This routine should be overriden if there is detail diagnostics to be provided by a real implementation. <p> * Methods of Diagnosticable
Default implementation of diagnostic on the object. <p> This routine returns a string with whatever diagnostic information you would like to provide about this object. <p> This routine should be overriden by a real implementation of the diagnostic information you would like to provide. <p> Default implementation of detail diagnostic on the object. <p> This routine should be overriden if there is detail diagnostics to be provided by a real implementation. <p> * Methods of Diagnosticable
Deletes the named file and, if it is a directory, all the files and directories it contains. end of deleteAll Get an exclusive lock. This is used to ensure that two or more JVMs do not open the same database at the same time. end of getExclusiveFileLock Creates an input stream from a file name. Creates an output stream from a file name. Creates an output stream from a file name. Get the name of the parent directory if this name includes a parent. Get a random access (read/write) file. end of getRandomAccessFile Release the resource associated with an earlier acquired exclusive lock End of releaseExclusiveFileLock Rename the file denoted by this name. Note that StorageFile objects are immutable. This method renames the underlying file, it does not change this StorageFile object. The StorageFile object denotes the same name as before, however the exists() method will return false after the renameTo method executes successfully. <p> It is not specified whether this method will succeed if a file already exists under the new name.
Clone this file abstaction Force any changes out to the persistent store.
end of doInit Construct a persistent StorageFile from a path name. Construct a persistent StorageFile from a directory and path name. Construct a persistent StorageFile from a directory and path name. Construct a StorageFile from a path name. Construct a StorageFile from a directory and file name. Construct a StorageFile from a directory and file name. Determine whether the storage supports random access. If random access is not supported then it will only be accessed using InputStreams and OutputStreams (if the database is writable). This method tests whether the "rws" and "rwd" modes are implemented. If the "rws" and "rwd" modes are supported then the database engine will conclude that the write methods of "rws"/"rwd" mode StorageRandomAccessFiles are slow but the sync method is fast and optimize accordingly. Force the data of an output stream out to the underlying storage. That is, ensure that it has been made persistent. If the database is to be transient, that is, if the database does not survive a restart, then the sync method implementation need not do anything.

Set the allocation status of pageNumber to doStatus.  To undo this operation, set the allocation status of pageNumber to undoStatus Chain one allocation page to the next. Compress free pages. <p> Compress the free pages at the end of the range maintained by this allocation page.  All pages being compressed should be FREE. Only pages in the last allocation page can be compressed. <p>

end of close Return an Enumeration that can be used to scan entire table. <p> RESOLVE - is it worth it to support this routine? Get a row from the overflow structure. Privileged lookup of a Context. Must be private so that user code can't call this entry point. end of getRemove Put a new row in the overflow structure. end of put remove all rows with a given key from the hash table. end of remove end of rowMatches
Returns the name of this taglet disk_layout not expected to be used in constructor documentation. disk_layout not expected to be used in field documentation. disk_layout not expected to be used in constructor documentation. disk_layout can be used in overview documentation. disk_layout can be used in package documentation. disk_layout can be used in type documentation. disk_layout is not an inline tag. Register this Taglet. Embed the contents of the disk_layout tag as a row in the disk format table. Close the table. Embed multiple disk_layout tags as cells in the disk format table. Close the table.
Display the result of a new order. New order terminal i/o is described in clause 2.4.3. May need more parameters. Display the result of an order status. Order status terminal i/o is decribed in clause 2.6.3. Display the result of a payment. Payment terminal i/o is described in clause 2.5.3. Display the result of a delivery schedule. Display the result of a stock level. Stock level terminal i/o is described in clause 2.8.3.
Asserts the number of statistics entries for all relevant tables. Creates and populates the test tables. Fetches all relevant statistics. Number of disposable statistics entries. Total number of possible statistics entries. <p> This number includes orphaned and unnecessary statistics, and these entries are expected to be purged out when running with the current/ newest version of Derby. Converts the list of statistics to an array. Returns the names of the tables used by this test. Tells if the old version is affected by the DERBY-5681 bug. <p> The bug is that Derby fails to drop a statistics entry for a foreign key constraint, leaving an orphaned and outdated entry behind.

generate the distinct result set operating over the source result set. Return whether or not the underlying ResultSet tree is ordered on the specified columns. RESOLVE - This method currently only considers the outermost table of the query block. Optimize this DistinctNode. Optimizable interface
If the result set has been opened, close the open scan. Close the source of whatever we have been scanning. RESOLVE - THIS NEXT METHOD IS ONLY INCLUDED BECAUSE OF A JIT ERROR. THERE IS NO OTHER REASON TO OVERRIDE IT IN DistinctScalarAggregateResultSet.  THE BUG WAS FOUND IN 1.1.6 WITH THE JIT. Return the next row.  If it is a scalar aggregate scan /////////////////////////////////////////////////////////////////////////////  SCAN ABSTRACTION UTILITIES  ///////////////////////////////////////////////////////////////////////////// Get a row from the sorter.  Side effects: sets currentRow. /////////////////////////////////////////////////////////////////////////////  MISC UTILITIES  ///////////////////////////////////////////////////////////////////////////// Load up the sorter.  Feed it every row from the source scan.  If we have a vector aggregate, initialize the aggregator for each source row.  When done, close the source scan and open the sort.  Return the sort scan controller. /////////////////////////////////////////////////////////////////////////////  ResultSet interface (leftover from NoPutResultSet)  ///////////////////////////////////////////////////////////////////////////// Open the scan.  Load the sorter and prepare to get rows from it. reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
ResultSet interface (override methods from HashScanResultSet)  Return the next row (if any) from the scan (if open).
Clear all information to allow object re-use.
Add all property names in the Properties object {@code src} to the HashSet {@code dest}.

Open a connection using JDBC attributes with a JDBC URL. The attributes user and password are set from the configuration and then the passed in attribute is set. Load the JDBC driver defined by the JDBCClient for the configuration. Open a connection using the DriverManager. <BR> The JDBC driver is only loaded if DriverManager.getDriver() for the JDBC URL throws an exception indicating no driver is loaded. <BR> If the connection request fails with SQLState XJ004 (database not found) then the connection is retried with attributes create=true. Open a connection using the DriverManager. <BR> The JDBC driver is only loaded if DriverManager.getDriver() for the JDBC URL throws an exception indicating no driver is loaded. <BR> If the connection request fails with SQLState XJ004 (database not found) then the connection is retried with attributes create=true. Shutdown the database using the attributes shutdown=true with the user and password defined by the configuration. Shutdown the engine using the attributes shutdown=true and no database name with the user and password defined by the configuration. Always shutsdown using the embedded URL thus this method will not work in a remote testing environment.
Deregister all Derby drivers accessible from the class loader in which this class lives.
Do the necessary checks to see if database is in consistent state cleanup resources.  Run OE load parse arguments. Populate the OE database. Assumption is that the schema is already loaded in the database. prints the usage
INTERFACE METHODS This is the guts of the Execution-time logic for DROP ALIAS. OBJECT SHADOWS
returns the alias type name given the alias char type Bind this DropMethodAliasNode. inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution.
INTERFACE METHODS This is the guts of the Execution-time logic for DROP CONSTRAINT. OBJECT METHODS
Remove all the files in the list Shutdown the database and then remove all of its files.
Pump a SYSDEPENDS row through the Filter. If the providerID of the row matches our providerID, we return true. Otherwise we return false. Gets a BooleanDataValue representing FALSE Gets a BooleanDataValue representing TRUE. ////////////////////////////////////////////////////////////////////////////////////  MINIONS  //////////////////////////////////////////////////////////////////////////////////// Get the UUID factory ////////////////////////////////////////////////////////////////////////////////////  TupleFilter BEHAVIOR  //////////////////////////////////////////////////////////////////////////////////// Initialize a Filter with a vector of parameters. This is a NOP. We initialize this filter at Constructor time.
INTERFACE METHODS This is the guts of the Execution-time logic for DROP INDEX. OBJECT METHODS
Bind this DropIndexNode.  This means looking up the index, verifying it exists and getting the conglomerate number. inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution.
Called when the transaction is about to complete.
INTERFACE METHODS This is the guts of the Execution-time logic for DROP ROLE. /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
INTERFACE METHODS This is the guts of the Execution-time logic for DROP TABLE. /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
INTERFACE METHODS This is the guts of the Execution-time logic for DROP SEQUENCE. /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
Bind this DropSequenceNode. inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution.

Drop the sequence generator backing an identity column INTERFACE METHODS This is the guts of the Execution-time logic for DROP TABLE. OBJECT METHODS
Bind this LockTableNode.  This means looking up the table, verifying it exists and getting the heap conglomerate number. inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution. Return true if the node references SESSION schema tables (temporary or permanent) Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
This is the guts of the Execution-time logic for DROP STATEMENT.
Bind this DropTriggerNode.  This means looking up the trigger, verifying it exists and getting its table uuid. inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution.
INTERFACE METHODS This is the guts of the Execution-time logic for DROP VIEW. OBJECT METHODS
Bind the drop view node inherit generate() method from DDLStatementNode Create the Constant information that will drive the guts of Execution.

Gets the int value of the two byte unsigned codepoint. Query if trace is on. This is currently needed since the comBufferTrcOn flag is private. Start the communications buffer trace. The name of the file to place the trace is passed to this method. After calling this method, calls to isComBufferTraceOn() will return true. Stop the communications buffer trace. The trace file is flushed and closed.  After calling this method, calls to isComBufferTraceOn () will return false. Write the communication buffer data to the trace. The data is passed in via a byte array.  The start and length of the data is given. The type is needed to indicate if the data is part of the send or receive buffer. The class name, method name, and trcPt number are also written to the trace. Not much checking is performed on the parameters.  This is done to help performance.
/////////////////////////////////////////////////////////////////////////////////  StringColumnVTI BEHAVIOR  /////////////////////////////////////////////////////////////////////////////////


//////////////////////////////////////////////////////////////////////  FUNCTION FOR RETRIEVING THE FULL TRACE  //////////////////////////////////////////////////////////////////////  Don't need to bother implementing the rest of the behavior.  //////////////////////////////////////////////////////////////////////  BEHAVIOR  //////////////////////////////////////////////////////////////////////  We only provide implementation for these methods.

Shrink the buffer left by the amount given. Ie. bytes from 0 to amountToShrinkBy are thrown away Expand the buffer by at least the number of bytes requested in minExtension. To optimize performance and reduce memory copies and allocation, we have a staged buffer expansion. <UL> <LI> buf.length &lt; 128k - increase by 4k <LI> buf.length &lt; 1Mb - increase by 128k <LI> otherwise increase by 1Mb. </UL> In all cases, if minExpansion is greater than the value about then the buffer will be increased by minExtension. Get the current position in the stream Get a reference to the byte array stored in the byte array output stream. Note that the byte array may be longer that getPosition(). Bytes beyond and including the current poistion are invalid. Get the current position in the stream Get the number of bytes that was used. Specific methods Reset the stream for reuse Set the begin position of the stream pointer. If the newBeginPosition is larger than the stream itself, then, the begin position is not set. Set the position of the stream pointer. It is up to the caller to make sure the stream has no gap of garbage in it or useful information is not left out at the end because the stream does not remember anything about the previous position. OutputStream methods

This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  Create a new EXTDTAInputStream from a CallableStatement. Create a new EXTDTAInputStream.  Before read the stream must be initialized by the user with {@link #initInputStream()}  This method takes information of ResultSet and initializes the binaryInputStream variable of this object with not empty stream by calling getBinaryStream or getCharacterStream() as appropriate. The Reader returned from getCharacterStream() will be encoded in binarystream. Is the value null?  Null status is obtained from the underlying EngineResultSet or LOB, so that it can be determined before the stream is retrieved.   Requires {@link #initInputStream()} be called before we can read from the stream
Interprets the Derby-specific status byte, and throws an exception if an error condition has been detected on the client. Returns the status byte. <p> <em>NOTE:</em> Check if the status byte has been set by calling {@linkplain #isStatusSet()}. Returns whether the status has been set or not. Performs necessary clean up when an error is signalled by the client. Saves the status byte read off the wire. Private for now, as the method is currently used only by checkStatus. Sets whether or not to suppress the exception when setting the status. Throws an exception as mandated by the EXTDTA status byte.

Lookup the Element subtree that starts with the specified tag. If more than one, or no such tags exist an IllegalArgumentException is thrown. Lookup the text (as String) identified by the specified tag. If more than one, or no such tags exist an IllegalArgumentException is thrown. Produce a list of the texts specified by the instances of tag in the wrapped tree. An empty list is retured if there are no instances of tag in the tree.
Checks if the pattern (starting from the second byte) appears inside the Blob content. <p> At this point, the first byte of the pattern must already have been matched, and {@code pos} must be pointing at the second byte to compare. Checks if the pattern (starting from the second byte) appears inside the Blob content. Checks is isValid is true. If it is not true throws a SQLException stating that a method has been called on an invalid LOB object throws SQLException if isvalid is not true. If we have a stream, release the resources associated with it.  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  ///////////////////////////////////////////////////////////////////////  JDBC 4.0	-	New public methods  /////////////////////////////////////////////////////////////////////// This method frees the <code>Blob</code> object and releases the resources that it holds. The object is invalid once the <code>free</code> method is called. If <code>free</code> is called multiple times, the subsequent calls to <code>free</code> are treated as a no-op. Retrieves the <code>BLOB</code> designated by this <code>Blob</code> instance as a stream. Returns an <code>InputStream</code> object that contains a partial <code>Blob</code> value, starting with the byte specified by pos, which is length bytes in length. Returns as an array of bytes part or all of the <code>BLOB</code> value that this <code>Blob</code> object designates.  The byte array contains up to <code>length</code> consecutive bytes starting at position <code>startPos</code>. The starting position must be between 1 and the length of the BLOB plus 1. This allows for zero-length BLOB values, from which only zero-length byte arrays can be returned. If a larger length is requested than there are bytes available, characters from the start position to the end of the BLOB are returned. Return locator for this lob. Convert exceptions where needed before calling handleException to convert them to SQLExceptions. Constructs a Blob object on top of a stream. Returns if blob data is stored locally (using LOBStreamControl). Returns the number of bytes in the <code>BLOB</code> value designated by this <code>Blob</code> object. PT stream part may get pushed to store Determines the byte position at which the specified byte <code>pattern</code> begins within the <code>BLOB</code> value that this <code>Blob</code> object represents.  The search for <code>pattern</code>. begins at position <code>start</code> Determines the byte position in the <code>BLOB</code> value designated by this <code>Blob</code> object at which <code>pattern</code> begins.  The search begins at position <code>start</code>. Reads one byte from the Blob at the specified position. <p> Depending on the representation, this might result in a read from a byte array, a temporary file on disk or from a Derby store stream. JDBC 3.0 Retrieves a stream that can be used to write to the BLOB value that this Blob object represents. The stream begins at position pos. Sets the position of the Blob to {@code logicalPos}, where position 0 is the beginning of the Blob content. <p> The position is only guaranteed to be valid from the time this method is invoked until the synchronization monitor is released, or until the next invokation of this method. <p> The position is logical in the sense that it specifies the requested position in the Blob content. This position might be at a different position in the underlying representation, for instance the Derby store stream prepends the Blob content with a length field. @GuardedBy(getConnectionSynchronization()) Following methods are for the new JDBC 3.0 methods in java.sql.Blob (see the JDBC 3.0 spec). We have the JDBC 3.0 methods in Local20 package, so we don't have to have a new class in Local30. The new JDBC 3.0 methods don't make use of any new JDBC3.0 classes and so this will work fine in jdbc2.0 configuration. ///////////////////////////////////////////////////////////////////////  JDBC 3.0	-	New public methods  /////////////////////////////////////////////////////////////////////// Writes the given array of bytes to the BLOB value that this Blob object represents, starting at position pos, and returns the number of bytes written. Writes all or part of the given array of byte array to the BLOB value that this Blob object represents and returns the number of bytes written. Writing starts at position pos in the BLOB value; len bytes from the given byte array are written. JDBC 3.0 Truncates the BLOB value that this Blob object represents to be len bytes in length.
JDBC 2.0 Get an Array OUT parameter. JDBC 3.0 Retrieves the value of a JDBC ARRAY parameter as an Array object in the Java programming language. JDBC 2.0 Get the value of a NUMERIC parameter as a java.math.BigDecimal object.  JDBC 3.0 Retrieves the value of a JDBC NUMERIC parameter as a java.math.BigDecimal object with as many digits to the right of the decimal point as the value contains Get binary stream for a parameter. JDBC 2.0 Get a BLOB OUT parameter. JDBC 3.0 Retrieves the value of a JDBC BLOB parameter as a Blob object in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC BIT parameter as a boolean in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC TINYINT parameter as a byte in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC BINARY or VARBINARY parameter as an array of byte values in the Java programming language. JDBC 4.0 methods Retrieves the value of the designated parameter as a <code>java.io.Reader</code> object in the Java programming language. Introduced in JDBC 4.0. JDBC 2.0 Get a CLOB OUT parameter. JDBC 3.0 Retrieves the value of a JDBC CLOB parameter as a Clob object in the Java programming language. Get the value of a SQL DATE parameter as a java.sql.Date object  JDBC 3.0 Retrieves the value of a JDBC DATE parameter as a java.sql.Date object JDBC 3.0 Retrieves the value of a JDBC DATE parameter as a java.sql.Date object, using the given Calendar object to construct the date object.  JDBC 3.0 Retrieves the value of a JDBC DOUBLE parameter as a double in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC FLOAT parameter as a float in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC INTEGER parameter as a int in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC BIGINT parameter as a long in the Java programming language.  //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// JDBC 2.0 Returns an object representing the value of OUT parameter {@code i}. Use the map to determine the class from which to construct data of SQL structured and distinct types. JDBC 3.0 Retrieves the value of a parameter as an Object in the java programming language. JDBC 3.0 Returns an object representing the value of OUT parameter i and uses map for the custom mapping of the parameter value. JDBC 2.0 Get a REF(&lt;structured-type&gt;) OUT parameter. JDBC 3.0 Retrieves the value of a JDBC REF (structured-type) parameter as a Ref object in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC SMALLINT parameter as a short in the Java programming language.  JDBC 3.0 Retrieves the value of a JDBC CHAR, VARCHAR, or LONGVARCHAR parameter as a String in the Java programming language. Get the value of a SQL TIME parameter as a java.sql.Time object.  JDBC 3.0 Retrieves the value of a JDBC TIME parameter as ajava.sql.Time object JDBC 3.0 Retrieves the value of a JDBC TIME parameter as a java.sql.Time object, using the given Calendar object to construct the time object. Get the value of a SQL TIMESTAMP parameter as a java.sql.Timestamp object.  JDBC 3.0 Retrieves the value of a JDBC TIMESTAMP parameter as a java.sql.Timestamp object JDBC 3.0 Retrieves the value of a JDBC TIMESTAMP parameter as a java.sql.Timestamp object, using the given Calendar object to construct the Timestamp object. JDBC 3.0 Retrieve the value of the designated JDBC DATALINK parameter as a java.net.URL object JDBC 3.0 Retrieves the value of a JDBC DATALINK parameter as a java.net.URL object CallableStatement interface (the PreparedStatement part implemented by EmbedPreparedStatement)   JDBC 2.0 Derby ignores the typeName argument because UDTs don't need it. JDBC 3.0 methods JDBC 3.0 Registers the OUT parameter named parameterName to the JDBC type sqlType. All OUT parameters must be registered before a stored procedure is executed. JDBC 3.0 Registers the parameter named parameterName to the JDBC type sqlType. This method must be called before a stored procedure is executed. JDBC 3.0 Registers the designated output parameter. This version of the method registerOutParameter should be used for a user-named or REF output parameter. JDBC 3.0 Sets the designated parameter to the given input stream, which will have the specified number of bytes. Sets the designated parameter to the given input stream, which will have the specified number of bytes. JDBC 3.0 Sets the designated parameter to the given java.math.BigDecimal value. The driver converts this to an SQL NUMERIC value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given input stream, which will have the specified number of bytes. Sets the designated parameter to the given input stream, which will have the specified number of bytes. JDBC 3.0 Sets the designated parameter to the given Java boolean value. The driver converts this to an SQL BIT value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given Java byte value. The driver converts this to an SQL TINYINT value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given Java array of bytes. The driver converts this to an SQL VARBINARY OR LONGVARBINARY (depending on the argument's size relative to the driver's limits on VARBINARY values)when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given Reader object, which is the given number of characters long. Sets the designated parameter to the given Reader, which will have the specified number of bytes. JDBC 3.0 Sets the designated parameter to the given java.sql.Date value. The driver converts this to an SQL DATE value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given java.sql.Date value, using the given Calendar object. JDBC 3.0 Sets the designated parameter to the given Java double value. The driver converts this to an SQL DOUBLE value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given Java float value. The driver converts this to an SQL FLOAT value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given Java int value. The driver converts this to an SQL INTEGER value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given Java long value. The driver converts this to an SQL BIGINT value when it sends it to the database. JDBC 3.0 Sets the designated parameter to SQL NULL. JDBC 3.0 Sets the designated parameter to SQL NULL. JDBC 3.0 Sets the value of the designated parameter with the given object. The second parameter must be of type Object; therefore, the java.lang equivalent objects should be used for built-in types. JDBC 3.0 Sets the value of the designated parameter with the given object. This method is like the method setObject above, except that it assumes a scale of zero. JDBC 3.0 Sets the value of the designated parameter with the given object. The second argument must be an object type; for integral values, the java.lang equivalent objects should be used. JDBC 3.0 Sets the designated parameter to the given Java short value. The driver converts this to an SQL SMALLINT value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given Java String value. The driver converts this to an SQL VARCHAR OR LONGVARCHAR value (depending on the argument's size relative the driver's limits on VARCHAR values) when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given java.sql.Time value. The driver converts this to an SQL TIME value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given java.sql.Time value using the Calendar object JDBC 3.0 Sets the designated parameter to the given java.sql.Timestamp value. The driver converts this to an SQL TIMESTAMP value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given java.sql.Timestamp value, using the given Calendar object JDBC 3.0 Sets the designated parameter to the given java.net.URL object. The driver converts this to an SQL DATALINK value when it sends it to the database.

Checks if the Clob is valid. <p> A Clob is invalidated when {@link #free} is called or if the parent connection is closed. @throws SQLException if the Clob is not valid ///////////////////////////////////////////////////////////////////////  JDBC 4.0    -    New public methods  /////////////////////////////////////////////////////////////////////// Frees the <code>Clob</code> and releases the resources that it holds. <p> The object is invalid once the <code>free</code> method is called. If <code>free</code> is called multiple times, the subsequent calls to <code>free</code> are treated as a no-op. Gets the <code>CLOB</code> value designated by this <code>Clob</code> object as a stream of Ascii bytes. Gets the <code>Clob</code> contents as a stream of characters. Returns a <code>Reader</code> object that contains a partial <code>Clob</code> value, starting with the character specified by pos, which is length characters in length. Returns the current internal Clob representation. <p> Care should be taken, as the representation can change when the user performs operations on the Clob. An example is if the Clob content is served from a store stream and the user updates the content. The internal representation will then be changed to a temporary Clob copy that allows updates.  Returns a copy of the specified substring in the <code>CLOB</code> value designated by this <code>Clob</code> object. <p> The substring begins at position <code>pos</code> and has up to * <code>length</code> consecutive characters. The starting position must be between 1 and the length of the CLOB plus 1. This allows for zero-length CLOB values, from which only zero-length substrings can be returned. <p> If a larger length is requested than there are characters available, characters from the start position to the end of the CLOB are returned. <p> <em>NOTE</em>: If the starting position is the length of the CLOB plus 1, zero characters are returned regardless of the length requested. Returns the number of characters in the <code>CLOB</code> value designated by this <code>Clob</code> object. Makes a writable clone of the current Clob. <p> This is called when we have a {@link StoreStreamClob} and the user calls a method updating the content of the Clob. A temporary Clob will then be created to hold the updated content. Makes a writable clone of the current Clob. <p> This is called when we have a {@link StoreStreamClob} and the user calls a method updating the content of the Clob. A temporary Clob will then be created to hold the updated content. Determines the character position at which the specified substring <code>searchStr</code> appears in the <code>CLOB</code> value. <p> The search begins at position <code>start</code>. The method uses the following algorithm for the search: <p> If the <code>CLOB</code> value is materialized as a string, use <code>String.indexOf</code>. <p> If the <code>CLOB</code> value is represented as a stream, read a block of chars from the start position and compare the chars with <code>searchStr</code>. Then: <ul> <li>If a matching char is found, increment <code>matchCount</code>. <li>If <code>matchCount</code> is equal to the length of <code>searchStr</code>, return with the current start position. <li>If no match is found, and there is more data, restart search (see below). <li>If no match is found, return <code>-1</code>. </ul> <p> The position where the stream has a char equal to the first char of <code>searchStr</code> will be remembered and used as the starting position for the next search-iteration if the current match fails. If a non-matching char is found, start a fresh search from the position remembered. If there is no such position, next search will start at the current position <code>+1</code>. Determines the character position at which the specified <code>Clob</code> object <code>searchstr</code> appears in this <code>Clob</code> object.  The search begins at position <code>start</code>. JDBC 3.0 Retrieves a stream to be used to write Ascii characters to the CLOB value that this Clob object represents, starting at position pos. JDBC 3.0 Retrieves a stream to be used to write a stream of characters to the CLOB value that this Clob object represents, starting at position pos. Following methods are for the new JDBC 3.0 methods in java.sql.Clob (see the JDBC 3.0 spec). We have the JDBC 3.0 methods in Local20 package, so we don't have to have a new class in Local30. The new JDBC 3.0 methods don't make use of any new JDBC3.0 classes and so this will work fine in jdbc2.0 configuration. ///////////////////////////////////////////////////////////////////////  JDBC 3.0    -    New public methods  /////////////////////////////////////////////////////////////////////// JDBC 3.0 Writes the given Java String to the CLOB value that this Clob object designates at the position pos. JDBC 3.0 Writes len characters of str, starting at character offset, to the CLOB value that this Clob represents. JDBC 3.0 Truncates the CLOB value that this Clob designates to have a length of len characters
//////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Add the locator and the corresponding LOB object into the HashMap Adds an entry of the lob in WeakHashMap. These entries are used for cleanup during commit/rollback or close. Add a temporary lob file to the lobFiles set. This will get closed at transaction end or removed as the lob is freed. ///////////////////////////////////////////////////////////////////////  Implementation specific methods  /////////////////////////////////////////////////////////////////////// Add a warning to the current list of warnings, to follow this note from Connection.getWarnings. Note: Subsequent warnings will be chained to this SQLWarning. Begin aborting the connection Boot database. Cancels the current running statement. Examines the boot properties looking for conflicting cryptographic options and commands. Check that a database has already been booted. Throws an exception otherwise Checks that a user has the system privileges to create a database. To perform this check the following policy grants are required <ul> <li> to run the encapsulated test: permission javax.security.auth.AuthPermission "doAsPrivileged"; <li> to resolve relative path names: permission java.util.PropertyPermission "user.dir", "read"; <li> to canonicalize path names: permission java.io.FilePermission "...", "read"; </ul> or a SQLException will be raised detailing the cause. <p> In addition, for the test to succeed <ul> <li> the given user needs to be covered by a grant: principal org.apache.derby.authentication.SystemPrincipal "..." {} <li> that lists a permission covering the database location: permission org.apache.derby.security.DatabasePermission "directory:...", "create"; </ul> or it will fail with a SQLException detailing the cause. Check if the transaction is active so that we cannot close down the connection. If auto-commit is on, the transaction is committed when the connection is closed, so it is always OK to close the connection in that case. Otherwise, throw an exception if a transaction is in progress. Raises an exception if the connection is closed. Check if actual authenticationId is equal to the database owner's.  Check passed-in user's credentials.  If applicable, check that we don't connect with a user name that equals a role. Clear the HashMap of all entries. Called when a commit or rollback of the transaction happens. After this call, getWarnings returns null until a new warning is reported for this Connection. Synchronization node: Warnings are synchonized on nesting level In some cases, it is desirable to immediately release a Connection's database and JDBC resources instead of waiting for them to be automatically released; the close method provides this immediate release. <P><B>Note:</B> A Connection is automatically closed when it is garbage collected. Certain fatal errors also result in a closed Connection. This inner close takes the exception and calls the context manager to make the connection close. The exception must be a session severity exception.  NOTE: This method is not part of JDBC specs.  Commit makes all changes made since the previous commit/rollback permanent and releases any database locks currently held by the Connection. This method should only be used when auto commit has been disabled. If in autocommit, then commit. Used to force a commit after a result set closes in autocommit mode. The needCommit mechanism does not work correctly as there are times with cursors (like a commit, followed by a next, followed by a close) where the system does not think it needs a commit but we need to force the commit on close.  It seemed safer to just force a commit on close rather than count on keeping the needCommit flag correct for all cursor cases. Must have connection synchonization and context set up already. if a commit is needed, perform it. Must have connection synchonization and context set up already. Creates a savepoint with the given name (if it is a named savepoint else we will generate a name because Derby only supports named savepoints internally) in the current transaction and returns the new Savepoint object that represents it. Compare two user-specified database names to see if they identify the same database. ------------------------------------------------------- JDBC 4.0 ------------------------------------------------------- Constructs an object that implements the <code>Blob</code> interface. The object returned initially contains no data.  The <code>setBinaryStream</code> and <code>setBytes</code> methods of the <code>Blob</code> interface may be used to add data to the <code>Blob</code>. Examine the attributes set provided for illegal boot combinations and determine if this is a create boot. Constructs an object that implements the <code>Clob</code> interface. The object returned initially contains no data.  The <code>setAsciiStream</code>, <code>setCharacterStream</code> and <code>setString</code> methods of the <code>Clob</code> interface may be used to add data to the <code>Clob</code>. * Create database methods. Create a new database. Privileged startup. Must be private so that user code can't call this entry point. * Methods from java.sql.Connection SQL statements without parameters are normally executed using Statement objects. If the same SQL statement is executed many times, it is more efficient to use a PreparedStatement JDBC 2.0 Result sets created using the returned Statement will have forward-only type, and read-only concurrency, by default. JDBC 2.0 Same as createStatement() above, but allows the default result set type and result set concurrency type to be overridden. JDBC 3.0 Same as createStatement() above, but allows the default result set type, result set concurrency type and result set holdability type to be overridden. <p> Forbid empty or null usernames and passwords. </p> Filter out properties from the passed in set of JDBC attributes to remove any derby.* properties. This is to ensure that setting derby.* properties does not work this way, it's not a defined way to set such properties and could be a secuirty hole in allowing remote connections to override system, application or database settings.   This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  Privileged service lookup. Must be private so that user code can't call this entry point. Get the current auto-commit state. Return the Connection's current catalog name. <code>getClientInfo</code> always returns an empty <code>Properties</code> object since Derby doesn't support ClientInfoProperties. <code>getClientInfo</code> always returns a <code>null String</code> since Derby doesn't support ClientInfoProperties. Return the context manager for this connection. Obtain the name of the current schema. Not part of the java.sql.Connection interface, but is accessible through the EngineConnection interface, so that the NetworkServer can get at the current schema for piggy-backing Return the dbname for this connection. * methods to be overridden by subimplementations wishing to insert * their classes into the mix. Gets the EngineType of the connected database. JDBC 3.0 Retrieves the current holdability of ResultSet objects created using this Connection object. Return the current locator value/ 0x800x values are not  valid values as they are used to indicate the BLOB is being sent by value, so we skip those values (DERBY-3243) Get the LOB reference corresponding to the locator. A Connection's database is able to provide information describing its tables, its supported SQL grammar, its stored procedures, the capabilities of this connection, etc. This information is made available through a DatabaseMetaData object. ///////////////////////////////////////////////////////////////////////  SECURITY  /////////////////////////////////////////////////////////////////////// Privileged Monitor lookup. Must be package private so that user code can't call this entry point. Return prepare isolation Return a unique order number for a result set. A unique value is only needed if the result set is being created within procedure and thus must be using a nested connection. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Get the name of the current schema. Get this Connection's current transaction isolation mode. Returns the type map for this connection. The first warning reported by calls on this Connection is returned. <P><B>Note:</B> Subsequent warnings will be chained to this SQLWarning. Return the Hash Map in the root connection EmbedConnection30 overrides this method so it can release the savepoints array if the exception severity is transaction level Handle any type of Exception. <UL> <LI> Inform the contexts of the error <LI> Throw an Util based upon the thrown exception. </UL> REMIND: now that we know all the exceptions from our driver are Utils, would it make sense to shut down the system for unknown SQLExceptions? At present, we do not. Because this is the last stop for exceptions, it will catch anything that occurs in it and try to cleanup before re-throwing them. Used to authorize and verify the privileges of the user and initiate failover. Used to perform failover on a database in slave replication mode. Performs failover, provided that the database is in replication slave mode and has lost connection with the master database. If the connection with the master is up, the call to this method will be refused by raising an exception. The reason for refusing the failover command if the slave is connected with the master is that we cannot authenticate the user on the slave side (because the slave database has not been fully booted) whereas authentication is not a problem on the master side. If not refused, this method will apply all operations received from the master and complete the booting of the database so that it can be connected to. Stop replication slave when called from a client. Stops replication slave mode, provided that the database is in replication slave mode and has lost connection with the master database. If the connection with the master is up, the call to this method will be refused by raising an exception. The reason for refusing the stop command if the slave is connected with the master is that we cannot authenticate the user on the slave side (because the slave database has not been fully booted) whereas authentication is not a problem on the master side. If not refused, this operation will cause SlaveDatabase to call internalStopReplicationSlave Stop replication slave when called from SlaveDatabase. Called when slave replication mode has been stopped, and all that remains is to shutdown the database. This happens if handleStopReplicationSlave has successfully requested the slave to stop, if the replication master has requested the slave to stop using the replication network, or if a fatal exception has occurred in the database. Return true if the connection is aborting Tests to see if a Connection is closed. Examines boot properties and determines if a boot with the given attributes would entail a cryptographic operation on the database. Examines the boot properties and determines if the given attributes would entail dropping the database. Examine boot properties and determine if a boot with the given attributes would entail a hard upgrade.  Examine the boot properties and determine if a boot with the given attributes should stop slave replication mode. A connection with this property should only be made from SlaveDatabase. Make sure to call SlaveDatabase.verifyShutdownSlave() to verify that this connection is not made from a client. Tests to see if the connection is in read-only mode. used to verify if the failover attribute has been set. Tells if the attribute/property has been set. Examine the boot properties and determine if a boot with the given attributes should stop slave replication mode. Tells if the attribute/property has the value {@code true}. Checks if the connection has not been closed and is still valid. The validity is checked by checking that the connection is not closed. Returns false unless <code>interfaces</code> is implemented A driver may convert the JDBC sql grammar into its system's native SQL grammar prior to sending it; nativeSQL returns the native form of the statement that the driver would have sent. if auto commit is on, remember that we need to commit the current statement. A SQL stored procedure call statement is handled by creating a CallableStatement for it. The CallableStatement provides methods for setting up its IN and OUT parameters, and methods for executing it. <P><B>Note:</B> This method is optimized for handling stored procedure call statements. Some drivers may send the call statement to the database when the prepareCall is done; others may wait until the CallableStatement is executed. This has no direct affect on users; however, it does affect which method throws certain SQLExceptions. JDBC 2.0 Result sets created using the returned CallableStatement will have forward-only type, and read-only concurrency, by default. JDBC 2.0 Same as prepareCall() above, but allows the default result set type and result set concurrency type to be overridden. JDBC 3.0 Same as prepareCall() above, but allows the default result set type, result set concurrency type and result set holdability to be overridden. Class interface methods used by database metadata to ensure good relations with autocommit. A SQL statement with or without IN parameters can be pre-compiled and stored in a PreparedStatement object. This object can then be used to efficiently execute this statement multiple times. <P><B>Note:</B> This method is optimized for handling parametric SQL statements that benefit from precompilation. If the driver supports precompilation, prepareStatement will send the statement to the database for precompilation. Some drivers may not support precompilation. In this case, the statement may not be sent to the database until the PreparedStatement is executed.  This has no direct affect on users; however, it does affect which method throws certain SQLExceptions. JDBC 2.0 Result sets created using the returned PreparedStatement will have forward-only type, and read-only concurrency, by default. Creates a default PreparedStatement object that has the capability to retieve auto-generated keys. The given constant tells the driver whether it should make auto-generated keys available for retrieval. This parameter is ignored if the SQL statement is not an INSERT statement. JDBC 3.0 JDBC 2.0 Same as prepareStatement() above, but allows the default result set type and result set concurrency type to be overridden. JDBC 3.0 Same as prepareStatement() above, but allows the default result set type, result set concurrency type and result set holdability to be overridden. Creates a default PreparedStatement object capable of returning the auto-generated keys designated by the given array. This array contains the indexes of the columns in the target table that contain the auto-generated keys that should be made available. This array is ignored if the SQL statement is not an INSERT statement JDBC 3.0 Creates a default PreparedStatement object capable of returning the auto-generated keys designated by the given array. This array contains the names of the columns in the target table that contain the auto-generated keys that should be returned. This array is ignored if the SQL statement is not an INSERT statement JDBC 3.0 Private, privileged lookup of the lcc.. Removes the given Savepoint object from the current transaction. Any reference to the savepoint after it has been removed will cause an SQLException to be thrown Remove the key(LOCATOR) from the hash table. Remove LOBFile from the lobFiles set. This will occur when the lob is freed or at transaction end if the lobFile was removed from the WeakHashMap but not finalized. Privileged shutdown. Must be private so that user code can't call this entry point. Remove any encryption or upgarde properties from the given properties Reset the connection before it is returned from a PooledConnection to a new application request (wrapped by a BrokeredConnection). Examples of reset covered here is dropping session temporary tables and reseting IDENTITY_VAL_LOCAL. Most JDBC level reset is handled by calling standard java.sql.Connection methods from EmbedPooledConnection. Rollback drops all changes made since the previous commit/rollback and releases any database locks currently held by the Connection. This method should only be used when auto commit has been disabled. Undoes all changes made after the given Savepoint object was set. This method should be used only when auto-commit has been disabled. If a connection is in auto-commit mode, then all its SQL statements will be executed and committed as individual transactions.  Otherwise, its SQL statements are grouped into transactions that are terminated by either commit() or rollback().  By default, new connections are in auto-commit mode. The commit occurs when the statement completes or the next execute occurs, whichever comes first. In the case of statements returning a ResultSet, the statement completes when the last row of the ResultSet has been retrieved or the ResultSet has been closed. In advanced cases, a single statement may return multiple results as well as output parameter values. Here the commit occurs when all results and output param values have been retrieved. A sub-space of this Connection's database may be selected by setting a catalog name. If the driver does not support catalogs it will silently ignore this request. <code>setClientInfo</code> will always throw a <code>SQLClientInfoException</code> since Derby does not support any properties. <code>setClientInfo</code> will throw a <code>SQLClientInfoException</code> unless the <code>properties</code> parameter is empty, since Derby does not support any properties. All the property keys in the <code>properties</code> parameter are added to failedProperties of the exception thrown, with REASON_UNKNOWN_PROPERTY as the value. JDBC 3.0 Changes the holdability of ResultSet objects created using this Connection object to the given holdability. This is called from the EmbedConnectionContext to close on errors.  We assume all handling of the connectin is dealt with via the context stack, and our only role is to mark ourself as closed. Close the connection when processing errors, or when closing a nested connection. <p> This only marks it as closed and frees up its resources; any closing of the underlying connection or commit work is assumed to be done elsewhere. Called from EmbedConnectionContext's cleanup routine, and by proxy.close(). Set the transaction isolation level that will be used for the next prepare.  Used by network server to implement DB2 style isolation levels. You can put a connection in read-only mode as a hint to enable database optimizations. <P><B>Note:</B> setReadOnly cannot be called while in the middle of a transaction. ///////////////////////////////////////////////////////////////////////  JDBC 3.0    -   New public methods  /////////////////////////////////////////////////////////////////////// Creates an unnamed savepoint in the current transaction and returns the new Savepoint object that represents it. Creates a savepoint with the given name in the current transaction and returns the new Savepoint object that represents it. Set the default schema for the Connection. You can call this method to try to change the transaction isolation level using one of the TRANSACTION_* values. <P><B>Note:</B> setTransactionIsolation causes the current transaction to commit if the isolation level is changed. Otherwise, if the requested isolation level is the same as the current isolation level, this method is a no-op. ///////////////////////////////////////////////////////////////////////  JDBC 2.0	-	New public methods  /////////////////////////////////////////////////////////////////////// Install a type-map object as the default type-map for this connection. JDBC 2.0 - java.util.Map requires JDK 1 Install the context manager for this thread.  Check connection status here. Puts the current thread to sleep. <p> <em>NOTE</em>: This method guarantees that the thread sleeps at least {@code millis} milliseconds. Privileged startup. Must be private so that user code can't call this entry point. Strips any sub-sub-protocol prefix from a database name. ///////////////////////////////////////////////////////////////////////  OBJECT OVERLOADS  /////////////////////////////////////////////////////////////////////// Get a String representation that uniquely identifies this connection.  Include the same information that is printed in the log for various trace and error messages. In Derby the "physical" connection is a LanguageConnectionContext, or LCC. The JDBC Connection is an JDBC-specific layer on top of this.  Rather than create a new id here, we simply use the id of the underlying LCC. Note that this is a big aid in debugging, because much of the engine trace and log code prints the LCC id. returns false if there is an underlying transaction and that transaction has done work.  True if there is no underlying transaction or that underlying transaction is idle Returns <code>this</code> if this class implements the interface used by release/rollback to check savepoint argument used by setSavepoint to check autocommit is false and not inside the trigger code Returns true if the attribute exists and is set to true. Raises an exception if the attribute exists and is set to something else. Do not use this method directly use XATransactionState.xa_commit instead because it also maintains/cancels the timout task which is scheduled to cancel/rollback the global transaction. * methods to be overridden by subimplementations wishing to insert * their classes into the mix. * The reason we need to override them is because we want to create a * Local20/LocalStatment object (etc) rather than a Local/LocalStatment * object (etc). * XA support Do not use this method directly use XATransactionState.xa_prepare instead because it also maintains/cancels the timeout task which is scheduled to cancel/rollback the global transaction. Do not use this method directly use XATransactionState.xa_rollback instead because it also maintains/cancels the timout task which is scheduled to cancel/rollback the global transaction.
public java.sql.Connection getEmbedConnection() { /	return conn; } Get a connection equivalent to the call <PRE> DriverManager.getConnection("jdbc:default:connection"); </PRE> Get a jdbc ResultSet based on the execution ResultSet. Private, privileged lookup of the lcc.. Process a ResultSet from a procedure to be a dynamic result, but one that will be closed due to it being inaccessible. We cannot simply close the ResultSet as it the nested connection that created it might be closed, leading to its close method being a no-op. This performs all the conversion (linking the ResultSet to a valid connection) required but does not close the ResultSet.
Read the query descriptions from metadata.properties and metadata_net.properties. This method must be invoked from within a privileged block. ////////////////////////////////////////////////////////////  DatabaseMetaData interface  //////////////////////////////////////////////////////////// ---------------------------------------------------------------------- First, a variety of minor information about the target database. Can all the procedures returned by getProcedures be called by the current user? Can all the tables returned by getTable be SELECTed by the current user? Returns whether or not all open {@code ResultSet}s on a {@code Connection} are closed if an error occurs when auto-commit in enabled. Does a data definition statement within a transaction force the transaction to commit? Is a data definition statement within a transaction ignored? JDBC 2.0 Determine whether or not a visible row delete can be detected by calling ResultSet.rowDeleted().  If deletesAreDetected() returns false, then deleted rows are removed from the result set. Does the actual work for the getBestRowIdentifier metadata calls.  See getBestRowIdentifier() method above for parameter descriptions. Does the actual work for the getColumns metadata calls. See getColumns() method above for parameter descriptions. Does the actual work for the getIndexInfo metadata calls.  See getIndexInfo() method above for parameter descriptions. Does the actual work for the getProcedureColumns metadata calls. See getProcedureColumns() method above for parameter descriptions. Does the actual work for the getProcedures and getFunctions metadata calls.  See getProcedures() method above for parameter descriptions. Does the actual work for the getVersionColumns metadata calls.  See getVersionColumns() method above for parameter descriptions. Did getMaxRowSize() include LONGVARCHAR and LONGVARBINARY blobs? ///////////////////////////////////////////////////////////////////////  JDBC 4.1 - New public methods  /////////////////////////////////////////////////////////////////////// See DatabaseMetaData javadoc JDBC 3.0 Retrieves a description of the given attribute of the given type for a user-defined type (UDT) that is available in the given schema and catalog. Get a description of a table's optimal set of columns that uniquely identifies a row. They are ordered by SCOPE. <P>Each column description has the following columns: <OL> <LI><B>SCOPE</B> short =&gt; actual scope of result <UL> <LI> bestRowTemporary - very temporary, while using row <LI> bestRowTransaction - valid for remainder of current transaction <LI> bestRowSession - valid for remainder of current session </UL> <LI><B>COLUMN_NAME</B> String =&gt; column name <LI><B>DATA_TYPE</B> int =&gt; SQL data type from java.sql.Types <LI><B>TYPE_NAME</B> String =&gt; Data source dependent type name <LI><B>COLUMN_SIZE</B> int =&gt; precision <LI><B>BUFFER_LENGTH</B> int =&gt; not used <LI><B>DECIMAL_DIGITS</B> short	 =&gt; scale <LI><B>PSEUDO_COLUMN</B> short =&gt; is this a pseudo column like an Oracle ROWID <UL> <LI> bestRowUnknown - may or may not be pseudo column <LI> bestRowNotPseudo - is NOT a pseudo column <LI> bestRowPseudo - is a pseudo column </UL> </OL> Get a description of a table's optimal set of columns that uniquely identifies a row. They are ordered by SCOPE. Same as getBestRowIdentifier() above, except that the result set will conform to ODBC specifications. What's the separator between catalog and table name? What's the database vendor's preferred term for "catalog"? Get the catalog names available in this database.  The results are ordered by catalog name. <P>The catalog column is: <OL> <LI><B>TABLE_CAT</B> String =&gt; catalog name </OL> ////////////////////////////////////////////////////////////  MISC  //////////////////////////////////////////////////////////// Get metadata that the client driver will cache. The metadata is fetched using SYSIBM.METADATA (found in metadata_net.properties). ///////////////////////////////////////////////////////////////////////  JDBC 4.0 - New public methods  /////////////////////////////////////////////////////////////////////// JDBC 4.0 <p>Returns a list of the client info properties supported by the driver. The result set contains the following columns: <p> <ol> <li>NAME String=&gt; The name of the client info property.</li> <li>MAX_LEN int=&gt; The maximum length of the value for the property.</li> <li>DEFAULT_VALUE String=&gt; The default value of the property.</li> <li>DESCRIPTION String=&gt; A description of the property.</li> </ol> <p>The <code>ResultSet</code> is sorted by the NAME column. Get a description of the access rights for a table's columns. <P>Only privileges matching the column name criteria are returned.  They are ordered by COLUMN_NAME and PRIVILEGE. <P>Each privilige description has the following columns: <OL> <LI><B>TABLE_CAT</B> String =&gt; table catalog (may be null) <LI><B>TABLE_SCHEM</B> String =&gt; table schema (may be null) <LI><B>TABLE_NAME</B> String =&gt; table name <LI><B>COLUMN_NAME</B> String =&gt; column name <LI><B>GRANTOR</B> =&gt; grantor of access (may be null) <LI><B>GRANTEE</B> String =&gt; grantee of access <LI><B>PRIVILEGE</B> String =&gt; name of access (SELECT, INSERT, UPDATE, REFRENCES, ...) <LI><B>IS_GRANTABLE</B> String =&gt; "YES" if grantee is permitted to grant to others; "NO" if not; null if unknown </OL> Get a description of table columns available in a catalog. <P>Only column descriptions matching the catalog, schema, table and column name criteria are returned.  They are ordered by TABLE_SCHEM, TABLE_NAME and ORDINAL_POSITION. <P>Each column description has the following columns: <OL> <LI><B>TABLE_CAT</B> String =&gt; table catalog (may be null) <LI><B>TABLE_SCHEM</B> String =&gt; table schema (may be null) <LI><B>TABLE_NAME</B> String =&gt; table name <LI><B>COLUMN_NAME</B> String =&gt; column name <LI><B>DATA_TYPE</B> int =&gt; SQL type from java.sql.Types <LI><B>TYPE_NAME</B> String =&gt; Data source dependent type name <LI><B>COLUMN_SIZE</B> int =&gt; column size.  For char or date types this is the maximum number of characters, for numeric or decimal types this is precision. <LI><B>BUFFER_LENGTH</B> is not used. <LI><B>DECIMAL_DIGITS</B> int =&gt; the number of fractional digits <LI><B>NUM_PREC_RADIX</B> int =&gt; Radix (typically either 10 or 2) <LI><B>NULLABLE</B> int =&gt; is NULL allowed? <UL> <LI> columnNoNulls - might not allow NULL values <LI> columnNullable - definitely allows NULL values <LI> columnNullableUnknown - nullability unknown </UL> <LI><B>REMARKS</B> String =&gt; comment describing column (may be null) <LI><B>COLUMN_DEF</B> String =&gt; default value (may be null) <LI><B>SQL_DATA_TYPE</B> int =&gt; unused <LI><B>SQL_DATETIME_SUB</B> int =&gt; unused <LI><B>CHAR_OCTET_LENGTH</B> int =&gt; for char types the maximum number of bytes in the column <LI><B>ORDINAL_POSITION</B> int	=&gt; index of column in table (starting at 1) <LI><B>IS_NULLABLE</B> String =&gt; "NO" means column definitely does not allow NULL values; "YES" means the column might allow NULL values.  An empty string means nobody knows. <LI><B>SCOPE_CATALOG</B> String =&gt; catalog of table that is the scope of a reference attribute (<code>null</code> if DATA_TYPE isn't REF) <LI><B>SCOPE_SCHEMA</B> String =&gt; schema of table that is the scope of a reference attribute (<code>null</code> if the DATA_TYPE isn't REF) <LI><B>SCOPE_TABLE</B> String =&gt; table name that this the scope of a reference attribure (<code>null</code> if the DATA_TYPE isn't REF) <LI><B>SOURCE_DATA_TYPE</B> short =&gt; source type of a distinct type or user-generated Ref type, SQL type from java.sql.Types (<code>null</code> if DATA_TYPE isn't DISTINCT or user-generated REF) <LI><B>IS_AUTOINCREMENT</B> String =&gt; Indicates whether this column is auto incremented <UL> <LI> YES --- if the column is auto incremented <LI> NO --- if the column is not auto incremented <LI> empty string --- if it cannot be determined whether the column is auto incremented parameter is unknown </UL> <LI><B>SCOPE_CATLOG</B> A redundant copy of SCOPE_CATALOG. The name of this column is deliberately mis-spelled in order to support a typo in the javadoc for DatabaseMetaData.getColumns() which was corrected by JDBC 4.1. </OL> Get a description of table columns available in a catalog. Same as getColumns() above, except that the result set will conform to ODBC specifications. JDBC 2.0 Return the connection that produced this metadata object. Get a description of the foreign key columns in the foreign key table that reference the primary key columns of the primary key table (describe how one table imports another's key.) This should normally return a single foreign key/primary key pair (most tables only import a foreign key from a table once.)  They are ordered by FKTABLE_CAT, FKTABLE_SCHEM, FKTABLE_NAME, and KEY_SEQ. <P>Each foreign key column description has the following columns: <OL> <LI><B>PKTABLE_CAT</B> String =&gt; primary key table catalog (may be null) <LI><B>PKTABLE_SCHEM</B> String =&gt; primary key table schema (may be null) <LI><B>PKTABLE_NAME</B> String =&gt; primary key table name <LI><B>PKCOLUMN_NAME</B> String =&gt; primary key column name <LI><B>FKTABLE_CAT</B> String =&gt; foreign key table catalog (may be null) being exported (may be null) <LI><B>FKTABLE_SCHEM</B> String =&gt; foreign key table schema (may be null) being exported (may be null) <LI><B>FKTABLE_NAME</B> String =&gt; foreign key table name being exported <LI><B>FKCOLUMN_NAME</B> String =&gt; foreign key column name being exported <LI><B>KEY_SEQ</B> short =&gt; sequence number within foreign key <LI><B>UPDATE_RULE</B> short =&gt; What happens to foreign key when primary is updated: <UL> <LI> importedNoAction - do not allow update of primary key if it has been imported <LI> importedKeyCascade - change imported key to agree with primary key update <LI> importedKeySetNull - change imported key to NULL if its primary key has been updated <LI> importedKeySetDefault - change imported key to default values if its primary key has been updated <LI> importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility) </UL> <LI><B>DELETE_RULE</B> short =&gt; What happens to the foreign key when primary is deleted. <UL> <LI> importedKeyNoAction - do not allow delete of primary key if it has been imported <LI> importedKeyCascade - delete rows that import a deleted key <LI> importedKeySetNull - change imported key to NULL if its primary key has been deleted <LI> importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility) <LI> importedKeySetDefault - change imported key to default if its primary key has been deleted </UL> <LI><B>FK_NAME</B> String =&gt; foreign key name (may be null) <LI><B>PK_NAME</B> String =&gt; primary key name (may be null) <LI><B>DEFERRABILITY</B> short =&gt; can the evaluation of foreign key constraints be deferred until commit <UL> <LI> importedKeyInitiallyDeferred - see SQL92 for definition <LI> importedKeyInitiallyImmediate - see SQL92 for definition <LI> importedKeyNotDeferrable - see SQL92 for definition </UL> </OL> In contrast to the JDBC version of getCrossReference, this method allows null values for table names. JDBC 3.0 Retrieves the major version number of the underlying database. JDBC 3.0 Retrieves the minor version number of the underlying database. What's the name of this database product? What's the version of this database product? ---------------------------------------------------------------------- What's the database's default transaction isolation level?  The values are defined in java.sql.Connection. What's this JDBC driver's major version number? What's this JDBC driver's minor version number? What's the name of this JDBC driver? What's the version of this JDBC driver? Get a description of the foreign key columns that reference a table's primary key columns (the foreign keys exported by a table).  They are ordered by FKTABLE_CAT, FKTABLE_SCHEM, FKTABLE_NAME, and KEY_SEQ. <P>Each foreign key column description has the following columns: <OL> <LI><B>PKTABLE_CAT</B> String =&gt; primary key table catalog (may be null) <LI><B>PKTABLE_SCHEM</B> String =&gt; primary key table schema (may be null) <LI><B>PKTABLE_NAME</B> String =&gt; primary key table name <LI><B>PKCOLUMN_NAME</B> String =&gt; primary key column name <LI><B>FKTABLE_CAT</B> String =&gt; foreign key table catalog (may be null) being exported (may be null) <LI><B>FKTABLE_SCHEM</B> String =&gt; foreign key table schema (may be null) being exported (may be null) <LI><B>FKTABLE_NAME</B> String =&gt; foreign key table name being exported <LI><B>FKCOLUMN_NAME</B> String =&gt; foreign key column name being exported <LI><B>KEY_SEQ</B> short =&gt; sequence number within foreign key <LI><B>UPDATE_RULE</B> short =&gt; What happens to foreign key when primary is updated: <UL> <LI> importedNoAction - do not allow update of primary key if it has been imported <LI> importedKeyCascade - change imported key to agree with primary key update <LI> importedKeySetNull - change imported key to NULL if its primary key has been updated <LI> importedKeySetDefault - change imported key to default values if its primary key has been updated <LI> importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility) </UL> <LI><B>DELETE_RULE</B> short =&gt; What happens to the foreign key when primary is deleted. <UL> <LI> importedKeyNoAction - do not allow delete of primary key if it has been imported <LI> importedKeyCascade - delete rows that import a deleted key <LI> importedKeySetNull - change imported key to NULL if its primary key has been deleted <LI> importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility) <LI> importedKeySetDefault - change imported key to default if its primary key has been deleted </UL> <LI><B>FK_NAME</B> String =&gt; foreign key name (may be null) <LI><B>PK_NAME</B> String =&gt; primary key name (may be null) <LI><B>DEFERRABILITY</B> short =&gt; can the evaluation of foreign key constraints be deferred until commit <UL> <LI> importedKeyInitiallyDeferred - see SQL92 for definition <LI> importedKeyInitiallyImmediate - see SQL92 for definition <LI> importedKeyNotDeferrable - see SQL92 for definition </UL> </OL> Get all the "extra" characters that can be used in unquoted identifier names (those beyond a-z, A-Z, 0-9 and _). Implements DatabaseMetaData.getFunctionColumns() for an embedded database. Queries the database to get information about function parameters. Executes the 'getFunctionColumns' query from metadata.properties to obtain the ResultSet.<p> Compatibility: This is a new method in the API which is only available with with Derby versions &gt; 10.1 and JDK versions &gt;= 1.6 <p>Upgrade: Since this is a new query it does not have an SPS, and will be available as soon as any database, new or old, is booted with the new version of Derby, (in <b>soft and hard</b> upgrade). Implements DatabaseMetaData.getFunctions() for an embedded database. Queries the database to get information about functions (procedures returning values). Executes the 'getFunctions' query from metadata.properties to obtain the ResultSet to return.<p> Compatibility: This is a new method in the API which is only available with with Derby versions &gt; 10.1 and JDK versions &gt;= 1.6 <p>Upgrade: Since this is a new query it does not have an SPS, and will be available as soon as any database, new or old, is booted with the new version of Derby, (in <b>soft and hard</b> upgrade). What's the string used to quote SQL identifiers? This returns a space " " if identifier quoting isn't supported. A JDBC-Compliant driver always uses a double quote character. Get a description of the primary key columns that are referenced by a table's foreign key columns (the primary keys imported by a table).  They are ordered by PKTABLE_CAT, PKTABLE_SCHEM, PKTABLE_NAME, and KEY_SEQ. <P>Each primary key column description has the following columns: <OL> <LI><B>PKTABLE_CAT</B> String =&gt; primary key table catalog being imported (may be null) <LI><B>PKTABLE_SCHEM</B> String =&gt; primary key table schema being imported (may be null) <LI><B>PKTABLE_NAME</B> String =&gt; primary key table name being imported <LI><B>PKCOLUMN_NAME</B> String =&gt; primary key column name being imported <LI><B>FKTABLE_CAT</B> String =&gt; foreign key table catalog (may be null) <LI><B>FKTABLE_SCHEM</B> String =&gt; foreign key table schema (may be null) <LI><B>FKTABLE_NAME</B> String =&gt; foreign key table name <LI><B>FKCOLUMN_NAME</B> String =&gt; foreign key column name <LI><B>KEY_SEQ</B> short =&gt; sequence number within foreign key <LI><B>UPDATE_RULE</B> short =&gt; What happens to foreign key when primary is updated: <UL> <LI> importedNoAction - do not allow update of primary key if it has been imported <LI> importedKeyCascade - change imported key to agree with primary key update <LI> importedKeySetNull - change imported key to NULL if its primary key has been updated <LI> importedKeySetDefault - change imported key to default values if its primary key has been updated <LI> importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility) </UL> <LI><B>DELETE_RULE</B> short =&gt; What happens to the foreign key when primary is deleted. <UL> <LI> importedKeyNoAction - do not allow delete of primary key if it has been imported <LI> importedKeyCascade - delete rows that import a deleted key <LI> importedKeySetNull - change imported key to NULL if its primary key has been deleted <LI> importedKeyRestrict - same as importedKeyNoAction (for ODBC 2.x compatibility) <LI> importedKeySetDefault - change imported key to default if its primary key has been deleted </UL> <LI><B>FK_NAME</B> String =&gt; foreign key name (may be null) <LI><B>PK_NAME</B> String =&gt; primary key name (may be null) <LI><B>DEFERRABILITY</B> short =&gt; can the evaluation of foreign key constraints be deferred until commit <UL> <LI> importedKeyInitiallyDeferred - see SQL92 for definition <LI> importedKeyInitiallyImmediate - see SQL92 for definition <LI> importedKeyNotDeferrable - see SQL92 for definition </UL> </OL> Get a description of a table's indices and statistics. They are ordered by NON_UNIQUE, TYPE, INDEX_NAME, and ORDINAL_POSITION. <P>Each index column description has the following columns: <OL> <LI><B>TABLE_CAT</B> String =&gt; table catalog (may be null) <LI><B>TABLE_SCHEM</B> String =&gt; table schema (may be null) <LI><B>TABLE_NAME</B> String =&gt; table name <LI><B>NON_UNIQUE</B> boolean =&gt; Can index values be non-unique? false when TYPE is tableIndexStatistic <LI><B>INDEX_QUALIFIER</B> String =&gt; index catalog (may be null); null when TYPE is tableIndexStatistic <LI><B>INDEX_NAME</B> String =&gt; index name; null when TYPE is tableIndexStatistic <LI><B>TYPE</B> short =&gt; index type: <UL> <LI> tableIndexStatistic - this identifies table statistics that are returned in conjuction with a table's index descriptions <LI> tableIndexClustered - this is a clustered index <LI> tableIndexHashed - this is a hashed index <LI> tableIndexOther - this is some other style of index </UL> <LI><B>ORDINAL_POSITION</B> short =&gt; column sequence number within index; zero when TYPE is tableIndexStatistic <LI><B>COLUMN_NAME</B> String =&gt; column name; null when TYPE is tableIndexStatistic <LI><B>ASC_OR_DESC</B> String =&gt; column sort sequence, "A" =&gt; ascending, "D" =&gt; descending, may be null if sort sequence is not supported; null when TYPE is tableIndexStatistic <LI><B>CARDINALITY</B> int =&gt; When TYPE is tableIndexStatistic, then this is the number of rows in the table; otherwise, it is the number of unique values in the index. <LI><B>PAGES</B> int =&gt; When TYPE is  tableIndexStatisic then this is the number of pages used for the table, otherwise it is the number of pages used for the current index. <LI><B>FILTER_CONDITION</B> String =&gt; Filter condition, if any. (may be null) </OL> Get a description of a table's indices and statistics. They are ordered by NON_UNIQUE, TYPE, INDEX_NAME, and ORDINAL_POSITION. Same as getIndexInfo above, except that the result set will conform to ODBC specifications. JDBC 3.0 Retrieves the major JDBC version number for this driver. JDBC 3.0 Retrieves the minor JDBC version number for this driver. Gets the LanguageConnectionContext for this connection. ---------------------------------------------------------------------- The following group of methods exposes various limitations based on the target database with the current driver. Unless otherwise specified, a result of zero means there is no limit, or the limit is not known. How many hex characters can you have in an inline binary literal? What's the maximum length of a catalog name? What's the max length for a character literal? What's the limit on column name length? What's the maximum number of columns in a "GROUP BY" clause? What's the maximum number of columns allowed in an index? What's the maximum number of columns in an "ORDER BY" clause? What's the maximum number of columns in a "SELECT" list? we don't have a limit... What's the maximum number of columns in a table? How many active connections can we have at a time to this database? What's the maximum cursor name length? What's the maximum length of an index (in bytes)? Added in JDBC 4.2. What's the maximum length of Derby LOB? This is the maximum number of bytes in a LOB. We return the default value of 0, which means "unknown". The maximum size of a CLOB is a complicated because it depends on how many bytes are needed to encode its string value on disk. What's the maximum length of a procedure name? What's the maximum length of a single row? What's the maximum length allowed for a schema name? What's the maximum length of a SQL statement? How many active statements can we have open at one time to this database? What's the maximum length of a table name? What's the maximum number of tables in a SELECT? What's the maximum length of a user name? Get a comma separated list of JDBC escaped numeric functions. Must be a complete or sub set of functions in appendix C.1 of JDBC 3.0 specification (pp. 183). Get a prepared query from system tables or metadata.properties. Either get the prepared query for the metadata call from the system tables, or from the metadata.properties or metadata_net.properties file. In soft upgrade mode, the queries stored in the system tables might not be upto date with the Derby engine release because system tables can't be modified in backward incompatible way in soft upgrade mode. Because of this, if the database is in soft upgrade mode, get the queries from metadata.properties file rather than from the system tables. Getting queries from metadata(_net).properties might cause problems if system catalogs have been changed between versions either by addition of columns or have new catalogs. To continue to support soft upgrade from older versions of database, find query that most closely matches database dictionary version. Get a stored prepared statement from the system tables. Get a description of a table's primary key columns.  They are ordered by COLUMN_NAME. <P>Each primary key column description has the following columns: <OL> <LI><B>TABLE_CAT</B> String =&gt; table catalog (may be null) <LI><B>TABLE_SCHEM</B> String =&gt; table schema (may be null) <LI><B>TABLE_NAME</B> String =&gt; table name <LI><B>COLUMN_NAME</B> String =&gt; column name <LI><B>KEY_SEQ</B> short =&gt; sequence number within primary key <LI><B>PK_NAME</B> String =&gt; primary key name (may be null) </OL> Get a description of a catalog's stored procedure parameters and result columns. <P>Only descriptions matching the schema, procedure and parameter name criteria are returned.  They are ordered by PROCEDURE_SCHEM and PROCEDURE_NAME. Within this, the return value, if any, is first. Next are the parameter descriptions in call order. The column descriptions follow in column number order. <P>Each row in the ResultSet is a parameter description or column description with the following fields: <OL> <LI><B>PROCEDURE_CAT</B> String =&gt; procedure catalog (may be null) <LI><B>PROCEDURE_SCHEM</B> String =&gt; procedure schema (may be null) <LI><B>PROCEDURE_NAME</B> String =&gt; procedure name <LI><B>COLUMN_NAME</B> String =&gt; column/parameter name <LI><B>COLUMN_TYPE</B> Short =&gt; kind of column/parameter: <UL> <LI> procedureColumnUnknown - nobody knows <LI> procedureColumnIn - IN parameter <LI> procedureColumnInOut - INOUT parameter <LI> procedureColumnOut - OUT parameter <LI> procedureColumnReturn - procedure return value <LI> procedureColumnResult - result column in ResultSet </UL> <LI><B>DATA_TYPE</B> int =&gt; SQL type from java.sql.Types <LI><B>TYPE_NAME</B> String =&gt; SQL type name <LI><B>PRECISION</B> int =&gt; precision <LI><B>LENGTH</B> int =&gt; length in bytes of data <LI><B>SCALE</B> short =&gt; scale <LI><B>RADIX</B> short =&gt; radix <LI><B>NULLABLE</B> short =&gt; can it contain NULL? <UL> <LI> procedureNoNulls - does not allow NULL values <LI> procedureNullable - allows NULL values <LI> procedureNullableUnknown - nullability unknown </UL> <LI><B>REMARKS</B> String =&gt; comment describing parameter/column <LI><B>COLUMN_DEF</B> String <LI><B>SQL_DATA_TYPE</B> int <LI><B>SQL_DATETIME_SUB</B> int <LI><B>CHAR_OCTET_LENGTH</B> int <LI><B>ORDINAL_POSITION</B> int <LI><B>IS_NULLABLE</B> String <LI><B>SPECIFIC_NAME</B> String </OL> <P><B>Note:</B> Some databases may not return the column descriptions for a procedure. Additional columns beyond SPECIFIC_NAME can be defined by the database. Get a description of a catalog's stored procedure parameters and result columns.  Same as getProcedureColumns() above, except that the result set will conform to ODBC specifications. What's the database vendor's preferred term for "procedure"? Get a description of stored procedures available in a catalog. <P>Only procedure descriptions matching the schema and procedure name criteria are returned.  They are ordered by PROCEDURE_SCHEM, and PROCEDURE_NAME. <P>Each procedure description has the the following columns: <OL> <LI><B>PROCEDURE_CAT</B> String =&gt; procedure catalog (may be null) <LI><B>PROCEDURE_SCHEM</B> String =&gt; procedure schema (may be null) <LI><B>PROCEDURE_NAME</B> String =&gt; procedure name <LI> reserved for future use <LI> reserved for future use <LI> reserved for future use <LI><B>REMARKS</B> String =&gt; explanatory comment on the procedure <LI><B>PROCEDURE_TYPE</B> short =&gt; kind of procedure: <UL> <LI> procedureResultUnknown - May return a result <LI> procedureNoResult - Does not return a result <LI> procedureReturnsResult - Returns a result </UL> <LI><B>SPECIFIC_NAME</B> String =&gt; The name which uniquely identifies this procedure within its schema (since JDBC 4.0) </OL> Get a description of stored procedures available in a catalog.  Same as getProcedures() above, except that the result set will conform to ODBC specifications. See DatabaseMetaData javadoc. Empty ResultSet because Derby does not support pseudo columns. Return all queries found in either metadata.properties or metadata_net.properties. Given a queryName, find closest match in queryDescriptions. This method should be called in soft-upgrade mode only, where current software version doesn't match dictionary version. For these cases, there may be multiple entries in queryDescriptions for given queryName. Find a version of the query that closely matches dictionary version. This method is currently coded to handle two specific queries, getColumnPrivileges and getTablePrivileges. Derby databases that are 10.1 or earlier will not have new system tables added for 10.2 for privileges. It should be possible to automate finding closest match by generating all Major_Minor versions between software version and dictionary version and try each one from Dictionary version to current version. Since only needed for two queries, overhead may not be worth it yet. JDBC 3.0 Retrieves the default holdability of this ResultSet object. Get a comma separated list of all a database's SQL keywords that are NOT also SQL92 keywords. includes reserved and non-reserved keywords. JDBC 3.0 Indicates whether the SQLSTATEs returned by SQLException.getSQLState is X/Open (now known as Open Group) SQL CLI or SQL99. What's the database vendor's preferred term for "schema"? Get the schema names available in this database.  The results are ordered by schema name. <P>The schema columns are: <OL> <li><strong>TABLE_SCHEM</strong> String =&gt; schema name</li> <li><strong>TABLE_CATALOG</strong> String =&gt; catalog name (may be <code>null</code>)</li> </OL> JDBC 4.0 <p>Get the schema names available in this database. The results are ordered by schema name. <p>The schema columns are: <ol> <li><strong>TABLE_SCHEM</strong> String =&gt; schema name</li> <li><strong>TABLE_CATALOG</strong> String =&gt; catalog name (may be <code>null</code>)</li> </ol> This is the string that can be used to escape '_' or '%' in the string pattern style catalog search parameters. we have no default escape value, so = is the end of the next line <P>The '_' character represents any single character. <P>The '%' character represents any sequence of zero or more characters. Execute a query in metadata.properties, or an SPS in the SYS schema. utility helper routines: Execute a query in metadata.properties (or SPS in the SYS schema) or metadata_net.properties (or SPS in the SYSIBM schema). Get a comma separated list of JDBC escaped string functions. Must be a complete or sub set of functions in appendix C.2 of JDBC 3.0 specification (pp. 184). JDBC 3.0 Retrieves a description of the table hierarchies defined in a particular schema in this database. JDBC 3.0 Retrieves a description of the user-defined type (UDT) hierarchies defined in a particular schema in this database. Only the immediate super type/ sub type relationship is modeled. Get a comma separated list of JDBC escaped system functions. Must be a complete or sub set of functions in appendix C.4 of JDBC 3.0 specification (pp. 185). Get a description of the access rights for each table available in a catalog. Note that a table privilege applies to one or more columns in the table. It would be wrong to assume that this priviledge applies to all columns (this may be true for some systems but is not true for all.) <P>Only privileges matching the schema and table name criteria are returned.  They are ordered by TABLE_SCHEM, TABLE_NAME, and PRIVILEGE. <P>Each privilige description has the following columns: <OL> <LI><B>TABLE_CAT</B> String =&gt; table catalog (may be null) <LI><B>TABLE_SCHEM</B> String =&gt; table schema (may be null) <LI><B>TABLE_NAME</B> String =&gt; table name <LI><B>GRANTOR</B> =&gt; grantor of access (may be null) <LI><B>GRANTEE</B> String =&gt; grantee of access <LI><B>PRIVILEGE</B> String =&gt; name of access (SELECT, INSERT, UPDATE, REFRENCES, ...) <LI><B>IS_GRANTABLE</B> String =&gt; "YES" if grantee is permitted to grant to others; "NO" if not; null if unknown </OL> Get the table types available in this database.  The results are ordered by table type. <P>The table type is: <OL> <LI><B>TABLE_TYPE</B> String =&gt; table type.  Typical types are "TABLE", "VIEW",	"SYSTEM TABLE", "GLOBAL TEMPORARY", "LOCAL TEMPORARY", "ALIAS", "SYNONYM". </OL> Get a description of tables available in a catalog. <P>Only table descriptions matching the catalog, schema, table name and type criteria are returned.  They are ordered by TABLE_TYPE, TABLE_SCHEM and TABLE_NAME. <P>Each table description has the following columns: <OL> <LI><B>TABLE_CAT</B> String =&gt; table catalog (may be null) <LI><B>TABLE_SCHEM</B> String =&gt; table schema (may be null) <LI><B>TABLE_NAME</B> String =&gt; table name <LI><B>TABLE_TYPE</B> String =&gt; table type.  Typical types are "TABLE", "VIEW",	"SYSTEM TABLE", "GLOBAL TEMPORARY", "LOCAL TEMPORARY", "ALIAS", "SYNONYM". <LI><B>REMARKS</B> String =&gt; explanatory comment on the table <LI><B>TYPE_CAT</B> String =&gt; the types catalog (may be <code>null</code>) <LI><B>TYPE_SCHEM</B> String =&gt; the types schema (may be <code>null</code>) <LI><B>TYPE_NAME</B> String =&gt; type name (may be <code>null</code>) <LI><B>SELF_REFERENCING_COL_NAME</B> String =&gt; name of the designated "identifier" column of a typed table (may be <code>null</code>) <LI><B>REF_GENERATION</B> String =&gt; specifies how values in SELF_REFERENCING_COL_NAME are created. Values are "SYSTEM", "USER", "DERIVED". (may be <code>null</code>) </OL> <P><B>Note:</B> Some databases may not return information for all tables. Get a comma separated list of JDBC escaped time date functions. Must be a complete or sub set of functions in appendix C.3 of JDBC 3.0 specification. Get a description of all the standard SQL types supported by this database. They are ordered by DATA_TYPE and then by how closely the data type maps to the corresponding JDBC SQL type. <P>Each type description has the following columns: <OL> <LI><B>TYPE_NAME</B> String =&gt; Type name <LI><B>DATA_TYPE</B> int =&gt; SQL data type from java.sql.Types <LI><B>PRECISION</B> int =&gt; maximum precision <LI><B>LITERAL_PREFIX</B> String =&gt; prefix used to quote a literal (may be null) <LI><B>LITERAL_SUFFIX</B> String =&gt; suffix used to quote a literal (may be null) <LI><B>CREATE_PARAMS</B> String =&gt; parameters used in creating the type (may be null) <LI><B>NULLABLE</B> short =&gt; can you use NULL for this type? <UL> <LI> typeNoNulls - does not allow NULL values <LI> typeNullable - allows NULL values <LI> typeNullableUnknown - nullability unknown </UL> <LI><B>CASE_SENSITIVE</B> boolean=&gt; is it case sensitive? <LI><B>SEARCHABLE</B> short =&gt; can you use "WHERE" based on this type: <UL> <LI> typePredNone - No support <LI> typePredChar - Only supported with WHERE .. LIKE <LI> typePredBasic - Supported except for WHERE .. LIKE <LI> typeSearchable - Supported for all WHERE .. </UL> <LI><B>UNSIGNED_ATTRIBUTE</B> boolean =&gt; is it unsigned? <LI><B>FIXED_PREC_SCALE</B> boolean =&gt; can it be a money value? <LI><B>AUTO_INCREMENT</B> boolean =&gt; can it be used for an auto-increment value? <LI><B>LOCAL_TYPE_NAME</B> String =&gt; localized version of type name (may be null) <LI><B>MINIMUM_SCALE</B> short =&gt; minimum scale supported <LI><B>MAXIMUM_SCALE</B> short =&gt; maximum scale supported <LI><B>SQL_DATA_TYPE</B> int =&gt; unused <LI><B>SQL_DATETIME_SUB</B> int =&gt; unused <LI><B>NUM_PREC_RADIX</B> int =&gt; usually 2 or 10 </OL> Get a description of all the standard SQL types supported by this database. They are ordered by DATA_TYPE and then by how closely the data type maps to the corresponding JDBC SQL type. Same as getTypeInfo above, except that the result set will conform to ODBC specifications. Get a description of the standard SQL types supported by this database. JDBC 2.0 Get a description of the user-defined types defined in a particular schema.  Schema specific UDTs may have type JAVA_OBJECT, STRUCT, or DISTINCT. <P>Only types matching the catalog, schema, type name and type criteria are returned.  They are ordered by DATA_TYPE, TYPE_SCHEM and TYPE_NAME.  The type name parameter may be a fully qualified name.  In this case, the catalog and schemaPattern parameters are ignored. <P>Each type description has the following columns: <OL> <LI><B>TYPE_CAT</B> String =&gt; the type's catalog (may be null) <LI><B>TYPE_SCHEM</B> String =&gt; type's schema (may be null) <LI><B>TYPE_NAME</B> String =&gt; type name <LI><B>CLASS_NAME</B> String =&gt; Java class name <LI><B>DATA_TYPE</B> String =&gt; type value defined in java.sql.Types. One of JAVA_OBJECT, STRUCT, or DISTINCT <LI><B>REMARKS</B> String =&gt; explanatory comment on the type <LI><B>BASE_TYPE</B> short =&gt; type code of the source type of a DISTINCT type or the type that implements the user-generated reference type of the SELF_REFERENCING_COLUMN of a structured type as defined in java.sql.Types (<code>null</code> if DATA_TYPE is not DISTINCT or not STRUCT with REFERENCE_GENERATION = USER_DEFINED) </OL> <P><B>Note:</B> If the driver does not support UDTs then an empty result set is returned. What's the url for this database? What's our user name as known to the database? Get a description of a table's columns that are automatically updated when any value in a row is updated.  They are unordered. <P>Each column description has the following columns: <OL> <LI><B>SCOPE</B> short =&gt; is not used <LI><B>COLUMN_NAME</B> String =&gt; column name <LI><B>DATA_TYPE</B> int =&gt; SQL data type from java.sql.Types <LI><B>TYPE_NAME</B> String =&gt; Data source dependent type name <LI><B>COLUMN_SIZE</B> int =&gt; precision <LI><B>BUFFER_LENGTH</B> int =&gt; length of column value in bytes <LI><B>DECIMAL_DIGITS</B> short	 =&gt; scale <LI><B>PSEUDO_COLUMN</B> short =&gt; is this a pseudo column like an Oracle ROWID <UL> <LI> versionColumnUnknown - may or may not be pseudo column <LI> versionColumnNotPseudo - is NOT a pseudo column <LI> versionColumnPseudo - is a pseudo column </UL> </OL> Get a description of a table's columns that are automatically updated when any value in a row is updated.  They are unordered.  Same as getVersionColumns() above, except that the result set will conform to ODBC specifications. JDBC 2.0 Determine whether or not a visible row insert can be detected by calling ResultSet.rowInserted(). Does a catalog appear at the start of a qualified table name? (Otherwise it appears at the end) Is the database in read-only mode? java.sql.Wrapper interface methods Returns whether or not this instance implements the specified interface. * Priv block code, moved out of the old Java2 version. Loads the query descriptions from metadata.properties and metadata_net.properties into <code>queryDescriptions</code> and <code>queryDescriptions_net</code>. JDBC 3.0 Indicates whether updates made to a LOB are made on a copy or directly to the LOB. <p> For the embedded driver, all updates will be made to a copy. Hence, this call will always return <code>true</code> check if the dictionary is at the same version as the engine. If not, then that means stored versions of the JDBC database metadata queries may not be compatible with this version of the software. This can happen if we are in soft upgrade mode. Since in soft upgrade mode, we can't change these stored metadata queries in a backward incompatible way, engine needs to read the metadata sql from metadata.properties or metadata_net.properties file rather than rely on system tables. Are concatenations between NULL and non-NULL values NULL? A JDBC-Compliant driver always returns true. Are NULL values sorted at the end regardless of sort order? Are NULL values sorted at the start regardless of sort order? Are NULL values sorted high? Are NULL values sorted low? JDBC 2.0 Determine whether deletes made by others are visible. JDBC 2.0 Determine whether inserts made by others are visible. Since Derby materializes a forward only ResultSet incrementally, it is possible to see changes made by others and hence following 3 metadata calls will return true for forward only ResultSets. JDBC 2.0 Determine whether updates made by others are visible. JDBC 2.0 Determine whether a result set's deletes are visible. JDBC 2.0 Determine whether a result set's inserts are visible. JDBC 2.0 Determine whether a result set's updates are visible. * Given a SPS name and a query text it returns a * java.sql.PreparedStatement for the SPS. If the SPS * doeesn't exist is created. * Performs a privileged action. Reads the query descriptions. Does the database treat mixed case unquoted SQL identifiers as case insensitive and store them in lower case? Does the database treat mixed case quoted SQL identifiers as case insensitive and store them in lower case? Does the database treat mixed case unquoted SQL identifiers as case insensitive and store them in mixed case? Does the database treat mixed case quoted SQL identifiers as case insensitive and store them in mixed case? Does the database treat mixed case unquoted SQL identifiers as case insensitive and store them in upper case? Does the database treat mixed case quoted SQL identifiers as case insensitive and store them in upper case? Is the ANSI92 entry level SQL grammar supported? All JDBC-Compliant drivers must return true. Is the ANSI92 full SQL grammar supported? Is the ANSI92 intermediate SQL grammar supported? -------------------------------------------------------------------- Functions describing which features are supported. Is "ALTER TABLE" with add column supported? Is "ALTER TABLE" with drop column supported? JDBC 2.0 Return true if the driver supports batch updates, else return false. Can a catalog name be used in a data manipulation statement? Can a catalog name be used in an index definition statement? Can a catalog name be used in a privilege definition statement? Can a catalog name be used in a procedure call statement? Can a catalog name be used in a table definition statement? Is column aliasing supported? <P>If so, the SQL AS clause can be used to provide names for computed columns or to provide alias names for columns as required. A JDBC-Compliant driver always returns true. Is the CONVERT function between SQL types supported? Is CONVERT between the given SQL types supported? Is the ODBC Core SQL grammar supported? Are correlated subqueries supported? A JDBC-Compliant driver always returns true. Are both data definition and data manipulation statements within a transaction supported? Are only data manipulation statements within a transaction supported? If table correlation names are supported, are they restricted to be different from the names of the tables? Are expressions in "ORDER BY" lists supported? Is the ODBC Extended SQL grammar supported? Are full nested outer joins supported? JDBC 3.0 Retrieves whether auto-generated keys can be retrieved after a statement has been executed. Is some form of "GROUP BY" clause supported? Can a "GROUP BY" clause add columns not in the SELECT provided it specifies all the columns in the SELECT? Can a "GROUP BY" clause use columns not in the SELECT? Is the SQL Integrity Enhancement Facility supported? Is the escape character in "LIKE" clauses supported? A JDBC-Compliant driver always returns true. Is there limited support for outer joins?  (This will be true if supportFullOuterJoins is true.) Is the ODBC Minimum SQL grammar supported? All JDBC-Compliant drivers must return true. Does the database treat mixed case unquoted SQL identifiers as case sensitive and as a result store them in mixed case? A JDBC-Compliant driver will always return false. Does the database treat mixed case quoted SQL identifiers as case sensitive and as a result store them in mixed case? A JDBC-Compliant driver will always return true. JDBC 3.0 Retrieves whether it is possible to have multiple ResultSet objects returned from a CallableStatement object simultaneously. Are multiple ResultSets from a single execute supported? Can we have multiple transactions open at once (on different connections)? JDBC 3.0 Retrieves whether this database supports named parameters to callable statements. Can columns be defined as non-nullable? A JDBC-Compliant driver always returns true. Can cursors remain open across commits? returns false because Derby does not support cursors that are open across commits for XA transactions. Can cursors remain open across rollbacks? Can statements remain open across commits? Can statements remain open across rollbacks? Can an "ORDER BY" clause use columns not in the SELECT? Is some form of outer join supported? Is positioned DELETE supported? Is positioned UPDATE supported? Added in JDBC 4.2. Derby does not support the Types.REF_CURSOR type. JDBC 2.0 Does the database support the concurrency type in combination with the given result set type? JDBC 3.0 Retrieves whether this database supports the given result set holdability. ///////////////////////////////////////////////////////////////////////  JDBC 2.0	-	New public methods  /////////////////////////////////////////////////////////////////////// JDBC 2.0 Does the database support the given result set type? JDBC 3.0 Retrieves whether this database supports savepoints. Can a schema name be used in a data manipulation statement? Can a schema name be used in an index definition statement? Can a schema name be used in a privilege definition statement? Can a schema name be used in a procedure call statement? Can a schema name be used in a table definition statement? Is SELECT for UPDATE supported? Following methods are for the new JDBC 3.0 methods in java.sql.DatabaseMetaData (see the JDBC 3.0 spec). We have the JDBC 3.0 methods in Local20 package, so we don't have to have a new class in Local30. The new JDBC 3.0 methods don't make use of any new JDBC3.0 classes and so this will work fine in jdbc2.0 configuration. ///////////////////////////////////////////////////////////////////////  JDBC 3.0	-	New public methods  /////////////////////////////////////////////////////////////////////// JDBC 3.0 Retrieves whether this database supports statement pooling. Are stored procedure calls using the stored procedure escape syntax supported? Are subqueries in comparison expressions supported? A JDBC-Compliant driver always returns true. Are subqueries in 'exists' expressions supported? A JDBC-Compliant driver always returns true. Are subqueries in 'in' statements supported? A JDBC-Compliant driver always returns true. Are subqueries in quantified expressions supported? A JDBC-Compliant driver always returns true. Are table correlation names supported? A JDBC-Compliant driver always returns true. Does the database support the given transaction isolation level? DatabaseMetaData.supportsTransactionIsolation() should return false for isolation levels that are not supported even if a higher level can be substituted. Are transactions supported? If not, commit is a noop and the isolation level is TRANSACTION_NONE. Is SQL UNION supported? Is SQL UNION ALL supported? Returns {@code this} if this class implements the interface. JDBC 2.0 Determine whether or not a visible row update can be detected by calling ResultSet.rowUpdated(). Does the database use a file for each table? Does the database store tables in a local file?
Check the position number for a parameter and throw an exception if it is out of range. Retrieves the fully-qualified name of the Java class whose instances should be passed to the method PreparedStatement.setObject. Retrieves the number of parameters in the PreparedStatement object for which this ParameterMetaData object contains information. Retrieves the designated parameter's mode. Retrieves the designated parameter's SQL type. Retrieves the designated parameter's database-specific type name. Retrieves the designated parameter's number of decimal digits. Retrieves the designated parameter's number of digits to right of the decimal point. Retrieves whether null values are allowed in the designated parameter. Retrieves whether values for the designated parameter can be signed numbers. java.sql.Wrapper interface methods Returns false unless {@code iface} is implemented. Returns {@code this} if this class implements the specified interface.
Add an event listener. Registers a {@code StatementEventListener} with this {@code PooledConnection} object. Components that wish to be notified when {@code PreparedStatement}s created by the connection are closed or are detected to be invalid may use this method to register a {@code StatementEventListener} with this {@code PooledConnection} object. Allow control over setting auto commit mode.  Allow control over calling commit. Are held cursors allowed. Allow control over calling rollback. Allow control over creating a Savepoint (JDBC 3.0) Close the Pooled connection. In this case the Listeners are *not* notified. JDBC 3.0 spec section 11.4 Close called on BrokeredConnection. If this call returns true then getRealConnection().close() will be called. Notify listners that connection is closed. Don't close the underlying real connection as it is pooled. Fire all the {@code ConnectionEventListener}s registered. Callers must synchronize on {@code this} to prevent others from modifying the list of listeners. Create an object handle for a database connection.  Gets the LanguageConnectionContext for this connection. class specific method called by ConnectionHandle when it needs to forward things to the underlying connection getter function for isActive  * BrokeredConnectionControl api Returns true if isolation level has been set using either JDBC api or SQL my conneciton handle has caught an error (actually, the real connection has already handled the error, we just need to nofity the listener an error is about to be thrown to the app). Notify the control class that a SQLException was thrown during a call on one of the brokered connection's methods. ----------------------------------------------------------------- These methods are from the BrokeredConnectionControl interface. These methods are needed to provide StatementEvent support for derby. Raise the statementClosed event for all the listeners when the corresponding events occurs Raise the statementErrorOccurred event for all the listeners when the corresponding events occurs Remove an event listener. JDBC 4.0 methods Removes the specified {@code StatementEventListener} from the list of components that will be notified when the driver detects that a {@code PreparedStatement} has been closed or is invalid. Reset the isolation level flag used to keep state in BrokeredConnection. It will get set to true when isolation level is set using JDBC/SQL. It will get reset to false at the start and the end of a global transaction. Get the string representation of this pooled connection. A pooled connection is assigned a separate id from a physical connection. When a container calls PooledConnection.toString(), it gets the string representation of this id. This is useful for developers implementing connection pools when they are trying to debug pooled connections. Call the setBrokeredConnectionControl method inside the EmbedCallableStatement class to set the BrokeredConnectionControl variable to this instance of EmbedPooledConnection This will then be used to call the onStatementErrorOccurred and onStatementClose events when the corresponding events occur on the CallableStatement Call the setBrokeredConnectionControl method inside the EmbedPreparedStatement class to set the BrokeredConnectionControl variable to this instance of EmbedPooledConnection This will then be used to call the onStatementErrorOccurred and onStatementClose events when the corresponding events occur on the PreparedStatement No need to wrap statements for PooledConnections.
JDBC 2.0 Add a set of parameters to the batch. Check general preconditions for setAsciiStream methods. Check general preconditions for setBinaryStream methods. Check general (pre)conditions for setBlob methods. Check general preconditions for setCharacterStream methods. Check general (pre)conditions for setClob methods. Method calls onStatementError occurred on the BrokeredConnectionControl class after checking the SQLState of the SQLException thrown. In the case that a XAConnection is involved in the creation of this PreparedStatement for e.g in the following case <code> XAConnection xaconn = xadatasource.getXAConnection();//where xadatasource is an object of XADataSource Connection conn = xaconnection.getConnection(); PreparedStatement ps = conn.preparedStatement("values 1"); </code> In the above case the PreparedStatement will actually be a BrokeredPreparedStatement object. Hence when we call bcc.onStatementClose and pass the PreparedStatement that caused it applicationStatement will be the appropriate choice since it will contain the appropriate instance of PreparedStatement in each case <P>In general, parameter values remain in force for repeated use of a Statement. Setting a parameter value automatically clears its previous value.  However, in some cases it is useful to immediately release the resources used by the current parameter values; this can be done by calling clearParameters. Additional close to close our activation. In the case that a XAConnection is involved in the creation of this PreparedStatement for e.g in the following case <code> XAConnection xaconn = xadatasource.getXAConnection();//where xadatasource is an object of XADataSource Connection conn = xaconnection.getConnection(); PreparedStatement ps = conn.preparedStatement("values 1"); </code> In the above case the PreparedStatement will actually be a BrokeredPreparedStatement object. Hence when we call bcc.onStatementClose and pass the PreparedStatement that caused it applicationStatement will be the appropriate choice since it will contain the appropriate instance of PreparedStatement in each case  Statement interface we override all Statement methods that take a SQL string as they must thrown an exception in a PreparedStatement. See the JDBC 3.0 spec. Execute a SQL INSERT, UPDATE or DELETE statement. In addition, SQL statements that return nothing such as SQL DDL statements can be executed. For use with statements which may touch more than Integer.MAX_VALUE rows. PreparedStatement interface; we have inherited from EmbedStatement to get the Statement interface for EmbedPreparedStatement (needed by PreparedStatement) These are the JDBC interface comments, so we know what to do. A prepared SQL query is executed and its ResultSet is returned. Execute a SQL INSERT, UPDATE or DELETE statement. In addition, SQL statements that return nothing such as SQL DDL statements can be executed. JDBC states that a Statement is closed when garbage collected. JDBC 2.0 The number, types and properties of a ResultSet's columns are provided by the getMetaData method. Get the target JDBC type for a parameter. Will throw exceptions if the parameter index is out of range. The parameterIndex is 1-based. JDBC 3.0 Retrieves the number, types and properties of this PreparedStatement object's parameters. Return the SQL type name for the parameter. Get the ParameterValueSet from the activation. The caller of this method should be aware that the activation associated with a Statement can change and hence the ParameterValueSet returned by this call should not be hold onto. An example of this can be seen in EmbedCallableStatement.executeStatement where at the beginning of the method, we check the validity of the parameters. But we donot keep the parameters in a local variable to use later. The reason for this is that the next call in the method, super.executeStatement can recompile the statement and create a new activation if the statement plan has been invalidated. To account for this possibility, EmbedCallableStatement.executeStatement makes another call to get the ParameterValueSet before stuffing the output parameter value into the ParameterValueSet object.  methods to be overridden in subimplementations that want to stay within their subimplementation.  JDBC 2.0 Set an Array parameter. jdbc 4.0 methods Sets the designated parameter to the given input stream. When a very large ASCII value is input to a <code>LONGVARCHAR</code> parameter, it may be more practical to send it via a <code>java.io.InputStream</code>. Data will be read from the stream as needed until end-of-file is reached. The JDBC driver will do any necessary conversion from ASCII to the database char format. <em>Note:</em> This stream object can either be a standard Java stream object or your own subclass that implements the standard interface. We do this inefficiently and read it all in here. The target type is assumed to be a String. We do this inefficiently and read it all in here. The target type is assumed to be a String. Set a parameter to a java.lang.BigDecimal value. The driver converts this to a SQL NUMERIC value when it sends it to the database. Sets the designated parameter to the given input stream. When a very large binary value is input to a <code>LONGVARBINARY</code> parameter, it may be more practical to send it via a <code>java.io.InputStream</code> object. The data will be read from the stream as needed until end-of-file is reached. <em>Note:</em> This stream object can either be a standard Java stream object or your own subclass that implements the standard interface. sets the parameter to the binary stream sets the parameter to the Binary stream Set the given stream for the specified parameter. If <code>lengthLess</code> is <code>true</code>, the following conditions are either not checked or verified at the execution time of the prepared statement: <ol><li>If the stream length is negative. <li>If the stream's actual length equals the specified length.</ol> The <code>lengthLess</code> variable was added to differentiate between streams with invalid lengths and streams without known lengths. Sets the designated parameter to a <code>InputStream</code> object. This method differs from the <code>setBinaryStream(int, InputStream) </code>  method because it informs the driver that the parameter value should be sent to the server as a <code>BLOB</code>. When the <code>setBinaryStream</code> method is used, the driver may have to do extra work to determine whether the parameter data should be sent to the server as a <code>LONGVARBINARY</code> or a <code>BLOB</code> Sets the designated parameter to a InputStream object. JDBC 2.0 Set a BLOB parameter. Set a parameter to a Java boolean value.  According to the JDBC API spec, the driver converts this to a SQL BIT value when it sends it to the database. But we don't have to do this, since the database engine supports a boolean type. This method is used to initialize the BrokeredConnectionControl variable with its implementation. This method will be called in the BrokeredConnectionControl class Set a parameter to a Java byte value.  The driver converts this to a SQL TINYINT value when it sends it to the database. Set a parameter to a Java array of bytes.  The driver converts this to a SQL VARBINARY or LONGVARBINARY (depending on the argument's size relative to the driver's limits on VARBINARYs) when it sends it to the database. Sets the designated parameter to the given <code>Reader</code> object. When a very large UNICODE value is input to a LONGVARCHAR parameter, it may be more practical to send it via a <code>java.io.Reader</code> object. The data will be read from the stream as needed until end-of-file is reached. The JDBC driver will do any necessary conversion from UNICODE to the database char format. <em>Note:</em> This stream object can either be a standard Java stream object or your own subclass that implements the standard interface. Using this lengthless overload is not less effective than using one where the stream length is specified, but since there is no length specified, the exact length check will not be performed. When a very large UNICODE value is input to a LONGVARCHAR parameter, it may be more practical to send it via a java.io.Reader. JDBC will read the data from the stream as needed, until it reaches end-of-file.  The JDBC driver will do any necessary conversion from UNICODE to the database char format. <P><B>Note:</B> This stream object can either be a standard Java stream object or your own subclass that implements the standard interface. When a very large UNICODE value is input to a LONGVARCHAR parameter, it may be more practical to send it via a java.io.Reader. JDBC will read the data from the stream as needed, until it reaches end-of-file.  The JDBC driver will do any necessary conversion from UNICODE to the database char format. <P><B>Note:</B> This stream object can either be a standard Java stream object or your own subclass that implements the standard interface. Set the given character stream for the specified parameter. If <code>lengthLess</code> is <code>true</code>, the following conditions are either not checked or verified at the execution time of the prepared statement: <ol><li>If the stream length is negative. <li>If the stream's actual length equals the specified length.</ol> The <code>lengthLess</code> variable was added to differentiate between streams with invalid lengths and streams without known lengths. Sets the designated parameter to a <code>Reader</code> object. This method differs from the <code>setCharacterStream(int,Reader)</code> method because it informs the driver that the parameter value should be sent to the server as a <code>CLOB</code>. When the <code>setCharacterStream</code> method is used, the driver may have to do extra work to determine whether the parameter data should be sent to the server as a <code>LONGVARCHAR</code> or a <code>CLOB</code>. Sets the designated parameter to a Reader object. JDBC 2.0 Set a CLOB parameter. Set a parameter to a java.sql.Date value.  The driver converts this to a SQL DATE value when it sends it to the database. Set a parameter to a java.sql.Date value.  The driver converts this to a SQL DATE value when it sends it to the database. Set a parameter to a Java double value.  The driver converts this to a SQL DOUBLE value when it sends it to the database. Set a parameter to a Java float value.  The driver converts this to a SQL FLOAT value when it sends it to the database. Set a parameter to a Java int value.  The driver converts this to a SQL INTEGER value when it sends it to the database. Set a parameter to a Java long value.  The driver converts this to a SQL BIGINT value when it sends it to the database. Set a parameter to SQL NULL. <P><B>Note:</B> You must specify the parameter's SQL type. ///////////////////////////////////////////////////////////////////////  JDBC 2.0	-	New public methods  /////////////////////////////////////////////////////////////////////// JDBC 2.0 Sets the designated parameter to SQL <code>NULL</code>. This version of the method <code>setNull</code> should be used for user-defined types and REF type parameters.  Examples of user-defined types include: STRUCT, DISTINCT, JAVA_OBJECT, and named array types. <p>Set the value of a parameter using an object; use the java.lang equivalent objects for integral values. <p>The JDBC specification specifies a standard mapping from Java Object types to SQL types.  The given argument java object will be converted to the corresponding SQL type before being sent to the database. <p>Note that this method may be used to pass datatabase specific abstract data types, by using a Driver specific Java type. This method is like setObject above, but assumes a scale of zero. ---------------------------------------------------------------------- Advanced features: The interface says that the type of the Object parameter must be compatible with the type of the targetSqlType. We check that, and if it flies, we expect the underlying engine to do the required conversion once we pass in the value using its type. So, an Integer converting to a CHAR is done via setInteger() support on the underlying CHAR type. <p>If x is null, it won't tell us its type, so we pass it on to setNull JDBC 2.0 Set a REF(&lt;structured-type&gt;) parameter. Set the scale of a parameter. Set a parameter to a Java short value.  The driver converts this to a SQL SMALLINT value when it sends it to the database. Set a parameter to a Java String value.  The driver converts this to a SQL VARCHAR or LONGVARCHAR value (depending on the arguments size relative to the driver's limits on VARCHARs) when it sends it to the database. Set a parameter to a java.sql.Time value.  The driver converts this to a SQL TIME value when it sends it to the database. Set a parameter to a java.sql.Time value.  The driver converts this to a SQL TIME value when it sends it to the database. Set a parameter to a java.sql.Timestamp value.  The driver converts this to a SQL TIMESTAMP value when it sends it to the database. Set a parameter to a java.sql.Timestamp value.  The driver converts this to a SQL TIMESTAMP value when it sends it to the database. JDBC 3.0 Sets the designated parameter to the given java.net.URL value. The driver converts this to an SQL DATALINK value when it sends it to the database. Deprecated in JDBC 3.0 Determines which header format to use for CLOBs when writing them to the store.

JDBC 2.0 <p> Move to an absolute row number in the result set. <p> If row is positive, moves to an absolute row with respect to the beginning of the result set. The first row is row 1, the second is row 2, etc. <p> If row is negative, moves to an absolute row position with respect to the end of result set. For example, calling absolute(-1) positions the cursor on the last row, absolute(-2) indicates the next-to-last row, etc. <p> An attempt to position the cursor beyond the first/last row in the result set, leaves the cursor before/after the first/last row, respectively. <p> Note: Calling absolute(1) is the same as calling first(). Calling absolute(-1) is the same as calling last(). Adds a warning to the end of the warning chain. <p> Adjust the scale of a type. </p> JDBC 2.0 <p> Moves to the end of the result set, just after the last row. Has no effect if the result set contains no rows. JDBC 2.0 <p> Moves to the front of the result set, just before the first row. Has no effect if the result set contains no rows. JDBC 2.0 The cancelRowUpdates() method may be called after calling an updateXXX() method(s) and before calling updateRow() to rollback the updates made to a row.  If no updates have been made or updateRow() has already been called, then this method has no effect. Throw an exception if this ResultSet is closed or its Connection has been closed. If the ResultSet has not been explictly closed but the Connection is closed, then this ResultSet will be marked as closed. Throw an exception if this ResultSet is closed. Checks if a stream or a LOB object has already been created for the specified LOB column. <p> Accessing a LOB column more than once is not forbidden by the JDBC specification, but the Java API states that for maximum portability, result set columns within each row should be read in left-to-right order, and each column should be read only once. The restriction was implemented in Derby due to complexities with the positioning of store streams when the user was given multiple handles to the stream. checkOnRow protects us from making requests of resultSet that would fail with NullPointerExceptions or milder problems due to not having a row. Checks if the result set has a scrollable cursor. do following few checks before accepting insertRow 1) Make sure JDBC ResultSet is not closed 2) Make sure this is an updatable ResultSet 3) Make sure JDBC ResultSet is positioned on insertRow Check whether it is OK to update a column using <code>updateAsciiStream()</code>. Check whether it is OK to update a column using <code>updateBinaryStream()</code>. Check whether it is OK to update a column using <code>updateCharacterStream()</code>. do following few checks before accepting updateRow or deleteRow 1)Make sure JDBC ResultSet is not closed 2)Make sure this is an updatable ResultSet 3)Make sure JDBC ResultSet is positioned on a row do following few checks before accepting updateXXX resultset api After this call getWarnings returns null until a new warning is reported for this ResultSet. In some cases, it is desirable to immediately release a ResultSet's database and JDBC resources instead of waiting for this to happen when it is automatically closed; the close method provides this immediate release. <P><B>Note:</B> A ResultSet is automatically closed by the Statement that generated it when that Statement is closed, re-executed, or is used to retrieve the next result from a sequence of multiple results. A ResultSet is also automatically closed when it is garbage collected. Documented behaviour for streams is that they are implicitly closed on the next get*() method call. close result set if we have a transaction level error * Comparable (for ordering dynamic result sets from procedures) JDBC 2.0 Delete the current row from the result set and the underlying database.  Cannot be called when on the insert row. ---------------------------------------------------------------- Map a Resultset column name to a ResultSet column index. * End of JDBC public methods. Map a Resultset column name to a ResultSet column index. JDBC 2.0 <p> Moves to the first row in the result set. JDBC 2.0 Get an array column. JDBC 2.0 Get an array column. Pushes a converter on top of getCharacterStream(). A column value can be retrieved as a stream of ASCII characters and then read in chunks from the stream.  This method is particularly suitable for retrieving large LONGVARCHAR values.  The JDBC driver will do any necessary conversion from the database format into ASCII. <P><B>Note:</B> All the data in the returned stream must be read prior to getting the value of any other column. The next call to a get method implicitly closes the stream. Get the value of a column in the current row as a java.lang.BigDecimal object. JDBC 2.0 Get the value of a column in the current row as a java.math.BigDecimal object. Get the value of a column in the current row as a java.lang.BigDecimal object. Get the column as an InputStream. If the column is already of type InputStream then just return it, otherwise convert the column to a set of bytes and create a stream out of the bytes. A column value can be retrieved as a stream of uninterpreted bytes and then read in chunks from the stream.  This method is particularly suitable for retrieving large LONGVARBINARY values. <P><B>Note:</B> All the data in the returned stream must be read prior to getting the value of any other column. The next call to a get method implicitly closes the stream. JDBC 2.0 Get a BLOB column. JDBC 2.0 Get a BLOB column. Get the value of a column in the current row as a Java boolean. Get the value of a column in the current row as a Java boolean. Get the value of a column in the current row as a Java byte. Get the value of a column in the current row as a Java byte. Get the value of a column in the current row as a Java byte array. The bytes represent the raw values returned by the driver. Get the value of a column in the current row as a Java byte array. The bytes represent the raw values returned by the driver. JDBC 2.0 <p>Get the value of a column in the current row as a java.io.Reader. JDBC 2.0 <p>Get the value of a column in the current row as a java.io.Reader. JDBC 2.0 Get a CLOB column. JDBC 2.0 Get a CLOB column. Get the column value for a getXXX() call. This method: <UL> <LI> Closes the current stream (as per JDBC) <LI> Throws a SQLException if the result set is closed <LI> Throws a SQLException if the ResultSet is not on a row <LI> Throws a SQLException if the columnIndex is out of range <LI> Returns the DataValueDescriptor for the column. </UL> Check the column is in range *and* return the JDBC type of the column. JDBC 2.0 Return the concurrency of this result set. The concurrency is determined as follows If Statement object has CONCUR_READ_ONLY concurrency, then ResultSet object will also have the CONCUR_READ_ONLY concurrency. But if Statement object has CONCUR_UPDATABLE concurrency, then the concurrency of ResultSet object depends on whether the underlying language resultset is updatable or not. If the language resultset is updatable, then JDBC ResultSet object will also have the CONCUR_UPDATABLE concurrency. If lanugage resultset is not updatable, then JDBC ResultSet object concurrency will be set to CONCUR_READ_ONLY. Get the name of the SQL cursor used by this ResultSet. <P>In SQL, a result table is retrieved through a cursor that is named. The current row of a result can be updated or deleted using a positioned update/delete statement that references the cursor name. <P>JDBC supports this SQL feature by providing the name of the SQL cursor used by a ResultSet. The current row of a ResultSet is also the current row of this SQL cursor. <P><B>Note:</B> If positioned update is not supported a SQLException is thrown mark the column as updated and return DataValueDescriptor for it. It will be used by updateXXX methods to put new values Get the value of a column in the current row as a java.sql.Date object. JDBC 2.0 Get the value of a column in the current row as a java.sql.Date object.  Use the calendar to construct an appropriate millisecond value for the Date, if the underlying database doesn't store timezone information. Get the value of a column in the current row as a java.sql.Date object. JDBC 2.0 Get the value of a column in the current row as a java.sql.Date object. Use the calendar to construct an appropriate millisecond value for the Date, if the underlying database doesn't store timezone information. Get the value of a column in the current row as a Java double. Get the value of a column in the current row as a Java double. JDBC 2.0 Return the fetch direction for this result set. JDBC 2.0 Return the fetch size for this result set. Get the value of a column in the current row as a Java float. Get the value of a column in the current row as a Java float. JDBC 4.0 <p> Retrieves the holdability for this <code>ResultSet</code> object. Get the value of a column in the current row as a Java int. Get the value of a column in the current row as a Java int. Get the value of a column in the current row as a Java long. Get the value of a column in the current row as a Java long. Return the user-defined maximum size of the column. Note that this may be different from the maximum column size Derby is able, or allowed, to handle (called 'maximum maximum length'). The number, types and properties of a ResultSet's columns are provided by the getMetaData method. <p>Get the value of a column in the current row as a Java object. <p>This method will return the value of the given column as a Java object.  The type of the Java object will be the default Java Object type corresponding to the column's SQL type, following the mapping specified in the JDBC spec. <p>This method may also be used to read datatabase specific abstract data types. JDBC 2.0 New behavior for getObject(). The behavior of method getObject() is extended to materialize data of SQL user-defined types.  When the column @columnIndex is a structured or distinct value, the behavior of this method is as if it were a call to: getObject(columnIndex, this.getStatement().getConnection().getTypeMap()). //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Retrieve the column as an object of the desired type. JDBC 2.0 Returns the value of column {@code i} as a Java object. Use the param map to determine the class from which to construct data of SQL structured and distinct types. <p>Get the value of a column in the current row as a Java object. <p>This method will return the value of the given column as a Java object.  The type of the Java object will be the default Java Object type corresponding to the column's SQL type, following the mapping specified in the JDBC spec. <p>This method may also be used to read datatabase specific abstract data types. JDBC 2.0 New behavior for getObject(). The behavior of method getObject() is extended to materialize data of SQL user-defined types.  When the column @columnName is a structured or distinct value, the behavior of this method is as if it were a call to: getObject(columnName, this.getStatement().getConnection().getTypeMap()). JDBC 2.0 Returns the value of column {@code i} as a Java object. Use the param map to determine the class from which to construct data of SQL structured and distinct types. Try to see if we can fish the pvs out of the local statement. JDBC 2.0 Get a REF(&lt;structured-type&gt;) column. JDBC 2.0 Get a REF(&lt;structured-type&gt;) column. JDBC 2.0 <p> Determine the current row number. The first row is number 1, the second number 2, etc. JDBC 4.0 methods Try to see if we can fish the SQL Statement out of the local statement. Get the value of a column in the current row as a Java short. Get the value of a column in the current row as a Java short. ///////////////////////////////////////////////////////////////////////  JDBC 2.0        -       New public methods  /////////////////////////////////////////////////////////////////////// --------------------------------------------------------------------- Getter's and Setter's --------------------------------------------------------------------- JDBC 2.0 Return the Statement that produced the ResultSet. ====================================================================== Methods for accessing results by column index ====================================================================== Get the value of a column in the current row as a Java String. ====================================================================== Methods for accessing results by column name ====================================================================== Get the value of a column in the current row as a Java String. Get the value of a column in the current row as a java.sql.Time object. JDBC 2.0 Get the value of a column in the current row as a java.sql.Time object. Use the calendar to construct an appropriate millisecond value for the Time, if the underlying database doesn't store timezone information. Get the value of a column in the current row as a java.sql.Time object. JDBC 2.0 Get the value of a column in the current row as a java.sql.Time object. Use the calendar to construct an appropriate millisecond value for the Time, if the underlying database doesn't store timezone information. Get the value of a column in the current row as a java.sql.Timestamp object. JDBC 2.0 Get the value of a column in the current row as a java.sql.Timestamp object. Use the calendar to construct an appropriate millisecond value for the Timestamp, if the underlying database doesn't store timezone information. Get the value of a column in the current row as a java.sql.Timestamp object. JDBC 2.0 Get the value of a column in the current row as a java.sql.Timestamp object. Use the calendar to construct an appropriate millisecond value for the Timestamp, if the underlying database doesn't store timezone information. JDBC 2.0 Return the type of this result set. The type is determined based on the statement that created the result set. JDBC 3.0 Retrieves the value of the designated column in the current row of this ResultSet object as a java.net.URL object in the Java programming language. JDBC 3.0 Retrieves the value of the designated column in the current row of this ResultSet object as a java.net.URL object in the Java programming language. JDBC 2.0 Deprecated in JDBC 2.0, not supported by JCC. Deprecated in JDBC 2.0, not supported by JCC. ===================================================================== Advanced features: ===================================================================== <p>The first warning reported by calls on this ResultSet is returned. Subsequent ResultSet warnings will be chained to this SQLWarning. <P>The warning chain is automatically cleared each time a new row is read. <P><B>Note:</B> This warning chain only covers warnings caused by ResultSet methods.  Any warning caused by statement methods (such as reading OUT parameters) will be chained on the Statement object. Initializes the currentRowHasBeenUpdated and columnGotUpdated fields JDBC 2.0 Insert the contents of the insert row into the result set and the database. Must be on the insert row when this method is called. JDBC 2.0 <p> Determine if the cursor is after the last row in the result set. --------------------------------------------------------------------- Traversal/Positioning --------------------------------------------------------------------- JDBC 2.0 <p> Determine if the cursor is before the first row in the result set. JDBC 4.0 <p> Checks whether this <code>ResultSet</code> object has been closed, either automatically or because <code>close()</code> has been called. JDBC 2.0 <p> Determine if the cursor is on the first row of the result set. * Is this result set from a select for update statement? JDBC 2.0 <p> Determine if the cursor is on the last row of the result set. Note: Calling isLast() may be expensive since the JDBC driver might need to fetch ahead one row in order to determine whether the current row is the last row in the result set. @see org.apache.derby.iapi.jdbc.EngineResultSet#isNull(int) Returns false unless <code>interfaces</code> is implemented JDBC 2.0 <p> Moves to the last row in the result set. Mark this ResultSet as closed and trigger the closing of the Statement if necessary. JDBC 2.0 Move the cursor to the remembered cursor position, usually the current row. Has no effect unless the cursor is on the insert row. JDBC 2.0 Move to the insert row. The current cursor position is remembered while the cursor is positioned on the insert row. The insert row is a special row associated with an updatable result set. It is essentially a buffer where a new row may be constructed by calling the updateXXX() methods prior to inserting the row into the result set. Only the updateXXX(), getXXX(), and insertRow() methods may be called when the cursor is on the insert row. All of the columns in a result set must be given a value each time this method is called before calling insertRow(). UpdateXXX()must be called before getXXX() on a column. java.sql.ResultSet interface A ResultSet is initially positioned before its first row; the first call to next makes the first row the current row; the second call makes the second row the current row, etc. <P>If an input stream from the previous row is open, it is implicitly closed. The ResultSet's warning chain is cleared when a new row is read. An exception on many method calls to JDBC objects does not change the state of the transaction or statement, or even the underlying object. This method simply wraps the excecption in a SQLException. Examples are: <UL> <LI> getXXX() calls on ResultSet - ResultSet is not closed. <LI> setXXX() calls on PreparedStatement - ResultSet is not closed. </UL> In addition these exceptions must not call higher level objects to be closed (e.g. when executing a server side Java procedure). See bug 4397 JDBC 2.0 <p> Moves to the previous row in the result set. <p> Note: previous() is not the same as relative(-1) since it makes sense to call previous() when there is no current row. JDBC 2.0 Refresh the value of the current row with its current value in the database. Cannot be called when on the insert row. The refreshRow() method provides a way for an application to explicitly tell the JDBC driver to refetch a row(s) from the database. An application may want to call refreshRow() when caching or prefetching is being done by the JDBC driver to fetch the latest value of a row from the database. The JDBC driver may actually refresh multiple rows at once if the fetch size is greater than one. All values are refetched subject to the transaction isolation level and cursor sensitivity. If refreshRow() is called after calling updateXXX(), but before calling updateRow() then the updates made to the row are lost. Calling refreshRow() frequently will likely slow performance. JDBC 2.0 <p> Moves a relative number of rows, either positive or negative. Attempting to move beyond the first/last row in the result set positions the cursor before/after the the first/last row. Calling relative(0) is valid, but does not change the cursor position. <p> Note: Calling relative(1) is different than calling next() since is makes sense to call next() when there is no current row, for example, when the cursor is positioned before the first row or after the last row of the result set. JDBC 2.0 Determine if this row has been deleted. A deleted row may leave a visible "hole" in a result set. This method can be used to detect holes in a result set. The value returned depends on whether or not the result set can detect deletions. JDBC 2.0 Determine if the current row has been inserted. The value returned depends on whether or not the result set can detect visible inserts. --------------------------------------------------------------------- Updates --------------------------------------------------------------------- JDBC 2.0 Determine if the current row has been updated. The value returned depends on whether or not the result set can detect updates. Set the application Statement object that created this ResultSet. Used when the Statement objects returned to the application are wrapped for XA. A dynamic result was created in a procedure by a nested connection. Once the procedure returns, there is a good chance that connection is closed, so we re-attach the result set to the connection of the statement the called the procedure, which will be still open. <BR> In the case where the dynamic result will not be accessible then owningStmt will be null, the ResultSet will be linked to the root connection to allow its close method to work. It will remain attached to its original statement. --------------------------------------------------------------------- Properties --------------------------------------------------------------------- JDBC 2.0 Give a hint as to the direction in which the rows in this result set will be processed. The initial value is determined by the statement that produced the result set. The fetch direction may be changed at any time. JDBC 2.0 Give the JDBC driver a hint as to the number of rows that should be fetched from the database when more rows are needed for this result set. If the fetch size specified is zero, then the JDBC driver ignores the value, and is free to make its own best guess as to what the fetch size should be. The default value is set by the statement that creates the result set. The fetch size may be changed at any time. Debug method used to test the setLargeMaxRows() method added by JDBC 4.2. This method is a NOP on a production (insane) build of Derby. Returns <code>this</code> if this class implements the interface JDBC 3.0 Updates the designated column with a java.sql.Array value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 3.0 Updates the designated column with a java.sql.Array value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. Updates the designated column with a character stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or </code>insertRow</code> methods are called to update the database. JDBC 2.0 Update a column with an ascii stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Update a column with an ascii stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a character stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or </code>insertRow</code> methods are called to update the database. JDBC 2.0 Update a column with an ascii stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 4.0 Update a column with an ascii stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a BigDecimal value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a binary stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. JDBC 2.0 Update a column with a binary stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Update a column with a binary stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a binary stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. JDBC 2.0 Update a column with a binary stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 4.0 Update a column with a binary stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Set the given binary stream for the specified parameter. If <code>lengthLess</code> is <code>true</code>, the following conditions are either not checked or verified at the execution time of <code>updateRow</code>/<code>insertRow</code>: <ol><li>If the stream length is negative. <li>If the stream's actual length equals the specified length.</ol> The <code>lengthLess</code> variable was added to differentiate between streams with invalid lengths and streams without known lengths. Updates the designated column using the given input stream. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 4.0 Updates the designated column with a java.sql.Blob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 3.0 Updates the designated column with a java.sql.Blob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. Updates the designated column using the given input stream. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 4.0 Updates the designated column with a java.sql.Blob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 3.0 Updates the designated column with a java.sql.Blob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 2.0 Update a column with a boolean value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a boolean value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a byte value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a byte value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a byte array value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a byte array value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a character stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or </code>insertRow</code> methods are called to update the database. JDBC 2.0 Update a column with a character stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 4.0 Update a column with a character stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column with a character stream value. The data will be read from the stream as needed until end-of-stream is reached. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or </code>insertRow</code> methods are called to update the database. JDBC 2.0 Update a column with a character stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 4.0 Update a column with a character stream value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Set the given character stream for the specified parameter. If <code>lengthLess</code> is <code>true</code>, the following conditions are either not checked or verified at the execution time of the prepared statement: <ol><li>If the stream length is negative. <li>If the stream's actual length equals the specified length.</ol> The <code>lengthLess</code> variable was added to differentiate between streams with invalid lengths and streams without known lengths. Updates the designated column using the given <code>Reader</code> object. The data will be read from the stream as needed until end-of-stream is reached. The JDBC driver will do any necessary conversion from <code>UNICODE</code> to the database char format. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. JDBC 4.0 Updates the designated column with a java.sql.Clob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 3.0 Updates the designated column with a java.sql.Clob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. Updates the designated column using the given <code>Reader</code> object. The data will be read from the stream as needed until end-of-stream is reached. The JDBC driver will do any necessary conversion from <code>UNICODE</code> to the database char format. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the <code>updateRow</code> or <code>insertRow</code> methods are called to update the database. JDBC 4.0 Updates the designated column with a java.sql.Clob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 3.0 Updates the designated column with a java.sql.Clob value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 2.0 Update a column with a Date value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a Date value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a Double value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a double value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a float value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a float value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with an integer value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with an integer value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a long value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a long value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Updates the designated column using the given Reader object, which is the given number of characters long. Updates the designated column using the given Reader object, which is the given number of characters long. JDBC 2.0 Give a nullable column a null value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a null value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with an Object value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with an Object value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with an Object value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with an Object value. The updateXXX() methods are used to update column values in the current row, or the insert row.  The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 3.0 Updates the designated column with a java.sql.Ref value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 3.0 Updates the designated column with a java.sql.Ref value. The updater methods are used to update column values in the current row or the insert row. The updater methods do not update the underlying database; instead the updateRow or insertRow methods are called to update the database. JDBC 2.0 Update the underlying database with the new contents of the current row.  Cannot be called when on the insert row. JDBC 2.0 Update a column with a short value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a short value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a String value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a String value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a Time value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a Time value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a Timestamp value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. JDBC 2.0 Update a column with a Timestamp value. The updateXXX() methods are used to update column values in the current row, or the insert row. The updateXXX() methods do not update the underlying database, instead the updateRow() or insertRow() methods are called to update the database. Mark a column as already having a stream or LOB accessed from it. If the column was already accessed, throw an exception. A column may have the value of SQL NULL; wasNull reports whether the last column read had this special value. Note that you must first call getXXX on a column to try to read its value and then call wasNull() to find if the value was the SQL NULL. <p> we take the least exception approach and simply return false if no column has been read yet.

What's a column's table's catalog name? ///////////////////////////////////////////////////////////////////////  JDBC 2.0	-	New public methods  /////////////////////////////////////////////////////////////////////// JDBC 2.0 <p>Return the fully qualified name of the Java class whose instances are manufactured if ResultSet.getObject() is called to retrieve a value from the column.  ResultSet.getObject() may return a subClass of the class returned by this method.  ResultSetMetaData interface  What's the number of columns in the ResultSet? What's the column's normal max width in chars? What's the suggested column title for use in printouts and displays? What's a column's name? What's a column's SQL type? What's a column's data source specific type name? What's a column's number of decimal digits? What's a column's number of digits to right of the decimal point? What's a column's table's schema? What's a column's table name? Is the column automatically numbered, thus read-only? Does a column's case matter? Is the column a cash value? Always returns false since there are no currency data types in Derby. Will a write on the column definitely succeed? Can you put a NULL in this column? Is a column definitely not writable? Can the column be used in a where clause? Is the column a signed number? JDBC 4.0 - java.sql.Wrapper interface Returns whether or not this instance implements the specified interface. Is it possible for a write on the column to succeed? Returns {@code this} if this class implements the interface. class interface
Derby internally keeps name for both named and unnamed savepoints Retrieves the generated ID for the savepoint that this Savepoint object represents. Retrieves the name of the savepoint that this Savepoint object represents. bug 4468 - verify that savepoint rollback/release is for a savepoint from the current connection
JDBC 2.0 Adds a SQL command to the current batch of commmands for the statement. This method is optional. Add a SQLWarning to this Statement object. If the Statement already has a SQLWarning then it is added to the end of the chain. Cancel can be used by one thread to cancel a statement that is being executed by another thread. A heavier weight version of checkStatus() that ensures the application's Connection object is still open. This is to stop errors or unexpected behaviour when a [Prepared]Statement object is used after the application has been closed. In particular to ensure that a Statement obtained from a PooledConnection cannot be used after the application has closed its connection (as the underlying Connection is still active). To avoid this heavier weight check on every method of [Prepared]Statement it is only used on those methods that would end up using the database's connection to read or modify data. E.g. execute*(), but not setXXX, etc. <BR> If this Statement's Connection is closed an exception will be thrown and the active field will be set to false, completely marking the Statement as closed. <BR> If the Statement is not currently connected to an active transaction, i.e. a suspended global transaction, then this method will throw a SQLException but the Statement will remain open. The Statement is open but unable to process any new requests until its global transaction is resumed. <BR> Upon return from the method, with or without a SQLException the field active will correctly represent the open state of the Statement. Check to see if a statement requires to be executed via a callable statement. Throw an exception if this Statement has been closed explictly or it has noticed it has been closed implicitly. JDBC specifications require nearly all methods throw a SQLException if the Statement has been closed, thus most methods call this method or checkExecStatus first. JDBC 2.0 Make the set of commands in the current batch empty. This method is optional. Close and clear all result sets associated with this statement from the last execution. don't call this directly. call clearResultSets() instead. After this call getWarnings returns null until a new warning is reported for this Statement. In many cases, it is desirable to immediately release a Statements's database and JDBC resources instead of waiting for this to happen when it is automatically closed; the close method provides this immediate release. <P><B>Note:</B> A Statement is automatically closed when it is garbage collected. When a Statement is closed its current ResultSet, if one exists, is also closed. allow sub-classes to execute additional close logic while holding the synchronization.  For tracking all of the ResultSets produced by this Statement so that the Statement can be cleaned up if closeOnCompletion() was invoked.  //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// ----------------------- Multiple Results -------------------------- Execute a SQL statement that may return multiple results. Under some (uncommon) situations a single SQL statement may return multiple result sets and/or update counts.  Normally you can ignore this, unless you're executing a stored procedure that you know may return multiple results, or unless you're dynamically executing an unknown SQL string.  The "execute", "getMoreResults", "getResultSet" and "getUpdateCount" methods let you navigate through multiple results. The "execute" method executes a SQL statement and indicates the form of the first result.  You can then use getResultSet or getUpdateCount to retrieve the result, and getMoreResults to move to any subsequent result(s). Execute a SQL statement that may return multiple results. Under some (uncommon) situations a single SQL statement may return multiple result sets and/or update counts.  Normally you can ignore this, unless you're executing a stored procedure that you know may return multiple results, or unless you're dynamically executing an unknown SQL string.  The "execute", "getMoreResults", "getResultSet" and "getUpdateCount" methods let you navigate through multiple results. The "execute" method executes a SQL statement and indicates the form of the first result.  You can then use getResultSet or getUpdateCount to retrieve the result, and getMoreResults to move to any subsequent result(s). JDBC 3.0 Executes the given SQL statement, which may return multiple results, and signals the driver that any auto-generated keys should be made available for retrieval. The driver will ignore this signal if the SQL statement is not an INSERT/UPDATE statement. JDBC 3.0 Executes the given SQL statement, which may return multiple results, and signals the driver that the auto-generated keys indicated in the given array should be made available for retrieval. This array contains the indexes of the columns in the target table that contain the auto-generated keys that should be made available. The driver will ignore the array if the given SQL statement is not an INSERT/UPDATE statement. JDBC 3.0 Executes the given SQL statement, which may return multiple results, and signals the driver that the auto-generated keys indicated in the given array should be made available for retrieval. This array contains the names of the columns in the target table that contain the auto-generated keys that should be made available. The driver will ignore the array if the given SQL statement is not an INSERT/UPDATE statement. JDBC 2.0 Submit a batch of commands to the database for execution. This method is optional. Moving jdbc2.0 batch related code in this class because callableStatement in jdbc 20 needs this code too and it doesn't derive from prepared statement in jdbc 20 in our implementation. BatchUpdateException is the only new class from jdbc 20 which is being referenced here and in order to avoid any jdk11x problems, using reflection code to make an instance of that class. Execute a single element of the batch. Overridden by EmbedPreparedStatement JDBC 4.2 Submit a batch of commands to the database for execution. This method is optional. For use with statements which may touch more than Integer.MAX_VALUE rows. JDBC 4.2 Execute a SQL INSERT, UPDATE or DELETE statement. For use with statements which may touch more than Integer.MAX_VALUE rows. JDBC 4.2 Execute the given SQL statement and signals the driver with the given flag about whether the auto-generated keys produced by this Statement object should be made available for retrieval. For use with statements which may touch more than Integer.MAX_VALUE rows. JDBC 4.2 Executes the given SQL statement and signals the driver that the auto-generated keys indicated in the given array should be made available for retrieval. The driver will ignore the array if the SQL statement is not an INSERT/UPDATE statement. For use with statements which may touch more than Integer.MAX_VALUE rows. JDBC 4.2 Executes the given SQL statement and signals the driver that the auto-generated keys indicated in the given array should be made available for retrieval. The driver will ignore the array if the SQL statement is not an INSERT/UPDATE statement. For use with statements which may touch more than Integer.MAX_VALUE rows.  java.sql.Statement interface the comments are those from the JDBC interface, so we know what we're supposed to to. Execute a SQL statement that returns a single ResultSet. ///////////////////////////////////////////////////////////////////////  Implementation specific methods  /////////////////////////////////////////////////////////////////////// Execute the current statement. Execute a SQL INSERT, UPDATE or DELETE statement. In addition, SQL statements that return nothing such as SQL DDL statements can be executed. JDBC 3.0 Execute the given SQL statement and signals the driver with the given flag about whether the auto-generated keys produced by this Statement object should be made available for retrieval. JDBC 3.0 Executes the given SQL statement and signals the driver that the auto-generated keys indicated in the given array should be made available for retrieval. The driver will ignore the array if the SQL statement is not an INSERT statement JDBC 3.0 Executes the given SQL statement and signals the driver that the auto-generated keys indicated in the given array should be made available for retrieval. The driver will ignore the array if the SQL statement is not an INSERT statement Mark the statement and its single-use activation as unused. This method should be called from <code>EmbedPreparedStatement</code>'s finalizer as well, even though prepared statements reuse activations, since <code>getGeneratedKeys()</code> uses a single-use activation regardless of statement type. <BR> Dynamic result sets (those in dynamicResults array) need not be handled here as they will be handled by the statement object that created them. In some cases results will point to a ResultSet in dynamicResults but all that will happen is that the activation will get marked as unused twice.  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  JDBC 2.0 Return the Connection that produced the Statement. Get the execute time holdability for the Statement. When in a global transaction holdabilty defaults to false. JDBC 2.0 Determine the fetch direction. JDBC 2.0 Determine the default fetch size. JDBC 3.0 Retrieves any auto-generated keys created as a result of executing this Statement object. If this Statement is a non-insert statement, a null ResultSet object is returned. JDBC 4.2 The maxRows limit is the maximum number of rows that a ResultSet can contain.  If the limit is exceeded, the excess rows are silently dropped. For use with statements which may touch more than Integer.MAX_VALUE rows. JDBC 4.2 getLargeUpdateCount returns the current result as an update count; if the result is a ResultSet or there are no more results -1 is returned.  It should only be called once per result. For use with statements which may touch more than Integer.MAX_VALUE rows. ---------------------------------------------------------------------- The maxFieldSize limit (in bytes) is the maximum amount of data returned for any column value; it only applies to BINARY, VARBINARY, LONGVARBINARY, CHAR, VARCHAR, and LONGVARCHAR columns.  If the limit is exceeded, the excess data is silently discarded. The maxRows limit is the maximum number of rows that a ResultSet can contain.  If the limit is exceeded, the excess rows are silently dropped. getMoreResults moves to a Statement's next result.  It returns true if this result is a ResultSet.  getMoreResults also implicitly closes any current ResultSet obtained with getResultSet. There are no more results when (!getMoreResults() &amp;&amp; (getUpdateCount() == -1) JDBC 3.0 Moves to this Statement obect's next result, deals with any current ResultSet object(s) according to the instructions specified by the given flag, and returns true if the next result is a ResultSet object The queryTimeout limit is the number of seconds the driver will wait for a Statement to execute. If the limit is exceeded a SQLException is thrown. getResultSet returns the current result as a ResultSet.  It should only be called once per result. JDBC 2.0 Determine the result set concurrency. JDBC 3.0 Retrieves the result set holdability for ResultSet objects generated by this Statement object. ///////////////////////////////////////////////////////////////////////  JDBC 2.0 methods that are implemented here because EmbedPreparedStatement and EmbedCallableStatement in Local20 need access to them, and those classes extend their peer classes in Local, instead of EmbedStatement in Local20  We do the same of JDBC 3.0 methods. /////////////////////////////////////////////////////////////////////// JDBC 2.0 Determine the result set type. package getUpdateCount returns the current result as an update count; if the result is a ResultSet or there are no more results -1 is returned.  It should only be called once per result. <P>The only way to tell for sure that the result is an update count is to first test to see if it is a ResultSet. If it is not a ResultSet it is either an update count or there are no more results. The first warning reported by calls on this Statement is returned.  A Statment's execute methods clear its SQLWarning chain. Subsequent Statement warnings will be chained to this SQLWarning. <p>The warning chain is automatically cleared each time a statement is (re)executed. <P><B>Note:</B> If you are processing a ResultSet then any warnings associated with ResultSet reads will be chained on the ResultSet object. Tell whether this statment has been closed or not. Returns the value of the EmbedStatement's poolable hint, indicating whether pooling is requested. Returns false unless {@code interfaces} is implemented. Process a ResultSet created in a Java procedure as a dynamic result. To be a valid dynamic result the ResultSet must be: <UL> <LI> From a Derby system <LI> From a nested connection of connection passed in or from the connection itself. <LI> Open </UL> Any invalid ResultSet is ignored. Go through a holder of dynamic result sets, remove those that should not be returned, and sort the result sets according to their creation. Callback on the statement when one of its result sets is closed. This allows the statement to control when it completes and hence when it commits in auto commit mode. Must have connection synchronization and setupContextStack(), this is required for the call to commitIfNeeded(). Set the application statement for this Statement. setCursorName defines the SQL cursor name that will be used by subsequent Statement execute methods. This name can then be used in SQL positioned update/delete statements to identify the current row in the ResultSet generated by this statement.  If the database doesn't support positioned update/delete, this method is a noop. <P><B>Note:</B> By definition, positioned update/delete execution must be done by a different Statement than the one which generated the ResultSet being used for positioning. Also, cursor names must be unique within a Connection. If escape scanning is on (the default) the driver will do escape substitution before sending the SQL to the database. JDBC 2.0 Give a hint as to the direction in which the rows in a result set will be processed. The hint applies only to result sets created using this Statement object.  The default value is ResultSet.FETCH_FORWARD. JDBC 2.0 Give the JDBC driver a hint as to the number of rows that should be fetched from the database when more rows are needed.  The number of rows specified only affects result sets created using this statement. If the value specified is zero, then the hint is ignored. The default value is zero. The maxRows limit is set to limit the number of rows that any ResultSet can contain.  If the limit is exceeded, the excess rows are silently dropped. The maxFieldSize limit (in bytes) is set to limit the size of data that can be returned for any column value; it only applies to BINARY, VARBINARY, LONGVARBINARY, CHAR, VARCHAR, and LONGVARCHAR fields.  If the limit is exceeded, the excess data is silently discarded. The maxRows limit is set to limit the number of rows that any ResultSet can contain.  If the limit is exceeded, the excess rows are silently dropped. Requests that an EmbedStatement be pooled or not. The queryTimeout limit is the number of seconds the driver will wait for a Statement to execute. If the limit is exceeded a SQLException is thrown. Transfer my batch of Statements to a newly created Statement. Returns {@code this} if this class implements the interface.
* BrokeredConnectionControl api Allow control over setting auto commit mode.  Allow control over calling commit. Are held cursors allowed. If the connection is attached to a global transaction then downgrade the result set holdabilty to CLOSE_CURSORS_AT_COMMIT if downgrade is true, otherwise throw an exception. If the connection is in a local transaction then the passed in holdabilty is returned. Allow control over calling rollback. Allow control over creating a Savepoint (JDBC 3.0) Override getRealConnection to create a a local connection when we are not associated with an XA transaction. This can occur if the application has a Connection object (conn) and the following sequence occurs. conn = xac.getConnection(); xac.start(xid, ...) // do work with conn xac.end(xid, ...); // do local work with conn // need to create new connection here. * XAConnection methods Check if this connection is part of a global XA transaction.  Wrap and control a PreparedStatement Wrap and control a PreparedStatement Wrap and control a Statement
Compares the user name and password of the XAResource with user name and password of this and throws XAException if there is a mismatch Checks if currently associated connection is active throws exception if not Close  an underlying connection object when there is no active XAResource to hand it to. Commit the global transaction specified by xid. Ends the work performed on behalf of a transaction branch. The resource manager disassociates the XA resource from the transaction branch specified and let the transaction be completed. <p> If TMSUSPEND is specified in flags, the transaction branch is temporarily suspended in incomplete state. The transaction context is in suspened state and must be resumed via start with TMRESUME specified. <p> If TMFAIL is specified, the portion of work has failed. The resource manager may mark the transaction as rollback-only <p> If TMSUCCESS is specified, the portion of work has completed successfully. Tell the resource manager to forget about a heuristically completed transaction branch. Privileged lookup of the ContextManager. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Resturns currently active xid Returns the default value for the transaction timeout in milliseconds setted up by the system properties. Gets the LanguageConnectionContext for this connection. Privileged LCC lookup. Must be private so that user code can't call this entry point. Returns the XATransactionState of the the transaction Obtain the current transaction timeout value set for this XAResource instance. If XAResource.setTransactionTimeout was not use prior to invoking this method, the return value is 0; otherwise, the value used in the previous setTransactionTimeout call is returned. This method is called to determine if the resource manager instance represented by the target object is the same as the resouce manager instance represented by the parameter xares. Ask the resource manager to prepare for a transaction commit of the transaction specified in xid. Obtain a list of prepared transaction branches from a resource manager. The transaction manager calls this method during recovery to obtain the list of transaction branches that are currently in prepared or heuristically completed states. Removes the xid from currently active transactions Return an underlying connection object back to its XAResource if possible. If not close the connection. Inform the resource manager to roll back work done on behalf of a transaction branch Set the current transaction timeout value for this XAResource instance. Once set, this timeout value is effective until setTransactionTimeout is invoked again with a different value. To reset the timeout value to the default value used by the resource manager, set the value to zero. If the timeout operation is performed successfully, the method returns true; otherwise false. If a resource manager does not support transaction timeout value to be set explicitly, this method returns false. Start work on behalf of a transaction branch specified in xid If TMJOIN is specified, the start is for joining a transaction previously seen by the resource manager. If TMRESUME is specified, the start is to resume a suspended transaction specified in the parameter xid. If neither TMJOIN nor TMRESUME is specified and the transaction specified by xid has previously been seen by the resource manager, the resource manager throws the XAException exception with XAER_DUPID error code. Map a SQL exception to appropriate XAException. Return the mapped XAException. Map a Standard exception to appropriate XAException. Return the mapped XAException.

Create and return an EmbedPooledConnection from this instance of EmbeddedConnectionPoolDataSource. ConnectionPoolDataSource methods Attempt to establish a database connection. Attempt to establish a database connection.


Add Java Bean properties to the reference using StringRefAddr for each property. List of bean properties is defined from the public getXXX() methods on this object that take no arguments and return short, int, boolean or String. The {@link StringRefAddr} has a key of the Java bean property name, converted from the method name. E.g. traceDirectory for traceDirectory. {@code javax.naming.Referenceable} interface This method creates a new {@code Reference} object to represent this data source.  The class name of the data source object is saved in the {@code Reference}, so that an object factory will know that it should create an instance of that class when a lookup operation is performed. The class is also stored in the reference.  This is not required by JNDI, but is recommend in practice.  JNDI will always use the object factory class specified in the reference when reconstructing an object, if a class name has been specified.  See the JNDI SPI documentation for further details on this topic, and for a complete description of the {@code Reference} and {@code StringRefAddr} classes. <p/> Derby data source classes class provides several standard JDBC properties.  The names and values of the data source properties are also stored in the reference using the {@code StringRefAddr} class. This is all the information needed to reconstruct an embedded data source object.


* Methods from java.sql.Driver. Accept anything that starts with <CODE>jdbc:derby:</CODE>. * Find the appropriate driver for our JDBC level and boot it. This is package protected so that AutoloadedDriver can call it. Connect to the URL if possible Lookup the booted driver module appropriate to our JDBC level. Returns the driver's major version number. Returns the driver's minor version number. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Returns an array of DriverPropertyInfo objects describing possible properties. Report whether the Driver is a genuine JDBC COMPLIANT (tm) driver.
Instantiate and return an EmbedXAConnection from this instance of EmbeddedXADataSource.   XADataSource methods Attempt to establish a database connection. Attempt to establish a database connection with the given user name and password. {@inheritDoc } <p/> Also clear the cached value of {@link #ra}.



ExecutionFactory interface   ResultSetStatisticsFactory interface
Create a UTF-16BE encoded stream from the given <code>Reader</code>. Create a UTF-8 encoded stream from the given <code>Reader</code>.
Containers are not encryped on a redo. Nothing to do in this method. Generate a Compensation (EncryptContainerUndoOperation) that will rollback the changes made to the container during container encryption. Loggable methods the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. Encrypt Operation has no optional data to write out Return my format identifier. A space operation is a RAWSTORE log record Check if this operation needs to be redone during recovery redo. Returns true if this op should be redone during recovery redo, debug Undo of encrytpion of the container. Original version of the container that existed before the start of the database encryption is put back.
Apply the undo operation, in this implementation of the RawStore, it can only call the undoMe method of undoOp the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. Encrypt Conatainer Undo Operation has no optional data to write out Return my format identifier. Undo operation is a COMPENSATION log operation Loggable methods Check if this operation needs to be redone during recovery redo. Returns true if this op should be redone during recovery redo,  make sure resource found in undoOp is released Compensation method Set up a Container undo operation during recovery redo. DEBUG: Print self.
Finds all the all the containers stored in the data directory and decrypts them. Find all the all the containers stored in the data directory and encrypt them. Encrypts or decrypts all containers in the database data directory. Encrypts or decrypts the specified container. Get file handle to a container file that is used to keep temporary versions of the container file. Get path to a container file that is used to keep temporary versions of the container file. Removes old versions of the containers after a cryptographic operation on the database. Restore the contaier to the state it was before it was encrypted with new encryption key. This function is called during undo of the EncryptContainerOperation log record incase of a error/crash before database was successfuly configured with new encryption properties. @param ckey the key of the container that needs to be restored. @exception StandardException Standard Derby error policy PrivilegedAction method
Find the blocks containing the data we are interested in. Returns the currrent position in the file. Returns file length. Reads len or remaining bytes in the file (whichever is lower) bytes into buff starting from off position of the buffer. Reads one byte from file. Sets the current file pointer to specific location. Sets the file length to a given size. If the new size is smaller than the file length the file is truncated. Write the buffer into file at current position. It overwrites the data if current position is in the middle of the file and appends into the file if the total length exceeds the file size. Writes length number of bytes from buffer starting from off position. Writes one byte into the file.
This method is used to calculate the encryption token. DES encrypts the data using a token and the generated shared private key. The token used depends on the type of security mechanism being used: USRENCPWD - The userid is used as the token. The USRID is zero-padded to 8 bytes if less than 8 bytes or truncated to 8 bytes if greater than 8 bytes. EUSRIDPWD - The middle 8 bytes of the server's connection key is used as the token. @param  int     securityMechanism @param  byte[]  userid or server's connection key @return byte[]  the encryption token This method decrypts the usreid/password with the middle 8 bytes of the generated secret key and an encryption token. Then it returns the decrypted data in a byte array. plainText   The byte array form userid/password to encrypt. initVector  The byte array which is used to calculate the encryption token. targetPublicKey   DERBY' public key. Returns the decrypted data in a byte array. This method encrypts the usreid/password with the middle 8 bytes of the generated secret key and an encryption token. Then it returns the encrypted data in a byte array. plainText   The byte array form userid/password to encrypt. initVector  The byte array which is used to calculate the encryption token. targetPublicKey   DERBY' public key. Returns the encrypted data in a byte array. This method generates a secret key using the application server's public key ************************************************************** Below are methods for the SECMEC_USRSSBPWD security mechanism. ************************************************************** This method generates an 8-Byte random seed for the client (source). JDK 1.4 has a parity check on the DES encryption key. Each byte needs to have an odd number of "1"s in it, and this is required by DES. Otherwise JDK 1.4 throws InvalidKeyException. Older JDK doesn't check this. In order to make encryption work with JDK1.4, we are going to check each of the 8 byte of our key and flip the last bit if it has even number of 1s. This method generates the public key and returns it. This shared public key is the application requester's connection key and will be exchanged with the application server's connection key. This connection key will be put in the sectkn in ACCSEC command and send to the application server. @param   null @return  a byte array that is the application requester's public key Strong Password Substitution (USRSSBPWD). This method generates a password substitute to send to the target server. Substitution algorithm works as follow: PW_TOKEN = SHA-1(PW, ID) The password (PW) and user name (ID) can be of any length greater than or equal to 1 byte. The client generates a 20-byte password substitute (PW_SUB) as follows: PW_SUB = SHA-1(PW_TOKEN, RDr, RDs, ID, PWSEQs) w/ (RDs) as the random client seed and (RDr) as the server one. See PWDSSB - Strong Password Substitution Security Mechanism (DRDA Vol.3 - P.650) Convert a string into a byte array in hex format. <BR> For each character (b) two bytes are generated, the first byte represents the high nibble (4 bits) in hexadecimal ({@code b & 0xf0}), the second byte represents the low nibble ({@code b & 0x0f}). <BR> The character at {@code str.charAt(0)} is represented by the first two bytes in the returned String. Convert a byte array to a String with a hexadecimal format. The String may be converted back to a byte array using fromHexString. <BR> For each byte (b) two characters are generated, the first character represents the high nibble (4 bits) in hexadecimal ({@code b & 0xf0}), the second character represents the low nibble ({@code b & 0x0f}). <BR> The byte at {@code data[offset]} is represented by the first two characters in the returned String.
Set of tests which are run for each encryption algorithm. Runs tests with a set of encryption algorithms. The set comes from the set of algorithms used for the same purpose in the old harness. Very simple test that ensures we can get a connection to the booted encrypted database.
Returns the name of this taglet end_format not expected to be used in constructor documentation. end_format not expected to be used in field documentation. end_format not expected to be used in method documentation. end_format can be used in overview documentation. end_format can be used in package documentation. end_format can be used in type documentation. end_format is not an inline tag. Register this Taglet. No-op. Not currently used. No-op. Not currently used.
Loggable methods Apply the change indicated by this operation and optional data. the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. EndXact has no optional data to write out Return my format identifier. EndXact is a RAWSTORE log record. Always redo an EndXact. EndXact has no resource to release DEBUG: Print self.
JDBC 4.1 methods that use generics and won't compile on CDC.
Add a SQLWarning to this Connection object. Obtain the name of the current schema, so that the NetworkServer can use it for piggy-backing Get the LOB reference corresponding to the locator. Return prepare isolation //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Get the name of the current schema. Is this a global transaction Resets the connection before it is returned from a PooledConnection to a new application request (wrapped by a BrokeredConnection). <p> Note that resetting the transaction isolation level is not performed as part of this method. Temporary tables, IDENTITY_VAL_LOCAL and current schema are reset. Set the DRDA identifier for this connection. Set the transaction isolation level that will be used for the next prepare.  Used by network server to implement DB2 style isolation levels. Note the passed in level using the Derby constants from ExecutionContext and not the JDBC constants from java.sql.Connection. Set the default schema for the Connection.
Frees all resources assoicated with this LOB. Returns LOB locator key. <p> The key can be used with {@link org.apache.derby.impl.jdbc.EmbedConnection#getLOBMapping} to retrieve this LOB at a later time.
//////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.2 IN JAVA 8  ////////////////////////////////////////////////////////////////// Get the version of the prepared statement. If this has not been changed, the caller may assume that a recompilation has not taken place, i.e. meta-data are (also) unchanged.
Return the length of the designated columnIndex data value. Implementation is type dependent. Is this result set from a select for update statement? Is the designated columnIndex a null data value? This is used by EXTDTAInputStream to get the null value without retrieving the underlying data value.
//////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.2 IN JAVA 8  //////////////////////////////////////////////////////////////////


All columns in the Db2jLogReader VTI have a of String type.
*  VTICosting methods  * * *  *

* Append an error string * * @param s 	the string to append * Append an error string with a newline * * @param s 	the string to append * Get the buffer * Reset the buffer -- truncate it down to nothing. * * Print a stacktrace from the throwable in the error * buffer. * * @param t	the error
************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Public Methods of XXXX class: *************************************************************************
Get the singleton exception factory instance. Construct an SQLException whose message and severity are specified explicitly. Construct an SQLException whose message and severity are derived from the message id.
returnTokensOnly is true only when exception tracing is enabled so that we don't try to go to the server for a message while we're in the middle of parsing an Sqlca reply. Without this, if e.getMessage() fails, we would have infinite recursion when TRACE_DIAGNOSTICS is on  because tracing occurs within the exception constructor.

Dumps stack traces for all the threads if the JVM supports it. The result is returned as a string, ready to print. If the JVM doesn't have the method Thread.getAllStackTraces i.e, we are on a JVM &lt; 1.5, or  if we don't have the permissions: java.lang.RuntimePermission "getStackTrace" and "modifyThreadGroup", a message saying so is returned instead. Convert a message identifier from org.apache.derby.shared.common.reference.SQLState to a SQLState five character string. Get the severity given a message identifier from {@code SQLState}. Determine if the given {@code SQLState} string constant is a deferred constraint transactional error. If this is so, return {@code true}, else return {@code false}.
Iteratively accumulates the addend into the aggregator. Called on each member of the set of values that is being aggregated. Return true if the aggregation eliminated at least one null from the input data set. Produces the result to be returned by the query. The last processing of the aggregate. Merges one aggregator into a another aggregator. Merges two partial aggregates results into a single result. Needed for: <UL> <LI> parallel aggregation </LI> <LI> vector aggregation (GROUP BY) </LI> <LI> distinct aggregates (e.g. MAX(DISTINCT Col)) </LI></UL><p> An example of a merge would be: given two COUNT() aggregators, C1 and C2, a merge of C1 into C2 would set C1.count += C2.count.  So, given a <i>CountAggregator</i> with a <i>getCount()</i> method that returns its counts, its merge method might look like this: <pre> public void merge(ExecAggregator inputAggregator) throws StandardException { &nbsp;&nbsp;&nbsp;count += ((CountAccgregator)inputAggregator).getCount(); } </pre> Return a new initialized copy of this aggregator, any state set by the setup() method of the original Aggregator must be copied into the new aggregator. Set's up the aggregate for processing.
Return the base name of the table Return the exposed name of the table.  Exposed name is another term for correlation name.  If there is no correlation, this will return the base name. Return the schema for the table.
Return true if orderedNulls was called on this ExecIndexRow for the given column position. Turn the ExecRow into an ExecIndexRow. These two methods are a sort of a hack.  The store implements ordered null semantics for start and stop positioning, which is correct for IS NULL and incorrect for everything else.  To work around this, TableScanResultSet will check whether the start and stop positions have NULL in any column position other than for an IS NULL check. If so, it won't do the scan (that is, it will return no rows). This method is to inform this ExecIndexRow (which can be used for start and stop positioning) that the given column uses ordered null semantics.
Mark the statement as unusable, i.e. the system is finished with it and no one should be able to use it. Get the class generated for this prepared statement. Used to confirm compatability with auxilary structures. Get a new prepared statement that is a shallow copy of the current one. Get the Execution constants. This routine is called at Execution time. Get the saved cursor info.  Used for stored prepared statements. Get the initial row count of the specified result set. If the initial row count has not yet been set, initialize it with the value of the current row count.  Get a saved object by number.  This is called during execution to access objects created at compile time.  These are meant to be read-only at run time. Get all the saved objects.  Used for stored prepared statements. Get the stale plan check interval. the target table of the cursor Methods from old CursorPreparedStatement the update mode of the cursor Check if this prepared statement has a cursor with columns that can be updated. Methods for stale plan checking. Increment and return the execution count for this statement. Check if the specified column name is one of the update columns. Does this statement need a savpoint Indicate that the statement represents an SPS action set the statement text Set the stale plan check interval. set this prepared statement to be valid <p> Checks whether this PreparedStatement is up to date and its activation class is identical to the supplied generated class. A call to {@code upToDate(gc)} is supposed to perform the same work as the following code in one atomic operation: </p> <pre> getActivationClass() == gc &amp;&amp; upToDate() </pre>
Get a clone of a DataValueDescriptor from an ExecRow. Clone the Row and its contents. Clone the Row.  The cloned row will contain clones of the specified columns and the same object as the original row for the other columns. Get a new row with the same columns type as this one, containing nulls. Get a new DataValueDescriptor[] Return the array of objects that the store needs. Get a clone of the array form of the row that Access expects. Reset all the <code>DataValueDescriptor</code>s in the row array to (SQL) null values. This method may reuse (and therefore modify) the objects currently contained in the row array. Set the array of objects
Build a new {@code ExecRow} instance with the columns specified by the {@link #setColumn(int, Object)} method initialized to empty (NULL) values. Reset a row by creating fresh NULL values. Add a template from which a NULL value of the correct type can be created. It should either be a {@code DataValueDescriptor} or a {@code DataTypeDescriptor}. Methods required by the Formatable interface.
Shouldn't be called Bind this ExecSPSNode.  This means doing any static error checking that can be done before actually creating the table. For example, verifying that the ResultColumnList does not contain any duplicate column names.   Do code generation for this statement.  Overrides the normal generation path in StatementNode. Get information about this cursor.  For sps, this is info saved off of the original query tree (the one for the underlying query). Return a description of the ? parameters for the statement represented by this query tree.  Just return the params stored with the prepared statement. Get the name of the SPS that is used to execute this statement.  Only relevant for an ExecSPSNode -- otherwise, returns null. SPSes are atomic if its underlying statement is atomic. Create the Constant information that will drive the guts of Execution. This is assumed to be the first action on this node. Make the result description.  Really, we are just copying it from the stored prepared statement. We need a savepoint if we will do transactional work. We'll ask the underlying statement if it needs a savepoint and pass that back.  We have to do this after generation because getting the PS now might cause us to basically do DDL (for a stmt recompilation) which is explicitly banned during binding.  So the caller can only call this after generate() has retrieved the target PS. ///////////////////////////////////////////////////////////////////  PRIVATE  /////////////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////////////  MISC  ///////////////////////////////////////////////////////////////////
Get the ExecutionFactory from this ExecutionContext.
This returns an indexable row This returns the value row as an indexable row Get a Qualifier to use with a scan of a conglomerate. Create an execution time ResultColumnDescriptor from a compile time RCD. Create a result description given parameters for it. Only one result set factory is needed for a database in the system. We require that an execution factory be configured for each database. Each execution factory then needs to know about the result set factory it is maintaining for its database, so that it can provide it through calls to this method. So, we reuse the result set factory by making it available to each connection in that connection's execution context. Get the ResultSetStatisticsFactory from this ExecutionFactory. Create a new RowChanger for performing update and delete operations based on full before and after rows. Create a new RowChanger for doing insert update and delete operations based on partial before and after. Get an array of ScanQualifiers for a scan.  ScanQualifiers are used with the DataDictionary. Methods from old RowFactory interface This returns a new row that is storable but not indexable Get the XPLAINFactory from this ExecutionFactory. Get the ExecutionFactory from this ExecutionContext. We want an execution context so that we can push it onto the stack.  We could instead require the implementation push it onto the stack for us, but this way we know which context object exactly was pushed onto the stack. Release a ScanQualifier[] (back to the pool or free it).
Validate the statement.
Checks whether the data file exists . SYSCS_EXPORT_QUERY  system Procedure from ij or from a Java application invokes  this method to perform export of the data retrieved by select statement to a file. SYSCS_EXPORT_QUERY_LOBS_TO_EXTFILE system Procedure from ij or from a Java application invokes this method to perform export of the data retrieved by select  statement to a file. Large object data is exported to a different file  and the reference to it is stored in the main output file. SYSCS_EXPORT_TABLE  system Procedure from ij or from a Java application invokes  this method to perform export of  a table data to a file. SYSCS_EXPORT_TABLE_LOBS_TO_EXTFILE system procedure from ij or from a Java application invokes  this method to perform export of a table data to a file. Large object data is exported to a different file and the reference to it is stored in the main output file. Checks if the specified file exists. For internal use only returns the control file reader corresponding to the control file passed Checks whether the lobs file exists . Set the file name to which larg object data has to be exported, and also set flag to indicate that large objects are exported to a different file.
returns the control file reader corresponding to the control file passed convert resultset data for the current row to string array. If large objects are being exported to an external file, then write the lob  data into the external file and store the lob data location  in the string array for that column. following makes the resultset using select * from entityName write a Serializable as a string
if the entity to be exported has non-sql types in it, an exception will be thrown
Convert the input string into double delimiter format for export. double character delimiter recognition in delimited format files applies to the export and import utilities. Character delimiters are permitted within the character-based fields of a file. This applies to fields of type CHAR, VARCHAR, LONGVARCHAR, or CLOB. Any pair of character delimiters found between the enclosing character delimiters is imported into the database. For example with doble quote(") as character delimiter "What a ""nice""day!" will be imported as: What a "nice"day! In the case of export, the rule applies in reverse. For example, I am 6"tall. will be exported to a file as: "I am 6""tall." if nothing more to write, then close the file and write a message of completion in message file prepares the o/p file for writing Writes the binary data in the given input stream to an external lob export file, and return it's location information in the file as string. Location information is written in the main export file. Writes the clob data in the given input Reader to an external lob export file, and return it's location information in the file as string. Location information is written in the main export file. if control file says true for column definition, write it as first line of the data file write the passed row into the data file puts the start and stop delimiters only if column value contains field/record separator in it
load properties locally for faster reference to them periodically if nothing more to write, then close the file and write a message of completion in message file used in case of fixed format Writes the binary data in the given input stream to an external lob export file, and return it's location information in the file as string. Location information is written in the main export file. @param istream   input streams that contains a binary column data. @return Location where the column data written in the external file. @exception Exception  if any error occurs while writing the data. Writes the clob data in the given input Reader to an external lob export file, and return it's location information in the file as string. Location information is written in the main export file. @param ir   Reader that contains a clob column data. @return Location where the column data written in the external file. @exception Exception  if any error occurs while writing the data. if control file says true for column definition, write it as first line of the data file write the passed row into the data file
Add a column to the existing Ordering list.  Takes a column id and only adds it if it isn't in the list. Generate the assignment for row = new ExecRow[numResultSets] /////////////////////////////////////////////////////////////////////  CONSTRUCTOR MANAGEMENT  ///////////////////////////////////////////////////////////////////// Finish the constructor by newing the array of Rows and putting a return at the end of it. Generate a data value. The value is to be set in the SQL data value is required on the stack and will be popped, a SQL data value is pushed. Generate a Null data value. Nothing is required on the stack, a SQL null data value is pushed. Generate a Null data value. The express value is required on the stack and will be popped, a SQL null data value is pushed. /////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////// Return the base class of the activation's hierarchy (the subclass of Object). This class is expected to hold methods used by all compilation code, such as datatype compilation code, e.g. getDataValueFactory. /////////////////////////////////////////////////////////////////////  COLUMN ORDERING  ///////////////////////////////////////////////////////////////////// These utility methods buffers compilation from the IndexColumnOrder class. They create an ordering based on their parameter, stuff that into the prepared statement, and then return the entry # for use in the generated code. We could write another utility method to generate code to turn an entry # back into an object, but so far no-one needs it. WARNING: this is a crafty method that ASSUMES that you want every column in the list ordered, and that every column in the list is the entire actual result colunm. It is only useful for DISTINCT in select. /////////////////////////////////////////////////////////////////////  CURRENT DATE/TIME SUPPORT  ///////////////////////////////////////////////////////////////////// This utility method returns an expression for CURRENT_DATE. Get the expression this way, because the activation needs to generate support information for CURRENT_DATE, that would otherwise be painful to create manually. /////////////////////////////////////////////////////////////////////  CLASS IMPLEMENTATION  ///////////////////////////////////////////////////////////////////// The first time a current datetime is needed, create the class level support for it. This utility method returns an expression for CURRENT_TIME. Get the expression this way, because the activation needs to generate support information for CURRENT_TIME, that would otherwise be painful to create manually. This utility method generates an expression for CURRENT_TIMESTAMP. Get the expression this way, because the activation needs to generate support information for CURRENT_TIMESTAMP, that would otherwise be painful to create manually. Get a method builder for adding code to the execute() method. The method builder does not actually build a method called execute. Instead, it creates a method that overrides the reinit() method, which is called from execute() on every execution in order to reinitialize the data structures. /////////////////////////////////////////////////////////////////////  GENERATE BYTE CODE  ///////////////////////////////////////////////////////////////////// Take the generated class, and turn it into an actual class. <p> This method assumes, does not check, that the class and its parts are all complete. /////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN  ///////////////////////////////////////////////////////////////////// Get the name of the package that the generated class will live in. Get the number of ExecRows that must be allocated return the Name of ResultSet with the RowLocations to be modified (deleted or updated). /////////////////////////////////////////////////////////////////////  DEBUG  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  DATATYPES  ///////////////////////////////////////////////////////////////////// Get the TypeCompiler associated with the given TypeId "ExprFun"s are the "expression functions" that are specific to a given JSQL statement. For example, an ExprFun is generated to evaluate the where clause of a select statement and return a boolean result. <p> All methods return by this are expected to be called via the GeneratedMethod interface. Thus the methods are public and return java.lang.Object. <p> Once the exprfun has been created, the caller will need to add statements to it, minimally a return statement. <p> ExprFuns  return Object types, since they are invoked through reflection and thus their return type would get wrapped in an object anyway. For example: return java.lang.Boolean, not boolean. Add an arbitrarily named field to the generated class. This is used to generate fields where the caller doesn't care what the field is named.  It is especially useful for generating arbitrary numbers of fields, where the caller doesn't know in advance how many fields will be used.  For example, it is used for generating fields to hold intermediate values from expressions. /////////////////////////////////////////////////////////////////////  ADD FIELDS TO GENERATED CLASS  ///////////////////////////////////////////////////////////////////// Add a field declaration to the generated class generated the next field name available. these are of the form 'e#', where # is incremented each time. This shares the name space with the expression methods as Java allows names and fields to have the same name. This reduces the number of constant pool entries created for a generated class file. /////////////////////////////////////////////////////////////////////  ADD FUNCTIONS TO GENERATED CLASS  ///////////////////////////////////////////////////////////////////// Activations might have need of internal functions that are not used by the result sets, but by other activation functions. Thus, we make it possible for functions to be generated directly as well as through the newExprFun interface.  newExprFun should be used when a static field pointing to the expression function is needed. <p> The generated function will generally have a generated name that can be viewed through the MethodBuilder interface. This name is generated to ensure uniqueness from other function names in the activation class. If you pass in a function name, think carefully about whether it will collide with other names. generates a variable name for the rowscanresultset. This can not be a fixed name because in cases like cascade delete same activation class will be dealing more than one RowScanResultSets for dependent tables. Start a user expression.  The difference between a normal expression (returned by newExprFun) and a user expression is that a user expression catches all exceptions (because we don't want random exceptions thrown from user methods to propagate to the rest of the system. Generate a reference to the row array that all activations use. private void pushRowArrayReference(MethodBuilder mb) { PUSHCOMPILE - cache mb.pushThis(); mb.getField(ClassName.BaseActivation, "row", ClassName.ExecRow + "[]"); } Generate a reference to a colunm in a result set. Push an expression that is a GeneratedMethod reference to the passed in method. aka. a "function pointer". Generate a reference to the parameter value set that all activations use. Get a "this" expression declared as an Activation. This is the commonly used type of the this expression. Sets the number of subqueries under this expression

Force an ambigutity with Object's wait.
* Methods of ModuleControl. * Methods of SortCostController * Methods of SortFactory Create a sort. This method could choose among different sort options, depending on the properties etc., but currently it always returns a merge sort. * Methods of MethodFactory There are no default properties for the external sort.. Returns merge sort implementation. Extending classes can overide this method to customize sorting. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Short one line description of routine. <p> The sort algorithm is a N * log(N) algorithm.  The following numbers on a PII, 400 MHZ machine, jdk117 with jit, insane.zip.  This test is a simple "select * from table order by first_int_column.  I then subtracted the time it takes to do "select * from table" from the result. number of rows       elaspsed time in seconds --------------       ----------------------------- 1000                  0.20 10000                10.5 100000               80.0 We assume that the formula for sort performance is of the form: performance = K * N * log(N).  Solving the equation for the 1000 and 100000 case we come up with: performance = 1 + 0.08 N ln(n) NOTE: Apparently, these measurements were done on a faster machine than was used for other performance measurements used by the optimizer. Experiments show that the 0.8 multiplier is off by a factor of 4 with respect to other measurements (such as the time it takes to scan a conglomerate).  I am correcting the formula to use 0.32 rather than 0.08. -	Jeff <p> RESOLVE (mikem) - this formula is very crude at the moment and will be refined later.  known problems: 1) internal vs. external sort - we know that the performance of sort is discontinuous when we go from an internal to an external sort. A better model is probably a different set of contants for internal vs. external sort and some way to guess when this is going to happen. 2) current row size is never considered but is critical to performance. 3) estimatedExportRows is not used.  This is a critical number to know if an internal vs. an external sort will happen. <p> Return an open SortCostController. <p> Return an open SortCostController which can be used to ask about the estimated costs of SortController() operations. <p>
Bind this operator
Bind this constraint definition.  Figure out some information about the table we are binding against.
Comb through the FKInfo structures and pick out the ones that have columns that intersect with the input columns. Get the formatID which corresponds to this class. Read this object from a stream of stored objects. ////////////////////////////////////////////////////////////  Misc  //////////////////////////////////////////////////////////// ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out
Check if a file exists ? This method is  called by some tests through a SQL function: fileExists(fileName varchar(128))returns VARCHAR(100) Create a file. Remove a directory and all of its contents. Remove a directory and all of its contents. This method is  called by some tests through a SQL function: removeDirectory(fileName varchar(128)) returns VARCHAR(100) rename a file. This method is  called by some tests through a SQL procedure: RENAME_FILE(LOCATION VARCHAR(32000), NAME VARCHAR(32000), NEW_NAME  VARCHAR(32000))
<P> Returns the Database object associated with the current connection. Get the TriggerExecutionContext for the current connection of the connection.
<code>getFirstKey</code> returns the first property key. Used when SQLClientInfoException is thrown with a parameterized error message. <code>getFirstValue</code> returns the first property value. Used when SQLClientInfoException is thrown with a parameterized error message. <code>getProperties</code> provides a <code>Map&lt;String,ClientInfoStatus&gt;</code> object describing the failed properties (as specified in the javadoc for java.sql.SQLClientInfoException). Helper method that creates a Propery object with the name-value pair given as arguments.
Throws an exception.
/////////////////////////////////////////////////////////////////////////////////  FUNCTIONS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  OTHER Object OVERRIDES  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  Externalizable BEHAVIOR  /////////////////////////////////////////////////////////////////////////////////
* Map jdbctype to fdoca drda type

Internal to store. Internal to store. Return the qualifier array. <p> Return the array of qualifiers in this FetchDescriptor.  The array of qualifiers which, applied to each key, restricts the rows returned by the scan.  Rows for which any one of the qualifiers returns false are not returned by the scan. If null, all rows are returned.  Qualifiers can only reference columns which are included in the scanColumnList. The column id that a qualifier returns in the column id the table, not the column id in the partial row being returned. <p> A null qualifier array means there are no qualifiers. ************************************************************************ Public Methods of This class: ************************************************************************* Return the column list bit map. <p> A description of which columns to return from every fetch in the scan. A row array and a valid column bit map work together to describe the row to be returned by the scan - see RowUtil for description of how these two parameters work together to describe a "row". Internal to store.
end exec The arguments should be the names of the input and output files
backup the container. increment the version by one and return the new version. <BR> MT - caller must synchronized this in the same sync block that modifies the container header.  Compress free space from container. <BR> MT - thread aware - It is assumed that our caller (our super class) has already arranged a logical lock on page allocation to only allow a single thread through here. Compressing free space is done in allocation page units, working it's way from the end of the container to the beginning.  Each loop operates on the last allocation page in the container. Freeing space in the container page involves 2 transactions, an update to an allocation page, N data pages, and possibly the delete of the allocation page. The User Transaction (UT) initiated the compress call. The Nested Top Transaction (NTT) is the transaction started by RawStore inside the compress call.  This NTT is committed before compress returns. The NTT is used to access high traffic data structures such as the AllocPage. This is outline of the algorithm used in compressing the container. Until a non free page is found loop, in each loop return to the OS all space at the end of the container occupied by free pages, including the allocation page itself if all of it's pages are free. 1) Find last 2 allocation pages in container (last if there is only one). 2) invalidate the allocation information cached by the container. Without the cache no page can be gotten from the container.  Pages already in the page cache are not affected.  Thus by latching the allocPage and invalidating the allocation cache, this NTT blocks out all page gets from this container until it commits. 3) the allocPage determines which pages can be released to the OS, mark that in its data structure (the alloc extent).  Mark the contiguous block of nallocated/free pages at the end of the file as unallocated.  This change is associated with the NTT. 4) The NTT calls the OS to deallocate the space from the file.  Note that the system can handle being booted and asked to get an allocated page which is past end of file, it just extends the file automatically. 5) If freeing all space on the alloc page, and there is more than one alloc page, then free the alloc page - this requires an update to the previous alloc page which the loop has kept latched also. 6) if the last alloc page was deleted, restart loop at #1 All NTT latches are released before this routine returns. If we use an NTT, the caller has to commit the NTT to release the allocPage latch.  If we don't use an NTT, the allocPage latch is released as this routine returns. * Container creation, opening, and closing Create a new container. <p> Create a new container, all references to identity must be through the passed in identity, this object will no identity until after this method returns. should be same name as createIdentity but seems to cause method resolution ambiguities Set container properties from the passed in ByteArray, which is created by logCreateContainerInfo.  This information is used to recreate the container during recovery load tran. The following container properties are set: pageSize spareSpace minimumRecordSize isReusableRecordId initialPages Set container properties from the passed in createArgs. The following container properties are set: pageSize spareSpace minimumRecordSize isReusableRecordId initialPages RESOLVE - in the future setting parameters should be overridable by sub-class, e.g. one implementation of Container may require a minimum page size of 4k. Deallocate a page from the container. deallocate the page from the alloc page Encryption/decryption Decrypts a page <BR>MT - MT safe. Preallocate the pages - actually doing it, called by subclass only Drop Container. <p> Encrypts a page. <BR> MT - not safe, call within synchronized block and only use the returned byte array withing synchronized block. Find or allocate an allocation page which can handle adding a new page. Return a latched allocPage. <BR> MT - single thread required - called as part of add page Find the last alloc page, returns null if no alloc page is found Get an alloc page - only accessible to the raw store (container and recovery) Get any old page - turn off all validation Request the system properties associated with a container. <p> Request the value of properties that are associated with a container. The following properties can be requested: derby.storage.pageSize derby.storage.pageReservedSpace derby.storage.minimumRecordSize derby.storage.reusableRecordId derby.storage.initialPages <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(ConglomerateController cc) { Properties prop = new Properties(); prop.put("derby.storage.pageSize", ""); cc.getContainerProperties(prop); System.out.println( "table's page size = " + prop.getProperty("derby.storage.pageSize"); } Privileged lookup of the ContextService. Must be limited to package visibility so that user code can't call this entry point. Get an embryonic page from the dataInput stream. The embryonic page will be read in from the input stream (fileData), which is assumed to be positioned at the beginning of the first allocation page. Read an embryonic page (that is, a section of the first alloc page that is so large that we know all the borrowed space is included in it) from the specified offset in a {@code StorageRandomAccessFile}. This method is not thread safe, so the caller must make sure that no other thread is performing operations that may change current position in the file. Get encryption buffer. MT - not safe, call within synchronized block and only use the returned byte array withing synchronized block.  Cost estimates <BR>MT - this routine is NOT MT-safe and clients don't need to provide synchronization. Get the first valid page in the container Get only a valid, non-overflow page.  If page number is either invalid or overflow, returns null Setting and getting lastInserted Page and lastUnfilledPage in a thead safe manner. Get a latched page. Incase of backup page Latch is necessary to prevent modification to the page when it is being written to the backup. Backup process relies on latches to get consistent snap shot of the page , user level table/page/row locks are NOT acquired  by the online backup mechanism. Get the next page in the container. Get a valid (non-deallocated or free) page in the container. Overflow page is OK. Resulting page is latched. <BR> MT - thread safe Get candidate page to move a row for compressing the table. <p> The caller is moving rows from the end of the table toward the beginning, with the goal of freeing up a block of empty pages at the end of the container which can be returned to the OS. <p> On entry pageno will be latched by the caller.  Only return pages with numbers below pageno.  Attempting to return pageno will result in a latch/latch deadlock on the same thread. Get a potentially suitable page for insert and latch it. Get the reusable RecordId sequence number for the container. Privileged module lookup. Must be private so that user code can't call this entry point. Get information about space used by the container. format Id must fit in 4 bytes Return my format identifier. Get a page in the container. Get User page is the generic base routine for all user (client to raw store) getPage.  This routine coordinate with allocation/deallocation to ensure that no page can be gotten from the container while page is in the middle of being allocated or deallocated. This routine latches the page. Increment the reusable RecordId version sequence number. initialize header information so this container object can be safely reused as if this container object has just been new'ed Initialize a page We treat this container as dirty if it has the container file open. * Hide our super-classes methods to ensure that cache management * is correct when the container is obtained and release. The container is kept by the find() in File.openContainer. Log all information on the container creation necessary to recreate the container during a load tran. Make a new alloc page, latch it with the passed in container handle. Create a new page in the container. <BR> MT - thread aware - It is assumed that our caller (our super class) has already arranged a logical lock on page allocation to only allow a single thread through here. Adding a new page involves 2 transactions and 2 pages. The User Transaction (UT) initiated the addPage call and expects a latched page (owns by the UT) to be returned. The Nested Top Transaction (NTT) is the transaction started by RawStore inside an addPage call.  This NTT is committed before the page is returned.  The NTT is used to accessed high traffic data structure such as the AllocPage. This is outline of the algorithm used in adding a page: 1) find or make an allocPage which can handle the addding of a new page. Latch the allocPage with the NTT. 2) invalidate the allocation information cached by the container. Without the cache no page can be gotten from the container.  Pages already in the page cache is not affected.  Thus by latching the allocPage and invalidating the allocation cache, this NTT blocks out all page gets from this container until it commits. 3) the allocPage determines which page can be allocated, mark that in its data structure (the alloc extent) and returns the page number of the new page.  This change is associated with the NTT. 4) the NTT gets or creates the new page in the page cache (bypassing the lookup of the allocPage since that is already latched by the NTT and will deadlock). 5) the NTT initializes the page (mark it is being a VALID page). 6) the page latch is transfered to the UT from the NTT. 7) the new page is returned, latched by UT If we use an NTT, the caller has to commit the NTT to release the allocPage latch.  If we don't use an NTT, the allocPage latch is released as this routine returns. Open a container. <p> Longer descrption of routine. <p> Open a container. Open the file that maps to this container, if the file does not exist then we assume the container was never created. If the file exists but we have trouble opening it then we throw some exception. <BR> MT - single thread required - Enforced by cache manager. page preallocation preAllocate writes out the preallocated pages to disk if necessary. <BR>Make sure the container is large enough and the pages are well formatted.  The only reason to do this is to save some I/O during page initialization.  Once the initPage log record is written, it is expected that the page really do exist and is well formed or recovery will fail.  However, we can gain some performance by writing a bunch of pages at a time rather than one at a time. <BR>If it doesn't make sense for the the implementation to have pre-allocation, just return 0. <BR>If the container is not being logged, don't actually do anything, just return 0. ReCreate a page for rollforward recovery. <p> During redo recovery it is possible for the system to try to redo the creation of a page (ie. going from non-existence to version 0). It first trys to read the page from disk, but a few different types of errors can occur: o the page does not exist at all on disk, this can happen during rollforward recovery applied to a backup where the file was copied and the page was added to the file during the time frame of the backup but after the physical file was copied. o space in the file exists, but it was never initalized.  This can happen if you happen to crash at just the right moment during the allocation process.  Also on some OS's it is possible to read from a part of the file that was not ever written - resulting in garbage from the store's point of view (often the result is all 0's). All these errors are easy to recover from as the system can easily create a version 0 from scratch and write it to disk. Because the system does not sync allocation of data pages, it is also possible at this point that whlie writing the version 0 to disk to create it we may encounter an out of disk space error (caught in this routine as a StandardException from the create() call.  We can't recovery from this without help from outside, so the caught exception is nested and a new exception thrown which the recovery system will output to the user asking them to check their disk for space/errors. Read the container's header. When this method is called, the embryonic page that is passed in must have been read directly from the file or the input stream, even if the alloc page may still be in cache.  This is because a stubbify operation only writes the stub to disk, it does not get rid of any stale page from the page cache.  So if it so happens that the stubbified container object is aged out of the container cache but the first alloc page hasn't, then when any stale page of this container wants to be written out, the container needs to be reopened, which is when this routine is called.  We must not get the alloc page in cache because it may be stale page and it may still say the container has not been dropped. <BR> MT - single thread required - Enforced by caller. Read containerInfo from a byte array The container Header array must be written by or of the same format as put together by writeHeaderFromArray. * Methods used solely by StoredPage Read a page into the supplied array. <BR> MT - thread safe  Open the container. <p> Open the container with key "newIdentity". <p> should be same name as setIdentity but seems to cause method resolution ambiguities * Methods of Cacheable * * getIdentity() and clearIdentity() are implemented by BaseContainer Containers Open the container. Update estimated row count by page as it leaves the cache. The estimated row count is updated without logging! Write a sequence of bytes at the given offset in a file. This method is not thread safe, so the caller must make sure that no other thread is performing operations that may change current position in the file. Write the container header to a page array (the first allocation page) Write the container header directly to file. Subclasses that can writes the container header is expected to manufacture a DataOutput stream which is used here. <BR> MT - single thread required - Enforced by caller Write containerInfo into a byte array The container Header thus put together can be read by readHeaderFromArray. Write a page from the supplied array. <BR> MT - thread safe
Provider interface     class interface   Gets the generationId for the current version of this file. The triple (schemaName,SQLName,generationId) are unique for the life of this database.
Close the logger. MT - caller provide synchronization (RESOLVE: not called by anyone ??) Flush the log up to the given log instant. <P>MT - not needed, wrapper method Flush all outstanding log to disk. <P>MT - not needed, wrapper method * Methods of Logger Writes out a log record to the log stream, and call its doMe method to apply the change to the rawStore. <BR>Any optional data the doMe method need is first written to the log stream using operation.writeOptionalData, then whatever is written to the log stream is passed back to the operation for the doMe method. <P>MT - there could be multiple threads running in the same raw transactions and they can be calling the same logger to log different log operations.  This whole method is synchronized to make sure log records are logged one at a time. Writes out a compensation log record to the log stream, and call its doMe method to undo the change of a previous log operation. <P>MT - Not needed. A transaction must be single threaded thru undo, each RawTransaction has its own logger, therefore no need to synchronize. The RawTransaction must handle synchronizing with multiple threads during rollback. Read the next log record from the scan. <P>MT - caller must provide synchronization (right now, it is only called in recovery to find the checkpoint log record.  When this method is called by a more general audience, MT must be revisited). Recovery Redo loop. <P> The log stream is scanned from the beginning (or from the undo low water mark of a checkpoint) forward until the end. The purpose of the redo pass is to repeat history, i.e, to repeat exactly the same set of changes the rawStore went thru right before it stopped.   With each log record that is encountered in the redo pass: <OL> <LI>if it isFirst(), then the transaction factory is called upon to create a new transaction object. <LI>if it needsRedo(), its doMe() is called (if it is a compensation operation, then the undoable operation needs to be created first before the doMe is called). <LI>if it isComplete(), then the transaction object is closed. </OL> <P> MT - caller provides synchronization During recovery re-prepare a transaction. <p> After redo() and undo(), this routine is called on all outstanding in-doubt (prepared) transactions.  This routine re-acquires all logical write locks for operations in the xact, and then modifies the transaction table entry to make the transaction look as if it had just been prepared following startup after recovery. <p> Undo a part of or the entire transaction.  Begin rolling back the log record at undoStartAt and stopping at (inclusive) the log record at undoStopAt. <P>MT - Not needed. A transaction must be single threaded thru undo, each RawTransaction has its own logger, therefore no need to synchronize.  The RawTransaction must handle synchronizing with multiple threads during rollback.
SECURITY WARNING. This method is run in a privileged block in a Java 2 environment. Return a property from the JVM's system set. In a Java2 environment this will be executed as a privileged block if and only if the property starts with 'derby.'. If a SecurityException occurs, null is returned. SECURITY WARNING. This method is run in a privileged block in a Java 2 environment. Set the system home directory.  Returns false if it couldn't for some reason. Create a ThreadGroup and set the daemon property to make sure the group is destroyed and garbage collected when all its members have finished (i.e., either when the driver is unloaded, or when the last database is shut down). * Priv block code, moved out of the old Java2 version. Initialize the system in a privileged block.
Add a file resource, copying from the input stream. The InputStream will be closed by this method. Get the StorageFile for a file resource.  Remove the current generation of a file resource from the database. During hard upgrade to &lt;= 10.9, remove a jar directory (at post-commit time) from the database. Replace a file resource with a new version. <P>The InputStream will be closed by this method.
end of copyDirectory( StorageFactory sf, StorageFile from, File to, byte[] buf, String[] filter) end of copyDirectory( StorageFactory sf, StorageFile from, File to, byte[] buf, String[] filter) end of copyFile( StorageFactory storageFactory, StorageFile from, File to, byte[] buf) end of copyFile end of copyFile <p> Use when creating new files. If running on Unix, limit read and write permissions on {@code file} to owner if {@code derby.storage.useDefaultFilePermissions == false}. </p> <p> If the property is not specified, we use restrictive permissions anyway iff running with the server server started from the command line. </p> <p> On Unix, this is equivalent to running with umask 0077. </p> <p> On Windows, with FAT/FAT32, we lose, since the fs does not support permissions, only a read-only flag. </p> <p> On Windows, with NTFS with ACLs, we limit access also for Windows using the new {@code java.nio.file.attribute} package. </p> <p> When restricted file access is enabled (either explicitly or by default) errors are handled like this: When running on JDK 7 or higher, and the file system can be accessed either via a PosixFileAttributeView or via an AclFileAttributeView, any IOException reported when trying to restrict the permissions will also be thrown by this method. In all other cases, it will do its best to limit the permissions using the {@code java.io.File} methods ({@code setReadable()}, {@code setWritable()}, {@code setExecutable()}), but it won't throw any exceptions if the permissions cannot be set that way. </p> Limit access to owner using methods in the {@code java.io.File} class. Those methods are available on all Java versions from 6 and up, but they are not fully functional on all file systems. Limit access to owner using a {@code java.nio.file.attribute.FileAttributeView}. Such views are only available on Java 7 and higher, and only on file systems that support changing file permissions. Currently, this is supported on POSIX file systems and file systems that maintain access control lists (ACLs). Remove a directory and all of its contents. The results of executing File.delete() on a File object that represents a directory seems to be platform dependent. This method removes the directory and all of its contents. Remove the leading 'file://' protocol from a filename which has been expressed as an URL. If the filename is not an URL, then nothing is done. Otherwise, an URL like 'file:///tmp/foo.txt' is transformed into the legal file name '/tmp/foo.txt'.
Compares this <code>Filter</code> object to another object. Returns the hashCode for this <code>Filter</code> object. Filter using a <code>Dictionary</code> object. The Filter is executed using the <code>Dictionary</code> object's keys and values. The keys are case insensitively matched with the filter. Filter using a service's properties. <p> The filter is executed using the keys and values of the referenced service's properties. The keys are case insensitively matched with the filter. Filter with case sensitivity using a <code>Dictionary</code> object. The Filter is executed using the <code>Dictionary</code> object's keys and values. The keys are case sensitively matched with the filter. Returns this <code>Filter</code> object's filter string. <p> The filter string is normalized by removing whitespace which does not affect the meaning of the filter.
Releases resources associated with the client. Fetches JIRA issues matched by a predefined filter search. <p> The filter must be created manually and before the release notes are generated. Fetches JIRA issues matching a generated JQL (Jira Query Language) search. Computes the ancestors for the specified version. Returns a list of sorted and filtered Derby releases. <p> If a target release is specified, all later releases will be filtered out. The filtering happens at two levels: <ul> <li>version number (i.e. 10.6.2.1 > 10.5.1.0)</li> <li>release date</li> </ul> <p> The target version will always be found at index zero. <p> Not specifying a target version will return all Derby releases sorted by version number. Returns the version object for the specified Derby version. Tells if the issue has the specified custom field value set. Tells if the issue has a release note. Logs status/convenience messages. Interface for running from the command line. Executes a JIRA filter and writes the matching JIRA issues to file. Generates a list of Derby JIRA issues addressed by the target release version and writes these to a file for further processing. <p> <b>Important:</b> Although some sanity checks are performed, it is crucial that the manually created filter is set up correctly. If the filter misses issues addressed by the release, they will not make it into the generated release notes. Short description: <ul> <li>include bugs and improvements</li> <li>include issues resolved as Fixed</li> <li>include issues marked as Resolved or Closed</li> <li>include all release candidates in the fix version field (if not already released)</li> </ul> Prints the list of ancestors, i.e. earlier releases down the release chain, for specified Derby version. <p> Note that only released versions are considered to be ancestors. Prints all Derby releases. Converts an array of fix versions into a string representation.

/////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Get the current line number. </p> /////////////////////////////////////////////////////////////////////////////////  StringColumnVTI BEHAVIOR TO BE IMPLEMENTED BY SUBCLASSES  ///////////////////////////////////////////////////////////////////////////////// <p> Get the string value of the column in the current row identified by the 1-based columnNumber. </p> <p> Get the name of the file being read. </p> /////////////////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  FlatFileVTI BEHAVIOR TO BE IMPLEMENTED BY SUBCLASSES  ///////////////////////////////////////////////////////////////////////////////// <p> Parse the next chunk of text, using readLine(), and return the next row. Returns null if the file is exhausted. </p> <p> Read the next line from the file and return it. Return null if the file is exhausted. </p> <p> Wrap an exception in a SQLException. </p> <p> Wrap an exception in a SQLException. </p>
--------------------------private helper methods---------------------------- Convert the byte array to an int. Convert the byte array to a long. Build a Java double from an 8-byte floating point representation. <p/> <p/> This includes DERBY types: <ul> <li> FLOAT <li> DOUBLE [PRECISION] </ul> --------------entry points for runtime representation----------------------- <p> Build a Java float from a 4-byte floating point representation. </p> <p> This includes DERBY types: </p> <ul> <li> REAL <li> FLOAT(1&lt;=n&lt;=24) </ul>
Close the scan. Return the log instant (as an integer) the scan is currently on - this is the log instant of the log record that was returned by getNextRecord. Return the log instant the scan is currently on - this is the log instant of the log record that was returned by getNextRecord. Get the log instant that is right after the record just retrived * Methods of LogScan Read a log record into the byte array provided.  Resize the input stream byte array if necessary. returns true if there is partially writen log records before the crash in the last log file. Partiall wrires are identified during forward scans for log recovery. Reset the scan to the given LogInstant. Private methods.
Get the group for the current log record. Get the Loggable associated with the currentLogRecord This may be called only once per log record.
<p> Create a Derby schema if it does not already exist. </p> //////////////////////////////////////////////////////////////////////  SQL MINIONS  ////////////////////////////////////////////////////////////////////// <p> Turn a Derby schema name into a schema name suitable for use in a dot-separated object name. </p> <p> Drop a Derby schema. </p> //////////////////////////////////////////////////////////////////////  UNREGISTRATION MINIONS  ////////////////////////////////////////////////////////////////////// <p> Drop a schema object. If the object does not exist, silently swallow the error. </p> <p> Get the name of the local Derby schema corresponding to a foreign schema name. Returns null if the default (current) schema is to be used. </p> //////////////////////////////////////////////////////////////////////  Connection MANAGEMENT  ////////////////////////////////////////////////////////////////////// <p> Get a cursor through the user tables in the foreign database. </p> /////////////////////////////////////////////////////////////////////////////////  OptionalTool BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Creates a local Derby schema for every foreign schema which contains a user table. Then creates a table function and convenience view for every user table found in the foreign database. The parameters to this method are: </p> <ul> <li>foreignConnectionURL (required) - URL to connect to the foreign database</li> <li>schemaPrefix (optional) - If not specified, then the local Derby schema which is created has the same name as the foreign schema. Otherwise, this prefix is prepended to the names of the local Derby schemas which are created.</li> </ul> <p> Get the type of an external database's column as a Derby type name. </p> <p> Build a precision and scale designator. </p> <p> Turns precision into a length designator. </p> /////////////////////////////////////////////////////////////////////////////////  REGISTRATION MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Removes the schemas, table functions, and views created by loadTool(). </p> <ul> <li>connectionURL (required) - URL to connect to the foreign database</li> <li>schemaPrefix (optional) - See loadTool() for more information on this argument.</li> </ul>
Gets an identifier telling what type of descriptor it is (UNIQUE, PRIMARY KEY, FOREIGN KEY, CHECK). Gets a referential action rule on a  DELETE Gets a referential action rule on a UPDATE Get the constraint that this FK references.  Will return either a primary key or a unique key constraint. Get the constraint id for the constraint that this FK references. Will return either a primary key or a unique key constriant. Am I a self-referencing FK?  True if my referenced constraint is on the same table as me. Does this constraint need to fire on this type of DML?  True if insert or update and columns intersect
Check that the row either has a null column(s), or corresponds to a row in the referenced key. <p> If the referenced key is found, then it is locked when this method returns.  The lock is held until the next call to doCheck() or close(). Get the isolation level for the scan for the RI check. NOTE: The level will eventually be instantaneous locking once the implementation changes.
//////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ////////////////////////////////////////////////////////////////////// <p> This function is useful for verifying that the connection to the foreign database was dropped when the foreignViews tool was unloaded. </p> //////////////////////////////////////////////////////////////////////  SUPPORT FUNCTIONS  ////////////////////////////////////////////////////////////////////// <p> Remove the cached connection to the foreign database. This method is called by ForeignDBViews.unloadTool(). </p> //////////////////////////////////////////////////////////////////////  Connection MANAGEMENT  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  RestrictedVTI BEHAVIOR  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  QUERY FACTORY  ////////////////////////////////////////////////////////////////////// <p> Build the query which will be sent to the foreign database. </p> //////////////////////////////////////////////////////////////////////  UTILITY METHODS  ////////////////////////////////////////////////////////////////////// <p> Map a 1-based Derby column number to a 1-based column number in the foreign query. </p> //////////////////////////////////////////////////////////////////////  TABLE FUNCTION  ////////////////////////////////////////////////////////////////////// <p> Table function to read a table in a foreign database. </p>
* CloneableStream interface **   Privileged lookup of a Context. Must be private so that user code can't call this entry point. * Class private methods Handle an error that happened within {@code readObject()} when reading a {@code Serializable} object.  Read an object from this stream. * Resetable interface **  Set the InputStream for this FormatIdInputStream to the stream provided.
Methods of ErrorInfo, used here for SQLData error reporting Set the OutputStream for this FormatIdOutputStream to the stream provided. It is the responsibility of the caller to flush or close (as required) the previous stream this class was attached to. Write a format id for the object provied followed by the object itself to this FormatIdOutputStream.
Returns the name of this taglet format_id not expected to be used in constructor documentation. format_id not expected to be used in field documentation. format_id not expected to be used in method documentation. format_id can be used in overview documentation. format_id can be used in package documentation. format_id can be used in type documentation. format_id is not an inline tag. Register this Taglet. Embed the contents of the format_id tag as a row in the disk format table. Embed multiple format_id tags as cells in the disk format table.


Get the held array of formatables, and return it in an array that is an instance of {@code arrayClass}. Get the formatID which corresponds to this class. Read this array from a stream of stored objects. Set the held array to the input array. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this array out
Bitwise AND this FormatableBitSet with another FormatableBitSet. The result is stored in this bitset. The operand is unaffected. A null operand is treated as an empty bitset (i.e. clearing this bitset). A bitset that is smaller than its operand is expanded to the same size. If any bit is set, return the zero-based bit index of the first bit that is set. If no bit is set, return -1. By using anySetBit() and anySetBit(beyondBit), one can quickly go thru the entire bit array to return all set bit. Like anySetBit(), but return any set bit whose number is bigger than beyondBit. If no bit is set after beyondBit, -1 is returned. By using anySetBit() and anySetBit(beyondBit), one can quickly go thru the entire bit array to return all set bit. Clear all the bits in this FormatableBitSet Bit clear Cloneable Bit comparison.  Compare this with other. Will always do a byte by byte compare. Given 2 similar bits of unequal lengths (x and y), where x.getLength() &lt; y.getLength() but where: x[0..x.getLength()] == y[0..x.getLength()] then x &lt; y. Copy the bytes from another FormatableBitSet. Assumes that this bit set is at least as large as the argument's bit set. * Some of the operators required by SQL.  These could alternatively * be in SQLBit, but since they are so tightly bound to the implementation * rather than return something that undermines the encapsulation * of this type, i have chosen to put them in here. Bit equivalence.  Compare this with other. If the length is different, then cannot be equal so short circuit.  Otherwise, rely on compare().  Note that two zero length bits are considered equal. A utility method which treats the byte argument as an 8-bit bitset and finds the first set bit in that byte. Assumes that at least one bit in v is set (v!=0). Bit get -- alias for isSet() Get the value of the byte array * Get the length in bits * * @return The length in bits for this value * * NOTE: could possibly be changed to a long.  As is * we are restricted to 2^(31-3) -&gt; 256meg instead * of 2^31 (Integer.MAX_VALUE) like other datatypes * (or 2 gig).  If it is ever changed to a long * be sure to change read/writeExternal which write * out the length in bits. Get the length in bytes of a Bit value Get a count of the number of bits that are set. Get the formatID which corresponds to this class. Grow (widen) a FormatableBitSet so that it contains at least N bits. If the bitset already has more than n bits, this is a noop. Negative values of n are not allowed. ASSUMPTIONS: that all extra bits in the last byte are zero. Produce a hash code by putting the value bytes into an int, exclusive OR'ing if there are more than 4 bytes. This method returns true if the following conditions hold: 1. The number of bits in the bitset will fit into the allocated byte array. 2. 'lengthAsBits' and 'bitsInLastByte' are consistent. 3. All unused bits in the byte array are unset. This represents an invariant for the class, so this method should always return true. The method is public, but is primarily intended for testing and ASSERTS. Bit isSet Statically calculates how many bits can fit into the number of bytes if this Bit object is externalized.  Only valid for this implementation of Bit. Figure out how many bits are in the last byte from the total number of bits. Figure out how many bytes are needed to store the input number of bits. Bitwise OR this FormatableBitSet with another FormatableBitSet. The result is stored in this bitset. The operand is unaffected. A null operand is treated as an empty bitset (i.e. a noop). A bitset that is smaller than its operand is expanded to the same size. Note: gracefully handles zero length bits -- will create a zero length array with no bits being used.  Fortunately in.read() is ok with a zero length array so no special code. <p> WARNING: this method cannot be changed w/o changing SQLBit because SQLBit calls this directly w/o calling read/writeObject(), so the format id is not stored in that case. Bit set Shrink (narrow) a FormatableBitSet to N bits. N may not be larger than the current bitset size, or negative. Get the length in bits -- alias for getLength() Format the string into BitSet format: {0, 2, 4, 8} if bits 0, 2, 4, 8 are set. Division, multiplication and remainder calcuation of a positive number with a power of two can be done using shifts and bit masking. The compiler attempts this optimization but since Java does not have unsigned ints it will also have to create code to handle negative values. In this class the argument is frequently an array index or array length, which is known not to be negative. These utility methods allow us to perform "unsigned" operations with 8. Hopefully the extra function call will be inlined by the compiler. ///////////////////////////////////////////////////////  EXTERNALIZABLE  /////////////////////////////////////////////////////// Format: <UL> <LI>int		length in bits  </LI> <LI>byte[]					</LI></UL> Bitwise XOR this FormatableBitSet with another FormatableBitSet. The result is stored in this bitset. The operand is unaffected. A null operand is treated as an empty bitset (i.e. a noop). A bitset that is smaller than its operand is expanded to the same size.
Get the formatID which corresponds to this class. Our special put method that wont barf on a null value. Read the hash table from a stream of stored objects. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write the hash table out.  Step through the enumeration and write the strings out in UTF.
Set the format identifier that this instance will be loading from disk.
Create and return an array of FormatableIntHolders given an array of ints. Get the held int. Get the formatID which corresponds to this class. Read this formatable from a stream of stored objects. Set the held int to the input int. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this formatable out
Create and return an array of FormatableLongHolders given an array of ints. Get the held int. Get the formatID which corresponds to this class. Read this formatable from a stream of stored objects. Set the held long to the input int. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this formatable out
Clear the defaults from this Properties set. This sets the default field to null and thus breaks any link with the Properties set that was the default. Get the formatID which corresponds to this class. Read the properties from a stream of stored objects. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write the properties out.  Step through the enumeration and write the strings out in UTF.
This method converts the non-ASCII characters in the input parameter to unicode escape sequences. Pads out a string to the specified size repeatChar is used to create strings of varying lengths. called from various tests to test edge cases and such.
//////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ////////////////////////////////////////////////////////////////////// Get the wrapped ResultSet. This overridable method maps the ForwardVTI's column numbers to those of the wrapped ResultSet //////////////////////////////////////////////////////////////////////  SUPPORT FUNCTIONS  ////////////////////////////////////////////////////////////////////// Poke in another ResultSet to which we forward method calls.
Returns the bundle associated with the event. This bundle is also the source of the event. Returns the exception related to this event. Returns the type of framework event. <p> The type values are: <ul> <li>{@link #STARTED} <li>{@link #ERROR} <li>{@link #WARNING} <li>{@link #INFO} <li>{@link #PACKAGES_REFRESHED} <li>{@link #STARTLEVEL_CHANGED} </ul>
Receives notification of a general <code>FrameworkEvent</code> object.
Creates a <code>Filter</code> object. This <code>Filter</code> object may be used to match a <code>ServiceReference</code> object or a <code>Dictionary</code> object. <p> If the filter cannot be parsed, an {@link InvalidSyntaxException} will be thrown with a human readable message where the filter became unparsable.
no LOJ reordering for base table. Accept the visitor for all visitable children of this node. Augment the RCL to include the columns in the FormatableBitSet. If the column is already there, don't add it twice. Column is added as a ResultColumn pointing to a ColumnReference.   Bind the expressions in this FromBaseTable.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the table in this FromBaseTable. This is where view resolution occurs Bind the result columns of this ResultSetNode when there is no base table to bind them to.  This is useful for SELECT statements, where the result columns get their types from the expressions that live under them. Bind the table descriptor for this table. If the tableName is a synonym, it will be resolved here. The original table name is retained in origTableName. Tell super-class that this Optimizable can be ordered  Clear the bits from the dependency map when join nodes are flattened  Is this a table that has a FOR UPDATE clause? Turn off bulk fetch Do a special scan for max. <p> Estimate the cost of scanning this {@code FromBaseTable} using the given predicate list with the given conglomerate. </p> <p> If the table contains little data, the cost estimate might be adjusted to make it more likely that an index scan will be preferred to a table scan, and a unique index will be preferred to a non-unique index. Even though such a plan may be slightly suboptimal when seen in isolation, using indexes, unique indexes in particular, needs fewer locks and allows more concurrency. </p>  Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Build a ResultColumnList based on all of the columns in this FromBaseTable. NOTE - Since the ResultColumnList generated is for the FromBaseTable, ResultColumn.expression will be a BaseColumnNode. Generation on a FromBaseTable creates a scan on the optimizer-selected conglomerate. Generation on a FromBaseTable for a referential action dependent table. Generation on a FromBaseTable for a SELECT. This logic was separated out so that it could be shared with PREPARE SELECT FILTER. Return a ResultColumnList with all of the columns in this table. (Used in expanding '*'s.) NOTE: Since this method is for expanding a "*" in the SELECT list, ResultColumn.expression will be a ColumnReference.  Does this FBT represent an EXISTS FBT. Get the exposed name for this table, which is the name that can be used to refer to it in the rest of the query. Get the exposed table name for this table, which is the name that can be used to refer to it in the rest of the query. Get the final CostEstimate for this ResultSetNode. Determine whether or not the specified name is an exposed name in the current query block. Try to find a ResultColumn in the table represented by this FromBaseTable that matches the name in the given ColumnReference. * RESOLVE: This whole thing should probably be moved somewhere else, * like the optimizer or the data dictionary.  Return a TableName node representing this FromTable. Return the table name for this table.     Return whether or not this is actually a EBT for NOT EXISTS. Return whether or not the underlying ResultSet tree will return a single row, at most.  This method is intended to be used during generation, after the "truly" best conglomerate has been chosen. This is important for join nodes where we can save the extra next on the right side if we know that it will return at most 1 row. Is this a one-row result set with the given conglomerate descriptor?  Return whether or not this index is ordered on a permutation of the specified columns. Return whether or not the underlying ResultSet tree is ordered on the specified columns. RESOLVE - This method currently only considers the outermost table of the query block. RESOLVE - We do not currently push method calls down, so we don't worry about whether the equals comparisons can be against a variant method. Is it possible to do a distinct scan on this ResultSet tree. (See SelectNode for the criteria.) Return whether or not this index is ordered on a permutation of the specified columns.    Convert an absolute to a relative 0-based column position. Return a node that represents invocation of the virtual table for the given table descriptor. The mapping of the table descriptor to a specific VTI class name will occur as part of the "init" phase for the NewInvocationNode that we create here. Currently only handles no argument VTIs corresponding to a subset of the diagnostic tables. (e.g. lock_table). The node returned is a FROM_VTI node with a passed in NEW_INVOCATION_NODE representing the class, with no arguments. Other attributes of the original FROM_TABLE node (such as resultColumns) are passed into the FROM_VTI node. Mark this ResultSetNode as the target table of an updatable cursor. Mark the underlying scan as a distinct scan. Mark as updatable all the columns in the result column list of this FromBaseTable that match the columns in the given update column list.  Create a new ResultColumnList to reflect the columns in the index described by the given ConglomerateDescriptor.  The columns in the new ResultColumnList are based on the columns in the given ResultColumnList, which reflects the columns in the base table. Optimizable interface.   Preprocess a ResultSetNode - this currently means: o  Generating a referenced table map for each ResultSetNode. o  Putting the WHERE and HAVING clauses in conjunctive normal form (CNF). o  Converting the WHERE and HAVING clauses into PredicateLists and classifying them. o  Ensuring that a ProjectRestrictNode is generated on top of every FromBaseTable and generated in place of every FromSubquery. o  Pushing single table predicates down to the new ProjectRestrictNodes.  helper method used by generateMaxSpecialResultSet and generateDistinctScan to return the name of the index if the conglomerate is an index. @param cd   Conglomerate for which we need to push the index name @param mb   Associated MethodBuilder @throws StandardException  Tells if the given table qualifies for a statistics update check in the current configuration. Return true if the node references SESSION schema tables (temporary or permanent) Search to see if a query references the specifed table name. Set whether or not this FBT represents an EXISTS FBT. set the Information gathered from the parent table that is required to perform a referential action on dependent table. Set the name of the row location column Set the table properties for this table.  Determine whether or not the columns marked as true in the passed in array are a superset of any unique index on this table. This is useful for subquery flattening and distinct elimination based on a uniqueness condition. Determine whether or not the columns marked as true in the passed in join table matrix are a superset of any single column unique index on this table. This is useful for distinct elimination based on a uniqueness condition. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.  Get the lock mode for the target table heap of an update or delete statement.  It is not always MODE_RECORD.  We want the lock on the heap to be consistent with optimizer and eventually system's decision. This is to avoid deadlock (beetle 4318).  During update/delete's execution, it will first use this lock mode we return to lock heap to open a RowChanger, then use the lock mode that is the optimizer and system's combined decision to open the actual source conglomerate. We've got to make sure they are consistent.  This is the lock chart (for detail reason, see comments below): BEST ACCESS PATH			LOCK MODE ON HEAP ----------------------		----------------------------------------- index					  row lock heap					  row lock if READ_COMMITTED, REPEATBLE_READ, or READ_UNCOMMITTED and not specified table lock otherwise, use optimizer decided best acess path's lock mode
This method reorders LOJs in the FROM clause. For now, we process only a LOJ.  For example, "... from LOJ_1, LOJ2 ..." will not be processed. Add a table to the FROM list. <p> Bind a column reference to one of the tables in this FromList.  The column name must be unique within the tables in the FromList.  An exception is thrown if a column name is not unique. This method fills in various fields in the column reference. </p> <p> NOTE: Callers are responsible for ordering the FromList by nesting level, with tables at the deepest (current) nesting level first.  We will try to match against all FromTables at a given nesting level.  If no match is found at a nesting level, then we proceed to the next level.  We stop walking the list when the nesting level changes and we have found a match. </p> <p> NOTE: If the ColumnReference is qualified, then we will stop the search at the first nesting level where there is a match on the exposed table name. For example, <p> <pre> s (a, b, c), t (d, e, f) select * from s where exists (select * from t s where s.c = a) </pre> <p> will not find a match for s.c, which is the expected ANSI behavior. </p> <p> bindTables() must have already been called on this FromList before calling this method. </p> Bind the expressions in this FromList.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the result columns of the ResultSetNodes in this FromList when there is no base table to bind them to.  This is useful for SELECT statements, where the result columns get their types from the expressions that live under them. Bind any untyped null nodes to the types in the given ResultColumnList. Decrement (query block) level (0-based) for all of the tables in this from list. This is useful when flattening a subquery. Expand a "*" into the appropriate ResultColumnList. If the "*" is unqualified it will expand into a list of all columns in all of the base tables in the from list at the current nesting level; otherwise it will expand into a list of all of the columns in the base table that matches the qualification. NOTE: Callers are responsible for ordering the FromList by nesting level, with tables at the deepest (current) nesting level first. We will expand the "*" into a list of all columns from all tables having the same nesting level as the first FromTable in this list. The check for nesting level exists because it's possible that this FromList includes FromTables from an outer query, which can happen if there is a "transparent" FromList above this one in the query tree.  Ex: select j from onerow where exists (select 1 from somerow union select * from diffrow where onerow.j &lt; diffrow.k) If "this" is the FromList for the right child of the UNION then it will contain both "diffrow" and "onerow", the latter of which was passed down via a transparent FromList (to allow binding of the WHERE clause). In that case the "*" should only expand the result columns of "diffrow"; it should not expand the result columns of "onerow" because that table is from an outer query.  We can achieve this selective expansion by looking at nesting levels. Flatten all the FromTables that are flattenable. RESOLVE - right now we just flatten FromSubqueries.  We should also flatten flattenable JoinNodes here. Mark all of the FromBaseTables in the list as EXISTS FBTs. Each EBT has the same dependency list - those tables that are referenced minus the tables in the from list. Determine whether or not the specified name is an exposed name in the current query block. Get the FromTable from this list which has the specified ResultColumn in its RCL. OptimizableList interface   Return whether or not the user specified a hash join for any of the tables in this list.  Go through the list of the tables and see if the passed ResultColumn is a join column for a right outer join with USING/NATURAL clause.  Indicate that this FromList is "transparent", which means that its FromTables should be bound to tables from an outer query. Generally this is not allowed, but there are exceptions.  See SetOperatorNode.setResultToBooleanTrueNode() for more.  Preprocess the query tree - this currently means: o  Generating a referenced table map for each ResultSetNode. o  Putting the WHERE and HAVING clauses in conjunctive normal form (CNF). o  Converting the WHERE and HAVING clauses into PredicateLists and classifying them. o  Flatten those FromSubqueries which can be flattened. o  Ensuring that a ProjectRestrictNode is generated on top of every FromBaseTable and generated in place of every FromSubquery which could not be flattened. o  Pushing single table predicates down to the new ProjectRestrictNodes. Categorize and push the predicates that are pushable.  Return true if the node references SESSION schema tables (temporary or permanent) Search to see if a query references the specifed table name. Check for (and reject) all ? parameters directly under the ResultColumns. This is done for SELECT statements. This method is used for both subquery flattening and distinct elimination based on a uniqueness condition.  For subquery flattening we want to make sure that the query block will return at most 1 row.  For distinct elimination we want to make sure that the query block will not return any duplicates. This is true if every table in the from list is (a base table and the set of columns from the table that are in equality comparisons with expressions that do not include columns from the same table is a superset of any unique index on the table) or an EXISTS FBT.  In addition, at least 1 of the tables in the list has a set of columns in equality comparisons with expressions that do not include column references from the same query block is a superset of a unique index on that table.  (This ensures that the query block will onlyr return a single row.) This method is expected to be called after normalization and after the from list has been preprocessed. It can be called both before and after the predicates have been pulled from the where clause. The algorithm for this is as follows If any table in the query block is not a base table, give up. For each table in the query Ignore exists table since they can only produce one row create a matrix of tables and columns from the table (tableColMap) (this is used to keep track of the join columns and constants that can be used to figure out whether the rows from a join or in a select list are distinct based on unique indexes) create an array of columns from the table(eqOuterCol) (this is used to determine that only one row will be returned from a join) if the current table is the table for the result columns set the result columns in the eqOuterCol and tableColMap (if these columns are a superset of a unique index and all joining tables result in only one row, the results will be distinct) go through all the predicates and update tableColMap  and eqOuterCol with join columns and correlation variables, parameters and constants since setting constants, correlation variables and parameters, reduces the number of columns required for uniqueness in a multi-column index, they are set for all the tables (if the table is not the result table, in this case only the column of the result table is set) join columns are just updated for the column in the row of the joining table. check if the marked columns in tableColMap are a superset of a unique index (This means that the join will only produce 1 row when joined with 1 row of another table) check that there is a least one table for which the columns in eqOuterCol(i.e. constant values) are a superset of a unique index (This quarantees that there will be only one row selected from this table). Once all tables have been evaluated, check that all the tables can be joined by unique index or will have only one row Set the (query block) level (0-based) for the FromTables in this FromList.  Set the Properties list for this FromList. Set windows field to the supplied value. determine whether this table is NOT EXISTS. This routine searches for the indicated table number in the fromlist and returns TRUE if the table is present in the from list and is marked NOT EXISTS, false otherwise. A table may be present in the from list for NOT EXISTS if it is used as a correlated NOT EXISTS subquery. In such a situation, when the subquery is flattened, it is important that we remember that this is a NOT EXISTS subquery, because the join semantics are different (we're looking for rows that do NOT match, rather than rows that do). And since the join semantics are different, we cannot include this table into a transitive closure of equijoins (See DERBY-3033 for a situation where this occurs). Get the lock mode for the target of an update statement (a delete or update).  The update mode will always be row for CurrentOfNodes.  It will be table if there is no where clause.
Bind the expressions in this FromSubquery.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind this subquery that appears in the FROM list. Bind any untyped null nodes to the types in the given ResultColumnList. Bind this subquery that appears in the FROM list. Decrement (query block) level (0-based) for this FromTable. This is useful when flattening a subquery. Extract out and return the subquery, with a PRN on top. (See FromSubquery.preprocess() for more details.) Flatten this FSqry into the outer query block. The steps in flattening are: o  Mark all ResultColumns as redundant, so that they are "skipped over" at generate(). o  Append the wherePredicates to the outer list. o  Return the fromList so that the caller will merge the 2 lists RESOLVE - FSqrys with subqueries are currently not flattenable.  Some of them can be flattened, however.  We need to merge the subquery list when we relax this restriction. NOTE: This method returns NULL when flattening RowResultSetNodes (the node for a VALUES clause).  The reason is that no reference is left to the RowResultSetNode after flattening is done - the expressions point directly to the ValueNodes in the RowResultSetNode's ResultColumnList. Expand a "*" into a ResultColumnList with all of the result columns from the subquery. Get the exposed name for this table, which is the name that can be used to refer to it in the rest of the query. Determine whether or not the specified name is an exposed name in the current query block. Try to find a ResultColumn in the table represented by this FromBaseTable that matches the name in the given ColumnReference. Return the "subquery" from this node. Preprocess a ResultSetNode - this currently means: o  Generating a referenced table map for each ResultSetNode. o  Putting the WHERE and HAVING clauses in conjunctive normal form (CNF). o  Converting the WHERE and HAVING clauses into PredicateLists and classifying them. o  Ensuring that a ProjectRestrictNode is generated on top of every FromBaseTable and generated in place of every FromSubquery. o  Pushing single table predicates down to the new ProjectRestrictNodes. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Return true if the node references SESSION schema tables (temporary or permanent) Search to see if a query references the specifed table name. Check for (and reject) ? parameters directly under the ResultColumns. This is done for SELECT statements.  For FromSubquery, we simply pass the check through to the subquery. Associate this subquery with the original compilation schema of a view.
no LOJ reordering for this FromTable. Assign the cost estimate in this node to the given cost estimate. Most Optimizables cannot be ordered Return true if some columns in this table are updatable. This method is used in deciding whether updateRow() or insertRow() are allowable.   Is this a table that has a FOR UPDATE clause.  Overridden by FromBaseTable. Decrement (query block) level (0-based) for this FromTable. This is useful when flattening a subquery.   Fill the referencedTableMap with this ResultSetNode. Flatten this FromTable into the outer query block. The steps in flattening are: o  Mark all ResultColumns as redundant, so that they are "skipped over" at generate(). o  Append the wherePredicates to the outer list. o  Return the fromList so that the caller will merge the 2 lists     Get this table's correlation name, if any.  Get the exposed name for this table, which is the name that can be used to refer to it in the rest of the query. Get the final CostEstimate for this FromTable. Determine whether or not the specified name is an exposed name in the current query block. Get the (query block) level (0-based) for this FromTable. get the merge table id   Gets the original or unbound table name for this FromTable. The tableName field can be changed due to synonym resolution. Use this method to retrieve the actual unbound tablename. end of getPerRowUsage  Return a ResultColumnList with all of the columns in this table. (Used in expanding '*'s.) NOTE: Since this method is for expanding a "*" in the SELECT list, ResultColumn.expression will be a ColumnReference. Get a schema descriptor for the given table. Uses this.corrTableName. Get a schema descriptor for the given table. * This gets a cost estimate for doing scratch calculations.  Typically, * it will hold the estimated cost of a conglomerate.  If the optimizer * decides the scratch cost is lower than the best cost estimate so far, * it will copy the scratch cost to the non-scratch cost estimate, * which is allocated above.  Return a TableName node representing this FromTable. Expect this to be overridden (and used) by subclasses that may set correlationName to null.   Return the user specified join strategy, if any for this table. Check if any columns containing large objects (BLOBs or CLOBs) are referenced in this table.       Is this FromTable a JoinNode which can be flattened into the parents FromList. No-op in FromTable.      Mark as updatable all the columns in the result column list of this FromBaseTable that match the columns in the given update column list. If the list is null, it means all the columns are updatable.     Optimizable interface  Optimize any subqueries that haven't been optimized any where else.  This is useful for a RowResultSetNode as a derived table because it doesn't get optimized otherwise.  Push expressions down to the first ResultSetNode which can do expression evaluation and has the same referenced table map. RESOLVE - This means only pushing down single table expressions to ProjectRestrictNodes today.  Once we have a better understanding of how the optimizer will work, we can push down join clauses.     This method is called when this table is placed in a potential join order, or when a new conglomerate is being considered. Set this join strategy number to 0 to indicate that no join strategy has been considered for this table yet. Set the cost estimate in this node to the given cost estimate.  Set the (query block) level (0-based) for this FromTable. set the merge table id Sets the original or unbound table name for this FromTable.  Set the table # for this table.   Tell the given RowOrdering about any columns that are constant due to their being equality comparisons with constant expressions. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Transform any Outer Join into an Inner Join where applicable. (Based on the existence of a null intolerant predicate on the inner table.)
Accept the visitor for all visitable children of this node. Add a FromList to the collection of FromLists which bindExpressions() checks when vetting VTI arguments which reference columns in other tables. See DERBY-5554 and DERBY-5779.  Bind the expressions in this VTI.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the non VTI tables in this ResultSetNode.  This includes getting their descriptors from the data dictionary and numbering them. Bind this VTI that appears in the FROM list. Return true if the predicate can be pushed into a RestrictedVTI If the referenced column appears in the indicated FROM list, then return the table it appears in. Fills in the array of projected column names suitable for handing to RestrictedVTI.initScan(). Returns a map of the exposed column names to the actual names of columns in the table function. This is useful because the predicate refers to the exposed column names. Compute the projection and restriction to be pushed to the external table function if it is a RestrictedVTI. This method is called by the parent ProjectRestrictNode at code generation time. See DERBY-4357. Fills in the restriction to be handed to a RestrictedVTI at run-time. Add result columns for a Derby-style Table Function Optimizable interface  Flip the sense of a comparison Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Generation on a FromVTI creates a wrapper around the user's java.sql.ResultSet Expand a "*" into a ResultColumnList with all of the result columns from the subquery. Get the DeferModification interface associated with this VTI end of getDeferralControl Get the exposed name for this table, which is the name that can be used to refer to it in the rest of the query.  Try to find a ResultColumn in the table represented by this FromBaseTable that matches the name in the given ColumnReference. Return the constructor or static method invoked from this node end of getNewInstance Get all of the nodes of the specified class from the parameters to this VTI. Get the ResultSetMetaData for the class/object.  We first look for the optional static method which has the same signature as the constructor. If it doesn't exist, then we instantiate an object and get the ResultSetMetaData from that object.  Check and see if we have a special trigger VTI. If it cannot be bound (because we aren't actually compiling or executing a trigger), then throw an exception. Get the VTICosting implementation for this optimizable VTI.  end of getVTIName This is a handy place to put instrumentation for tracing trees which we don't understand Return true if this Derby Style Table Function implements the VTICosting interface. The class must satisfy the following conditions: <ul> <li>Implements VTICosting</li> <li>Has a public, no-arg constructor</li> </ul> * VTIEnvironment Return true if this VTI is a constructor. Otherwise, it is a static method. Return true if this is a user-defined table function   Lookup the class that holds the VTI. Makes an IS NULL comparison of a column in the VTI. Makes a Restriction out of a comparison between a constant and a column in the VTI. Turn a compile-time WHERE clause fragment into a run-time Restriction. Returns null if the clause could not be understood. Map internal operator constants to user-visible ones  Return whether or not to materialize this ResultSet tree. Preprocess a ResultSetNode - this currently means: o  Generating a referenced table map for each ResultSetNode. o  Putting the WHERE and HAVING clauses in conjunctive normal form (CNF). o  Converting the WHERE and HAVING clauses into PredicateLists and classifying them. o  Ensuring that a ProjectRestrictNode is generated on top of every FromBaseTable and generated in place of every FromSubquery. o  Pushing single table predicates down to the new ProjectRestrictNodes. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Search to see if a query references the specifed table name. <p> Remap the column references in vti arguments. Point those column references at the result columns for the base table. This prevents us from code-generating the args from references to unfilled columns in higher join nodes. See DERBY-5554. </p> Mark this VTI as the target of a delete or update. Get the constant or parameter reference out of a comparand. Return null if we are confused. A parameter reference is wrapped in an integer array to distinguish it from a constant integer. Store an object in the prepared statement.  Returns -1 if the object is null. Otherwise returns the object's retrieval handle.  Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Comparable implementation methods to be registered as functions Externalizable implementation
<p> Compute a Student's age given their birthday. </p> <p> Format a double as a percentage, suitable for printing. </p> <p> Get the default connection, called from inside the database engine. </p> <p> Calculate the median score achieved on a Test. </p> <p> Compute the score for a question. A penalty is assessed for guessing the wrong answer. If actualChoice is -1, then the Student didn't guess and we don't assess a penalty. </p> //////////////////////////////////////////////////////  STATE  ////////////////////////////////////////////////////// //////////////////////////////////////////////////////  STATIC BEHAVIOR  ////////////////////////////////////////////////////// <p> Check that a legal answer was given to a question. Throws an exception if it is not. </p> <p> Compute the relative weight of a Question given its difficulty. </p>


Generate a mixture-of-Gaussian configuration.

Get the GeneratedClass object for this object. Initialize the generated class from a context. Called by the class manager just after creating the instance of the new class. Called by the class manager just after calling setGC(). Set the Generated Class. Call by the class manager just after calling initFromContext.
Return the class reload version that this class was built at. Obtain a handle to the method with the given name that takes no arguments. Return the name of the generated class. Return a new object that is an instance of the represented class. The object will have been initialised by the no-arg constructor of the represneted class. (Similar to java.lang.Class.newInstance).
Assert that a table has the correct column types. <p> Assert whether a routine is expected to be DETERMINISTIC. </p> Assert that the statement text, when executed, raises a warning. Assert that the statement text, when executed, raises a warning. Assert that the in-place insert raises the expected error. Assert that the statement text, when executed, raises no warnings. /////////////////////////////////////////////////////////////////////////////////  HELPER METHODS  ///////////////////////////////////////////////////////////////////////////////// Assert that the in-place update raises the expected error. <p> Fill an ArrayList from an array. </p> Test that a restricted drop is blocked by an object. Test that a privilege can't be revoked if an object depends on it.
Invoke a generated method that has no arguments. (Similar to java.lang.refect.Method.invoke) Returns the value returned by the method.
Accept the visitor for all visitable children of this node. /////////////////////////////////////////////////////////////////////////////////  QueryTreeNode BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Binding the generation clause. Return a list of columns referenced in the generation expression. Generate code for this node. Return the auxiliary provider list. /////////////////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////////////////// Get the defining text of this generation clause Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Set the auxiliary provider list. Stringify.
Add an item with a bold name to the end of a list. Add an item to the end of a list. Add a paragraph to the end of a parent element. Make some bold text. ///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// Note that this release is a delta from the previous one. Clone all of the children of a source node and add them as children of a target node. Create a header at the end of the parent node. Return the block created to hold the text following this header. Create a hotlink. Create a hotlink. Insert a list at the end of the parent element. Create a standard link to a local label. Create a standard link to a local label. //////////////////////////////////////////////////////  HTML MINIONS  ////////////////////////////////////////////////////// Create a section at the end of a parent element and link to it from a table of contents. Create a section at the end of a parent element and link to it from a table of contents. Insert a table at the end of the parent element. Create an html text element. Sets the width of the first column in the given table. //////////////////////////////////////////////////////  EXCEPTION PROCESSING MINIONS  ////////////////////////////////////////////////////// Format an error for later reporting. //////////////////////////////////////////////////////  XML MINIONS  ////////////////////////////////////////////////////// Search the tree rooted at <code>node</code> for nodes tagged with <code>childName</code>. Return the first such node, or throws an exception if none is found. Search the tree rooted at <code>node</code> for nodes tagged with <code>childName</code>. Return the index'th such node. Search the tree rooted at <code>node</code> for nodes tagged with <code>childName</code>. Return the first such node, or null if tag is not found. Retrieve the indented block inside a section Insert a column at the end of a row Insert a header column at the end of the row. Insert a horizontal line at the end of the parent element. Insert a row at the end of a table Inserts the specified element to the parent element. Make the tag for a header, given its level Print accumulated errors. Print the generated output document to the output file. //////////////////////////////////////////////////////  MISC MINIONS  ////////////////////////////////////////////////////// Format a line of text. Replace all instances of the tag with the indicated text. Replace the known parameters with their corresponding text values. Sets/overwrites the specified attribute. Ant mutator to set the name of the JIRA-generated list of bugs addressed by this release Ant mutator to set the name of the generated output file Ant mutator to set the id of the release ///////////////////////////////////////////////////////////////////////  ANT Task BEHAVIOR  /////////////////////////////////////////////////////////////////////// Ant accessor to set the name of the summary file prepared by the Release Manager Squeeze the text out of an Element. Print a stack trace as a string. Wraps the text content of the given node inside a div tag.
Temporary tables can be declared with ON COMMIT DELETE ROWS. But if the table has a held curosr open at commit time, data should not be deleted from the table. This method, (gets called at commit time) checks if this activation held cursor and if so, does that cursor reference the passed temp table name.               Dependable interface implementation         Get the number of subqueries in the entire query.             Is the activation in use?  Is the activation set up for a single execution. Dependent interface implementation   Class implementation Mark the activation as unused.  Activation interface             Set the activation for a single execution.
Finish the aggregation for the current row. Basically call finish() on each aggregator on this row.  Called once per grouping on a vector aggregate or once per table on a scalar aggregate. If the input row is null, then rowAllocator is invoked to create a new row.  That row is then initialized and used for the output of the aggregation. Get a template row of the right shape for sorting or returning results. The template is cached, so it may need to be cloned if callers use it for multiple purposes at the same time. For each AggregatorInfo in the list, generate a GenericAggregator and stick it in an array of GenericAggregators.
Accumulate the aggregate results.  This is the guts of the aggregation.  We will call the user aggregate on itself to do the aggregation. Accumulate the aggregate results.  This is the guts of the aggregation.  We will call the user aggregate on itself to do the aggregation. Accumulate the aggregate results.  This is the guts of the aggregation.  We will call the user aggregate on itself to do the aggregation. Get the results of the aggregation and put it in the result column. ////////////////////////////////////////////////////  MISC  //////////////////////////////////////////////////// Get a new instance of the aggregator and initialize it. ///////////////////////////////////////////////////////////  /////////////////////////////////////////////////////////// Return the column id that is being aggregated Initialize the aggregator Merge the aggregate results.  This is the guts of the aggregation.  We will call the user aggregate on itself to do the aggregation. Merge two partial aggregations.  This is how the sorter merges partial aggregates. Merge the aggregate results.  This is the guts of the aggregation.  We will call the user aggregate on itself to do the aggregation.
Used for operations that do not involve tables or routines.  Return true if the connection must remain readOnly Throw an exception if the user does not have all of the required permissions.

Get the position of the Column. NOTE - position is 1-based. Returns the name of the Column. Get the name of the schema for the Column's base table, if any. Following example queries will all return APP (assuming user is in schema APP) select t.a from t select b.a from t as b select app.t.a from t Get the name of the underlying(base) table this column comes from, if any. Following example queries will all return T select a from t select b.a from t as b select t.a from t When retrieving a DataTypeDescriptor, it might just be a regular DataTypeDescriptor or may be an OldRoutineType, as used for Routine parameters and return values prior to DERBY-2775. If it is not a regular DataTypeDescriptor, it must be an OldRoutineType, so convert it to a DataTypeDescriptor DERBY-4913 Returns a DataTypeDescriptor for the column. This DataTypeDescriptor will not represent an actual value, it will only represent the type that all values in the column will have. Get the formatID which corresponds to this class. Read this object from a stream of stored objects. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out
Compare this Orderable with a given Orderable for the purpose of index positioning.  This method treats nulls as ordered values - that is, it treats SQL null as equal to null and less than all other values. ************************************************************************ Public Methods implementing DataValueDescriptor interface. ************************************************************************* Gets the length of the data value.  The meaning of this is implementation-dependent.  For string types, it is the number of characters in the string.  For numeric types, it is the number of bytes used to store the number.  This is the actual length of this value, not the length of the type it was defined as. For example, a VARCHAR value may be shorter than the declared VARCHAR (maximum) length. Get a new null value of the same type as this data value. Gets the value in the data value descriptor as a Java Object. The type of the Object will be the Java object type corresponding to the data value's SQL type. JDBC defines a mapping between Java object types and SQL types - we will allow that to be extended through user type definitions. Throws an exception if the data value is not an object (yeah, right). Gets the value in the data value descriptor as a String. Throws an exception if the data value is not a string. Get the SQL name of the datatype Tells if there are columns with collations (other than UCS BASIC) in the given list of collation ids. Set the value of this DataValueDescriptor from another. Set the value based on the value for the specified DataValueDescriptor from the specified ResultSet.
************************************************************************ Fields of the class ************************************************************************* ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Public Methods implementing ConglomerateController which just delegate to OpenConglomerate: ************************************************************************* ************************************************************************ Public Methods implementing ConglomerateController: *************************************************************************  Close conglomerate controller as part of terminating a transaction. <p> Use this call to close the conglomerate controller resources as part of committing or aborting a transaction.  The normal close() routine may do some cleanup that is either unnecessary, or not correct due to the unknown condition of the controller following a transaction ending error. Use this call when closing all controllers as part of an abort of a transaction. <p> This call is meant to only be used internally by the Storage system, clients of the storage system should use the simple close() interface. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this.
Make ConstantAction to drop a constraint. Make the AlterAction for an ALTER TABLE statement. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Make the ConstantAction for a CREATE ALIAS statement. Make a ConstantAction for a constraint. Make the ConstantAction for a CREATE INDEX statement. Make the ConstantAction for a CREATE ROLE statement. Make the ConstantAction for a CREATE SCHEMA statement. Make the ConstantAction for a CREATE SEQUENCE statement. Make the ConstantAction for a CREATE TABLE statement. Make the ConstantAction for a CREATE TRIGGER statement. Make the ConstantAction for a CREATE VIEW statement. Make the ConstantAction for a Replicated DELETE statement. Make the ConstantAction for a DROP ALIAS statement. Make ConstantAction to drop a constraint. Make the ConstantAction for a DROP INDEX statement. Make the ConstantAction for a DROP ROLE statement. Make the ConstantAction for a DROP SCHEMA statement. Make the ConstantAction for a DROP SEQUENCE statement. Make the constant action for Drop Statistics statement. Make the ConstantAction for a DROP TABLE statement. Make the ConstantAction for a DROP TRIGGER statement. Make the ConstantAction for a DROP VIEW statement. Make the constant action for a Grant statement Make the ConstantAction for a GRANT role statement. Make the ConstantAction for a INSERT statement. Make the ConstantAction for a LOCK TABLE statement. Make the ConstantAction for a WHEN [ NOT ] MATCHED clause. Make the ConstantAction for a MERGE statement. Make the ConstantAction for a RENAME TABLE/COLUMN/INDEX statement. Make the constant action for a Revoke statement Make the ConstantAction for a REVOKE role statement. Make the ConstantAction for a savepoint statement (ROLLBACK savepoint, RELASE savepoint and SAVEPOINT). /////////////////////////////////////////////////////////////////////  CONSTANT ACTION MANUFACTORIES  ///////////////////////////////////////////////////////////////////// Get ConstantAction for SET CONSTRAINTS statement. Make the ConstantAction for a SET ROLE statement. Make the ConstantAction for a SET SCHEMA statement. Make the ConstantAction for a SET TRANSACTION ISOLATION statement. Make the ConstantAction for an updatable VTI statement. Make the ConstantAction for an updatable VTI statement. Make the ConstantAction for an UPDATE statement.
************************************************************************ Public Methods implementing ConglomerateController which just delegate to OpenConglomerate: ************************************************************************* Get the total estimated number of rows in the container. <p> The number is a rough estimate and may be grossly off.  In general the server will cache the row count and then occasionally write the count unlogged to a backing store.  If the system happens to shutdown before the store gets a chance to update the row count it may wander from reality. <p> This call is currently only supported on Heap conglomerates, it will throw an exception if called on btree conglomerates. ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* is the open btree table locked? Set the total estimated number of rows in the container. <p> Often, after a scan, the client of RawStore has a much better estimate of the number of rows in the container than what store has.  For instance if we implement some sort of update statistics command, or just after a create index a complete scan will have been done of the table.  In this case this interface allows the client to set the estimated row count for the container, and store will use that number for all future references. <p> This call is currently only supported on Heap conglomerates, it will throw an exception if called on btree conglomerates.
************************************************************************ Fields of the class ************************************************************************* ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* ************************************************************************ Public Methods implementing StoreCostController class: ************************************************************************* ************************************************************************ Public Methods implementing StoreCostController class, default impl just throws exception: ************************************************************************* Return the cost of exact key lookup. <p> Return the estimated cost of calling ScanController.fetch() on the current conglomerate, with start and stop positions set such that an exact match is expected. <p> This call returns the cost of a fetchNext() performed on a scan which has been positioned with a start position which specifies exact match on all keys in the row. <p> Example: <p> In the case of a btree this call can be used to determine the cost of doing an exact probe into btree, giving all key columns.  This cost can be used if the client knows it will be doing an exact key probe but does not have the key's at optimize time to use to make a call to getScanCost() <p>
Return whether or not the underlying system table has been scanned. Get the UniqueTupleDescriptor that matches the input uuid. Mark whether or not the underlying system table has been scanned.  (If a table does not have any constraints then the size of its CDL will always be 0.  We used these get/set methods to determine when we need to scan the table.
Context interface    ExecutionContext interface
This Factory is expected to be booted relative to a LanguageConnectionFactory. Privileged startup. Must be private so that user code can't call this entry point.  ModuleControl interface  Get the factory for constant actions.  Create an execution time ResultColumnDescriptor from a compile time RCD. Make a result description  ExecutionFactory interface  Factories are generic and can be used by all connections. We defer instantiation until needed to reduce boot time. We may instantiate too many instances in rare multi-user situation, but consistency will be maintained and at some point, usually always, we will have 1 and only 1 instance of each factory because assignment is atomic. Get the ResultSetStatisticsFactory from this ExecutionFactory.   @see ExecutionFactory#getScanQualifier Get a trigger execution context Old RowFactory interface Get the XPLAINFactory from this ExecutionContext. We want a dependency context so that we can push it onto the stack.  We could instead require the implementation push it onto the stack for us, but this way we know which context object exactly was pushed onto the stack.
Add the activation to those known about by this connection.   Flush the cache of autoincrement values being kept by the lcc. This will result in the autoincrement values being written to the SYSCOLUMNS table as well as the mapping used by lastAutoincrementValue Start a Nested User Transaction (NUT) with the store. If a NUT is already active simply increment a counter, queryNestingDepth, to keep track of how many times we have tried to start a NUT. check if there are any activations that reference this temporary table   Context interface  If worse than a transaction error, everything goes; we rely on other contexts to kill the context manager for this session. <p> If a transaction error, act like we saw a rollback. <p> If more severe or a java error, the outer cleanup will shutdown the connection, so we don't have to clean up. <p> REMIND: connection should throw out all contexts and start over when the connection is closed... perhaps by throwing out the context manager? <p> REVISIT: If statement error, should we do anything? <P> Since the access manager's own context takes care of its own resources on errors, there is nothing that this context has to do with the transaction controller. If dropAndRedeclare is true, that means we have come here for temp tables with on commit delete rows and no held curosr open on them. We will drop the existing conglomerate and redeclare a new conglomerate similar to old conglomerate. This is a more efficient way of deleting all rows from the table. If dropAndRedeclare is false, that means we have come here for the rollback cleanup work. We are trying to restore old definition of the temp table (because the drop on it is being rolled back). Clear deferred information for this transaction. Copies an existing autoincrement mapping into autoincrementHT, the cache of autoincrement values kept in the languageconnectioncontext.    Decrements the statement depth This is where the work on internalCommit(), userCOmmit() and internalCommitNoSync() actually takes place. <p> When a commit happens, the language connection context will close all open activations/cursors and commit the Store transaction. <p> REVISIT: we talked about having a LanguageTransactionContext, but since store transaction management is currently in flux and our context might want to delegate to that context, for now all commit/rollback actions are handled directly by the language connection context. REVISIT: this may need additional alterations when RELEASE SAVEPOINT/ROLLBACK TO SAVEPOINT show up. <P> Since the access manager's own context takes care of its own resources on commit, and the transaction stays open, there is nothing that this context has to do with the transaction controller. <p> Also, tell the data dictionary that the transaction is finished, if necessary (that is, if the data dictionary was put into DDL mode in this transaction. When a rollback happens, the language connection context will close all open activations and invalidate their prepared statements. Then the language will abort the Store transaction. <p> The invalidated statements can revalidate themselves without a full recompile if they verify their dependencies' providers still exist unchanged. REVISIT when invalidation types are created. <p> REVISIT: this may need additional alterations when RELEASE SAVEPOINT/ROLLBACK TO SAVEPOINT show up. <p> Also, tell the data dictionary that the transaction is finished, if necessary (that is, if the data dictionary was put into DDL mode in this transaction. Drop all the declared global temporary tables associated with this connection. This gets called when a getConnection() is done on a PooledConnection. This will ensure all the temporary tables declared on earlier connection handle associated with this physical database connection are dropped before a new connection handle is issued on that same physical database connection.   class implementation  If we are called as part of rollback code path, then we will reset all the activations that have resultset returning rows associated with them. DERBY-3304 Resultsets that do not return rows should be left alone when the rollback is through the JDBC Connection object. If the rollback is caused by an exception, then at that time, all kinds of resultsets should be closed. If we are called as part of commit code path, then we will do one of the following if the activation has resultset assoicated with it. Also, we will clear the conglomerate used while scanning for update/delete 1)Close result sets that return rows and are not held across commit. 2)Clear the current row of the resultsets that return rows and are held across commit. 3)Leave the result sets untouched if they do not return rows Additionally, clean up (close()) activations that have been marked as unused during statement finalization. Find the declared global temporary table in the list of temporary tables known by this connection. Finish the data dictionary transaction, if any. Return the number of activations known for this connection. Note that some of these activations may not be in use (when a prepared statement is finalized, its activations are marked as unused and later closed and removed on the next commit/rollback).        Return the current SQL session context based on statement context Return the current SQL session context of the activation    Get the data value factory to use with this language connection context.      Get the identity column value most recently generated. Get the computed value for the initial default schema.  Get the language connection factory to use with this language connection context. Get the language factory to use with this language connection context.  get the lock escalation threshold.   LanguageConnectionContext interface   Build a String for a statement name.      Get the session user  Reports how many statement levels deep we are.    Get the transaction controller to use at compile time with this language connection context. If a NUT is active then return NUT else return parent transaction. Get the topmost tec. Get the topmost trigger table descriptor Get a connection unique system generated name for a cursor. Get a connection unique system generated id for an unnamed savepoint. Get a connection unique system generated name for an unnamed savepoint.    //////////////////////////////////////////////////////////////////  MINIONS  ////////////////////////////////////////////////////////////////// Increments the statement depth. Compute the initial default schema and set cachedInitialDefaultSchemaDescr accordingly. Do a commit as appropriate for an internally generated commit (e.g. as needed by sync, or autocommit). Commit the language transaction by doing a commitNoSync() on the store's TransactionController. <p> Do *NOT* tell the data dictionary that the transaction is finished. The reason is that this would allow other transactions to see comitted DDL that could be undone in the event of a system crash. Do a rollback as appropriate for an internally generated rollback (e.g. as needed by sync, or autocommit). When a rollback happens, we close all open activations and invalidate their prepared statements.  We then tell the cache to age out everything else, which effectively invalidates them.  Thus, all prepared statements will be compiled anew on their 1st execution after a rollback. <p> The invalidated statements can revalidate themselves without a full recompile if they verify their dependencies' providers still exist unchanged. REVISIT when invalidation types are created. <p> REVISIT: this may need additional alterations when RELEASE SAVEPOINT/ROLLBACK TO SAVEPOINT show up. <p> Also, tell the data dictionary that the transaction is finished, if necessary (that is, if the data dictionary was put into DDL mode in this transaction. Let the context deal with a rollback to savepoint Invalidate a dropped temp table     Reports whether there is any outstanding work in the transaction. Sets a savepoint. Causes the Store to set a savepoint. lastAutoincrementValue searches for the last autoincrement value inserted into a column specified by the user. The search for the "last" value supports nesting levels caused by triggers (Only triggers cause nesting, not server side JDBC). If lastAutoincrementValue is called from within a trigger, the search space for ai-values are those values that are inserted by this trigger as well as previous triggers; i.e if a SQL statement fires trigger T1, which in turn does something that fires trigger t2, and if lastAutoincrementValue is called from within t2, then autoincrement values genereated by t1 are visible to it. By the same logic, if it is called from within t1, then it does not see values inserted by t2. See if a given cursor is available for use. if so return its activation. Returns null if not found. For use in execution. See if a given statement has already been compiled for this user, and if so use its prepared statement. Returns null if not found.  returns the <b>next</b> value to be inserted into an autoincrement col. This is used internally by the system to generate autoincrement values which are going to be inserted into a autoincrement column. This is used when as autoincrement column is added to a table by an alter table statemenet and during bulk insert. Make a note that some activations are marked unused  Pop a CompilerContext off the context stack. Remove the validator.  Does an object identity (validator == validator) comparison.  Asserts that the validator is found.  Pop a StatementContext of the context stack. Remove the tec.  Does an object identity (tec == tec) comparison.  Asserts that the tec is found. Remove the trigger table descriptor.   Push a CompilerContext on the context stack with the current default schema as the default schema which we compile against. Push a CompilerContext on the context stack with the passed in schema sd as the default schema we compile against. Push a new execution statement validator.  An execution statement validator is an object that validates the current statement to ensure that it is permitted given the current execution context. An example of a validator a trigger ExecutionStmtValidator that doesn't allow ddl on the trigger target table. <p> Multiple ExecutionStmtValidators may be active at any given time. This mirrors the way there can be multiple connection nestings at a single time.  The validation is performed by calling each validator's validateStatement() method.  This yields the union of all validations. {@inheritDoc } Push a StatementContext on the context stack. Inherit SQL session state a priori (statementContext will get its own SQL session state if this statement executes a call, cf. pushNestedSessionContext. Push a new trigger execution context. <p> Multiple TriggerExecutionContexts may be active at any given time. Set the trigger table descriptor.  Used to compile statements that may special trigger pseudo tables. Let the context deal with a release of a savepoint Remove the activation to those known about by this connection. This method will remove a statement from the  statement cache. It should only be called if there is an exception preparing the statement. The caller must have set the flag {@code preparedStmt.compilingStatement} in the {@code GenericStatement} before calling this method in order to prevent race conditions when calling {@link CacheManager#remove(Cacheable)}. This is called at the commit time for temporary tables with ON COMMIT DELETE ROWS If a temp table with ON COMMIT DELETE ROWS doesn't have any held cursor open on them, we delete the data from them by dropping the conglomerate and recreating the conglomerate. In order to store the new conglomerate information for the temp table, we need to replace the existing table descriptor with the new table descriptor which has the new conglomerate information Reset the connection before it is returned (indirectly) by a PooledConnection object. See EmbeddedConnection.  Reset all statement savepoints. Traverses the StatementContext stack from bottom to top, calling resetSavePoint() on each element.  Resets the statementDepth.   Set the constraint mode to deferred for the specified constraint.      Set the field of most recently generated identity column value.  debug methods           Do the necessary work at commit time for temporary tables <p> 1)If a temporary table was marked as dropped in this transaction, then remove it from the list of temp tables for this connection 2)If a temporary table was not dropped in this transaction, then mark it's declared savepoint level and modified savepoint level as -1 3)After savepoint fix up, then handle all ON COMMIT DELETE ROWS with no open held cursor temp tables. <p> do the necessary work at rollback time for temporary tables 1)If a temp table was declared in the UOW, then drop it and remove it from list of temporary tables. 2)If a temp table was declared and dropped in the UOW, then remove it from list of temporary tables. 3)If an existing temp table was dropped in the UOW, then recreate it with no data. 4)If an existing temp table was modified in the UOW, then get rid of all the rows from the table. After a release of a savepoint, we need to go through our temp tables list. If there are tables with their declare or drop or modified in savepoint levels set to savepoint levels higher than the current savepoint level, then we should change them to the current savepoint level Do a commmit as is appropriate for a user requested commit (e.g. a java.sql.Connection.commit() or a language 'COMMIT' statement.  Does some extra checking to make sure that users aren't doing anything bad. Do a rollback as is appropriate for a user requested rollback (e.g. a java.sql.Connection.rollback() or a language 'ROLLBACk' statement.  Does some extra checking to make sure that users aren't doing anything bad.  Validate a deferred constraint. Validate all deferred constraints. Validate a statement.  Does so by stepping through all the validators and executing them.  If a validator throws and exception, then the checking is stopped and the exception is passed up. Verify that there are no activations with open held result sets. This gets used in case of hold cursors. If there are any hold cursors open then user can't change the isolation level without closing them. At the execution time, set transaction isolation level calls this method before changing the isolation level. Verify that there are no activations with open result sets on the specified prepared statement. Same as userCommit except commit a distributed transaction. This commit always commit store and sync the commit. Same as userRollback() except rolls back a distrubuted transaction.
Start-up method for this instance of the language connection factory. Note these are expected to be booted relative to a Database. Privileged startup. Must be private so that user code can't call this entry point. ModuleControl interface this implementation will not support caching of statements. Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup. Must be private so that user code can't call this entry point. Get the ClassFactory to use with this language connection Get the DataValueFactory to use with this language connection Get the ExecutionFactory to use with this language connection Get the JavaFactory to use with this language connection REMIND: this is only used by the compiler; should there be a compiler module control class to boot compiler-only stuff? Privileged Monitor lookup. Must be package private so that user code can't call this entry point. Class methods Get the instance # for the next LCC. (Useful for logStatementText=true output. Get the OptimizerFactory to use with this language connection Get the PropertyFactory to use with this language connection Privileged module lookup. Must be private so that user code can't call this entry point. LanguageConnectionFactory interface these are the methods that do real work, not just look for factories Get a Statement for the connection returns the statement cache that this connection should use; currently there is a statement cache per database. Get the TypeCompilerFactory to use with this language connection these methods all look for factories that we booted. Get the UUIDFactory to use with this language connection REMIND: this is only used by the compiler; should there be a compiler module control class to boot compiler-only stuff? * Methods of PropertySetCallback  Get a LanguageConnectionContext. this holds things we want to remember about activity in the language system, where this factory holds things that are pretty stable, like other factories. <p> The returned LanguageConnectionContext is intended for use only by the connection that requested it. Privileged startup. Must be private so that user code can't call this entry point. Stop this module.
ModuleControl interface Start-up method for this instance of the language factory. This service is expected to be started and accessed relative to a database. * REMIND: we will need a row and column factory * when we make putResultSets available for users' * server-side JDBC methods. Privileged startup. Must be private so that user code can't call this entry point. Get a new result description Get a new result description from the input result description.  Picks only the columns in the column array from the inputResultDescription. LanguageFactory methods Factory method for getting a ParameterValueSet Stop this module.  In this case, nothing needs to be done.

Clear the parameter, unless it is a return output parameter Clone myself.  It is a shallow copy for everything but the underlying data wrapper and its value -- e.g. for everything but the underlying SQLInt and its int. //////////////////////////////////////////////////  CLASS IMPLEMENTATION  ////////////////////////////////////////////////// get string for param number Return the scale of the parameter. Get the parameter value.  Doesn't check to see if it has been initialized or not. Set the DataValueDescriptor and type information for this parameter ////////////////////////////////////////////////////////////////  CALLABLE STATEMENT  //////////////////////////////////////////////////////////////// Mark the parameter as an output parameter. Validate the parameters.  This is done for situations where we cannot validate everything in the setXXX() calls.  In particular, before we do an execute() on a CallableStatement, we need to go through the parameters and make sure that all parameters are set up properly.  The motivator for this is that setXXX() can be called either before or after registerOutputParamter(), we cannot be sure we have the types correct until we get to execute().
Check that there are not output parameters defined by the parameter set. If there are unknown parameter types they are forced to input types. i.e. Derby static method calls with parameters that are array. Check the position number for a parameter and throw an exception if it is out of range.  Returns the parameter value at the given position. Returns the number of parameters in this set. Return the mode of the parameter according to JDBC 3.0 ParameterMetaData Return the parameter number (in jdbc lingo, i.e. 1 based) for the given parameter.  Linear search. Return the precision of the given parameter index in this pvs. Get the value of the return parameter in order to set it. Return the scale of the given parameter index in this pvs. Is there a return output parameter in this pvs.  A return parameter is from a CALL statement of the following syntax: ? = CALL myMethod() * ParameterValueSet interface methods Initialize the set by allocating a holder DataValueDescriptor object for each parameter. ////////////////////////////////////////////////////////////////  CALLABLE STATEMENT  //////////////////////////////////////////////////////////////// Mark the parameter as an output parameter. Class implementation  Validate the parameters.  This is done for situations where we cannot validate everything in the setXXX() calls.  In particular, before we do an execute() on a CallableStatement, we need to go through the parameters and make sure that all parameters are set up properly.  The motivator for this is that setXXX() can be called either before or after registerOutputParamter(), we cannot be sure we have the types correct until we get to execute().
Signal that the statement is about to be compiled. This will block others from attempting to compile it.  class interface  Makes the prepared statement valid, assigning values for its query tree, generated class, and associated information. Signal that we're done compiling the statement and unblock others that are waiting for the compilation to finish.  The guts of execution.   Finish marks a statement as totally unusable. Get a new activation instance. Get the timestamp for the beginning of compilation Get the bind time for the associated query in milliseconds.  class implementation  Get the byte code saver for this statement. Overridden for StorablePreparedStatement.  We don't want to save anything Get the Dependable's class type. Get a new prepared statement that is a shallow copy of the current one. Get the total compile time for the associated query in milliseconds. Compile time can be divided into parse, bind, optimize and generate times. Get the Execution constants. This routine is called at Execution time. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Return the cursor info in a single chunk.  Used by StrorablePreparedStatement  Dependable interface   Get the timestamp for the end of compilation Get the generate time for the associated query in milliseconds.  Privileged Monitor lookup. Must be private so that user code can't call this entry point. Get the Dependable's UUID String. Return the name of this Dependable.  (Useful for errors.) Get the optimize time for the associated query in milliseconds. Return the type of the parameter (0-based indexing) Get the parse time for the associated query in milliseconds. Get the specified saved object. Get the saved objects.  the target table of the cursor  ExecPreparedStatement  the update mode of the cursor  Returns whether or not this Statement requires should behave atomically -- i.e. whether a user is permitted to do a commit/rollback during the execution of this statement. Check if this statement is currently being compiled. Is this dependent persistent?  A stored dependency will be required if both the dependent and provider are persistent. Unsynchronized helper method for {@link #upToDate()} and {@link #upToDate(GeneratedClass)}. Checks whether this statement is up to date.  Dependent interface  Check that all of the dependent's dependencies are valid. Mark the dependent as invalid (due to at least one of its dependencies being invalid). Does this statement need a savepoint? Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). Return true if the query node for this statement references SESSION schema tables/views. This method gets called at the very beginning of the compile phase of any statement. If the statement which needs to be compiled is already found in cache, then there is no need to compile it again except the case when the statement is referencing SESSION schema objects. There is a small window where such a statement might get cached temporarily (a statement referencing SESSION schema object will be removed from the cache after the bind phase is over because that is when we know for sure that the statement is referencing SESSION schema objects.) Return true if the QueryTreeNode references SESSION schema tables/views. The return value is also saved in the local field because it will be used by referencesSessionSchema() method. This method gets called when the statement is not found in cache and hence it is getting compiled. At the beginning of compilation for any statement, first we check if the statement's plan already exist in the cache. If not, then we add the statement to the cache and continue with the parsing and binding. At the end of the binding, this method gets called to see if the QueryTreeNode references a SESSION schema object. If it does, then we want to remove it from the cache, since any statements referencing SESSION schema objects should never get cached. cache holder stuff. Set the compile time for this prepared statement. Set the Execution constants. This routine is called as we Prepare the statement. Set the name of the statement and schema for an "execute statement" command. Set the stmts 'isAtomic' state. Set the stmts 'needsSavepoint' state.  Used by an SPS to convey whether the underlying stmt needs a savepoint or not. Indicate this prepared statement is an SPS action, currently used by GenericTriggerExecutor. Set the saved objects. Called when compilation completes.  set this prepared statement to be valid, currently used by GenericTriggerExecutor.  PreparedStatement interface  Check whether this statement is up to date and its generated class is identical to the supplied class object.
/////////////////////////////////////////////////////////////////////////////////  PrivilegeInfo BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// This is the guts of the Execution-time logic for GRANT/REVOKE generic privileges. end of executeGrantRevoke
Clear the DataValueDescriptor cache, if one exists. (The DataValueDescriptor can be 1 of 3 types: o  VARIANT		  - cannot be cached as its value can vary within a scan o  SCAN_INVARIANT - can be cached within a scan as its value will not change within a scan o  QUERY_INVARIANT- can be cached across the life of the query as its value will never change o  CONSTANT		  - never changes Qualifier interface  Get the operator to use in the comparison.  Get the getOrderedNulls argument to use in the comparison. Get the getOrderedNulls argument to use in the comparison. Should the result from the compare operation be negated?  If true then only rows which fail the compare operation will qualify. This method reinitializes all the state of the Qualifier.  It is used to distinguish between resetting something that is query invariant and something that is constant over every execution of a query.  Basically, clearOrderableCache() will only clear out its cache if it is a VARIANT or SCAN_INVARIANT value.  However, each time a query is executed, the QUERY_INVARIANT qualifiers need to be reset.

Clean up all scan controllers Check the validity of this row Get the isolation level for the scan for the RI check. NOTE: The level will eventually be instantaneous locking once the implementation changes. Get a scan controller positioned using searchRow as the start/stop position.  The assumption is that searchRow is of the same format as the index being opened. The scan is set up to return no columns. NOTE: We only need an instantaneous lock on the table that we are probing as we are just checking for the existence of a row.  All updaters, whether to the primary or foreign key tables, will hold an X lock on the table that they are updating and will be probing the other table, so instantaneous locks will not change the semantics. RESOLVE:  Due to the current RI implementation we cannot always get instantaneous locks.  We will call a method to find out what kind of locking to do until the implementation changes. Are any of the fields null in the row passed in.  The only fields that are checked are those corresponding to the colArray in fkInfo. * Do reference copy for the qualifier row.  No cloning. * So we cannot get another row until we are done with * this one.
Find a column name based upon the JDBC rules for getXXX and setXXX. Name matching is case-insensitive, matching the first name (1-based) if there are multiple columns that map to the same name.  position is 1-based. Get the saved meta data.  ResultDescription interface   Get the formatID which corresponds to this class. Read this object from a stream of stored objects. Set the meta data if it has not already been set. Get a new result description that has been truncated from input column number.   If the input column is 5, then columns 5 to getColumnCount() are removed. The new ResultDescription points to the same ColumnDescriptors (this method performs a shallow copy. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out
Table/Index scan where rows are read in bulk          a distinct scan generator, for ease of use at present.    a hash scan generator, for ease of use at present.     A last index key sresult set returns the last row from the index in question.  It is used as an ajunct to max().    Multi-probing scan that probes an index for specific values contained in the received probe list. All index rows for which the first column equals probeVals[0] will be returned, followed by all rows for which the first column equals probeVals[1], and so on.  Assumption is that we only get here if probeVals has at least one value.      a referential action dependent table scan generator.       a minimal table scan generator, for ease of use at present. a minimal union scan generator, for ease of use at present.
Close the scan.  This method always succeeds, and never throws any exceptions. Callers must not use the scan controller after closing it; they are strongly advised to clear out the scan controller reference after closing. Return ScanInfo object which describes performance of scan. <p> Return ScanInfo object which contains information about the current state of the scan. <p> The statistics gathered by the scan are not reset to 0 by a reopenScan(), rather they continue to accumulate. <p> Return whether this is a keyed conglomerate. <p> Return whether this scan is table locked. <p> Implementation of this is not complete.  Currently it does not give back the right information on covering locks or lock escalation.  If the openScan() caller specifies a MODE_TABLE as the lock_level then this routine will always return true.  If the openScan() caller specifies a MODE_RECORD as the lock_level then this routine will return true iff the lock level of the system has been overridden either by the derby.storage.rowLocking=false property, or by a shipped configuration which disables row locking. <p> Return a row location object to be used in calls to fetchLocation. <p> Return a row location object of the correct type to be used in calls to fetchLocation. <p> Reposition the current scan.  This call is semantically the same as if the current scan had been closed and a openScan() had been called instead. The scan is reopened with against the same conglomerate, and the scan is reopened with the same "scan column list", "hold" and "forUpdate" parameters passed in the original openScan. <p> The statistics gathered by the scan are not reset to 0 by a reopenScan(), rather they continue to accumulate. <p> Reposition the current scan.  This call is semantically the same as if the current scan had been closed and a openScan() had been called instead. The scan is reopened against the same conglomerate, and the scan is reopened with the same "scan column list", "hold" and "forUpdate" parameters passed in the original openScan. <p> The statistics gathered by the scan are not reset to 0 by a reopenScan(), rather they continue to accumulate. <p> Note that this operation is currently only supported on Heap conglomerates. Also note that order of rows within are heap are not guaranteed, so for instance positioning at a RowLocation in the "middle" of a heap, then inserting more data, then continuing the scan is not guaranteed to see the new rows - they may be put in the "beginning" of the heap.
Clear the DataValueDescriptor cache, if one exists. (The DataValueDescriptor can be 1 of 3 types: o  VARIANT		  - cannot be cached as its value can vary within a scan o  SCAN_INVARIANT - can be cached within a scan as its value will not change within a scan o  QUERY_INVARIANT- can be cached across the life of the query as its value will never change o  CONSTANT		  - immutable Qualifier interface  Get the operator to use in the comparison.  Get the getOrderedNulls argument to use in the comparison. Get the getOrderedNulls argument to use in the comparison. Should the result from the compare operation be negated?  If true then only rows which fail the compare operation will qualify. This method reinitializes all the state of the Qualifier.  It is used to distinguish between resetting something that is query invariant and something that is constant over every execution of a query.  Basically, clearOrderableCache() will only clear out its cache if it is a VARIANT or SCAN_INVARIANT value.  However, each time a query is executed, the QUERY_INVARIANT qualifiers need to be reset. ScanQualifier interface
* Identity Return the {@link PreparedStatement} currently associated with this statement. Statement interface RESOLVE: may need error checking, debugging code here Generates an execution plan given a set of named parameters. Does so for a storable prepared statement. Walk the AST, using a (user-supplied) Visitor
Track a Dependency within this StatementContext. (We need to clear any dependencies added within this context on an error. Cancels the statement which has allocated this StatementContext object. This is done by setting a flag in the StatementContext object. For this to have any effect, it is the responsibility of the executing statement to check this flag regularly.  Context interface  Close down the top ResultSet, if relevant, and rollback to the internal savepoint, if one was set.    Return the text of the current statement. Note that this may be null.  It is currently not set up correctly for ResultSets that aren't single row result sets (e.g SELECT), replication, and setXXXX/getXXXX jdbc methods. Get the subquery tracking array for this query. (Useful for runtime statistics.) Return true if this statement is system code. Returns whether we started from within the context of a trigger or not. Indicates whether the statement needs to be executed atomically or not, i.e., whether a commit/rollback is permitted by a connection nested in this statement. Tests whether the statement which has allocated this StatementContext object has been cancelled. This method is typically called from the thread which is executing the statement, to test whether execution should continue or stop.  Reports whether this StatementContext is on the context stack.  class implementation  Raise an exception if this Context is not in use, that is, on the Context Stack. Resets the savepoint to the current spot if it is set, otherwise, noop.  Used when a commit is done on a nested connection.  StatementContext Interface Indicate that, in the event of a statement-level exception, this context is NOT the last one that needs to be rolled back--rather, it is nested within some other statement context, and that other context needs to be rolled back, too.   Set the appropriate entry in the subquery tracking array for the specified subquery. Useful for closing down open subqueries on an exception. Set to indicate statement is system code. For example a system procedure, view, function etc. Set the top ResultSet in the ResultSet tree for close down on an error. Private minion of setTopResultSet() and clearInUse()
Get and load the activation class.  Will always return a loaded/valid class or null if the class cannot be loaded. Get our byte code array.  Used by others to save off our byte code for us. Privileged lookup of a Context. Must be private so that user code can't call this entry point. ///////////////////////////////////////////////////////////  FORMATABLE INTERFACE  /////////////////////////////////////////////////////////// Get the formatID which corresponds to this class. ///////////////////////////////////////////////////////////  MISC  /////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////  STORABLEPREPAREDSTATEMENT INTERFACE  /////////////////////////////////////////////////////////// Load up the class from the saved bytes.  ///////////////////////////////////////////////////////////  EXTERNALIZABLE INTERFACE  ///////////////////////////////////////////////////////////
Cleanup after executing the SPS for the WHEN clause and trigger action. Execute the given stored prepared statement.  We just grab the prepared statement from the spsd, get a new activation holder and let er rip. <p> Execute the WHEN clause SPS and the trigger action SPS. </p> <p> If there is no WHEN clause, the trigger action should always be executed. If there is a WHEN clause, the trigger action should only be executed if the WHEN clause returns TRUE. </p> Fire the trigger based on the event.
Bind this operator Bind a ? parameter operand of the char_length function. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. Check the reliability type of this java value.  Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Remap all ColumnReferences in this tree to be clones of the underlying expression.
Get the value of the specified data type from a column.  @exception SQLException  Thrown if there is a SQL error.    Get the value of the specified data type from a column.  @exception SQLException  Thrown if there is a SQL error.    Get the value of the specified data type from a column.  @exception SQLException  Thrown if there is a SQL error.

Provide a hashCode which is compatable with the equals() method.
Obtain the transaction branch qualifier as an array of bytes. ************************************************************************ Private/Protected methods of This class: ************************************************************************* Obtain the format id part of the GlobalTransactionId. Obtain the global transaction identifier as an array of bytes. Return my format identifier. Read this in Write this out.
Bind this GrantNode. Resolve all table, column, and routine references. end of bind Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. end of toString
This is the guts of the Execution-time logic for GRANT/REVOKE See ConstantAction#executeConstantAction
Check that allowing this grant to go ahead does nto create a circularity in the GRANT role relation graph, cf. Section 12.5, Syntax rule 1 of ISO/IEC 9075-2 2003. INTERFACE METHODS This is the guts of the Execution-time logic for GRANT role. OBJECT SHADOWS
Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. end of toString

Accept the visitor for all visitable children of this node. Bind this grouping column. Get the name of this column Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.
Add a column to the list Bind the group by list.  Verify: o  Number of grouping columns matches number of non-aggregates in SELECT's RCL. o  Names in the group by list are unique o  Names of grouping columns match names of non-aggregate expressions in SELECT's RCL. Find the matching grouping column if any for the given expression Get a column from the list Get the number of grouping columns that need to be added to the SELECT list. Remap all ColumnReferences in this tree to be clones of the underlying expression. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
In the query rewrite involving aggregates, add the columns for aggregation. Add the extra result columns required by the aggregates to the result list. Add any distinct aggregates to the order by list. Asserts that there are 0 or more distincts. Add a whole slew of columns needed for aggregation. Basically, for each aggregate we add 3 columns: the aggregate input expression and the aggregator column and a column where the aggregate result is stored.  The input expression is taken directly from the aggregator node.  The aggregator is the run time aggregator.  We add it to the RC list as a new object coming into the sort node. <P> At this point this is invoked, we have the following tree: <UL> PR - (PARENT): RCL is the original select list | PR - GROUP BY:  RCL is empty | PR - FROM TABLE: RCL is empty </UL> <P> For each ColumnReference in PR RCL <UL> <LI> clone the ref </LI> <LI> create a new RC in the bottom RCL and set it to the col ref </LI> <LI> create a new RC in the GROUPBY RCL and set it to point to the bottom RC </LI> <LI> reset the top PR ref to point to the new GROUPBY RC</LI></UL> For each aggregate in {@code aggregates} <UL> <LI> create RC in FROM TABLE.  Fill it with aggs Operator. <LI> create RC in FROM TABLE for agg result</LI> <LI> create RC in FROM TABLE for aggregator</LI> <LI> create RC in GROUPBY for agg input, set it to point to FROM TABLE RC </LI> <LI> create RC in GROUPBY for agg result</LI> <LI> create RC in GROUPBY for aggregator</LI> <LI> replace Agg with reference to RC for agg result </LI></UL>. <P> For a query like, <pre> select c1, sum(c2), max(c3) from t1 group by c1; </pre> the query tree ends up looking like this: <pre> ProjectRestrictNode RCL -&gt; (ptr to GBN(column[0]), ptr to GBN(column[1]), ptr to GBN(column[4])) | GroupByNode RCL-&gt;(C1, SUM(C2), &lt;agg-input&gt;, <aggregator>, MAX(C3), &lt;agg-input&gt;, &lt;aggregator&gt;) | ProjectRestrict RCL-&gt;(C1, C2, C3) | FromBaseTable </pre> The RCL of the GroupByNode contains all the unagg (or grouping columns) followed by 3 RC's for each aggregate in this order: the final computed aggregate value, the aggregate input and the aggregator function. <p> The Aggregator function puts the results in the first of the 3 RC's and the PR resultset in turn picks up the value from there. <p> The notation (ptr to GBN(column[0])) basically means that it is a pointer to the 0th RC in the RCL of the GroupByNode. <p> The addition of these unagg and agg columns to the GroupByNode and to the PRN is performed in addUnAggColumns and addAggregateColumns. <p> Note that that addition of the GroupByNode is done after the query is optimized (in SelectNode#modifyAccessPaths) which means a fair amount of patching up is needed to account for generated group by columns. Add a new PR node for aggregation.  Put the new PR under the sort. In the query rewrite for group by, add the columns on which we are doing the group by. Consider any optimizations after the optimizer has chosen a plan. Optimizations include: o  min optimization for scalar aggregates o  max optimization for scalar aggregates  Evaluate whether or not the subquery in a FromSubquery is flattenable. Currently, a FSqry is flattenable if all of the following are true: o  Subquery is a SelectNode. o  It contains no top level subqueries.  (RESOLVE - we can relax this) o  It does not contain a group by or having clause o  It does not contain aggregates. Generate the code to evaluate grouped aggregates. Generate the code to evaluate scalar aggregates. generate the sort result set operating over the source result set.  Adds distinct aggregates to the sort if necessary. /////////////////////////////////////////////////////////////  UTILITIES  ///////////////////////////////////////////////////////////// Method for creating a new result column referencing the one passed in. Get whether or not the source is in sorted order. Return the parent node to this one, if there is one.  It will return 'this' if there is no generated node above this one. Return whether or not the underlying ResultSet tree will return a single row, at most. This is important for join nodes where we can save the extra next on the right side if we know that it will return at most 1 row. Optimize this GroupByNode. Optimizable interface  Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.  Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Fetch the next N rows from the table. <p> The client allocates an array of N rows and passes it into the fetchNextSet() call.  The client must at least allocate a row and set row_array[0] to this row.  The client can optionally either leave the rest of array entries null, or allocate rows to the slots. If access finds an entry to be null, and wants to read a row into it, it will allocate a row to the slot.  Once fetchNextGroup() returns "ownership" of the row passes back to the client, access will not keep references to the allocated row.  Expected usage is that the client will specify an array of some number (say 10), and then only allocate a single row.  This way if only 1 row qualifies only one row will have been allocated. <p> This routine does the equivalent of N fetchNext() calls, filling in each of the rows in the array. Locking is performed exactly as if the N fetchNext() calls had been made. <p> It is up to Access how many rows to return.  fetchNextGroup() will return how many rows were filled in.  If fetchNextGroup() returns 0 then the scan is complete, (ie. the scan is in the same state as if fetchNext() had returned false).  If the scan is not complete then fetchNext() will return (1 &lt;= row_count &lt;= N). <p> The current position of the scan is undefined if fetchNextSet() is used (ie. mixing fetch()/fetchNext() and fetchNextSet() calls in a single scan does not work).  This is because a fetchNextSet() request for 5 rows from a heap where the first 2 rows qualify, but no other rows qualify will result in the scan being positioned at the end of the table, while if 5 rows did qualify the scan will be positioned on the 5th row. <p> If the row loc array is non-null then for each row fetched into the row array, a corresponding fetchLocation() call will be made to fill in the rowloc_array.  This array, like the row array can be initialized with only one non-null RowLocation and access will allocate the rest on demand. <p> Qualifiers, start and stop positioning of the openscan are applied just as in a normal scan. <p> The columns of the row will be the standard columns returned as part of a scan, as described by the validColumns - see openScan for description. <p> Expected usage: // allocate an array of 5 empty rows DataValueDescriptor[][] row_array = allocate_row_array(5); int row_cnt = 0; scan = openScan(); while ((row_cnt = scan.fetchNextSet(row_array, null) != 0) { // I got "row_cnt" rows from the scan.  These rows will be // found in row_array[0] through row_array[row_cnt - 1] } <p> Move to the next position in the scan.  If this is the first call to next(), the position is set to the first row. Returns false if there is not a next row to move to. It is possible, but not guaranteed, that this method could return true again, after returning false, if some other operation in the same transaction appended a row to the underlying conglomerate.
If the result set has been opened, close the open scan. Close the source of whatever we have been scanning. This result set has its row from the last fetch done. If the cursor is closed, a null is returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the next row. /////////////////////////////////////////////////////////////////////////////  SCAN ABSTRACTION UTILITIES  ///////////////////////////////////////////////////////////////////////////// Get the next output row for processing Get a row from the input result set. Get a row from the sorter.  Side effects: sets currentRow. /////////////////////////////////////////////////////////////////////////////  CursorResultSet interface  ///////////////////////////////////////////////////////////////////////////// This result set has its row location from the last fetch done. If the cursor is closed, a null is returned. Return the total amount of time spent in this ResultSet /////////////////////////////////////////////////////////////////////////////  AGGREGATION UTILITIES  ///////////////////////////////////////////////////////////////////////////// Run the aggregator initialization method for each aggregator in the row.  Accumulate the input column.  WARNING: initializiation performs accumulation -- no need to accumulate a row that has been passed to initialization. Load up the sorter.  Feed it every row from the source scan.  When done, close the source scan and open the sort.  Return the sort scan controller. Return the passed row, after ensuring that we call setCurrentRow Run the aggregator merge method for each aggregator in the row. Return the number of grouping columns. Since some additional sort columns may have been included in the sort for DISTINCT aggregates, this function is used to ignore those columns when computing the grouped results. /////////////////////////////////////////////////////////////////////////////  ResultSet interface (leftover from NoPutResultSet)  ///////////////////////////////////////////////////////////////////////////// Open the scan.  Load the sorter and prepare to get rows from it. Return whether or not the new row has the same values for the grouping columns as the current row.  (This allows us to process in-order group bys without a sorter.) We are performing a ROLLUP aggregation and we need to set the N rolled-up columns in this row to NULL.
This method re-binds the result columns which may be referenced in the ON clause in this node. This method recursively: <ul> <li>determines if this part of the query tree is a compound OJ of the shape required for reordering and if so,</li> <li>does a reordering.</li> </ul> <pre> OJ1  pT1T2                      OJ1  pT2T3 /  \                             / \ /    \                 can       /   t3 t1    OJ2 pT2T3       reorder    / /  \              to      OJ2  pT1T2 /    \                    /   \ t2    t3                  /     \ t1     t2 where pR1R2 is a null-rejecting predicate which references the schema of joinee R1 and R2, cf. terminology explanation in #isNullRejecting. <p/> OJ1 represents <em>this</em> before and after the reordering. </pre> <p/> The join predicates are assumed to be in CNF form. <p/> <em>Note:</em> Present implementation limitations <ul> <li>Only left outer joins are considered, i.e. both OJs in diagram above must be LOJ.</li> <li>Top left side must be a base table (t1 above). The bottow right side (t3 above) may be another OJ, so reordering can happen recursively.</li> </ul> return the Null-producing table references return the row-preserving table references Generate	and add any arguments specifict to outer joins. Generate	the methods (and add them as parameters) for returning an empty row from 1 or more sides of an outer join, if required.  Pass whether or not this was originally a right outer join.  Generate the code for an inner join node. Return the logical left result set for this qualified join node. (For RIGHT OUTER JOIN, the left is the right and the right is the left and the JOIN is the NIOJ). Return the logical right result set for this qualified join node. (For RIGHT OUTER JOIN, the left is the right and the right is the left and the JOIN is the NIOJ). Return the number of arguments to the join result set. If this is a right outer join node with USING/NATURAL clause, then check if the passed ResultColumn is a join column. If yes, then ResultColumn should be marked such. DERBY-4631 Tests pRiRj in the sense of Galindo-Legaria et al: <em>Outerjoin Simplification and Reordering for Query Optimization</em>, ACM Transactions on Database Systems, Vol. 22, No. 1, March 1997, Pages 43-74: <quote> "The set of attributes referenced by a predicate p is called the schema of p, and denoted sch(p). As a notational convention, we annotate predicates to reflect their schema. If sch(p) includes attributes of both Ri, Rj and only those relations, we can write the predicate as pRiRj. </quote> If a null-valued column is compared in a predicate that contains no OR connectives, the predicate evaluates to undefined, and the tuple is rejected. The relops satisfy this criterion. <p/> To simplify analysis, we only accept predicates of the form: <pre> X relop Y [and .. and X-n relop Y-n] </pre> At least one of the relops should reference both {@code leftTableMap} and {@code rightTableMap}, so that we know that sch(p) includes attributes of both Ri, Rj. I.e. <p/> {@code X} should be a table in {@code leftTableMap}, and {@code Y} should be a table in {@code rightTableMap}. <p/> <b>or</b> {@code X} should be a table in {@code rightTableMap}, and {@code Y} should be a table in {@code leftTableMap}. Return true if right outer join or false if left outer join Used to set Nullability correctly in JoinNode Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Push expressions down to the first ResultSetNode which can do expression evaluation and has the same referenced table map. RESOLVE - This means only pushing down single table expressions to DistinctNodes today.  Once we have a better understanding of how the optimizer will work, we can push down join clauses. Optimizable interface  Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Transform any Outer Join into an Inner Join where applicable. (Based on the existence of a null intolerant predicate on the inner table.)


Returns an input stream of this SerialObject. Returns a copied array of this SerialObject, starting at the <code> pos </code> with the given <code> length</code> number. If <code> pos </code> + <code> length </code> - 1 is larger than the length of this SerialObject array, the <code> length </code> will be shortened to the length of array - <code>pos</code> + 1. Gets the number of bytes in this SerialBlob object. Create a SQLException from Derby message arguments. Returns true if the bytes array contains exactly the same elements from start position to start + subBytes.length as subBytes. Otherwise returns false. Search for the position in this Blob at which the specified pattern begins, starting at a specified position within the Blob. Search for the position in this Blob at which a specified pattern begins, starting at a specified position within the Blob.
Returns true if the chars array contains exactly the same elements from start position to start + pattern.length as pattern. Otherwise returns false.
//////////////////////////////////////////////  CLASS INTERFACE  ////////////////////////////////////////////// Indicate whether we found the node in question Shortcut to set if hasCorrelatedCRs Stop traversal if we found the target node //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// If we have found the target node, we are done.
//////////////////////////////////////////////  CLASS INTERFACE  ////////////////////////////////////////////// Indicate whether we found the node in question Reset the status so it can be run again. Don't visit children under the skipOverClass node, if it isn't null. Stop traversal if we found the target node //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// If we have found the target node, we are done. Visit parent before children.
/////////////////////////////////////////////////////////////////////////////////  OVERRIDES  /////////////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////  CLASS INTERFACE  ////////////////////////////////////////////// Indicate whether we found the node in question Stop traversal if we found the target node //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// If we have found the target node, we are done.

Find the hash key columns, if any, to use with this join.

ResultSet interface (leftover from NoPutResultSet)  Can we get instantaneous locks when getting share row locks at READ COMMITTED. If the result set has been opened, close the open scan. This result set has its row from the last fetch done. If the cursor is closed, a null is returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the next row (if any) from the scan (if open).  CursorResultSet interface  This result set has its row location from the last fetch done. If the cursor is closed, a null is returned. Return the total amount of time spent in this ResultSet Is this ResultSet or it's source result set for update open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... Return a start or stop positioner as a String. reopen this ResultSet.
Accept the visitor for all visitable children of this node. For joins, the tree will be (nodes are left out if the clauses are empty): ProjectRestrictResultSet -- for the having and the select list SortResultSet -- for the group by list ProjectRestrictResultSet -- for the where and the select list (if no group or having) the result set for the fromList Logic shared by generate() and generateResultSet(). General logic shared by Core compilation and by the Replication Filter compiler. A couple ResultSets (the ones used by PREPARE SELECT FILTER) implement this method. Optimizable interface  Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.
ResultSet interface If the result set has been opened, close the open scan. Do the projection against the source row.  Use reflection where necessary, otherwise get the source column into our result row. Gets last row returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction and projection parameters are evaluated for each row. RowSource interface   CursorResultSet interface  Gets information from its source. We might want to have this take a CursorResultSet in its constructor some day, instead of doing a cast here? Return the total amount of time spent in this ResultSet Is this ResultSet or it's source result set for update  NoPutResultSet interface  open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
Return the header for the stream. Gets the name of the wrapped writer or stream Gets a PrintWriter object for writing to this HeaderPrintWriter. Users may use the HeaderPrintWriter to access methods not included in this interface or to invoke methods or constructors which require a PrintWriter. Interleaving calls to a printWriter and its associated HeaderPrintWriter is not supported. The routines that mimic java.io.PrintWriter...    Puts out some setup info for the current write and the write(s) that will be put out next. It ends with a \n\r. <p> All other writes to the stream use the PrintStream interface.
* Methods of Conglomerate Add a column to the heap conglomerate. <p> This routine update's the in-memory object version of the Heap Conglomerate to have one more column of the type described by the input template column. Create a heap conglomerate during the boot process. <p> Manufacture a Heap Conglomerate out of "thin" air, to boot strap the system.  Create an in-memory Heap Conglomerate with the input parameters, The caller will use this to open the conglomerate conglomerate and read the "real" values from disk.  Conglom-conglom is always on segment 0. Private/Protected methods of This class: Create a heap conglomerate. <p> Create a heap conglomerate.  This method is called from the heap factory to create a new instance of a heap. <p> Open a heap compress scan. <p> Drop this heap. end of estimateMemoryUsage Retrieve the maximum value row in an ordered conglomerate. <p> Returns true and fetches the rightmost row of an ordered conglomerate into "fetchRow" if there is at least one row in the conglomerate.  If there are no rows in the conglomerate it returns false. <p> Non-ordered conglomerates will not implement this interface, calls will generate a StandardException. <p> RESOLVE - this interface is temporary, long term equivalent (and more) functionality will be provided by the openBackwardScan() interface. ************************************************************************ Public Methods of StaticCompiledOpenConglomInfo Interface: ************************************************************************* return the "Conglomerate". <p> For heap just return "this", which both implements Conglomerate and StaticCompiledOpenConglomInfo. <p> Return dynamic information about the conglomerate to be dynamically reused in repeated execution of a statement. <p> The dynamic info is a set of variables to be used in a given ScanController or ConglomerateController.  It can only be used in one controller at a time.  It is up to the caller to insure the correct thread access to this info.  The type of info in this is a scratch template for btree traversal, other scratch variables for qualifier evaluation, ... <p> Get the id of the container of the conglomerate. <p> Will have to change when a conglomerate could have more than one container.  The ContainerKey is a combination of the container id and segment id. Return static information about the conglomerate to be included in a a compiled plan. <p> The static info would be valid until any ddl was executed on the conglomid, and would be up to the caller to throw away when that happened.  This ties in with what language already does for other invalidation of static info.  The type of info in this would be containerid and array of format id's from which templates can be created. The info in this object is read only and can be shared among as many threads as necessary. <p> ************************************************************************ Methods of Storable (via Conglomerate) Storable interface, implies Externalizable, TypedFormat ************************************************************************* Return my format identifier. Return whether the value is null or not. Is this conglomerate temporary? <p> Bulk load into the conglomerate. <p> Open a heap controller. <p> Open a heap controller given ContainerKey. <p> Static routine to open a container given input of the ContainerKey. Routine will lock the container first, and then get the Heap from the conglomerate cache.  This insures that interaction with the conglomerate cache is safe with respect to concurrent alter table's which may or may not commit. Currently only package accessible and only used by HeapPostCommit. Longer term would be better to change all of the open interfaces to get lock before accessing conglomerate cache rather than have a specific interface for HeapPostCommit. package Open a heap scan controller. <p> Return an open StoreCostController for the conglomerate. <p> Return an open StoreCostController which can be used to ask about the estimated row counts and costs of ScanController and ConglomerateController operations, on the given conglomerate. <p> Restore the in-memory representation from the stream. <p> Restore the in-memory representation to the null value. Print this heap. Store the stored representation of column value in stream. <p> This routine uses the current database version to either store the the 10.2 format (ACCESS_HEAP_V2_ID) or the current format (ACCESS_HEAP_V3_ID). <p> Store the stored representation of the column value in the stream. Store the 10.2 format stored representation of column value in stream. <p> This routine stores the 10.2 version the Heap, ie. the ACCESS_HEAP_V2_ID format.  It is used by any database which has been created in 10.2 or a previous release and has not been hard upgraded to a version subsequent to 10.2. <p>
only supports a single class at the moment
************************************************************************ Protected override implementation of routines in GenericController class: ************************************************************************* Fetch the next N rows from the table. <p> Utility routine used by both fetchSet() and fetchNextGroup(). ************************************************************************ Private/Protected methods of This class: ************************************************************************* Set scan position to just after current page. <p> Used to set the position of the scan if a record handle is not avaliable.  In this case current_rh will be set to null, and current_pageno will be set to the current page number. On resume of the scan, the scan will be set to just before the first row returned form a getNextPage(current_pageno) call. <p> A positionAtResumeScan(scan_position) is necessary to continue the scan after this call. Reposition the scan upon entering the fetchRows loop. <p> Called upon entering fetchRows() while in the SCAN_INPROGRESS state. Do work necessary to look at rows in the current page of the scan. <p> The default implementation uses a record handle to maintain a scan position.  It will get the latch again on the current scan position and set the slot to the current record handle. Move the scan from SCAN_INIT to SCAN_INPROGRESS. <p> This routine is called to move the scan from SCAN_INIT to SCAN_INPROGRESS.  Upon return from this routine it is expected that scan_position is set such that calling the generic scan loop will reach the first row of the scan.  Note that this usually means setting the scan_postion to one before the 1st row to be returned. <p>
* Methods of ModuleControl. Create the conglomerate and return a conglomerate object for it. Methods of MethodFactory (via ConglomerateFactory) Return the default properties for this kind of conglomerate. * Methods of ConglomerateFactory Return the conglomerate factory id. <p> Return a number in the range of 0-15 which identifies this factory. Code which names conglomerates depends on this range currently, but could be easily changed to handle larger ranges.   One hex digit seemed reasonable for the number of conglomerate types being currently considered (heap, btree, gist, gist btree, gist rtree, hash, others? ). <p> Privileged Monitor lookup. Must be private so that user code can't call this entry point. Interface to be called when an undo of an insert is processed. <p> Implementer of this class provides interface to be called by the raw store when an undo of an insert is processed.  Initial implementation will be by Access layer to queue space reclaiming events if necessary when a rows is logically "deleted" as part of undo of the original insert.  This undo can happen a lot for many applications if they generate expected and handled duplicate key errors. <p> Caller may decide to call or not based on deleted row count of the page, or if overflow rows/columns are present. Return the primary format that this access method supports. The heap currently only supports one format, HEAPFORMAT1. Return the primary implementation type for this access method. The heap only has one implementation type, "heap". Return Conglomerate object for conglomerate with container_key. <p> Return the Conglomerate Object.  This is implementation specific. Examples of what will be done is using the key to find the file where the conglomerate is located, and then executing implementation specific code to instantiate an object from reading a "special" row from a known location in the file.  In the btree case the btree conglomerate is stored as a column in the control row on the root page. <p> This operation is costly so it is likely an implementation using this will cache the conglomerate row in memory so that subsequent accesses need not perform this operation. Return whether this access method supports the format supplied in the argument. The heap currently only supports one format, HEAPFORMAT1. Return whether this access method implements the implementation type given in the argument string. The heap only has one implementation type, "heap".
Insert a new row into the heap. <p> Overflow policy: The current heap access method implements an algorithm that optimizes for fetch efficiency vs. space efficiency.  A row will not be over flowed unless it is bigger than a page.  If it is bigger than a page then it's initial part will be placed on a page and then subsequent parts will be overflowed to other pages. <p> ************************************************************************ Public Methods of XXXX class: ************************************************************************* ************************************************************************ Fields of the class ************************************************************************* ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Protected concrete impl of abstract methods of GenericConglomerateController class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* Lock the given record id/page num pair. <p> Should only be called by access, to lock "special" locks formed from the Recordhandle.* reserved constants for page specific locks. <p> This call can be made on a ConglomerateController that was opened for locking only. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this. Lock the given row location. <p> Should only be called by access. <p> This call can be made on a ConglomerateController that was opened for locking only. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this. ************************************************************************ Private/Protected methods of This class: ************************************************************************* Check and purge committed deleted rows on a page. <p> UnLock the given row location. <p> Should only be called by access. <p> This call can be made on a ConglomerateController that was opened for locking only. <p> RESOLVE (mikem) - move this call to ConglomerateManager so it is obvious that non-access clients should not call this.
Public Methods of This class: Public Methods of XXXX class: Return the cost of calling ConglomerateController.fetch(). <p> Return the estimated cost of calling ConglomerateController.fetch() on the current conglomerate.  This gives the cost of finding a record in the conglomerate given the exact RowLocation of the record in question. <p> The validColumns describes what kind of row is being fetched, ie. it may be cheaper to fetch a partial row than a complete row. <p> Calculate the cost of a scan. <p> Cause this object to calculate the cost of performing the described scan.  The interface is setup such that first a call is made to calcualteScanCost(), and then subsequent calls to accessor routines are made to get various pieces of information about the cost of the scan. <p> For the purposes of costing this routine is going to assume that a page will remain in cache between the time one next()/fetchNext() call and a subsequent next()/fetchNext() call is made within a scan. <p> The result of costing the scan is placed in the "cost_result". The cost of the scan is stored by calling cost_result.setEstimatedCost(cost). The estimated row count is stored by calling cost_result.setEstimatedRowCount(row_count). <p> The estimated cost of the scan assumes the caller will execute a fetchNext() loop for every row that qualifies between start and stop position.  Note that this cost is different than execution a next(),fetch() loop; or if the scan is going to be terminated by client prior to reaching the stop condition. <p> The estimated number of rows returned from the scan assumes the caller will execute a fetchNext() loop for every row that qualifies between start and stop position. <p> Private/Protected methods of This class: Initialize the cost controller. <p> Let super.init() do it's work and then get the initial stats about the table from raw store.
perform the work described in the postcommit work. <p> In this implementation the only work that can be executed by this post commit processor is this class itself. <p> ************************************************************************ Private/Protected methods of This class: ************************************************************************* Reclaim space taken of committed deleted rows or aborted inserted rows. <p> This routine assumes it has been called by an internal transaction which has performed no work so far, and that it has an exclusive intent table lock.  It will attempt obtain exclusive row locks on rows marked deleted, where successful those rows can be reclaimed as they must be "committed deleted" or "aborted inserted" rows. <p> This routine will latch the page and hold the latch due to interface requirement from Page.purgeAtSlot. ************************************************************************ Public Methods implementing the Serviceable interface: ************************************************************************* The urgency of this post commit work. <p> This determines where this Serviceable is put in the post commit queue.  Post commit work in the heap can be safely delayed until there is not user work to do. @return true, if this work needs to be done on a user thread immediately
* Methods of Orderable (from RowLocation) * * see description in * protocol/Database/Storage/Access/Interface/Orderable.java * *		Methods of Object Implement value equality. <BR> MT - Thread safe end of estimateMemoryUsage public void setFrom(long pageno, int recid) { this.pageno = pageno; this.recid  = recid; } InternalRowLocation interface Return a RecordHandle built from current RowLocation. <p> Build a RecordHandle from the current RowLocation.  The main client of this interface is row level locking secondary indexes which read the RowLocation field from a secondary index row, and then need a RecordHandle built from this RowLocation. <p> The interface is not as generic as one may have wanted in order to store as compressed a version of a RowLocation as possible.  So if an implementation of a RowLocation does not have the segmentid, and containerid stored, use the input parameters instead.  If the RowLocation does have the values stored use them and ignore the input parameters. <p> Example: <p> The HeapRowLocation implementation of RowLocation generated by the Heap class, only stores the page and record id.  The B2I conglomerate implements a secondary index on top of a Heap class.  B2I knows the segmentid and containerid of it's base table, and knows that it can find an InternalRowLocation in a particular column of it's rows.  It uses InternalRowLocation.getRecordHandle() to build a RecordHandle from the InternalRowLocation, and uses it to set a row lock on that row in the btree. public RecordHandle getRecordHandle( TransactionManager   tran, long                 segmentid, long                 containerid) throws StandardException { return( this.getRecordHandle( tran.getRawStoreXact(), segmentid, containerid)); } Storable interface, implies Externalizable, TypedFormat Return my format identifier. Return a hashcode based on value. <BR> MT - thread safe  Recycle this HeapRowLocation object. Standard toString() method.
************************************************************************ Public Methods of ScanController interface: *************************************************************************  Fetch the row at the next position of the Scan. If there is a valid next position in the scan then the value in the template storable row is replaced with the value of the row at the current scan position.  The columns of the template row must be of the same type as the actual columns in the underlying conglomerate. The resulting contents of templateRow after a fetchNext() which returns false is undefined. The result of calling fetchNext(row) is exactly logically equivalent to making a next() call followed by a fetch(row) call.  This interface allows implementations to optimize the 2 calls if possible. Return ScanInfo object which describes performance of scan. <p> Return ScanInfo object which contains information about the current scan. <p>   ************************************************************************ Protected concrete impl of abstract methods of GenericController class: ************************************************************************* Reposition the current scan and sets the necessary locks. Reposition the current scan.  This call is semantically the same as if the current scan had been closed and a openScan() had been called instead. The scan is reopened against the same conglomerate, and the scan is reopened with the same "scan column list", "hold" and "forUpdate" parameters passed in the original openScan. <p> The statistics gathered by the scan are not reset to 0 by a reopenScan(), rather they continue to accumulate. <p> Note that this operation is currently only supported on Heap conglomerates. Also note that order of rows within are heap are not guaranteed, so for instance positioning at a RowLocation in the "middle" of a heap, then inserting more data, then continuing the scan is not guaranteed to see the new rows - they may be put in the "beginning" of the heap. ************************************************************************ Private/Protected methods of This class: *************************************************************************
Return all information gathered about the scan. <p> This routine returns a list of properties which contains all information gathered about the scan.  If a Property is passed in, then that property list is appeneded to, otherwise a new property object is created and returned. <p> Not all scans may support all properties, if the property is not supported then it will not be returned.  The following is a list of properties that may be returned: numPagesVisited - the number of pages visited during the scan.  For btree scans this number only includes the leaf pages visited. numRowsVisited - the number of rows visited during the scan.  This number includes all rows, including: those marked deleted, those that don't meet qualification, ... numRowsQualified - the number of undeleted rows, which met the qualification. treeHeight (btree's only) - for btree's the height of the tree.  A tree with one page has a height of 1.  Total number of pages visited in a btree scan is (treeHeight - 1 + numPagesVisited). NOTE - this list will be expanded as more information about the scan is gathered and returned.
************************************************************************ Public Methods required by Storable interface, implies Externalizable, TypedFormat: ************************************************************************* Return my format identifier. <p> This identifier was used for Heap in all Derby versions prior to 10.3. Databases hard upgraded to a version 10.3 and later will write the new format, see Heap.  Databases created in 10.3 and later will also write the new format, see Heap. Store the stored representation of the column value in the stream. <p> For more detailed description of the format see documentation at top of file.

A call from the VTI execution layer back into the supplied VTI. Presents the row just processed as an array of DataValueDescriptors. This only called when the VTI is being executed as a regular ResultSet VTI Start a query. Returns true if the VTI will start out as a fast path query and thus rows will be returned by nextRow(). Returns false if the engine must call the VTI's PreparedStatement.executeQuery() method to execute as a regular ResultSet VTI. When operating in fast path mode return the next row into the passed in row parameter. Returns GOT_ROW if a valid row is found. Returns SCAN_COMPLETED if the scan is complete. Returns NEED_RS if the rest of the query must be handled as a regular ResultSet VTI by the engine calling the VTI's PreparedStatement.executeQuery() Called once the ResultSet returned by executeQuery() has emptied all of its rows (next() has returned false).


public boolean handleQualifier(int relOp, int Called at runtime before each scan of the VTI. The passed in qualifiers are only valid for the single execution that follows.
Given a case normal form SQL authorization identifier, convert it to a form that may be compared with the username of Derby builtin authentication, which uses Java properties of the form {@code derby.user.}&lt;username&gt;. <p> The returned form is suitable for comparing against the property string, cf.  {@code systemPropertiesExistsBuiltinUser}. <p> E.g.: <p> <pre> Argument -&gt; Return ------------------ EVE      -&gt; eve       [will match Java property: derby.user.eve] eVe      -&gt; "eVe"     [will match Java property: derby.user."eVe"] "eve"    -&gt; """eve""" [will match Java property: derby.user."""eVe"""] \eve\    -&gt; "\eve\"   [will match Java property: derby.user."\eve\"] The latter could look this if specified on a Unix shell command line: -Dderby.user.'"\eve\"'=&lt;password&gt; Note: The processing of properties specified on the command line do not interpret backslash as escape in the way done by the java.util.Properties#load method, so no extra backslash is needed above. </pre> Since parseSQLIdentifier maps many-to-one, the backward mapping is non-unique, so the chosen lower case canonical form is arbitrary, e.g. we will not be able to correctly match the non-canonical: <p> <pre> [Java property: derby.user.eVe] </pre> since this is internally EVE (but see DERBY-3150), and maps back as eve after the rules above. Append an identifier to a comma separated list of identifiers. The passed in identifier is its normal form, the list contains a list of SQL identifiers, either regular or delimited. This routine takes the easy way out and always appends a delimited identifier. Check that identifier is not too long Delete an normal value from a list of SQL identifiers. The returned list maintains its remaining identifiers in the format they were upon entry to the call. Return an IdList with all the ids that are repeated in l. Map userName to authorizationId in its normal form. Get user name from URL properties (key user) without any transformation. If the user property does not exist or is set to the empty string then Property.DEFAULT_USER_NAME is returned. Return true if the normalized value of an indentifier is on the list of SQL identifiers provided. Return an IdList with all the ids that in l1 and l2 or null if not ids are on both lists. Produce a string form of an idList from an array of normalized ids. Produce an id list from an array of ids in external form Produce a delimited two part qualified name from two un-delimited identifiers. Make a string form of a qualified name from the array of ids provided. Produce a delimited form of a normal value. Scan a database classpath from the string provided. This returns an array with one qualified name per entry on the classpath. The constants above describe the content of the returned names. This raises an an exception if the string does not contain a valid database class path. <PRE> classpath := item[:item]* item := id.id In the syntax braces ([]) show grouping. '*' means repeat 0 or more times. The syntax for id is defined in IdUtil. </PRE> <BR> Classpath returned is a two part name.	  <BR> If the class path is empty then this returns an array of zero length. Read an id from the StringReader provided. Parse a list of comma separated SQL identifiers returning them a as elements in an array. * Methods that operate on lists of identifiers. Scan a list of comma separated SQL identifiers from the string provided. This returns an array with containing the normalized forms of the identifiers. This raises an an exception if the string does not contain a valid list of names.  Parse a multi-part (dot separated) SQL identifier form the String provided. Raise an excepion if the string does not contain valid SQL indentifiers. The returned String array contains the normalized form of the identifiers. Parse a delimited (quoted) identifier returning either the value of the identifier or a delimited identifier. Parse role identifier to internal, case normal form. It should not be NONE nor exceed Limits.MAX_IDENTIFIER_LENGTH. Parse a SQL identifier from the String provided. Raise an excepion if the string does not contain a valid SQL indentifier. The returned String  contains the normalized form of the identifier. Parse a regular identifier (unquoted) returning returning either the value of the identifier or a delimited identifier. Ensures that all characters in the identifer are valid for a regular identifier. Return an IdList with all the duplicate ids removed Return an idList in external form with one id for every element of v. If v has no elements, return null. Verify the read is empty (no more characters in its stream).
/////////////////////////////////////////////////////////////////////////  VisitableFilter BEHAVIOR  /////////////////////////////////////////////////////////////////////////
Bump the import counter. Format a import error with line number virtual method from the abstract class SYSCS_IMPORT_DATA  system Procedure from ij or from a Java application invokes  this method to perform import to a table from a file. SYSCS_IMPORT_TABLE  system Procedure from ij or from a Java application invokes  this method to perform import to a table from a file. The extraArgs parameter is variadic, and is used when this method is called from SYSCS_IMPORT_TABLE_BULK, in which case extraArgs[0] specifies the number of header lines to skip. This function creates and executes  SQL Insert statement that performs the the import using VTI. eg: insert into T1 select  (cast column1 as DECIMAL), (cast column2 as INTEGER)  from new org.apache.derby.impl.load.Import('extin/Tutor1.asc') as importvti; Quote a string argument so that it can be used as a literal in an SQL statement. If the string argument is {@code null} an SQL NULL token is returned. Read the header lines to get column names to identify columns by name
closes the resultset Read an object which was serialized to a string using StringUtil Does all the work Returns <code> java.sql.Blob </code> type object that contains the column data from the import file. Returns byte array that contains the column data from the import file. Returns <code> java.sql.Clob </code> type object that contains the column data from the import file. gets the current line number Gets the resultset meta data Returns Object that contains the column data from the import file. all the resultset interface methods gets the next row  Close the stream and wrap exception in a SQLException Check if for this column type, real data is stored in an external file and only the reference is in the main import file. the column names will be Column# Read a serializable from a set of bytes.
Raise error, not used by import Returns  <code>BLOB</code> value designated by this <code>Blob</code> object as a input stream. Raise error, not used by import following rotines does not have implmentation because there are not used by the VTI that is used to import the data. This routine is not used by the VTI to read the data, so no implementatio  is provided , an exception is thrown if used. Returns the number of bytes in this <code>BLOB</code>  object. Return an unimplemented feature error This routine is not used by the VTI to read the data, so no implementatio  is provided , an exception is thrown if used. This routine is not used by the VTI to read the data, so no implementation is provided , an exception is thrown if used. This routine is not used by the VTI to read the data, so no implementation  is provided , an exception is thrown if used. This routine is not used by the VTI to read the data, so no implementation  is provided , an exception is thrown if used. This routine is not used by the VTI to read the data, so no implementation  is provided , an exception is thrown if used. This routine is not used by the VTI to read the data, so no implementation  is provided , an exception is thrown if used.
Raise error, not used by import This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. Returns  <code>CLOB</code> value designated by this <code>Clob</code> object as a <code> Reader </code>. Raise error, not used by import following rotines does not have implmentation because they are not used by the VTI that is used to import the data. This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. Returns the number of characters in this <code>CLOB</code>  object. Return an unimplemented feature error This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called. This routine is not used by the VTI to read the data, so no implementation is provided, an exception is thrown if it is called.
Returns the number of bytes that can be read from this stream. Closes this input stream and releases any associated resources Override the following InputStream-methods to read data from the current postion of the file. Reads a byte of data from this input stream. Reads up to <code>length</code> bytes of data from this input stream into given array. This method blocks until some input is available. Sets the file offset at which the next read will occur. Close all the resources realated to the lob file. Returns a stream that points to the lob data from file at the given <code>offset</code>. Returns a stream that points to the clob data from file at the given <code>offset</code>. Returns the clob data length in characters at the give location. Returns the clob data at the given location as <code>String</code>. Open the lob file and setup the stream required to read the data.
look for white spaces from the back towards the stop delimiter position. If there was no startdelimite & stopdelimiter combination, then we start from the back all the way to the beginning and stop when we find non-white char positionOfNonWhiteSpaceCharInBack keeps the count of whitespaces at the back keep track of white spaces in the front. We use positionOfNonWhiteSpaceCharInFront for that. It has the count of number of white spaces found so far before any non-white char in the token. Look for whitespace only if field start delimiter is not found yet. Any white spaces within the start and stop delimiters are ignored. Also if one of the white space chars is same as recordSeparator or fieldSeparator then disregard it. close the input data file actually looks at the data file to find how many columns make up a row Returns a blob columnn data stored at the specified location as a java.sql.Blob object. Returns a clob columnn data stored at the specified location as a java.sql.Clob object. following are the routines that are used to read lob data stored in a external import file for clob/blob columns, the reference to external file is stored in the main import file. Returns a clob columnn data stored at the specified location. returns the number of the current row just a getter returning number of columns for a row in the data file if columndefinition is true, ignore first row. The way to do that is to just look for the record separator if skipHeaderLines is greater than 0, ignore skipHeaderLines number of lines. The way to do that is to just look for the record separator Extract the file name, offset and length from the given lob location and setup the file resources to read the data from the file on first  invocaton. tells if a char array is field separator: load the column types from the meta data line to be analyzed later in the constructor of the ImportResultSetMetaData. read the first data row to find how many columns make a row and then save that column information for future use load the control file properties info locally, since we need to refer to them all the time while looking for tokens if not inside a start delimiter, then look for the delimiter passed else look for stop delimiter first. this routine returns -1 if it finds field delimiter or record delimiter omit the line feed character(\n) If after finding a few matching characters for a delimiter, find a mismatch, restart the matching process from character next to the one from which you were in the process of finding the matching pattern by this time, we know number of columns that make up a row in this data file so first look for number of columns-1 field delimites and then look for record delimiter read the specified column width for each column the way we read the next row from input file depends on it's format read one column's value at a time keep looking for field and record separators simultaneously because we don't yet know how many columns make up a row in this data file. Stop as soon as we get the record separator which is indicated by a return value of true from this function open the input data file for reading skips the duplicate delimeter characters inserd character stringd ata to get the original string. In Double Delimter recognigation Delimiter Format strings are written with a duplicate delimeter if a delimiter is found inside the data while exporting. For example with double quote(") as character delimiter "What a ""nice""day!" will be imported as: What a "nice"day! In the case of export, the rule applies in reverse. For example, I am 6"tall. will be exported to a file as: "I am 6""tall."
Get the class bound to a UDT column.
Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. Do code generation for this IN list operator. Generate the code to create an array of DataValueDescriptors that will hold the IN-list values at execution time.  The array gets created in the constructor.  All constant elements in the array are initialized in the constructor.  All non-constant elements, if any, are initialized each time the IN list is evaluated. Generate start/stop key for this IN list operator.  Bug 3858. Get the dominant type of all the operands in this IN list. Return whether or not the IN-list values for this node are ordered. This is used for determining whether or not we need to do an execution- time sort. Indicate that the IN-list values for this node are ordered (i.e. they are all constants and they have been sorted). Indicate that the IN-list values for this node must be sorted in DESCENDING order.  This only applies to in-list "multi-probing", where the rows are processed in the order of the IN list elements themselves.  In that case, any requirement to sort the rows in descending order means that the values in the IN list have to be sorted in descending order, as well. Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. The selectivity for an "IN" predicate is generally very small. This is an estimate applicable when in list are not all constants. See if this IN list operator is referencing the same table. Create a shallow copy of this InListOperatorNode whose operands are the same as this node's operands.  Copy over all other necessary state, as well. Return whether or not the IN-list values for this node must be sorted in DESCENDING order. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Close this IndexChanger. Close our index Conglomerate Controller Close our index ScanController. Perform index maintenance to support a delete of a base table row. Insert a row into the temporary conglomerate <P>This opens our deferred ConglomeratController the first time it is called. Delete a row from our index. This assumes our index ScanController is positioned before the row by setScan if we own the SC, otherwise it is positioned on the row by the underlying index scan. <P>This verifies the row exists and is unique. Insert a row into our indes. <P>This opens our index ConglomeratController the first time it is called. Finish doing the changes for this index.  This is intended for deferred inserts for unique indexes.  It has no effect unless we are doing an update of a unique index. Return the id of the corresponding unique or primary key constraint. Note: this only works because deferrable constraints do not share an index with other constraints and explicit indexes, so the mapping back from index conglomerate to constraint is one-to-one. Determine whether or not any columns in the current index row are being changed by the update.  No need to update the index if no columns changed. Perform index maintenance to support an insert of a base table row. Insert the given row into the given conglomerate and check for duplicate key error. If we're updating a unique index, the inserts have to be deferred.  This is to avoid uniqueness violations that are only temporary.  If we do all the deletes first, only "true" uniqueness violations can happen.  We do this here, rather than in open(), because this is the only operation that requires deferred inserts, and we only want to create the conglomerate if necessary. Open this IndexChanger. Open the ConglomerateController for this index if it isn't open yet. Propagate the heap's ConglomerateController to this index changer. Set the column values for 'ourIndexRow' to refer to a base table row and location provided by the caller. The idea here is to Set the column values for 'ourUpdatedIndexRow' to refer to a base table row and location provided by the caller. The idea here is to Set the row holder for this changer to use. If the row holder is set, it wont bother saving copies of rows needed for deferred processing.  Also, it will never close the passed in rowHolder. Position our index scan to 'ourIndexRow'. <P>This creates the scan the first time it is called. Perform index maintenance to support an update of a base table row.
ColumnOrdering interface Indicate whether NULL values should be ordered below non-NULL. This function returns TRUE if the user has specified, via the <null ordering> clause in the ORDER BY clause, that NULL values of this column should sort lower than non-NULL values. Get the formatID which corresponds to this class. Read this object from a stream of stored objects. ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out
CLASS METHODS Get the index name. Set the id for the constraint which may be driving this index action. This is called by CreateConstraintConstantAction. Set the index name at execution time. Useful for unnamed constraints which have a backing index.
Returns an array of column positions in the base table.  Each index column corresponds to a column position in the base table, except the column representing the location of the row in the base table. The returned array holds the column positions in the base table, so, if entry 2 is the number 4, the second column in the index is the fourth column in the table. Returns the postion of a column. <p> Returns the position of a column within the key (1-based). 0 means that the column is not in the key.  Same as the above method, but it uses int instead of Integer. Returns true if the index is used to support a deferrable constraint. Returns the type of the index.  For now, we only support B-Trees, so the value "BTREE" is returned. Returns array of boolean telling asc/desc info for each index key column for convenience of using together with baseColumnPositions method.  Both methods return an array with subscript starting from 0. Returns true if the specified column is ascending in the index (1-based). Returns true if the specified column is descending in the index (1-based).  In the current release, only ascending columns are supported. Returns true if the index is unique. The index represents a PRIMARY KEY or a UNIQUE NOT NULL constraint which is deferrable. {@code true} implies {@code isUnique() == false} and {@code isUniqueWithDuplicateNulls() == false} and {@code hasDeferrableChecking() == true}. Returns true if the index is duplicate keys only for null key parts. This is effective only if isUnique is false. Returns the number of ordered columns. <p> In the future, it will be possible to store non-ordered columns in an index.  These will be useful for covered queries.  The ordered columns will be at the beginning of the index row, and they will be followed by the non-ordered columns. For now, all columns in an index must be ordered. set the baseColumnPositions field of the index descriptor.  This is for updating the field in operations such as "alter table drop column" where baseColumnPositions is changed. set the isAscending field of the index descriptor.  This is for updating the field in operations such as "alter table drop column" where isAscending is changed. set the numberOfOrderedColumns field of the index descriptor.  This is for updating the field in operations such as "alter table drop column" where numberOfOrderedColumns is changed.
Test for value equality  TypedFormat interface        The index represents a PRIMARY KEY or a UNIQUE NOT NULL constraint which is deferrable. {@code true} implies {@code #isUnique() == false} and {@code #isUniqueWithDuplicateNulls() == false} and {@code #hasDeferrableChecking() == true}.   Externalizable interface
Get the base column position for a column within a catalog given the (0-based) column number for the column within the index. Get the column count for the index. Get the conglomerate number for the index. Get the index name for the index. Get the IndexRowGenerator for this index. Return whether or not this index is declared unique Set the conglomerate number for the index. Set the IndexRowGenerator for this index.

//////////////////////////////////////////////////////////////////////  MINIONS  ////////////////////////////////////////////////////////////////////// Reads all the indices on the table and populates arrays with the corresponding index row generators and index conglomerate ids. Returns an array of distinct index conglomerate ids on a table. erasing entries for duplicate indexes (which share same conglomerate). Returns an array of index names for all distinct indexes on a table. erasing entries for duplicate indexes (which share same conglomerate). Returns an array of distinct index row generators on a table, erasing entries for duplicate indexes (which share same conglomerate). Returns an array of all the index conglomerate ids on a table. //////////////////////////////////////////////////////////////////////  INDEXLISTER METHODS  ////////////////////////////////////////////////////////////////////// Returns an array of all the index row generators on a table.
Turn the ExecRow into an ExecIndexRow. /////////////////////////////////////////////////////////////////////  EXECINDEXROW INTERFACE  ///////////////////////////////////////////////////////////////////// Column positions are one-based, arrays are zero-based
Test for value equality Return an array of collation ids for this table. <p> Return an array of collation ids, one for each column in the columnDescriptorList.  This is useful for passing collation id info down to store, for instance in createConglomerate() to create the index. This is only expected to get called during ddl, so object allocation is ok. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Get the IndexDescriptor that this IndexRowGenerator is based on. Get an index row for this index given a row from the base table and the RowLocation of the base row.  This method can be used to get the new index row for inserts, and the old and new index rows for deletes and updates.  For updates, the result row has all the old column values followed by all of the new column values, so you must form a row using the new column values to pass to this method to get the new index row. Get a template for the index row, to be used with getIndexRow.  Get a NULL Index Row for this index. This is useful to create objects that need to be passed to ScanController. TypedFormat interface         //////////////////////////////////////////////////////////////////////////  EXTERNALIZABLE  //////////////////////////////////////////////////////////////////////////
If the result set has been opened, close the open scan. * Gets last row returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction and projection parameters are evaluated for each row.  CursorResultSet interface  Return the RowLocation of the base row. Return the total amount of time spent in this ResultSet Is this ResultSet or it's source result set for update. beetle 3865: updateable cursor using index scan.  We didn't need this function before because we couldn't use index for update cursor.  ResultSet interface (leftover from NoPutResultSet)  open this ResultSet.  reopen this ResultSet.
Close this IndexSetChanger. Perform index maintenance associated with deleting a row from a table. Finish processing the changes for this IndexSetChanger.  This means doing the deferred inserts for updates of unique indexes. Perform index maintenance associated with insering a row into a table. Open this IndexSetchanger. Open the indexes that must be fixed if they are not already open. Propagate the heap's ConglomerateController to all of the underlying index changers. Set the row holder for all underlying changers to use. If the row holder is set, underlying changers  wont bother saving copies of rows needed for deferred processing.  Also, it will never close the passed in rowHolder. Create a string describing the state of this IndexSetChanger Perform index maintenance associated with updating a row in a table.
Creates/updates index statistics for the specified conglomerates/indexes. Schedules creation/update of the index statistics associated with the specified table. <p> Note that the scheduling request may be denied. Typical situations where that will happen is if the work queue is full, or if work has already been scheduled for the specified table. Stops the background daemon. <p> Any ongoing tasks will be aborted as soon as possible, and it will not be possible to schedule new tasks. Note that <tt>runExplicitly</tt> can still be used.
Determines if the given work can be accepted. @GuardedBy("queue") Appends runtime statistics to the given string buffer. @GuardedBy("queue") Produces a textual representation of the cardinality numbers. Tells if the database is 10.9 or newer. Purely for debugging, to avoid printing too much info. Format array of scan durations as a string. Generates index statistics for all indexes associated with the given table descriptor. <p> This method is run as a background task. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Handles expected errors. <p> The logging of expected errors is for observability purposes only. The daemon is capable of dealing with these errors, and no interaction from the user is expected. Handles fatal errors that will cause the daemon to be shut down. Handles unexpected errors. <p> Unexpected errors are error conditions the daemon isn't set up to handle specifically. For this reason the stack trace will be logged to allow for later investigation. <p> In general it is expected that the daemon will be able to recover by dropping the current unit of work and move on to the next one (if any). Performs an invalidation action for the given table (the event being statistics update). Return true if we are being shutdown  Logs the information given. <p> Note that if {@code asBackgroundTask} is false, nothing will be logged currently. Logs the information given. Main processing loop which will compute statistics until the queue of scheduled work units has been drained. Drives the statistics generation. <p> This method will be run in a separate thread, and it will keep working as long as there is work to do. When the queue is exhausted, the method will exit (the thread dies). Runs the statistics update sequence explicitly as requested by the user. Schedules an update of the index statistics for the specified table. <p> Assume the descriptor will be valid until we get around to generate the statistics. If it turns out to be invalid, it will be discarded. Sets the row estimate for the heap conglomerate. Puts the current thread to sleep for maximum {@code ms} milliseconds. <p> No guarantee is provided for the minimum amount of time slept. If interrupted, the interrupt flag will be set again. Stops the daemon. <p> Will also clear the queue and print runtime statistics to the log the first time the method is invoked. Updates the index statistics for the given table and the specified indexes. <p> <strong>API note</strong>: Using {@code null} to update the statistics for all conglomerates is preferred over explicitly passing an array with all the conglomerates for the table. Doing so allows for some optimizations, and will cause a disposable statistics check to be performed. Writes updated statistics for the specified index to the data dictionary.
Asserts that the expected number of statistics exists for the specified index. Asserts that there are no existing statistics in the database. Asserts that there are no existing statistics for the specified table. Asserts that the expected number of statistics exists. Asserts that the expected number of statistics exists for the specified table. Waits until all given statistics entries have been changed, or until the call times out. <p> <em>NOTE</em>: The method is built on the assumption that the UUIDs of statistics objects aren't reused. That is, when statistics are updated, the old row in SYS.SYSSTATISTICS will be dropped and a new row will be inserted. Builds a human readable representation of a list of statistics objects. Builds an array of statistics objects from data from the {@code SYS.SYSSTATISTICS} system table. Generates a map from ids to names for conglomerates in the database. <p> Convenience method, used for better reporting. Waits for the current statistics to disappear and expects to fetch the same number of new statistics for the table. Obtains all existing statistics entries. Obtains statistics for the specified index. Obtains statistics for the specified index, fails if the number of statistics objects isn't as expected within the timeout. Obtains statistics for the specified table. Obtains statistics for the specified table, fails if the number of statistics objects isn't as expected within the timeout. Prints all entries in the {@code SYS.SYSSTATISTICS} system table. Queries the system table {@code SYS.SYSSTATISTICS} for statistics associated with a specific table or index. Releases resources and closes the associated connection. Releases resources.
Accept the visitor for all visitable children of this node.   Decrement (query block) level (0-based) for this FromTable. This is useful when flattening a subquery.  Generation of an IndexToBaseRowNode creates an IndexRowToBaseRowResultSet, which uses the RowLocation in the last column of an index row to get the row from the base conglomerate (heap). Fill in the column mapping for those columns coming from the index.  Return whether or not the underlying FBT is for NOT EXISTS. Return whether or not the underlying ResultSet tree will return a single row, at most. This is important for join nodes where we can save the extra next on the right side if we know that it will return at most 1 row. Return whether or not the underlying ResultSet tree is ordered on the specified columns. RESOLVE - This method currently only considers the outermost table of the query block. Get the lock mode for the target of an update statement (a delete or update).  The update mode will always be row for CurrentOfNodes.  It will be table if there is no where clause.
position is 1-based Turn the ExecRow into an ExecIndexRow. position is 1-based Row interface position is 1-based Get the array form of the row that Access expects. Get a clone of the array form of the row that Access expects. this is the actual current # of columns ExecIndexRow interface Reset all columns in the row array to null values. position is 1-based.  class interface
Return the default stream. If the default stream could not be set up as requested then it points indirectly to System.err.
Loggable methods Mark the page as valid, and clear out any crud from the page Override PageBasicOperation's getPageForRedoRecovery If we are in load tran, this page may not exist for the container yet. We need to create it first. This routine is called as the last resort of find page, the container handle has already been found and it is not dropped. Return my format identifier. Read this in PageBasicOperation methods restore the before image of the page PhysicalPageOperation method Mark the page as free Write this out.
** This method abstracts exception message printing for all exception messages. You may want to change ****it if more detailed exception messages are desired. ***Method is synchronized so that the output file will contain sensible stack traces that are not ****mixed but rather one exception printed at a time This starts the acutal inserts end of startInserts()
Backup the container. There is no support to backup this type of containers. It may not be any real use for users because users can simply  make copies of the read only database in Zip files easily using OS utilities. Write out the header information for this container. If an i/o exception occurs then ... * Container creation, opening, and closing Create a new container, all references to identity must be through the passed in identity, this object will no identity until after this method returns. Encrypts or decrypts the container. <p> These operations are unsupported for this type of container. Get an input stream positioned at the beginning of the file end of openContainer Preallocate page. * Methods used solely by StoredPage Read a page into the supplied array. <BR> MT - thread safe Read the page at the positioned offset. This default implementation, opens the stream and skips to the offset and then reads the data into pageData. Remove the container. Write a page from the supplied array. <BR> MT - thread safe
Determine whether the named file is writable. If the named file does not already exist then create it as an empty normal file. The implementation must synchronize with other threads accessing the same file (in the same or a different process). If two threads both attempt to create a file with the same name at the same time then at most one should succeed. Deletes the named file or empty directory. This method does not delete non-empty directories. Deletes the named file and, if it is a directory, all the files and directories it contains. Tests whether the named file exists. Get an exclusive lock with this name. This is used to ensure that two or more JVMs do not open the same database at the same time. Creates an input stream from a file name.  Creates an output stream from a file name. If a normal file already exists with this name it will first be truncated to zero length. Creates an output stream from a file name. If a normal file already exists with this name it will first be truncated to zero length. Get the name of the parent directory if this name includes a parent. Get the parent of this file. Converts this StorageFile into a pathname string. The character returned by StorageFactory.getSeparator() is used to separate the directory and file names in the sequence. <p> <b>The returned path may include the database directory. Therefore it cannot be directly used to make an StorageFile equivalent to this one.</b> end of getPath Get a random access file. Tests whether the named file is a directory, or not. This is only called in writable storage factories. Get the names of all files and sub-directories in the directory named by this path name. Creates the named directory. Creates the named directory, and all nonexistent parent directories. Release the resource associated with an earlier acquired exclusive lock Rename the file denoted by this name. Note that StorageFile objects are immutable. This method renames the underlying file, it does not change this StorageFile object. The StorageFile object denotes the same name as before, however the exists() method will return false after the renameTo method executes successfully. <p>It is not specified whether this method will succeed if a file already exists under the new name. Make the named file or directory read-only. This interface does not specify whether this also makes the file undeletable. Get the file name for diagnostic purposes. Usually the same as getPath().
Read a number of bytes into an array. Read a number of bytes into an array. Keep reading in a loop until len bytes are read or EOF is reached or an exception is thrown. Return the number of bytes read. (InputStream.read(byte[],int,int) does not guarantee to read len bytes even if it can do so without reaching EOF or raising an exception.) Read an unsigned byte from an InputStream, throwing an EOFException if the end of the input is reached. Skips requested number of bytes, throws EOFException if there is too few bytes in the stream. Tries harder to skip the requested number of bytes. <p> Note that even if the method fails to skip the requested number of bytes, it will not throw an exception. If this happens, the caller can be sure that end-of-stream has been reached. Skips until EOF, returns number of bytes skipped.
Get the 0-based position of the autogenerated column gets the increment value for a column. gets the row location gets the name of the desired column in the taget table. get the array of column names in the target table. Gets the name of the schema that the table is in Gets the name of the table being inserted into Get the formatID which corresponds to this class. Does the target table has autoincrement columns. INTERFACE METHODS Formatable methods Write this object to a stream of stored objects.
This is the method that is invoked from the outer query
Accept the visitor for all visitable children of this node. Bind this InsertNode.  This means looking up tables and columns and getting their types, and figuring out the result types of all expressions, as well as doing view resolution, permissions checking, etc. <p> Binding an insert will also massage the tree so that the collist and select column order/number are the same as the layout of the table in the store. Process ResultSet column lists for projection and autoincrement. This method recursively descends the result set node tree. When it finds a simple result set, it processes any autoincrement columns in that rs by calling checkAutoIncrement. When it finds a compound result set, like a Union or a PRN, it recursively descends to the child(ren) nodes. Union nodes can arise due to multi-rows in VALUES clause), PRN nodes can arise when the set of columns being inserted is a subset of the set of columns in the table. In addition to checking for autoincrement columns in the result set, we may need to enhance and re-order the column list to match the column list of the table we are inserting into. This work is handled by ResultsetNode.enhanceRCLForInsert. Note that, at the leaf level, we need to enhance the RCL first, then check for autoincrement columns. At the non-leaf levels, we have to enhance the RCL, but we don't have to check for autoincrement columns, since they only occur at the leaf level. This way, all ColumnDescriptor of all rows will be set properly. Code generation for insert creates an expression for: ResultSetFactory.getInsertResultSet(resultSet.generate(ps), generationClausesResult, checkConstrainResult, this ) Get the list of indexes on the table being inserted into.  This is used by INSERT.  This is an optimized version of what UPDATE and DELETE use. Create a boolean[] to track the (0-based) columns which are indexed. Return the type of statement, something from StatementType. Return the statement type, where it is dependent on the targetProperties.  (insertMode = replace causes statement type to be BULK_INSERT_REPLACE. Compile constants that Execution will use {@inheritDoc } <p> Remove any duplicate ORDER BY columns and push an ORDER BY if present down to the source result set, before calling super.optimizeStatement. </p> Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Return true if the node references SESSION schema tables (temporary or permanent) Request bulk insert optimization at run time. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Do the bind time checks to see if bulkInsert is allowed on this table.  bulkInsert is disallowed at bind time for: o  target databases o  (tables with triggers?) (It is disallowed at execution time if the table has at least 1 row in it or if it is a deferred mode insert.) Verify that the target properties that we are interested in all hold valid values. NOTE: Any target property which is valid but cannot be supported due to a target database, etc. will be turned off quietly.
Loggable methods  methods to support prepared log the following two methods should not be called during recover Return my format identifier. Read this in LogicalUndoable methods Restore the row stored in the optional data of the log record. method to support BeforeImageLogging restore the before image of the page DEBUG: Print self. PageOperation methods Undo the insert by simply marking the just inserted record as deleted. All logical undo logic has already been taken care of by generateUndo. Write this out. Writes out the row that is to be inserted as the optional data.
Do the work for a bulk insert * Bulk Referential Integrity Checker TargetResultSet interface   Clean up resources and call close on data members. Empty the indexes after doing a bulk insert replace where the table has 0 rows after the replace. RESOLVE: This method is ugly!  Prior to 2.0, we simply scanned back across the table to build the indexes.  We changed this in 2.0 to populate the sorters via a call back as we populated the table.  Doing a 0 row replace into a table with indexes is a degenerate case, hence we allow ugly and unoptimized code. Run the check constraints against the current row. Raise an error if a check constraint is violated, unless all the offending checks are deferred, in which case a false value will be returned. A NULL value will be interpreted as success (not violation). If user didn't provide columns list for auto-generated columns, then only include columns with auto-generated values in the resultset. Those columns would be ones with default value defined. Get an exclusive table lock on the target table (and check to see if the table is populated if this is not a bulk insert replace). Identity generation logic for bulk-insert used in pre-10.11 databases. Identity generation logic used in pre-10.11 databases. getSetAutoincrementValue will get the autoincrement value of the columnPosition specified for the target table. If increment is non-zero we will also update the autoincrement value. Get me a table scan result set, preferably a bulk table scan, thank you.  If we already have one, reopen it. The implementation of this method is slightly different than the one in UpdateResultSet. This code was originally written for insert but with DERBY-6414, we have started supporting update of auto generated column with keyword DEFAULT. The reason of different implementation is that the array used in the following method, namely, ColumnDescriptors in resultDescription hold different entries for insert and update case. For insert case, the array holds the column descriptors of all the columns in the table. This is because all the columns in the table are going to get some value into them whether or not they were included directly in the actual INSERT statement. The 2nd array, rla has a spot for each of the columns in the table, with non null value for auto generated column. But in case of Update, resultDescription does not include all the columns in the table. It only has the columns being touched by the Update statement(the rest of the columns in the table will retain their original values), and for each of those touched columns, it has a duplicate entry in resultDescription in order to have before and after values for the changed column values. Lastly, it has a row location information for the row being updated. This difference in array content of resultDescription requires us to have separate implementation of this method for insert and update. checks if source result set is a RowResultSet type. Is sourceResultSet a RowResultSet (values clause)? Make a template row with the correct columns. Do the work for a "normal" insert  Preprocess the source row.  Apply any check constraints here. Do an inplace cloning of all key columns.  For triggers, if we have a before row trigger, we fire it here if we can. This is useful for bulk insert where the store stands between the source and us. <p> Special handling if this is an INSERT action of a MERGE statement. </p> Set the estimated row count for this table. Set up to update all of the indexes on a table when doing a bulk insert on an empty table. Update all of the indexes on a table when doing a bulk insert on an empty table. Class implementation Verify that bulkInsert is allowed on this table. The execution time check to see if bulkInsert is allowed simply consists of checking to see if this is not a deferred mode insert and that the table is empty if this is not replace. A side effect of calling this method is to get an exclusive table lock on the table.
end of cleanUp Class implementation end of finish  end of normalInsertCore
Create an instance of a class.
/////////////////////////////////////////////////////////////////////////////////  Comparable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  FUNCTIONS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  OTHER Object OVERRIDES  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  Externalizable BEHAVIOR  /////////////////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////////////////  OTHER PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  RestrictedVTI BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  ResultSet OVERRIDES  ///////////////////////////////////////////////////////////////////////////////// Return true if the qualification succeeds on the current row
Gets the number of characters in the Clob. Gets the number of characters in the Clob if it is already known. <p> This method will not do any work to obtain the length if it isn't already known. Due to special handling of zero in the code, this method will return {@code -1} if a length of zero is cached internally. <p> If a positive value is returned, it is expected to be equal to the actual length of the Clob (i.e., no stale values must be returned). Returns an internal reader for the Clob content, initialized at the specified character position. <p> This method can return a shared reader object, avoiding instantiation and repositioning costs for internal operations where the stream itself is not published to the end-user. One such example is {@code Clob.getSubString}. Returns a stream serving the raw bytes of the Clob. <p> Note that it is up to the caller of this method to handle the issue of encoding. There is no predetermined encoding associated with this byte stream, it is up to the Clob representation which one it uses. <p> This stream may be an internal store stream, and should not be directly published to the end user (returned through the JDBC API). There are three reasons for this: <ul> <li>the stream may be closed by the end user when it is not supposed to</li> <li>operations on the stream might throw exceptions we don't want to present to the end user unwrapped</li> <li>the stream may contain a Derby specific end-of-stream marker </li> </ul> <p> The primary use of this method is to clone the Clob contents without going via char (or String). Make sure the clone uses the same encoding as the original Clob representation. Returns a reader for the Clob content, initialized at the specified character position. Returns the update count of the Clob. <p> The update count is increased each time a modification of the Clob content is made. Returns a writer to write data into the Clob. <p> The semantics of the writer is the same as for {@link #insertString}. Inserts the given string at the specified character position. <p> The behavior of this method can be defined by the following examples on the Clob <code>clob</code> with value <code>"ABCDEFG"</code>; <ul> <li><code>clob.setString(2, "XX")</code> - "AXXDEFG" <li><code>clob.setString(1, "XX")</code> - "XXCDEFG" <li><code>clob.setString(8, "XX")</code> - "ABCDEFGXX" <li><code>clob.setString(7, "XX")</code> - "ABCDEFXX" <li><code>clob.setString(9, "XX")</code> - throws exception </ul> Tells if the the Clob has been released. <p> Depending on the context, a Clob is released either because the internal representation has been changed, or because the Clob itself has been closed. The former can happen when a user modifies a stream that is currently represented as a store stream. The latter can happen if {@code Clob.free} has been called, or if Derby implicitly closes the Clob. Tells if the Clob representation is intended to be writable. <p> Note that even if this method returns <code>true</code>, it might not be possible to write to the Clob. If this happens, it is because the assoicated database is read-only, and the internal Clob representation is unable to obtain the resources it require (could be an area on disk to write temporary data). Frees the resources held by the internal Clob representation. <p> After calling this method, all other operations on the Clob will be invalid and throw an exception.
* Methods from java.sql.Driver *	Methods from ModuleControl Checks for shutdown System Privileges. To perform this check the following policy grant is required <ul> <li> to run the encapsulated test: permission javax.security.auth.AuthPermission "doAsPrivileged"; </ul> or a SQLException will be raised detailing the cause. <p> In addition, for the test to succeed <ul> <li> the given user needs to be covered by a grant: principal org.apache.derby.authentication.SystemPrincipal "..." {} <li> that lists a shutdown permission: permission org.apache.derby.security.SystemPermission "shutdown"; </ul> or it will fail with a SQLException detailing the cause. Checks for System Privileges. * This method can be called by AutoloadedDriver so that we * don't accidentally boot Derby while answering the question "Can * you handle this URL?" Privileged service lookup. Must be private so that user code can't call this entry point. * URL manipulation Convert all the attributes in the url into properties and combine them with the set provided. <BR> If the caller passed in a set of attributes (info != null) then we set that up as the default of the returned property set as the user's set. This means we can easily break the link with the user's set, ensuring that we don't hang onto the users object. It also means that we don't add our attributes into the user's own property object. returns the authenticationService handle Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Get the database name from the url. Copes with three forms jdbc:derby:dbname jdbc:derby:dbname;... jdbc:derby:;subname=dbname Check whether {@code AutoloadedDriver} should deregister itself on shutdown. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Methods to be overloaded in sub-implementations such as a tracing driver. Get a new nested connection. Create and return an EmbedPooledConnection from the received instance of EmbeddedDataSource. Create and return an EmbedXAConnection from the received instance of BasicEmbeddedDataSource40. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// <p>The getPropertyInfo method is intended to allow a generic GUI tool to discover what properties it should prompt a human for in order to get enough information to connect to a database.  Note that depending on the values the human has supplied so far, additional values may become necessary, so it may be necessary to iterate though several calls to getPropertyInfo. Privileged module lookup. Must be private so that user code can't call this entry point. Return true if this driver is active. Package private method. Return a new BrokeredConnection for this implementation.  Return a new java.sql.DatabaseMetaData instance for this implementation.  Return a new java.sql.ResultSet instance for this implementation. Returns a new java.sql.ResultSetMetaData for this implementation * methods to be overridden by subimplementations wishing to insert * their classes into the mix. Process exceptions raised while running a timed login. Indicate to {@code AutoloadedDriver} whether it should deregister itself on shutdown. Enforce the login timeout.
Cleanup the trigger execution context.  <B>MUST</B> be called when the caller is done with the trigger execution context. <p> We go to somewhat exaggerated lengths to free up all our resources here because a user may hold on to a TEC after it is valid, so we clean everything up to be on the safe side. Copy a map of auto increment values into the trigger execution context hash table of auto increment values. Make sure that the user isn't trying to get a result set after we have cleaned up. Get the text of the statement that caused the trigger to fire. Get the type for the event that caused the trigger to fire. Like getAfterResultSet(), but returns a result set positioned on the first row of the before result set.  Used as a convenience to get a column for a row trigger.  Equivalent to getAfterResultSet() followed by next(). Returns a result set row the new images of the changed rows. For a row trigger, the result set will have a single row.  For a statement trigger, this result set has every row that has changed or will change.  If a statement trigger does not affect a row, then the result set will be empty (i.e. ResultSet.next() will return false). Like getBeforeResultSet(), but returns a result set positioned on the first row of the before result set.  Used as a convenience to get a column for a row trigger.  Equivalent to getBeforeResultSet() followed by next(). Returns a result set row the old images of the changed rows. For a row trigger, the result set will have a single row.  For a statement trigger, this result set has every row that has changed or will change.  If a statement trigger does not affect a row, then the result set will be empty (i.e. ResultSet.next() will return false). Get the target table UUID upon which the trigger event is declared. ///////////////////////////////////////////////////////  TriggerExectionContext  /////////////////////////////////////////////////////// Get the target table name upon which the trigger event is declared. Reset Autoincrement counters to the beginning or the end. Update auto increment counters from the last row inserted. ///////////////////////////////////////////////////////  ExecutionStmtValidator  /////////////////////////////////////////////////////// Make sure that whatever statement is about to be executed is ok from the context of this trigger. <p> Note that we are sub classed in replication for checks for replication specific language.
* Methods of RawTransaction Internal transactions don't allow logical operations. *	Implementation specific methods  Yes, we do want to be rolled back first in recovery. * Methods of Transaction Savepoints are not supported in internal transactions.

Privileged lookup of a Context. Must be private so that user code can't call this entry point. Checks if the thread has been interrupted in NIO, presumably because we saw an exception indicating this. Make a note of this and clear the thread's interrupt status flag (NIO doesn't clear it when throwing) so we can retry whatever we are doing. It will be set back ON before control is transferred back to the application, cf. {@code restoreIntrFlagIfSeen}. <p/> The note that we saw an interrupt is stored in the lcc if available, if not, in thread local {@code exception}. Check if the we ever noticed and reset the thread's interrupt status flag to allow safe operation during execution.  Called from JDBC API methods before returning control to user application. Typically, this happens just prior to return in methods that catch {@code Throwable} and invoke {@code handleException} (directly or indirectly) on it, e.g. <pre> : InterruptStatus.restoreIntrFlagIfSeen(); return ...; } catch (Throwable t) { throw handleException(t); } </pre> {@code handleException} does its own calls to {@code restoreIntrFlagIfSeen}. If {@code setupContextStack} has been called consider using the overloaded variant of {@code restoreIntrFlagIfSeen} with an lcc argument. <p/> If an interrupt status flag was seen, we set it back <em>on</em> here. Same purpose as {@code restoreIntrFlagIfSeen()}. This variant presumes we are sure we have a {@code lcc != null}, i.e. {@code setupContextStack} has been called and not yet restored.  Note that we cannot merge this code with {@code restoreContextStack}, since that is typically called in a {@code finally} block, at which point in time, the {@code lcc} may be gone due to errors of severity {@code SESSION_SEVERITY} or {@code DATABASE_SEVERITY}. <p/> If no {@code lcc} is available, use the zero-arg variant. We only need this variant for performance reasons. Use when lcc is dying to save info in thread local instead. Useful under shutdown. Make a note that this thread saw an interrupt. Thread's intr status flag is presumably off already, but we reset it here also. Use lcc if available, else thread local variable. Check if the we ever noticed and reset the thread's interrupt status flag to allow safe operation during execution, or if the interrupt status flag is set now.  Called when operations want to be prematurely terminated due to interrupt. <p/> If an interrupt status flag was seen, but temporarily switched off, we set it back ON here.
Add any new ResultSetNodes that are necessary to the tree. We wait until after optimization to do this in order to make it easier on the optimizer. end of addNewNodes  End of estimateCost Generate the code. end of generate  end of getRowCountEstimate   Push order by lists down to the children so that we can implement the intersect/except by scan of the two sorted inputs. end of preprocess end of pushOrderingDown
Returns the cause of this exception or <code>null</code> if no cause was specified when this exception was created. Returns the filter string that generated the <code>InvalidSyntaxException</code> object. The cause of this exception can only be set when constructed.
Loggable methods Mark the page as being invalidated Return my format identifier. Read this in PageBasicOperation restore the before image of the page PhysicalPageOperation Mark the page as being valid If this page can be reused in the same transaction (of if committed transaction needs to be undone, then we need the before image of the page.  Right now, the transaction that deallocate a page must commit before the page can be freed and reused, so we don't need to log the before image of the page
Bind this logical operator.  All that has to be done for binding a logical operator is to bind the operands, check that both operands are BooleanDataValue, and set the result type to BooleanDataValue. Finish putting an expression into conjunctive normal form.  An expression tree in conjunctive normal form meets the following criteria: o  If the expression tree is not null, the top level will be a chain of AndNodes terminating in a true BooleanConstantNode. o  The left child of an AndNode will never be an AndNode. o  Any right-linked chain that includes an AndNode will be entirely composed of AndNodes terminated by a true BooleanConstantNode. o  The left child of an OrNode will never be an OrNode. o  Any right-linked chain that includes an OrNode will be entirely composed of OrNodes terminated by a false BooleanConstantNode. o  ValueNodes other than AndNodes and OrNodes are considered leaf nodes for purposes of expression normalization. In other words, we won't do any normalization under those nodes. In addition, we track whether or not we are under a top level AndNode. SubqueryNodes need to know this for subquery flattening. Eliminate NotNodes in the current query block. We just mark whether this IS node is under an eliminated NOT node. Do code generation for this logical binary operator. Do the 1st step in putting child expressions into conjunctive normal form.  This step ensures that the top level of the child expression is a chain of AndNodes terminated by a true BooleanConstantNode. Verify that changeToCNF() did its job correctly.  Verify that: o  AndNode  - rightOperand is not instanceof OrNode leftOperand is not instanceof AndNode o  OrNode	- rightOperand is not instanceof AndNode leftOperand is not instanceof OrNode Verify that putAndsOnTop() did its job correctly.  Verify that the top level of the expression is a chain of AndNodes terminated by a true BooleanConstantNode.
Bind a ? parameter operand of the IS [NOT] NULL predicate.      Negate the comparison.  null operators are defined on DataValueDescriptor. Overrides method in UnaryOperatorNode for code generation purposes.      RelationalOperator interface
Return a new DataSource corresponding to the current configuration. The getPooledConnection() method is configured to use the user name and password from the configuration. Create a new DataSource object setup from the passed in TestConfiguration. The getPooledConnection() method is configured to use the user name and password from the configuration. Return a new XA DataSource corresponding to the current configuration. The getXAConnection() method is configured to use the user name and password from the configuration. Create a new DataSource object setup from the passed in TestConfiguration. The getXAConnection() method is configured to use the user name and password from the configuration. Set a bean property for a data source. This code can be used on any data source type.
Return the string form of the URL for the jar file that contains whichever JAXP parser implementation is picked up from the user's classpath.  If the JAXP parser is not in the user's classpath, then it must be embedded within the JVM (either implicitly or else through use of "endorsed standards" jars), in which case we return null. NOTE: Assumption is that we only get here if we know there is in fact a JAXP parser available to the JVM.  I.e. if a call to the "classpathHasXalanAndJAXP()" method of junit/XML.java returns true.
Clear all of the bits in this JBitSet Test to see if one JBitSet contains another one of the same size. Get the first set bit (starting at index 0) from a JBitSet. Grow an existing JBitSet to the specified size. See of a JBitSet has exactly 1 bit set. Set the BitSet to have the exact same bits set as the parameter's BitSet. Return the size of bitSet Wrapper methods for BitSet's methods
Decrypt the secretKey with the user key . This includes the following steps, retrieve the encryptedKey, generate the muck from the boot password and generate an appropriate IV using the muck,and using the key and IV decrypt the encryptedKey Encrypt the secretKey with the boot password. This includes the following steps, getting muck from the boot password and then using this to generate a key, generating an appropriate IV using the muck using the key and IV thus generated to create the appropriate cipher provider and encrypting the secretKey Generate an IV using the input secretKey that can be used by JCECipherProvider to encrypt or decrypt. Generate a Key object using the input secretKey that can be used by JCECipherProvider to encrypt or decrypt. get the secretkey used for encryption and decryption when boot password mechanism is used for encryption Steps include retrieve the stored key, decrypt the stored key and verify if the correct boot password was passed There is a possibility that the decrypted key includes the original key and padded bytes in order to have been block size aligned during encryption phase. Hence extract the original key Use MD5 MessageDigest algorithm to generate checksum Initilize the new instance of this class. For block ciphers, and  algorithms using the NoPadding scheme, the data that has to be encrypted needs to be a multiple of the expected block size for the cipher Pad the key with appropriate padding to make it blockSize align access a file for either read/write access a InputStream for a given file for reading. put all the encyrpion cipger related properties that has to be made peristent into the database service properties list. @param  properties  properties object that is used to store cipher properties persistently. The database can be encrypted with an encryption key given in connection url. For security reasons, this key is not made persistent in the database. But it is necessary to verify the encryption key when booting the database if it is similar to the one used when creating the database This needs to happen before we access the data/logs to avoid the risk of corrupting the database because of a wrong encryption key. This method performs the steps necessary to verify the encryption key if an external encryption key is given. At database creation, 4k of random data is generated using SecureRandom and MD5 is used to compute the checksum for the random data thus generated.  This 4k page of random data is then encrypted using the encryption key. The checksum of unencrypted data and encrypted data is made persistent in the database in file by name given by Attribute.CRYPTO_EXTERNAL_KEY_VERIFYFILE (verifyKey.dat). This file exists directly under the database root directory. When trying to boot an existing encrypted database, the given encryption key is used to decrypt the data in the verifyKey.dat and the checksum is calculated and compared against the original stored checksum. If these checksums dont match an exception is thrown. Please note, this process of verifying the key  does not provide any added security but only is intended to allow to fail gracefully if a wrong encryption key is used StandardException is thrown if there are any problems during the process of verification of the encryption key or if there is any mismatch of the encryption key. <p> Verify that a decrypter matches an encrypter. Raises an exception if they don't. The verification is performed by encrypting a block of text and checking that it decrypts to the same block. </p>
Create an instance of the cipher factory.



Boot a system requesting a JDBC driver but only if there is no current JDBC driver that is handling the required protocol. Privileged startup. Must be private so that user code can't call this entry point.
Get ConnectionPoolDataSource class name. Get DataSource class name. Return the default embedded client for this JVM. Get JDBC driver class name. Get the name of the client Return the base JDBC url. The JDBC base url specifies the protocol and possibly the subprotcol in the JDBC connection string. Get XADataSource class name. Is this DB2's Universal JDBC Is this Derby's network client. Is this the embdded client. Return string representation of this object.

Clear a String Java bean property by setting it to null. Get a bean property for a data source. This code can be used on any data source type. Return a new DataSource corresponding to the current configuration. The getConnection() method will return a connection identical to TestConfiguration.openDefaultConnection(). Return a new DataSource corresponding to the current configuration except that the database name is different. Create a new DataSource object setup from the passed in TestConfiguration. The getConnection() method will return a connection identical to TestConfiguration.openDefaultConnection(). Create a new DataSource object setup from the passed in TestConfiguration using the received properties and data source class name. Return a DataSource corresponding to one of the logical databases in the current configuration. Return a DataSource object of the passed in type configured with the passed in Java bean properties. This will actually work with any object that has Java bean setter methods. <BR> If a thread context class loader exists then it is used to try and load the class. Create a HashMap with the set of Derby DataSource Java bean properties corresponding to the configuration. Set a bean property for a data source. This code can be used on any data source type. Shutdown the engine described by this data source. The shutdownDatabase property is cleared by this method. Shutdown the database described by this data source. The shutdownDatabase property is cleared by this method.
Print a banner containing the column labels separated with '|'s and a line of '-'s.  Each field is as wide as the display width reported by the metadata. Display the current row of the result set along with a banner. Assume the result set is on a row.   Fetch the next row of the result set, and if it exists format and display a banner and the row.  Pretty-print the results of a statement that has been executed. If it is a select, gathers and prints the results.  Display partial results up to the first error. If it is not a SELECT, determine if rows were involved or not, and print the appropriate message. DisplayRow Print one row of a result set, padding each field to the display width and separating them with '|'s DisplayRow  ================ ----------------------------------------------------------------- Methods for displaying and checking errors Print information about the exception to the given PrintWriter. For non-SQLExceptions, does a stack trace. For SQLExceptions, print a standard error message and walk the list, if any. Print information about the SQL exception to the given PrintWriter. Walk the list of exceptions, if any. ShowWarnings ShowResultSetWarnings ShowStatementWarnings Print information about the SQL warnings for the connection to the given PrintWriter. Walks the list of exceptions, if any. ShowWarnings Print information about the SQL warnings for the ResultSet to the given PrintWriter. Walk the list of exceptions, if any. ShowResultSetWarnings  Print information about the SQL warnings for the Statement to the given PrintWriter. Walks the list of exceptions, if any. ShowStatementWarnings Check if an object is null, and if it is, throw an exception with an informative parameter about what was null. The exception is a run-time exception that is internal to ij. checkNotNull If the property ij.exceptionTrace is true, display the stack trace to the print stream. Otherwise, do nothing. Calculates column display widths from the default widths of the result set. ----------------------------------------------------------------- Accessors ========================== Get an ij boolean system property. DisplayBanner DisplayBanner DisplayNextRow DisplayNextRow DisplayNextRow DisplayNextRow DisplayResults DisplayResults Map the string to the value if it is null.
Does the driver accept the passed in JDBC URL Get the JDBC driver's implementation level Return the JDBC driver's major version. Return the JDBC driver's minor version. Is the JDBC driver compliant.
Cache the prepared statement if it does not already exist. Retrieves a cached prepared statement if one exists.
Creates a URL for connecting to the platform MBean server on the host specified by the network server hostname of this test configuration. The JMX port number used is also retreived from the test configuration. Decorate a test to use JMX connections directly from the platform MBean Server. Decorate a test so to use JMX connections from the passed in url.
Get a connection to the platform MBean Server.
Start the management service if derby.system.jmx is true. <P> Starting the service means: <UL> <LI> getting the platform MBeanServer which may require starting it <LI> registering a Version mbean representing the system </UL> Require SystemPermission("jmx", "control") to change the management state. Initialize the management service by obtaining the platform MBeanServer and registering system beans. Separate from boot() to allow future changes where the jmx management can be enabled on the fly. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Register an mbean with the platform mbean server. Unregister an mbean from the JMX plaform server but leave it registered to this service. This is so that if jmx is reenabled we can reestablish all vaid mbeans (that are still registered with this service). Registers an MBean with the MBean server as a StandardMBean. Use of the StandardMBean allows the implementation details of Derby's mbeans to be hidden from JMX, thus only exposing the MBean's interface in org.apache.derby.mbeans. Unregister an mbean using an object previous returned from registerMBean. Unregisters an mbean from this service and JMX plaform server
Construct the initial JNDI directory context environment Properties object. We retrieve JNDI environment properties that the user may have set at the database level. To be OVERRIDEN by subclasses. This basically tests and sets default/expected JNDI properties for the JNDI provider scheme.
ModuleControl implementation (overriden)  Check if we should activate the JNDI authentication service.
What kind of type is this: If this is a JAVA_CLASS, what is it's name? Translate the name of a java primitive to an id If this is a JAVA_PRIMITIVE, what is its name? Give read-only access to array of strings What's our SQLTYPE? /////////////////////////////////////////////////////////////////////  Formatable BEHAVIOR  ///////////////////////////////////////////////////////////////////// Get the formatID which corresponds to this class. /////////////////////////////////////////////////////////////////////  GENERAL MINIONS  ///////////////////////////////////////////////////////////////////// Gets the name of the java wrapper class corresponding to a primitive. /////////////////////////////////////////////////////////////////////  INITIALIZATION MINIONS  ///////////////////////////////////////////////////////////////////// Initialize this JSQL type. Minion of all constructors.
Return Derby's understanding of the virtual machine's environment. Get system property. Determine whether we are running in a constrained environment. Check whether this is IBM jvm. Check whether this is sun jvm. For IBM jvm, this method will dump more diagnostic information to file. JVM specific code for other vender can be added. DERBY-4856
Tests whether the named file exists. end of exists Creates an input stream from a file name. end of getInputStream Get the name of the parent directory if this name includes a parent. Get the file name for diagnostic purposes. Usually the same as getPath().
Return the SQL name for the installed jar. Used for error and informational messages. Get a stream from a zip file that is itself a stream. We copy to the contents to a byte array and return a stream around that to the caller. Though a copy is involved it has the benefit of: <UL> <LI> Isolating the application from the JarInputStream, thus denying any possibility of the application reading more of the jar that it should be allowed to. E.g. the contents class files are not exposed through getResource. <LI> Avoids any possibility of the application holding onto the open stream beyond shutdown of the database, thus leading to leaked file descriptors or inability to remove the jar. </UL> * Routines to get an InputStream for a namedResource Get a stream for a resource directly from a JarFile. In this case we can safely return the stream directly. It's a new stream set up by the zip code to read just the contents of this entry.  Validate the security certificates (signers) for the class data. Get an InputStream for the given resource. Provide a SecurityManager with information about the class name and the jar file. Initialize the class loader so it knows if it is loading from a ZipFile or an InputStream Handle all requests to the top-level loader. Load the class data when the installed jar is accessible only as an input stream (the jar is itself in a database jar). Load and optionally resolve the class given its JarEntry and an InputStream to the class fiel format. This is common code for when the jar is accessed directly using JarFile or through InputStream. * Private api Load the class data when the installed jar is accessible as a java.util.jarFile. Read the raw data for the class file format into a byte array that can be used for loading the class. If this is a signed class and it has been compromised then a SecurityException will be thrown. Set this loader to be invaid so that it will not resolve any classes or resources. Return the jar name if toString() is called on this class loader.
Get the StorageFile for an installed jar file.
end of doInit end of getJarFile Construct a persistent StorageFile from a path name. Construct a StorageFile from a directory and file name. Construct a StorageFile from a directory and file name. Close the opened jar/zip file on shutdown. (Fix for DERBY-2083).
Add a jar file to the current connection's database. <P> The reason for adding the jar file in this private instance method is that it allows us to share set up logic with drop and replace. Drop a jar file from the current connection's database. <P> The reason for dropping  the jar file in this private instance method is that it allows us to share set up logic with add and replace. Drop a jar file from the current connection's database. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Get the FileInfoDescriptor for the Jar file or null if it does not exist. install a jar file to the current connection's database. Make an external name for a jar file stored in the database. Open an input stream to read a URL or a file. URL is attempted first, if the string does not conform to a URL then an attempt to open it as a regular file is tried. <BR> Attempting the file first can throw a security execption when a valid URL is passed in. The security exception is due to not have the correct permissions to access the bogus file path. To avoid this the order was reversed to attempt the URL first and only attempt a file open if creating the URL throws a MalformedURLException. Replace a jar file in the current connection's database with the content of an external file. <P> The reason for adding the jar file in this private instance method is that it allows us to share set up logic with add and drop. Replace a jar file from the current connection's database with the content of an external file. Copy the jar from the externally obtained input stream into the database Upgrade code: upgrade one jar file to new style (&gt;= 10.9)
/////////////////////////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Load or unload an optional tool package. If the tool name is the special CUSTOM_TOOL_CLASS_NAME tool, then the first optionalArg is the name of a user-supplied class which implements OptionalTool. </p> Lookup the class name corresponding to the name of an optional tool Privileged lookup of a Context. Must be private so that user code can't call this entry point. <p> For a custom tool, we strip the first arg from the list of optional args. By the time we get to this method, it has already been determined that there is at least one arg and it is the name of a class which implements OptionalTool. </p> /////////////////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////////////////
a class.  Once it is created, fields, methods, interfaces, static initialization code, and constructors can be added to it. <verbatim> Java: package #packageName; #modifiers #className extends #superClass { } // modifiers is the | of the JVM constants for // the modifiers such as static, public, etc. </verbatim>
Accept the visitor for all visitable children of this node. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. Do code generation for this conversion of a value from the Java to the SQL domain. Get the JavaValueNode that lives under this JavaToSQLValueNode. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) {@inheritDoc } Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression.
Toggles whether the code generator should add a cast to extract a primitive value from an object.  Check the reliability type of this java value. Do the code generation for this node.  Call the more general routine that generates expressions. General logic shared by Core compilation and by the Replication Filter compiler. Every child of ValueNode must implement one of these methods. Generate the expression that evaluates to the receiver. This is for the case where a java expression is being returned to the SQL domain, and we need to check whether the receiver is null (if so, the SQL value should be set to null, and this Java expression not evaluated). Instance method calls and field references have receivers, while class method calls and calls to constructors do not. If this Java expression does not have a receiver, this method returns null. The implementation of this method should only generate the receiver once and cache it in a field. This is because there will be two references to the receiver, and we want to evaluate it only once. Generate the expression that evaluates to the receiver. This is for the case where a java expression is being returned to the SQL domain, and we need to check whether the receiver is null (if so, the SQL value should be set to null, and this Java expression not evaluated). Instance method calls and field references have receivers, while class method calls and calls to constructors do not. If this Java expression does not have a receiver, this method returns null. This also covers the case where a java expression is being returned to the Java domain. In this case, we need to check whether the receiver is null only if the value returned by the Java expression is an object (not a primitive type). We don't want to generate the expression here if we are returning a primitive type to the Java domain, because there's no point in checking whether the receiver is null in this case (we can't make the expression return a null value). Only generate the receiver once and cache it in a field. This is because there will be two references to the receiver, and we want to evaluate it only once.   Get the resolved data type of this node. May be overridden by descendants. Get the JSQLType that corresponds to this node. Could be a SQLTYPE, a Java primitive, or a Java class. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Get an expression that has the value of the receiver. If a field holding the receiver value was already generated, use that.  If not, generate the receiver value. Map a JSQLType to a compilation type id. Mark this node as being for a CALL Statement. (void methods are only okay for CALL Statements) Tell this node that nothing is done with the returned value Reports whether the code generator should add a cast to extract a primitive value from an object.   Tell whether the return value from this node is discarded Inform this node that it returns its value to the SQL domain Set the collation type. This will be used to determine the collation type for the SQLToJavaValueNode. Tell whether this node returns its value to the SQL domain
<p> Return true if we are at least at the passed in version. </p>
other urls to some cute jira reports in xml. all (warning: avoid using this - it's tough on apache infrastructure. public static String jira_report="http://issues.apache.org/jira/secure/IssueNavigator.jspa?view=rss&pid=10594&sorter/field=issuekey&sorter/order=DESC&tempMax=5000&reset=true&decorator=none"; all open bugs public static String jira_BUG_OPEN="http://issues.apache.org/jira/secure/IssueNavigator.jspa?view=rss&pid=10594&types=1&statusIds=1&sorter/field=issuekey&sorter/order=DESC&tempMax=1000&reset=true&decorator=none"; one bug - the following two would be joined with in the middle a string like 'DERBY-225' to make the http to get xml for 1 bug. public static String onejirabegin="http://issues.apache.org/jira/browse/"; // public static String onejiraend="?decorator=none&view=rss";
Factory method which extracts a list of JiraIssue objects from a data file generated by the Derby JIRA SOAP client.        Predicate for finding out if issue has a given fixVersion.
Currently we don't reordering any outer join w/ inner joins. This method returns the table references in Join node, and this may be needed for LOJ reordering.  For example, we may have the following query: (T JOIN S) LOJ (X LOJ Y) The top most LOJ may be a join betw T and X and thus we can reorder the LOJs.  However, as of 10/2002, we don't reorder LOJ mixed with join. Accept the visitor for all visitable children of this node. Generate	and add any arguments specifict to outer joins. (Expected to be overriden, where appropriate, in subclasses.) Some types of joins (e.g. outer joins) will return a different number of rows than is predicted by optimizeIt() in JoinNode. So, adjust this value now. This method does nothing for most join types. Bind an expression against the child tables of the JoinNode. May update the subquery and aggregate lists in the JoinNode. Assumes that the subquery and aggregate lists for the JoinNode have already been created. Bind the expressions under this node. Bind the result columns for this ResultSetNode to a base table. This is useful for INSERT and UPDATE statements, where the result columns get their types from the table being updated or inserted into. If a result column list is specified, then the verification that the result column list does not contain any duplicates will be done when binding them by name. Bind the result columns of this ResultSetNode when there is no base table to bind them to.  This is useful for SELECT statements, where the result columns get their types from the expressions that live under them. Build the RCL for this node.  We propagate the RCLs up from the children and splice them to form this node's RCL. Extract all the column names from a result column list. Flatten this JoinNode into the outer query block. The steps in flattening are: o  Mark all ResultColumns as redundant, so that they are "skipped over" at generate(). o  Append the joinPredicates to the outer list. o  Create a FromList from the tables being joined and return that list so that the caller will merge the 2 lists For joins, the tree will be (nodes are left out if the clauses are empty): ProjectRestrictResultSet -- for the having and the select list SortResultSet -- for the group by list ProjectRestrictResultSet -- for the where and the select list (if no group or having) the result set for the fromList Generate the code for a qualified join node. Do the generation work for the join node hierarchy. Return a ResultColumnList with all of the columns in this table. (Used in expanding '*'s.) NOTE: Since this method is for expanding a "*" in the SELECT list, ResultColumn.expression will be a ColumnReference. Return a ResultColumnList with all of the columns in this table. (Used in expanding '*'s.) NOTE: Since this method is for expanding a "*" in the SELECT list, ResultColumn.expression will be a ColumnReference. NOTE: This method is handles the case when there is no USING clause. The caller handles the case when there is a USING clause. Generate a result column list with all the column names that appear on both sides of the join operator. Those are the columns to use as join columns in a natural join.  Get the arguments to the join result set. Return the logical left result set for this qualified join node. (For RIGHT OUTER JOIN, the left is the right and the right is the left and the JOIN is the NIOJ). Return the logical right result set for this qualified join node. (For RIGHT OUTER JOIN, the left is the right and the right is the left and the JOIN is the NIOJ). Try to find a ResultColumn in the table represented by this FromTable that matches the name in the given ColumnReference. Return the number of arguments to the join result set.  This will be overridden for other types of joins (for example, outer joins). Is this FromTable a JoinNode which can be flattened into the parents FromList. Return whether or not the underlying ResultSet tree is ordered on the specified columns. RESOLVE - This method currently only considers the outermost table of the query block. Convert the joinType to a string. Make a FromList for binding  Put the expression trees in conjunctive normal form Mark this node and its children as not being a flattenable join. Optimizable interface  Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Find the unreferenced result columns and project them out. This is used in pre-processing joins that are not flattened into the where clause. Push expressions down to the first ResultSetNode which can do expression evaluation and has the same referenced table map. RESOLVE - This means only pushing down single table expressions to DistinctNodes today.  Once we have a better understanding of how the optimizer will work, we can push down join clauses.  Flag this as a natural join so that an implicit USING clause will be generated in the bind phase. Transform any Outer Join into an Inner Join where applicable. (Based on the existence of a null intolerant predicate on the inner table.) Get the lock mode for the target of an update statement (a delete or update).  The update mode will always be row for CurrentOfNodes.  It will be table if there is no where clause.
ResultSet interface (leftover from NoPutResultSet)  Clear any private state that changes during scans. This includes things like the last row seen, etc. THis does not include immutable things that are typically set up in the constructor. <p> This method is called on open()/close() and reopen() If the result set has been opened, close the open scan. <n> <B>WARNING</B> does not track close time, since it is expected to be called directly by its subclasses, and we don't want to skew the times close the rightResultSet A join is combining rows from two sources, so it should never be used in a positioned update or delete.  CursorResultSet interface  A join is combining rows from two sources, so it has no single row location to return; just return a null. open a scan on the join. For a join, this means: o  Open the left ResultSet o  Do a getNextRow() on the left ResultSet to establish a position and get "parameter values" for the right ResultSet. NOTE: It is possible for the getNextRow() to return null, in which case there is no need to open the RightResultSet.  We must remember this condition. o  If the getNextRow() on the left ResultSet succeeded, then open() the right ResultSet. scan parameters are evaluated at each open, so there is probably some way of altering their values... Class implementation open the rightResultSet.  If already open, just reopen. reopen a a join.
Is it OK to use bulk fetch with this join strategy? Divide up the predicates into different lists for different phases of the operation. When this method is called, all of the predicates will be in restrictionList.  The effect of this method is to remove all of the predicates from restrictionList except those that will be pushed down to the store as start/stop predicates or Qualifiers.  The remaining predicates will be put into nonBaseTableRestrictionList. All predicate lists will be ordered as necessary for use with the conglomerate. Some operations (like hash join) materialize results, and so require requalification of rows when doing a non-covering index scan.  The predicates to use for requalification are copied into baseTableRestrictionList. Is materialization built in to the join strategy? Get the estimated cost for the join. Is this join strategy feasible under the circumstances? Get the base predicates for this join strategy.  The base predicates are the ones that can be used while scanning the table.  For some join strategies (for example, nested loop), all predicates are base predicates.  For other join strategies (for example, hash join), the base predicates are those that involve comparisons with constant expressions. Also, order the base predicates according to the order in the proposed conglomerate descriptor for the inner table. Get the name of this join strategy Get the operator symbol used to represent this join strategy in optimizer traces Get the appropriate arguments to the scan for this type of join. Get the name of the join result set method for the half outerjoin Should we just ignore bulk fetch with this join strategy? Is this a form of hash join? Get the name of the join result set method for the join  Returns true if the base cost of scanning the conglomerate should be multiplied by the number of outer rows. Get the extra selectivity of the non-base predicates (those that were left in the predicate list by getBasePredicates() that are not applied to the scan of the base conglomerate. NOTE: For some types of join strategy, it may not remove any predicates from the original predicate list.  The join strategy is expected to know when it does this, and to return 1.0 as the extra selectivity in these cases. Put back and base predicates that were removed from the list by getBasePredicates (see above). NOTE: Those join strategies that treat all predicates as base predicates may treat the get and put methods as no-ops. Get the name of the result set method for base table scans Get the costing type, for use with StoreCostController.getScanCost
Return the boolean value of a system property
Get the UUID of the backing index, if one exists. Gets the index conglomerate descriptor Gets the UUID of the backing index for the constraint. Gets the UUID String of the backing index for the constraint. Does this constraint have a backing index? Convert the SubConstraintDescriptor to a String.
Static method to return the object to hash on. (Object stored in specifed array, if only a single object, otherwise a KeyHasher wrapping the objects to hash on. (NOTE: We optimize for in-memory hash tables, hence we only create a wrapper when needed.) Get the object stored at the specified index. * Methods from java.lang.Object Set array element at the specified index to the specified object.
Adds this class to the *existing server* suite. Test killing master during replication.
Adds this class to the *existing server* suite. Test killing slave during replication.
Authenticate the passed-in user's credentials. We authenticate against a LDAP Server. Search for the full user's DN in the LDAP server. LDAP server bind may or not be anonymous. If the admin does not want us to do anonymous bind/search, then we must have been given principal/credentials in order to successfully bind to perform the user's DN search. Call new InitialDirContext in a privilege block This method basically tests and sets default/expected JNDI properties for the JNDI provider scheme (here it is LDAP).
Closes the file. Returns the current position of the file pointer. Get the {@code StorageFile} which represents the file where the contents of the LOB are stored. Returns length of the file. Reads len number of bytes from the file starting from off position in the buffer. Reads one byte from file. Sets the file pointer to a given position. Sets the file length to a given size. Writes a buffer completely into the file. Writes a segment of bytes into the file. Writes one bytes into the file.
Implementation of the PositionedStream interface: - asInputStream - getPosition - reposition Closes this input stream and releases any system resources associated with the stream. <p> The <code>close</code> method of <code>InputStream</code> does nothing. Returns the current byte position. Checks if underlying StreamControl has been updated. Returns size of stream in bytes. Reinitializes the stream and sets the current pointer to zero. Reads the next byte of data from the input stream. The value byte is returned as an <code>int</code> in the range <code>0</code> to <code>255</code>. If no byte is available because the end of the stream has been reached, the value <code>-1</code> is returned. This method blocks until input data is available, the end of the stream is detected, or an exception is thrown. <p> A subclass must provide an implementation of this method. Reads up to <code>len</code> bytes of data from the input stream into an array of bytes.  An attempt is made to read as many as <code>len</code> bytes, but a smaller number may be read. The number of bytes actually read is returned as an integer. <p> This method blocks until input data is available, end of file is detected, or an exception is thrown. <p> If <code>b</code> is <code>null</code>, a <code>NullPointerException</code> is thrown. <p> If <code>off</code> is negative, or <code>len</code> is negative, or <code>off+len</code> is greater than the length of the array <code>b</code>, then an <code>IndexOutOfBoundsException</code> is thrown. <p> If <code>len</code> is zero, then no bytes are read and <code>0</code> is returned; otherwise, there is an attempt to read at least one byte. If no byte is available because the stream is at end of file, the value <code>-1</code> is returned; otherwise, at least one byte is read and stored into <code>b</code>. <p> The first byte read is stored into element <code>b[off]</code>, the next one into <code>b[off+1]</code>, and so on. The number of bytes read is, at most, equal to <code>len</code>. Let <i>k</i> be the number of bytes actually read; these bytes will be stored in elements <code>b[off]</code> through <code>b[off+</code><i>k</i><code>-1]</code>, leaving elements <code>b[off+</code><i>k</i><code>]</code> through <code>b[off+len-1]</code> unaffected. <p> In every case, elements <code>b[0]</code> through <code>b[off]</code> and elements <code>b[off+len]</code> through <code>b[b.length-1]</code> are unaffected. <p> If the first byte cannot be read for any reason other than end of file, then an <code>IOException</code> is thrown. In particular, an <code>IOException</code> is thrown if the input stream has been closed. <p> The <code>read(b,</code> <code>off,</code> <code>len)</code> method for class <code>InputStream</code> simply calls the method <code>read()</code> repeatedly. If the first such call results in an <code>IOException</code>, that exception is returned from the call to the <code>read(b,</code> <code>off,</code> <code>len)</code> method.  If any subsequent call to <code>read()</code> results in a <code>IOException</code>, the exception is caught and treated as if it were end of file; the bytes read up to that point are stored into <code>b</code> and the number of bytes read before the exception occurred is returned.  Subclasses are encouraged to provide a more efficient implementation of this method. Repositions the stream to the requested byte position.
Closes this output stream and releases any system resources associated with this stream. The general contract of <code>close</code> is that it closes the output stream. A closed stream cannot perform output operations and cannot be reopened. <p> The <code>close</code> method of <code>OutputStream</code> does nothing. Writes <code>len</code> bytes from the specified byte array starting at offset <code>off</code> to this output stream. The general contract for <code>write(b, off, len)</code> is that some of the bytes in the array <code>b</code> are written to the output stream in order; element <code>b[off]</code> is the first byte written and <code>b[off+len-1]</code> is the last byte written by this operation. <p> The <code>write</code> method of <code>OutputStream</code> calls the write method of one argument on each of the bytes to be written out. Subclasses are encouraged to override this method and provide a more efficient implementation. <p> If <code>b</code> is <code>null</code>, a <code>NullPointerException</code> is thrown. <p> If <code>off</code> is negative, or <code>len</code> is negative, or <code>off+len</code> is greater than the length of the array <code>b</code>, then an <tt>IndexOutOfBoundsException</tt> is thrown. Writes the specified byte to this output stream. The general contract for <code>write</code> is that one byte is written to the output stream. The byte to be written is the eight low-order bits of the argument <code>b</code>. The 24 high-order bits of <code>b</code> are ignored. <p> Subclasses of <code>OutputStream</code> must provide an implementation for this method.
Checks the current row, updating state and releasing locators on the server as required. <p> This method should only be called once per valid row in the result set. Discards all recorded dynamic state about LOBs. <p> Typically called after connection commit or rollback, as those operations will release all locators on the server automatically. There is no need to release them from the client side in this case. Marks the specified column of the current row as published, which implies that the tracker should not release the associated locator. <p> Columns must be marked as published when a LOB object is created on the client, to avoid releasing the corresponding locator too early.
Creates a new empty Blob and registers it in the HashMap in the Connection and returns the locator value corresponding to this Blob. Reads up to len bytes from the associated {@code Blob} and returns a byte array containing the bytes read. <p> Note that a smaller number of bytes than requested might be returned. The number of bytes returned can be found by checking the length of the returned byte array. Returns the length in bytes of the Blob. Returns the first occurrence of the byte array in the Blob. Returns the first occurrence of locator in the Blob. Removes the supplied LOCATOR entry from the hash map. Replaces the bytes at pos with len bytes truncates the Blob value represented by LOCATOR to have a length of length. Creates a new empty Clob and registers it in the HashMap in the Connection and returns the locator value corresponding to this Clob. returns the length of the Clob corresponding to the LOCATOR value. returns the first occurrence of the given search string from the given start search position inside the Clob. returns the first occurrence of the given search string from the given start search position inside the Clob. Returns the {@code String} starting from {@code pos} and consisting of up to {@code len} consecutive characters from the {@code Clob} corresponding to {@code LOCATOR}. Removes the supplied LOCATOR entry from the hash map. replaces the characters starting at fromPosition and with length ForLength truncates the Clob value represented by LOCATOR to have a length of length. returns the Blob object corresponding to the locator. returns the Clob object corresponding to the locator. Returns the EmbedConnection object. Generate the SQLException with the appropriate SQLState.
Copies bytes from stream to local storage. <p> Note that specifying the length as {@code Long.MAX_VALUE} results in reading data from the stream until EOF is reached, but no length checking will be performed. Copies UTF-8 encoded chars from a stream to local storage. <p> Note that specifying the length as {@code Long.MAX_VALUE} results in reading data from the stream until EOF is reached, but no length checking will be performed.  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  Privileged service lookup. Must be private so that user code can't call this entry point. Privileged startup. Must be private so that user code can't call this entry point. Invalidates all the variables and closes file handle if open. returns input stream linked with this object. Returns length of data. returns output stream linked with this object Returns the running sequence number to check if the lob is updated since last access. Reads bytes starting from 'position' into bytes array. starting from 'offset' Reads one byte. Close and release all resources held by a temporary file. The file will also be deleted from the file system and removed from the list of {@code LOBFile}s in {@code EmbedConnection}. Replaces a block of bytes in the middle of the LOB with a another block of bytes, which may be of a different size. <p> The new byte array may not be be of same length as the original, thus it may result in resizing the total lob. Resets the size. Writes {@code len} bytes from the specified byte array to the LOB. Writes one byte.
Tell whether this type (LOB) is compatible with the given type. Tell whether this type (LOB) can be converted to the given type.    Tell whether this type (LOB) can be stored into from the given type.
Return a suite of language SQL tests from the list of script names. Each test is surrounded in a decorator that cleans the database. Run a set of language SQL scripts (.sql files) passed in on the command line. Note the .sql suffix must not be provided as part of the script name. <code> example java org.apache.derbyTesting.functionTests.tests.lang.LangScripts case union </code> Return the suite that runs all the langauge SQL scripts.
Add the activation to those known about by this connection. Add the declared global temporary table to the list of temporary tables known by this connection. Create an autoincrement counter to be used on behalf of a SQL-J statement. The counter is identified by (schemaName, tableName, columnName). The counter must be freed up by calling autoincrementFlushCache at the end of the statement. It is expected that a ai-counter with the same signaure doesn't exist when the method is called. Flush the cache of autoincrement values being kept by the lcc. This will result in the autoincrement values being written to the SYSCOLUMNS table as well as the mapping used by lastAutoincrementValue Begin a nested transaction. Check if there are any global temporary tables declared for this connection. Check that deferred constraints are valid, if not roll back the transaction. Close any unused activations in this connection context. commit a nested transaction. We do not provide a abortNestedTransaction. If a nested xaction is aborted, then this results in the parent xaction also being aborted. This is not what we need for releasing compile time locks or autoincrement-- hence we do not provide abortNestedTransaction. Copy a map of autoincrement key value pairs into the cache of ai values stored in the language connection context. Create a fresh SQLSessionContext for this connection. Return true if the data dictionary is in write mode (that is, this context was informed that is is in write mode by the method call setDataDictionaryWriteMode(). Decrement the DataDictionary bind count. Drop (mark the declared global temporary table for dropping) from the list of temporary tables known by this connection. Get the Visitor which should walk the AST. Return the number of activation known for this connection. Get an Authorizer for this connection. Returns the current value of autoincrementUpdate. Get the DataDictionary bind count. Get the current isolation level. Get the current isolation level in DB2 format. Get the current role authorization identifier of the dynamic call context associated with this activation. Get the current role authorization identifier in external delimited form (not case normal form) of the dynamic call context associated with this activation. Get the SQL session context of the given activation. Get the current schema name (at compile-time, see explanations for getDefaultSchema overloads). Get the current schema name (at execution time, see explanations for getDefaultSchema overloads); This version is used by CURRENT SCHEMA. Get the Authorization Id of the current user Get the data dictionary Get the data value factory to use with this language connection context. Returns the Database of this connection. Get the database name of this LCC. Get the default schema (used at compile-time when no activation is yet available, cf. the activation argument overload version. Get the default schema (used at execution time).  At execution time, the current statement context is not always a reliable place to find the correct SQL session context, viz. when a dynamic result set referencing CURRENT SCHEMA is accessed after a called procedure has returned only the activation of the call is live and still holds the correct session context. Get the set of disk backed hash tables containing any index rows saved for deferred unique/PK constraints in this transaction, keyed by the conglomerate id, or rows saved containing row locations violating rows for deferred check constraints. Get the DRDA ID of this LCC. Get the identity column value most recently generated. Get the instance number of this LCC. Get exception created when we detected interruped status flag. Get the language connection factory to use with this language connection context. Get the language factory to use with this language connection context. Return the last activation added This is used to find the drop activation in dropViewCascade so we can add warning messages to the activation Debug method for retrieving the last query tree. get the lock escalation threshold to use with this connection. Get value of logQueryPlan. (Whether or not to write query plan info on currently executing statement to error log.) Get value of logStatementText. (Whether or not to write info on currently executing statement to error log.) get the optimizer factory to use with this language connection context. Get the optimizer tracer (could be null if we aren't tracing the optimizer). Get the prepare isolation level. If the isolation level has been explicitly set with a SQL statement or embedded call to setTransactionIsolation, this will return TransactionControl.UNSPECIFIED_ISOLATION_LEVEL SET ISOLATION always takes priority. Return a map of AST nodes that have already been printed during a compiler phase, so as to be able to avoid printing a node more than once. Get the referenced column map for a table Get the RUNTIMESTATISTICS mode. Get the RUNTIMESTATISTICS object. Get the Authorization Id of the session user Get the current StatementContext. Reports how many statement levels deep we are. Get the STATISTICS TIMING mode. Get table descriptor for the declared global temporary table from the list of temporary tables known by this connection. Get the value of top level session context of the top level connection. Get the transaction controller to use with this language connection context at compile time. Get the transaction controller to use with this language connection context during execute time. Get the topmost tec. Get the topmost trigger table descriptor Get a connection unique system generated name for a cursor. Get a connection unique system generated id for an unnamed savepoint. Get a connection unique system generated name for an unnamed savepoint. gets the current set XplainOnlyMode gets the current set XplainSchema Increment the DataDictionary bind count.  This is for keeping track of nested binding, which can happen if SQL statements are bound from within static initializers. Initialize. For use after pushing the contexts that initialization needs. Do a commit, as internally needed by Derby.  E.g. a commit for sync, or a commit for autocommit.  Skips checks that a user isn't doing something bad like issuing a commit in a nested xact. Similar to internalCommit() but has logic for an unsynchronized commit Do a rollback, as internally needed by Derby.  E.g. a rollback for sync, or a rollback for an internal error.  Skips checks that a user isn't doing something bad like issuing a rollback in a nested xact. Let the context deal with a rollback to savepoint Determines if a check or foreign key constraint has deferred mode. Return true if this schema name is the initial default schema for the current session. Returns true if isolation level has been set using JDBC/SQL. Get the readOnly status for the current connection. Reports whether there is any outstanding work in the transaction. Sets a savepoint. Causes the Store to set a savepoint. Returns the last autoincrement value inserted by this connection. If no values have been inserted into the given column a NULL value is returned. See if a given cursor is available for use.  This is used to locate the cursor during its execution. Mark the passed temporary table as modified in the current unit of work. That information will be used at rollback time The compile phase will generate code to call this method if the DML is on a temporary table returns the <b>next</b> value to be inserted into an autoincrement col. This is used internally by the system to generate autoincrement values which are going to be inserted into a autoincrement column. This is used when as autoincrement column is added to a table by an alter table statemenet and during bulk insert. Make a note that some activations are marked unused Get whether or not optimizer trace is on. Pop a CompilerContext off the context stack. Remove the validator.  Does an object identity (validator == validator) comparison.  Asserts that the validator is found. If returning from a routine that can execute SQL, perform any actions needed when popping the SQL session context. Pop a StatementContext of the context stack. Remove the tec.  Does an object identity (tec == tec) comparison.  Asserts that the tec is found. Remove the trigger table descriptor. Return a PreparedStatement object for the query. This method first tries to locate the PreparedStatement object from a statement cache.  If the statement is not found in the cache, the query will be compiled and put into the cache. The schema used when compiling the statement is the same schema as returned by getDefaultSchema().  For internal statements, the read only status is set to true. Calling this method is equivalent to calling prepareExternalStatement(lcc.getDefaultSchema(), sqlText, true); Return a PreparedStatement object for the query. This method first tries to locate the PreparedStatement object from a statement cache.  If the statement is not found in the cache, the query will be compiled and put into the cache. Push a CompilerContext on the context stack with the current default schema as the default schema which we compile against. Push a CompilerContext on the context stack with the passed in default schema as the default schema we compile against. Push a new execution statement validator.  An execution statement validator is an object that validates the current statement to ensure that it is permitted given the current execution context. An example of a validator a trigger ExecutionStmtValidator that doesn't allow ddl on the trigger target table. <p> Multiple ExecutionStmtValidators may be active at any given time. This mirrors the way there can be multiple connection nestings at a single time.  The validation is performed by calling each validator's validateStatement() method.  This yields the union of all validations. Create a new SQL session context for the current activation on the basis of the existing SQL session context. This happens when a stored procedure or function that can contain SQL is invoked, cf. SQL 2003 section 4.27.3, since this gives rise to a nested connection. <p> Called from generated code, see {@link org.apache.derby.impl.sql.compile.StaticMethodCallNode#generatePushNestedSessionContext}. <p> The new SQL session context is also set in the current statement context (of the invocation). Push a StatementContext on the context stack. Push a new trigger execution context. <p> Multiple TriggerExecutionContexts may be active at any given time. Set the trigger table descriptor.  Used to compile statements that may special trigger pseudo tables. Let the context deal with a release of a savepoint Remove the activation from those known about by this connection. Reset the connection before it is returned (indirectly) by a PooledConnection object. See EmbeddedConnection. Reset the isolation level flag used to keep correct isolation level state in BrokeredConnection. This resetting will happen at the start and end of a global transaction, after the BrokeredConection's isolation level state is brought upto date with the EmbedConnection's isolation state. The flag gets set to true when isolation level is set using JDBC/SQL. Reset any occurence of schemaName as current default schema in the SQLSessionContext stack to the initial default, because schemaName is no longer a valid schema. Checks whether the given role can be legally set for the current user. This method will read (potentially) the dictionary, so it needs a transaction context. Set a Visitor which walks the AST at various stages. This is useful for poking user-written inspectors into the parse, bind, and optimize phases. Sets autoincrementUpdate-- this variable allows updates to autoincrement columns if it is set to true. The default is ofcourse false; i.e ai columns cannot be directly modified by the user. This is set to true by AlterTableConstantAction, when a new ai column is being added to an existing table. Set the constraint mode for this constraint to {@code deferred}. If {@code deferred} is {@code false}, to immediate checking, if {@code true} to deferred checking. Set the current role Remember that the DataDictionary is in write mode, so we can take it out of write mode at the end of the transaction. Set the default schema (at execution time, see explanations for getDefaultSchema overloads); This version is used by SET SCHEMA. Set the default schema (at compile-time, see explanations for getDefaultSchema overloads). Set the constraint mode of all deferrable constraints to the value of {@code deferred}. If the value is {@code false}, this method might throw with a constraint violation error, i.e. if some constraint has deferred mode before this call and had seen violations. Set the DRDA ID of this LCC. Set the field of most recently generated identity column value. Set the exception created and associated with the detected interruped status flag. Set current isolation level. Debug method for remembering the last query tree. Set value of logStatementText (Whether or not to write info on currently executing statement to error log.) Install an optimizer tracer (to enable tracing) or uninstall the current optimizer tracer (to disable tracing). Set the readOnly status for the current connection. This can only be called when the current transaction has not done any work. Set the referenced column map for a table Turn RUNTIMESTATISTICS  on or off. Set the RUNTIMESTATISTICS object. Turn STATISTICS TIMING on or off. sets the XplainOnlyMode. If a connection is in XplainOnlyMode, then the statements are not actually being executed, but are just being compiled and the runtime statistics collected into the XPLAIN tables. This can be set on and off by calling SYSCS_SET_XPLAIN_MODE. sets the XplainSchema Used when a statement as part of its operation executes an other statement. In contrast to pushNestedSessionContext, the activation (for the substatement) just inherits the current session context from the parent statements activation, it does <b>not</b> push a new copy on the stack of session contexts. Currently, this is used in the following situations: <ul> <li>With {@code ALTER TABLE} adding a column which has a default values, the default value for all the existing rows is added using an {@code UPDATE} substatement. <li>With {@code ALTER TABLE} adding a a check constraint, we will use a substatement {@code SELECT} to check if all rows satisfy the constraint. <li>{@code ResultSet.insertRow}, {@code updateRow} and {@code deleteRow}. <li>During trigger body execution. </ul> Do a commit, as issued directly by a user (e.g. via Connection.commit() or the JSQL 'COMMIT' statement. Do a rollback, as issued directly by a user (e.g. via Connection.rollback() or the JSQL 'ROLLBACK' statement. Check if in SQL standard mode, with support for Grant and Revoke Validate a statement.  Does so by stepping through all the validators and executing them.  If a validator throws and exception, then the checking is stopped and the exception is passed up. Verify that there are no activations with open held result sets. Verify that there are no activations with open result sets on the specified prepared statement. Commit a distrubuted transaction. Roll back a distrubuted transaction.
Get the ClassFactory to use with this language connection Get the DataValueFactory to use with this language connection This is expected to get stuffed into the language connection context and accessed from there. Get the ExecutionFactory to use with this language connection Get the JavaFactory to use with this language connection Get the OptimizerFactory to use with this language connection Get the PropertyFactory to use with this language connection Get a Statement Get the TypeCompilerFactory to use with this language connection Get the UUIDFactory to use with this language connection Get a new LanguageConnectionContext. this holds things we want to remember about activity in the language system, where this factory holds things that are pretty stable, like other factories. <p> The returned LanguageConnectionContext is intended for use only by the connection that requested it.
Privileged lookup of a Context. Must be private so that user code can't call this entry point.
Get a new result description Get a new result description from the input result description.  Picks only the columns in the column array from the inputResultDescription. Get a ParameterValueSet

///////////////////////////////////////////////////  ResultSet interface (leftover from NoPutResultSet)  /////////////////////////////////////////////////// Can we get instantaneous locks when getting share row locks at READ COMMITTED. If the result set has been opened, close the open scan. This result set has its row from the last fetch done. If the cursor is closed, a null is returned. Return the next row (if any) from the scan (if open). Return the total amount of time spent in this ResultSet open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
Get the compatibility space the latch is held in. Gte the object the latch is held on. Get the qualifier used when the latch was obtained.
Returns the number of bytes that can be read (or skipped over) from this input stream without blocking by the next caller of a method for this input stream. <p> This subclass implements this method by calling available on the current buffer, which is a ByteInputStreamReader. Reads the next byte of data from the input stream. <p> This subclass of InputStream implements this method by reading the next byte from the current buffer. If there is more data, it will be requested a new buffer from the DDMReader. Reads up to <code>len</code> bytes of data from the input stream into an array of bytes.  An attempt is made to read as many as <code>len</code> bytes, but a smaller number may be read, possibly zero. The number of bytes actually read is returned as an integer. This subclass implements this method by calling this method on the current buffer, which is an instance of ByteArrayInputStream. If the current buffer does not have any data, it will be requested a new buffer from the DDMReader.
Private/Protected methods of This class: Allocate a new leaf page to the conglomerate. * Debug/consistency check Methods of ControlRow: * Perform consistency checks on a leaf page. * * Check consistency of the page and its children, * returning the number of pages seen, and throwing * errors if inconsistencies are found. * The checks specific to a leaf page are: * <menu> * <li> Page is at level 0. * <li> Version is a valid leaf page version. * <li> Control row has right number of columns for leaf. * </menu> * This method also performs the consistency checks that * are common to both leaf and branch pages. * @see ControlRow#checkGeneric * * @exception StandardException Standard exception policy. Public Methods of LeafControlRow class: Perform page specific initialization. <p> Return the left child pointer for the page. <p> Leaf pages don't have children, so they override this and return null. * Non - Debug/consistency check Methods of ControlRow: Get the number of columns in the control row. <p> Control rows all share the first columns as defined by this class and then add columns to the end of the control row.  For instance a branch control row add a child page pointer field. <p> Return the right child pointer for the page. <p> Leaf pages don't have children, so they override this and return null. Methods of TypedFormat: Return my format identifier. Return the number of non-deleted rows from slot 1 through "startslot" <p> Return the number of non-deleted rows that exist on the page starting at slot one through "startslot". <p> RESOLVE (mikem) - is the expense of this routine worth it, it is only used for costing.  Could an estimate from the nonDeletedRecordCount() be used instead? * Grow a new root page from a leaf page.  Slightly * tricky because we want to retain page 0 as the root. * <P> * On entry, the current leaf root page is expected * to be latched.  On exit, all latches will have been * released. * <P> * The caller cannot not assume success.  If we have to release latches * this routine just returns and assumes the caller will retry the * grow root if necessary. Initialize conglomerate with one page, to be a 1 page btree. Given a conglomerate which already has one page allocated to it, initialize the page to be a leaf-root page with no entries.  Allocate the control row and store it on the page. Is the current page the leftmost leaf of tree? <p> Is the current page the rightmost leaf of tree? <p> * Recursively print the tree starting at current node in tree. * This is a leaf so return. * Perform a search of this leaf page, ultimately returning the latched * leaf page and row slot after which the given key belongs. * The slot is returned in the result structure.  If the key * exists on the page, the result.exact will be true.  Otherwise, * result.exact will be false, and the row slot returned will be * the one immediately preceding the position at which the key * belongs. Search and return the left most leaf page. <p> Perform a recursive search, ultimately returning the leftmost leaf page which is the first leaf page in the leaf sibling chain.  (This method might better be called getFirstLeafPage()). Search and return the right most leaf page. <p> Perform a recursive search, ultimately returning the rightmost leaf page which is the last leaf page in the leaf sibling chain.  (This method might better be called getLastLeafPage()). *	Perform a recursive shrink operation for the key. * If this method returns true, the caller should * remove the corresponding entry for the page. * This routine is not guaranteed to successfully * shrink anything.  The page lead to by the key might * turn out not to be empty by the time shrink gets * there, and shrinks will give up if there is a deadlock. * <P> * The receiver page must be latched on entry and is * returned unlatched. Perform a top down split pass making room for the the key in "row". <p> Perform a split such that a subsequent call to insert given the argument index row will likely find room for it.  Since latches are released the client must code for the case where another user has grabbed the space made available by the split pass and be ready to do another split. <p> On entry, the parent is either null or latched, and the current page is latched.  On exit, all pages will have been unlatched.  If the parent is null, then this page is a root leaf page.
Bind this operator Bind a ? parameter operand of the XXX_length function. This is a length operator node.  Overrides this method in UnaryOperatorNode for code generation purposes.
If the character in val matches the character in pat, then it does not matter if the database is UCS_BASIC or territory based, we simply return TRUE from the method. If the characters do not match and we are running with UCS_BASIC collation, then we will return FALSE. But if the database is territory based, then we want to use the Collator for the territory to determine if the Collator treats the 2 characters as equal (ie if their collation elements match, then the 2 characters are equal even if they  are not the same character). checkLengths Returns null if we are not done. Returns true if we are at the end of our value and pattern Returns false if there is more pattern left but out of input value Calculate the shortest length string that could match this pattern greaterEqualString -- for Escape clause only Walk the pattern character by character Return the substring from the pattern for the optimization &gt;= clause. Return whether or not the like comparison is still needed after performing the like transformation on a constant string.  The comparison is not needed if the constant string is of the form: CONSTANT%  (constant followed by a trailing %) Methods for LIKE transformation at preprocess time: Determine whether or not this LIKE can be transformed into optimizable clauses.  It can if the pattern is non-null and if the length == 0 or the first character is not a wild card. Return the substring from the pattern for the &lt; clause. This method gets called for UCS_BASIC and territory based character string types to look for a pattern in a value string. It also deals with escape character if user has provided one. Most typical interface for character string types with UCS_BASIC and territory based collation. For character string types with UCS_BASIC and territory based collation. Pad a string with null characters, in order to make it &gt; and &lt; comparable with SQLChar. stripEscapesNoPatternChars
Bind this operator implement binding for like expressions. <p> overrides BindOperatorNode.bindExpression because like has special requirements for parameter binding. Do code generation for this binary operator. This code was copied from BinaryOperatorNode and stripped down Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed.
Clear any limit set by setLimit. After this call no limit checking will be made on any read until a setLimit()) call is made. Set the limit of the data that can be read or written. After this call up to and including length bytes can be read from or skipped in the stream. <P> On input classes (e.g. InputStreams) any attempt to read or skip beyond the limit will result in an end of file indication (e.g. read() methods returning -1 or throwing EOFException). <P> On output classes (e.g. OutputStream) any attempt to write more beyond the limit will result in an EOFException
Clear any limit set by setLimit. After this call no limit checking will be made on any read until a setLimit()) call is made. This stream doesn't support mark/reset, independent of whether the underlying stream does so or not. <p> The reason for not supporting mark/reset, is that it is hard to combine with the limit functionality without always keeping track of the number of bytes read. Set the limit of the stream that can be read. After this call up to and including length bytes can be read from or skipped in the stream. Any attempt to read more than length bytes will result in an EOFException

Clear any limit set by setLimit. After this call no limit checking will be made on any read until a setLimit()) call is made. return limit of the stream that can be read without throwing EOFException Set the limit of the stream that can be read. After this call up to and including length characters can be read from or skipped in the stream. Any attempt to read more than length characters will result in an EOFException

/////////////////////////////////////////////////////////////////////////////////  TABLE FUNCTION METHOD  ///////////////////////////////////////////////////////////////////////////////// <p> This is the method which is registered as a table function. </p> /////////////////////////////////////////////////////////////////////////////////  FlatFileVTI BEHAVIOR TO BE IMPLEMENTED BY SUBCLASSES  ///////////////////////////////////////////////////////////////////////////////// <p> Parse the next chunk of text, using readLine(), and return the next row. Returns null if the file is exhausted. </p>
Follow the initial database population requirements in Section 4.3.3 and populate all the required tables. BE CAREFUL to use the correct starting identifiers for the data in the tables. In the specification, identifiers start at 1 (one), e.g. 1-10 for a district and is not zero based. Set the seed for the random number generator used to populate the data. Useful for testing to ensure consistent repeatable runs. If not set, defaults a value based upon current time. Must be called before setupLoad to have an effect. Set the number of total threads the loader is allowed to use to load data. If an implementation does not support multiple threads then the passed in value will be ignored. Perform the necessary setup before database population.
Raised if, the Derby database connection is null. Raised if, there is data found between the stop delimiter and field/record spearator. Raised if, data file exists. Raised if, the passed data file can't be found. Raised if, null is passed for data file url. Raised if same delimiter character is used for more than one delimiter type . For eg using ';' for both column delimter and character delimter Raised if, the entity (ie table/view) for import/export is missing in the database. Raised if, got IOException while writing data to the file. Raised if, field and record separators are substring of each other. Raised if, no column by given name is found in the resultset while importing. Raised if, no column by given number is found in the resultset while importing. Raised if, lob file exists. Raised if, trying to export/import from an entity which has non supported type columns in it. Raised if period(.) is used a character delimiter Raised if, in case of fixed format, don't find the record separator for a row in the data file. Raised if, in case of fixed format, reach end of file before reading data for all the columns. Wrapper to throw an unknown excepton duing Import/Export. Typically this can be some IO error which is not generic error like the above error messages.
Initialize the load generator. Print a report from the test run. Move from the warmup phase to the steady-state phase. Start collecting results. Start the warmup phase. This means that the load generator is started, but the results are not collected. Stop the load generator.
* Methods for subclass * Public methods from Generated Class
Calls SqlLength() to check if the Locator associated with the underlying Lob is valid. If it is not it throws an exception. Checks the <code>pos</code> and <code>length</code>. Checks if isValid is true and whether the transaction that created the Lob is still active. If any of which is not true throws a SQLException stating that a method has been called on an invalid LOB object. ----------------------------helper methods---------------------------------- Get locator for this Lob Get the length of locator based Lob from the server.  This is a dummy implementation that is supposed to be overridden by subclasses.  A stored procedure call will be made to get the length from the server. Returns the current updateCount of the Clob. Increments and returns the new updateCount of this <code>Lob</code>. The method needs to be synchronized since multiple updates can happen on this <code>Lob</code> simultaneously. It will be called from the 1) Locator Writers 2) Locator OutputStreams 3) From the update methods within the Lobs like setString, truncate. since all of the above acesses are inside the am package, this method will have default access. We do not need to worry about the non-locator streams since non-locator InputStreams would not depend on updateCount for invalidation Check whether this Lob is based on a locator -----------------------event callback methods------------------------------- Method to be implemented by subclasses, so that #materializedStream(InputStream, String) can be called with subclass specific parameters and the result assigned to the right stream. Materialize the given stream into memory and update the internal length variable. Update the registered length of the Lob value.  To be called by methods that make changes to the length of the Lob. NOTE: The caller needs to deal with synchronization. ---------------------------jdbc 2------------------------------------------ Return the length of the Lob value represented by this Lob object.  If length is not already known, and Lob is locator based, length will be retrieved from the server.  If not, locator based, Lob will first be materialized.  NOTE: The caller needs to deal with synchronization.

Write more messages to the standard output if property tvtdebug is true. Check that the file exists Check for characters in the range 0x00-0x1f (which are ASCII) and 0x7f-0xff If found, suggest native2ascii modification of file   Find uppercase strings in a string passed in Write message to the standard output. check that strings have doubled quotes. Note that we're passing in 'Character' but it's really only thought about for quotes/apostrophes. If the apostrophes are single FAIL, not OK. (except for tools) Note that we already know there *are* quotes in the string check that strings without replacements only have single quotes Note that we're passing in 'Character' but it's really only thought about for quotes. If the apostrophes are doubled, flag a warning. Note that it may still be ok, we just want to know. This is only relevant in the toolsmessages files.

Get a formatter for formatting dates. The implementation may cache this value, since it never changes for a given Locale. Get a formatter for formatting times. The implementation may cache this value, since it never changes for a given Locale. Get a formatter for formatting timestamps. The implementation may cache this value, since it never changes for a given Locale.


Get a new LocalizedOutput with the given encoding. Resets the 'local' field to null. This is not needed for normal operations, however, when executing sql files in our junit tests, we use the same jvm and thus the locale will get loaded only once, resulting in trouble when testing the localization for ij. get the message file resource according to the locale fall back to English message file if locale message file is not found
//////////////////////////////////////////////////////////////  ACCESSORS  ////////////////////////////////////////////////////////////// Get the array of column values Flatten this LocatedRow into a DataValueDescriptor[] where the last cell contains the RowLocation. //////////////////////////////////////////////////////////////  STATIC BEHAVIOR  ////////////////////////////////////////////////////////////// Append a RowLocation to the end of a column array Get the RowLocation
make a copy of this lock with the count set to zero, copies are only to be used in the LockSpace code. EXCLUDE-END-lockdiag- Return the compatibility space this lock is held in. MT - Thread safe Return the count of locks. MT - Thread safe * Methods of Control Return the object this lock is held on MT - Thread safe Return the qualifier lock was obtained with. MT - Thread safe * Methods of object EXCLUDE-START-lockdiag- We can return ourselves here because our identity is immutable and what we returned will not be accessed as a Lock, so the count cannot be changed.
Add a lock into this control, granted it if possible. This can be entered in several states. </OL> <LI>The Lockable is locked (granted queue not empty), and there are no waiters (waiting queue is empty) <LI>The Lockable is locked and there are waiters <LI>The Lockable is locked and there are waiters and the first is potentially granted <LI>The Lockable is unlocked and there are waiters and the first is potentially granted. Logically the item is still locked, it's just that the lock has just been released and the first waker has not woken up yet. </OL> This call is never entered when the object is unlocked and there are no waiters. 1) The Lockable has just been unlocked, EXCLUDE-END-lockdiag- Add a lock request to a list of waiters. * Deadlock support. Add the waiters of this lock into this Map object. <BR> Each waiting thread gets two entries in the hashtable <OL> <LI>key=compatibility space - value=ActiveLock <LI>key=ActiveLock - value={LockControl for first waiter|ActiveLock of previosue waiter} </OL> <p> Returns true if the childLock is blocked because its parent owns a conficting lock. This code was written to support the fix to DERBY-6554. The only known way that this condition arises is when a write attempt by a nested user transaction is blocked by a read lock held by the main transaction. This only happens while trying to write to SYS.SYSSEQUENCES while managing sequence generators. </p> Return the first lock in the wait line, null if the line is empty. Return a Stack of the held locks (Lock objects) on this Lockable. Find a granted lock matching this space and qualifier Return the lockable object controlled by me. Get the next waiting lock (if any). Give up waiting up on a lock Grant this lock.  This routine can be called to see if a lock currently on the wait list could be granted. If this lock has waiters ahead of it then we do not jump over the waiter(s) even if we can be granted. This avoids the first waiter being starved out. Remove and return the first lock request from a list of waiters. Remove and return the lock request at the given index from a list of waiters. Remove and return the given lock request from a list of waiters. EXCLUDE-START-lockdiag- make a shallow clone of myself package
Returns true if locks held by anyone are blocking anyone else Return true if locks are held in this compatibility space. Return true if locks are held in this compatibility space and this group. Clear a limit set by setLimit. Create an object which can be used as a compatibility space. A compatibility space object can only be used in the <code>LockFactory</code> that created it. Get the lock timeout in milliseconds. A negative number means that there is no timeout. Check to see if a specific lock is held. Lock an object within a compatibility space and associate the lock with a group object, waits up to timeout milli-seconds for the object to become unlocked. A timeout of 0 means do not wait for the lock to be unlocked. Note the actual time waited is approximate. <P> A compatibility space in an space where lock requests are assumed to be compatible and granted by the lock manager if the trio {compatibilitySpace, ref, qualifier} are equal (i.e. reference equality for qualifier and compatibilitySpace, equals() method for ref). Granted by the lock manager means that the Lockable object may or may not be queried to see if the request is compatible. <BR> A compatibility space is not assumed to be owned by a single thread. Make a virtual lock table for diagnostics. Install a limit that is called when the size of the group exceeds the required limit. <BR> It is not guaranteed that the callback method (Limit.reached) is called as soon as the group size exceeds the given limit. If the callback method does not result in a decrease in the number of locks held then the lock factory implementation may delay calling the method again. E.g. with a limit of 500 and a reached() method that does nothing, may result in the call back method only being called when the group size reaches 550. <BR> Only one limit may be in place for a group at any time. Transfer a set of locks from one group to another. Unlock a single lock on a single object held within this compatibility space and locked with the supplied qualifier. Unlock all locks in a group. Unlock all locks on a group that match the passed in value. Lock an object with zero duration within a compatibility space, waits up to timeout milli-seconds for the object to become unlocked. A timeout of 0 means do not wait for the lock to be unlocked. Note the actual time waited is approximate. <P> Zero duration means the lock is released as soon as it is obtained. <P> A compatibility space in an space where lock requests are assumed to be compatible and granted by the lock manager if the trio {compatibilitySpace, ref, qualifier} are equal (i.e. reference equality for qualifier and compatibilitySpace, equals() method for ref). Granted by the lock manager means that the Lockable object may or may not be queried to see if the request is compatible. <BR> A compatibility space is not assumed to be owned by a single thread.
<p> Return true if this is a nested owner, e.g., a nested user transaction. </p> <p> Return true if this owner nests under another owner. </p> Tells whether lock requests should time out immediately if the lock cannot be granted at once, even if {@code C_LockFactory.TIMED_WAIT} was specified in the lock request. <p> Normally, this method should return {@code false}, but in some very special cases it could be appropriate to return {@code true}. One example is when a stored prepared statement (SPS) is compiled and stored in a system table. In order to prevent exclusive locks in the system table from being held until the transaction that triggered the compilation is finished, the SPS will be compiled in a nested transaction that is committed and releases all locks upon completion. There is however a risk that the transaction that triggered the compilation is holding locks that the nested transaction needs, in which case the nested transaction will time out. The timeout will be detected by the calling code, and the operation will be retried in the parent transaction. To avoid long waits in the cases where the nested transaction runs into a lock conflict with its parent, the nested transaction's {@code LockOwner} instance could return {@code true} and thereby making it possible to detect lock conflicts instantly.
Add a lock to a group. Return true if locks are held in this compatibility space. Return true if locks are held in a group Clear a limit set by setLimit. Return a count of the number of locks held by this space. The argument bail indicates at which point the counting should bail out and return the current count. This routine will bail if the count is greater than bail. Thus this routine is intended to for deadlock code to find the space with the fewest number of locks. Get the object representing the owner of the compatibility space. Unlock all the locks in a group and then remove the group. Unlock all locks in the group that match the key
* Private methods Convert the lock information into a hashtable.  VTI costing interface   All columns in TransactionTable VTI are of String type.
INTERFACE METHODS This is the guts of the Execution-time logic for LOCK TABLE. OBJECT METHODS
Bind this LockTableNode.  This means looking up the table, verifying it exists and getting the heap conglomerate number. Create the Constant information that will drive the guts of Execution. Return true if the node references SESSION schema tables (temporary or permanent) Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.

If this lockable object wants to participate in a diagnostic virtual lock table, then put any relevant attributes of this lock into the attributes list (the attribute must be an immutable object).  The list of attributes of interest to the virtual lock table can be found in VirtualLockTable. The toString method will be called by the VirtualTable on the attribute value for display. <P> Note the fact the object is locked. Performs required actions to ensure that unlockEvent() work correctly. This method does not actually  perform any locking of the object, the locking mechanism is provided by the lock manager. <P> If the class supports multiple lockers of the object then this method will be called once per locker, each with their own qualifier. <P> Must only be called by the lock manager. Synchronization will be handled by the lock manager. Returns true if any lock request on a Lockable L in a compatibility space CS1 is compatible with any other lock held on L in CS1. Return true if the requested qualifier is compatible with the already granted qualifier. Note that the object has been unlocked <P> Must only be called by the lock manager. Synchronization will be handled by the lock manager.
Get the mode of this policy Called when a container is opened. Called before a record is fetched. Called before a record is inserted, updated or deleted. If zeroDuration is true then lock is released immediately after it has been granted. Called when a container is closed. Called after a record has been fetched. Request a write lock which will be released immediately upon grant.
Append a log record to a byte[]. Typically, the byte[] will be currentBuffer, but if a log record that is too big to fit in a buffer is added, buff will be a newly allocated byte[]. The database is being marked corrupted, get rid of file pointer without writing out anything more. Write data from all dirty buffers into the log file. <p> A call for clients of LogAccessFile to insure that all privately buffered data has been writen to the file - so that reads on the file using one of the various scan classes will see all the data which has been writen to this point. <p> Note that this routine only "writes" the data to the file, this does not mean that the data has been synced to disk unless file was opened in WRITE SYNC mode(rws/rwd).  The only way to insure that is by calling is to call syncLogAccessFile() after this call in Non-WRITE sync mode(rw) <p> MT-Safe : parallel thereads can call this function, only one threads does the flush and the other threads waits for the one that is doing the flush to finish. Currently there are two possible threads that can call this function in parallel 1) A Thread that is doing the commit 2) A Thread that is writing to the log and log buffers are full or a log records does not fit in a buffer. (Log Buffers full(switchLogBuffer() or a log record size that is greater than logbuffer size has to be writtern through writeToLog call directlty) Note: writeToLog() is not synchronized on the semaphore that is used to do  buffer management to allow writes to the free buffers when flush is in progress. flush all the the dirty buffers to disk Return the length of a checksum record reserve the space for the checksum log record in the log file. Make this LogAccessFile pass chunks of log records (byte[]) to the MasterFactory when the chunks are written to disk. Method to put this LogAccessFile object in replication slave mode, effectively disabling checksum writes. Because checksums are received from the replication master, the slave can not be allowed to add it's own checksums - that would invalidate the checksums and would stop the database from recovering. Replication slave mode must therefore be set before LogAccessFile decides whether to write it's own checksums, and this method is therefore indirectly called from the constructor of this class by calling LogFactory.checkForReplication If replication slave mode for the database is stopped after this object has been created, checksums cannot be reenabled without creating a new instance of this class. That is conveniently handled as LogToFile.recover completes (which automatically happens once replication slave mode is no longer active) Stop this LogAccessFile from passing chunks of log records to the MasterFactory. Appends the current Buffer to the dirty Buffer list and assigns a free buffer to be the currrent active buffer . Flushing of the buffer to disk is delayed if there is a free buffer available. dirty buffers will be  flushed to the disk when  flushDirtyBuffers() is invoked by  a commit call or when no more free buffers are available. Guarantee all writes up to the last call to flushLogAccessFile on disk. <p> A call for clients of LogAccessFile to insure that all data written up to the last call to flushLogAccessFile() are written to disk. This call will not return until those writes have hit disk. <p> Note that this routine may block waiting for I/O to complete so callers should limit the number of resource held locked while this operation is called.  It is expected that the caller Note that this routine only "writes" the data to the file, this does not mean that the data has been synced to disk.  The only way to insure that is to first call switchLogBuffer() and then follow by a call of sync(). Generate the checkum log record and write it into the log buffer. The checksum applies to all bytes from this checksum log record to the next one. Write a single log record to the stream. <p> For performance pass all parameters rather into a specialized routine rather than maintaining the writeInt, writeLong, and write interfaces that this class provides as a standard OutputStream.  It will make it harder to use other OutputStream implementations, but makes for less function calls and allows optimizations knowing when to switch buffers. <p> This routine handles all log records which are smaller than one log buffer.  If a log record is bigger than a log buffer it calls writeUnbufferedLogRecord(). <p> The log record written will always look the same as if the following code had been executed: writeInt(length) writeLong(instant) write(data, data_offset, (length - optional_data_length) ) if (optional_data_length != 0) write(optional_data, optional_data_offset, optional_data_length) writeInt(length) write to the log file
************************************************************************ Private/Protected methods of This class: *************************************************************************
The append methods should be changed to use java.nio.ByteBuffer if it is decided that replication will never use j2me. We use our own implementation for now so that j2me is not blocked. Append a byte[] to this LogBufferElement. Append a chunk of log records to this LogBufferElement.    Resets all variables to default values. Should be called before a LogBufferElement is reused.



create the tables that are used by this test. utility routine to generate random byte array of data. Insert some rows into the table and corrupt the log for the last row, so when we recover , there should be one row less even though we committed. Log is corrupted using the corrupt storage factory. setup offset/length where we want the transaction log to be corrupted. Transaction tat the corruption is simulated  on should be rolled back because the log check should identify that the writes were incomplete. update some rows in the table and corrupt the log for the last row, so when we recover , All checsum should be correct because corrupted log transaction should been rolled back. read the data from the table and verify the blob data using the checksum and make sure that expected number rows exist in the table.
Get the formatID which corresponds to this class. These following methods are only intended to be called by an implementation of a log factory. All other uses of this object should only see it as a log instant. LogScan methods Static functions that can only be used inside the RawStore's log factory which passes the log counter around encoded as a long make a log instant from 2 longs and return a long which is the long representatin of a LogCounter methods for the Formatable interface Read this in. Write this out.
Abort any activity related to backup in the log factory. Backup is not in progress any more, it failed for some reason. Check to see if a database has been upgraded to the required level in order to use a store feature. Checkpoint the rawstore. The frequency of checkpoint is determined by 2 persistent service properties, RawStore.LOG_SWITCH_INTERVAL and RawStore.CHECKPOINT_INTERVAL. By default, LOG_SWITCH_INTERVAL is every 1M bytes of log record written.  User can change this value by setting the property to some other values during boot time.   The legal range of LOG_SWITCH_INTERVAL is from 100K to 128M. By default, CHECKPOINT_INTERVAL equals 10M, but user can set it to less if more frequent checkpoint is desired.  The legal range of CHECKPOINT_INTERVAL is from 100K to 128M. redoing a checkpoint  during rollforward recovery Create readme file in log directory warning users against touching any files in the directory delete the log file after the checkpoint. <P>MT - synchronization provided by caller - RawStore boot, This method is called only if a crash occured while re-encrypting the database at boot time. Deletes the archived log files store in the log directory path. This call is typically used after a successful version level backup to clean up the old log files that are no more required to do roll-forward recovery from the last backup taken. Disable the log archive mode, when log archive mode is off the system will delete  old log files(not required for crash recovery) after each checkpoint. @exception StandardException - thrown on error Enable the log archive mode, when log archive mode is on the system keeps all the old log files instead of deleting them at the checkpoint. logArchive mode is persistent across the boots. @exception StandardException - thrown on error copy all the log files that has to go into the backup directory and mark that backup has come to an end. @param toDir - location where the log files should be copied to. @exception StandardException Standard Derby error policy Flush all unwritten log record up to the log instance indicated to disk. Backup restore support Stop making any change to the persistent store Return the canonical directory of the PARENT of the log directory.  The log directory live in the "log" subdirectory of this path.  If the log is at the default location (underneath the database directory), this returns null.  Should only be called after the log factory is booted. Get the instant for the last record in the log. Get the log instant long value of the first log record that has not been flushed. Only works after recover() has finished, or (if in slave replication mode) after calling initializeReplicationSlaveRole. Return the location of the log directory. Get JBMS properties relevant to the log factory Is the transaction in rollforward recovery Used to determine if the replication master mode has been started, and the logging for unlogged operations needs to be enabled. find if the checkpoint is in the last log file. <P>MT - synchronization provided by caller - RawStore boot, This method is called only if a crash occured while re-encrypting the database at boot time. @return <code> true </code> if if the checkpoint is in the last log file, otherwise <code> false </code>. checks whether is log archive mode is enabled or not. Get a ScanHandle to scan flushed records from the log. <P> MT- read only Get a LogScan to scan flushed records from the log. <P> MT- read only Get a LogScan to scan the log in a forward direction. <P> MT- read only Recover the database to a consistent state using the log. Each implementation of the log factory has its own recovery algorithm, please see the implementation for a description of the specific recovery algorithm it uses. Sets whether the database is encrypted, all the transaction log has to be encrypted, and flush the log if requested. <p> Log needs to be flushed first if the cryptographic state of the database changes (for instance re-encryption with a new key). Make log factory aware of which raw store factory it belongs to start the transaction log backup, the transaction log is  is required to bring the database to the consistent state on restore. copies the log control information , active log files to the given backup directory and marks that backup is in progress. @param toDir - location where the log files should be copied to. @exception StandardException Standard Derby error policy set up a new log file to start writing the log records into the new log file after this call. <P>MT - synchronization provided by caller - RawStore boot, This method is called while re-encrypting the database at database boot time. Make this LogFactory pass log records to the MasterFactory every time a log record is appended to the log on disk, and notify the MasterFactory when a log disk flush has taken place. Not implemented by ReadOnly. Stop this LogFactory from passing log records to the MasterFactory and from notifying the MasterFactory when a log disk flush has taken place. Not implemented by ReadOnly. Can start making change to the persistent store again
closing the log file logging the supplied message to the logfile

Return my format identifier. Read this in class specific methods Skip over the loggable.  Set the input stream to point ot after the loggable as if the entire log record has been sucked in by the log record Formatable methods Write this out.

Transmits all the log records in the log buffer to the slave. updates the information about the latest instance of the log record that has been flushed to the disk. Ships the next log record chunk, if available, from the log buffer to the slave. Used to notify the log shipper that a log buffer element is full.
backup is not in progress any more, it failed for some reason. * Implementation specific methods Append length bytes of data to the log prepended by a long log instant and followed by 4 bytes of length information. <P> This method is synchronized to ensure log records are added sequentially to the end of the log. <P>MT- single threaded through this log factory.  Log records are appended one at a time. copy the log files into the given backup location @param toDir               - location to copy the log files to @param lastLogFileToBackup - last log file that needs to be copied. Boot up the log factory. <P> MT- caller provide synchronization end of boot * Methods of ModuleControl Used by LogAccessFile to check if it should take the replication master role, and thereby send log records to the MasterFactory. In Java 1.4.2 and newer rws and rwd modes for RandomAccessFile are supported. Still, on some JVMs (e.g. early versions of 1.4.2 and 1.5 on Mac OS and FreeBSD) the support for rws and rwd is not working. This method attempts to detect this by opening an existing file in "rws" mode. If this fails, Derby should fall back to use "rw" mode for the log files followed by explicit syncing of the log. Note: it is important to use "rws" for the test. If "rwd" is used, no exception is thrown when opening the file, but the syncing does not take place. For more details see DERBY-1 (and DERBY-2020). Check to see if a database has been upgraded to the required level in order to use a store feature. Check to see if a database has been upgraded to the required level in order to use a store feature. Checkpoint the rawStore. <P> MT- Only one checkpoint is to be taking place at any given time. <P> The steps of a checkpoint are <OL> <LI> switch to a new log file if possible <PRE> freeze the log (for the transition to a new log file) flush current log file create and flush the new log file (with file number 1 higher than the previous log file). The new log file becomes the current log file. unfreeze the log </PRE> <LI> start checkpoint transaction <LI> gather interesting information about the rawStore: the current log instant (redoLWM) the earliest active transaction begin tran log record instant (undoLWM), all the truncation LWM set by clients of raw store (replication) <LI> clean the buffer cache <LI> log the next checkpoint log record, which contains (repPoint, undoLWM, redoLWM) and commit checkpoint transaction. <LI> synchronously write the control file containing the next checkpoint log record log instant <LI> the new checkpoint becomes the current checkpoint. Somewhere near the beginning of each log file should be a checkpoint log record (not guarenteed to be there) <LI> see if the log can be truncated <P> The earliest useful log record is determined by the repPoint and the undoLWM, whichever is earlier. <P> Every log file whose log file number is smaller than the earliest useful log record's log file number can be deleted. <P><PRE> Transactions can be at the following states w/r to a checkpoint - consider the log as a continous stream and not as series of log files for the sake of clarity. |(BT)-------(ET)| marks the begin and end of a transaction. .                          checkpoint started .       |__undoLWM          | .       V                   |___redoLWM .                           |___TruncationLWM .                           | .                           V 1 |-----------------| 2       |--------------------------------| 3           |-------| 4               |--------------------------------------(end of log) 5                                       |-^-| .                                   Checkpoint Log Record ---A---&gt;|&lt;-------B---------&gt;|&lt;-------------C----------- </PRE> <P> There are only 3 periods of interest : <BR> A) before undoLWM,  B) between undo and redo LWM, C) after redoLWM. <P> Transaction 1 started in A and terminates in B.<BR> During redo, we should only see log records and endXact from this transaction in the first phase (between undoLWM and redoLWM).  No beginXact log record for this transaction will be seen. <P> Transaction 2 started in B (right on the undoLWM) and terminated in C.<BR> Any transaction that terminates in C must have a beginXact at or after undoLWM.  In other words, no transaction can span A, B and C. During redo, we will see beginXact, other log records and endXact for this transaction. <P> Transaction 3 started in B and ended in B.<BR> During redo, we will see beginXact, other log records and endXact for this transaction. <P> Transaction 4 begins in B and never ends.<BR> During redo, we will see beginXact, other log records. In undo, this loser transaction will be rolled back. <P> Transaction 5 is the transaction taking the checkpoint.<BR> The checkpoint action started way back in time but the checkpoint log record is only written after the buffer cache has been flushed. <P> Note that if any time elapse between taking the undoLWM and the redoLWM, then it will create a 4th period of interest. redo a checkpoint during rollforward recovery checkpoint with pre-start transaction Create readme file in log directory warning users against touching any files in the directory Create the directory where transaction log should go. Misc private functions to access the log Get the current log instant - this is the log instant of the Next log record to be written out <P> MT - This method is synchronized to ensure that it always points to the end of a log record, not the middle of one.  delete the log file after the checkpoint. <P>MT - synchronization provided by caller - RawStore boot, This method is called only if a crash occured while re-encrypting the database at boot time. delete the log files, that might have been left around if we crashed immediately after the checkpoint before truncations of logs completed. see bug no: 3519 , for more details. delete the online archived log files disable the log archive mode enable the log archive mode  copy all the log files that have to go into the backup and mark that backup is compeleted. @param toDir - location where the log files should be copied to. @exception StandardException Standard Derby error policy Used to make the slave stop appending log records, complete recovery and boot the database. Find a checkpoint log record at the checkpointInstant <P> MT- read only Privileged startup. Must be private so that user code can't call this entry point. Get the first valid log instant - this is the beginning of the first log file <P>MT- synchronized on this Flush the log such that the log record written with the instant wherePosition is guaranteed to be on disk. <P>MT - only one flush is allowed to be taking place at any given time (RESOLVE: right now it single thread thru the log factory while the log is frozen) Flush all unwritten log record up to the log instance indicated to disk and sync. Also check to see if database is frozen or corrupt. <P>MT - not needed, wrapper method Flush all unwritten log record to disk and sync. Also check to see if database is frozen or corrupt. <P>MT - not needed, wrapper method Flush all unwritten log record up to the log instance indicated to disk without syncing. <P>MT - not needed, wrapper method Backup restore - stop sending log record to the log stream Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Return the control file name <P> MT- read only returns the length that will make the data to be multiple of encryption block size based on the given length. Block cipher algorithms like DES and Blowfish ..etc  require their input to be an exact multiple of the block size. return the encryption block size used during encrypted db creation Return the "oldest" log file still needed by recovery. <p> Returns the log file that contains the undoLWM, ie. the oldest log record of all uncommitted transactions in the given checkpoint. If no checkpoint is given then returns -1, indicating all log records may be necessary. Get the instant of the first record which was not flushed. <P>This only works after running recovery the first time. <P>MT - RESOLVE: end of getLogDirPath Return the directory the log should go. <P> MT- read only @exception StandardException Derby Standard Error Policy Methods to help a log scan switch from one log file to the next Open a log file and position the file at the beginning. Used by scan to switch to the next log file <P> MT- read only </p> <p> When the database is in slave replication mode only: Assumes that only recover() will call this method after initializeReplicationSlaveRole() has been called, and until slave replication has ended. If this changes, the current implementation will fail.</p> Get a read-only handle to the log file positioned at the stated position <P> MT- read only Given a log file number, return its file name <P> MT- read only Return the current log file number. <P> MT - this method is synchronized so that it is not in the middle of being changed by swithLogFile Get the log file to Simulate a log corruption FOR UNIT TESTING USAGE ONLY end of getLogStorageFactory * Methods of LogFactory MT- not needed Return the mirror control file name <P> MT- read only Privileged Monitor lookup. Must be private so that user code can't call this entry point. Privileged module lookup. Must be private so that user code can't call this entry point. format Id must fit in 4 bytes Return my format identifier. Is the transaction in rollforward recovery Used to determine if the replication master mode has been started, and the logging for unlogged operations needs to be enabled. Initialize the log to the correct format with the given version and log file number.  The new log file must be empty.  After initializing, the file is synchronously written to disk. <P>MT - synchornization provided by caller Initializes logOut so that log received from the replication master can be appended to the log file. Normally, logOut (the file log records are appended to) is set up as part of the recovery process. When the database is booted in replication slave mode, however, recovery will not get to the point where logOut is initialized until this database is no longer in slave mode. Since logOut is needed to append log records received from the master, logOut needs to be set up for replication slave mode. This method finds the last log record in the log file with the highest number. logOut is set up so that log records will be appended to the end of that file, and the endPosition and lastFlush variables are set to point to the end of the same file. All this is normally done as part of recovery. After the first log file switch resulting from applying log received from the master, recovery will be allowed to read up to, but not including, the current log file which is the file numbered logFileNumber. Note that this method must not be called until LogToFile#boot() has completed. Currently, this is ensured because RawStore#boot starts the SlaveFactory (in turn calling this method) after LogFactory.boot() has completed. Race conditions for logFileNumber may occur if this is changed. find if the checkpoint is in the last log file. <P>MT - synchronization provided by caller - RawStore boot, This method is called only if a crash occured while re-encrypting the database at boot time. @return <code> true </code> if if the checkpoint is in the last log file, otherwise <code> false </code>. Privileged startup. Must be private so that user code can't call this entry point. Backup restore - is the log being archived to some directory? if log archive mode is enabled return true else false * Sending information to the user without throwing exception. * There are times when unusual external or system related things happen in * the log which the user may be interested in but which doesn't impede on * the normal operation of the store.  When such an event occur, just send * a message or a warning message to the user rather than throw an * exception, which will rollback a user transaction or crash the database. * * logErrMsg - sends a warning message to the user Print error message to user about the log MT - not needed, informational only Print error message to user about the log MT - not needed, informational only In case of boot errors, and if database is either booted with derby.system.durability=test or was previously at any time booted in this mode, mention in the error message that the error is probably because the derby.system.durability was set. Dont want to waste time to resolve issues in such cases <p> MT - not needed, informational only Testing support Writes out a partial log record - takes the appendLogRecord. Need to shutdown the database before another log record gets written, or the database is not recoverable. * Methods of Corruptable Once the log factory is marked as corrupt then the raw store will shut down. Functions to help the Logger open a log scan on the log. Scan backward from start position. <P> MT- read only Scan backward from end of log. <P> MT- read only  Open a forward scan of the transaction log. <P> MT- read only Scan Forward from start position. <P> MT- read only Get a forwards scan open the given log file name for writes; if file can not be be opened in write sync mode then disable the write sync mode and open the file in "rw" mode. preallocate the given log File to the logSwitchInterval size; file is extended by writing zeros after the header till the log file size the set by the user. end of preAllocateNewLogFile print stack trace from the Throwable including its nested exceptions Carefully read the content of the control file. <P> MT- read only Recover the rawStore to a consistent state using the log. <P> In this implementation, the log is a stream of log records stored in one or more flat files.  Recovery is done in 2 passes: redo and undo. <BR> <B>Redo pass</B> <BR> In the redo pass, reconstruct the state of the rawstore by repeating exactly what happened before as recorded in the log. <BR><B>Undo pass</B> <BR> In the undo pass, all incomplete transactions are rolled back in the order from the most recently started to the oldest. <P>MT - synchronization provided by caller - RawStore boot. This method is guaranteed to be the only method being called and can assume single thread access on all fields. This function restores logs based on the  following attributes are specified on connection URL: Attribute.CREATE_FROM (Create database from backup if it does not exist) Attribute.RESTORE_FROM (Delete the whole database if it exists and then restore it from backup) Attribute.ROLL_FORWARD_RECOVERY_FROM:(Perform Rollforward Recovery; except for the log directory everything else is replaced by the copy from backup. log files in the backup are copied to the existing online log directory. In case of RESTORE_FROM, the whole database directory is removed in Directory.java while restoring service.properties so even the log directory is removed. In case of CREATE_FROM, log directory will not exist if we came so far because it should fail if a database already exists. In case ROLL_FORWARD_RECOVERY_FROM log directory should not be removed. So only thing that needs to be done here is create a a log directory if it does not exists and copy the log files(including control files) that exists in the backup from which we are are trying to restore the database to the online log directory. Serviceable methods @return true, if this work needs to be done on a user thread immediately {@inheritDoc } set the endPosition of the log and make sure the new position won't spill off the end of the log Make log factory aware of which raw store factory it belongs to Start the transaction log backup. The transaction log is required to bring the database to the consistent state on restore. All the log files that are created after the backup starts must be kept around until they are copied into the backup, even if there are checkpoints when backup is in progress. Copy the log control files to the backup (the checkpoint recorded in the control files is the backup checkpoint). Restore will use the checkpoint info in these control files to perform recovery to bring the database to the consistent state. Find first log file that needs to be copied into the backup to bring the database to the consistent state on restore. In the end, existing log files that are needed to recover from the backup checkpoint are copied into the backup, any log that gets generated after this call are also copied into the backup after all the information in the data containers is written to the backup, when endLogBackup() is called. @param toDir - location where the log files should be copied to. @exception StandardException Standard Derby error policy set up a new log file to start writing the log records into the new log file after this call. <P>MT - synchronization provided by caller - RawStore boot, This method is called while re-encrypting the database at database boot time. Make this LogFactory pass log records to the MasterFactory every time a log record is appended to the log on disk, and notify the MasterFactory when a log disk flush has taken place. Stop the log factory <P> MT- caller provide synchronization (RESOLVE: this should be called AFTER dataFactory and transFactory are stopped) Stop this LogFactory from passing log records to the MasterFactory and from notifying the MasterFactory when a log disk flush has taken place. Stop the slave functionality for this LogFactory. Calling this method causes the thread currently doing recovery to stop the recovery process and throw a StandardException with SQLState SHUTDOWN_DATABASE. This should only be done when the database will be shutdown. Switch to the next log file if possible. <P>MT - log factory is single threaded thru a log file switch, the log is frozen for the duration of the switch Utility routine to call sync() on the input file descriptor. <p> Simulate a log full condition if TEST_LOG_FULL is set to true, then the property TEST_RECORD_TO_FILL_LOG indicates the number of times this function is call before an IOException simulating a log full condition is raised. If TEST_RECORD_TO_FILL_LOG is not set, it defaults to 100 log record Get rid of old and unnecessary log files Get rid of old and unnecessary log files <P> MT- only one truncate log is allowed to be taking place at any given time.  Synchronized on this. Backup restore - start sending log record to the log stream Private methods that helps to implement methods of LogFactory Verify that we the log file is of the right format and of the right version and log file number. <P>MT - not needed, no global variables used Verify that we the log file is of the right format and of the right version and log file number.  The log file position is set to the beginning. <P>MT - MT-unsafe, caller must synchronize Carefully write out this value to the control file. We do safe write of this data by writing the data into two files every time we write the control data. we write checksum at the end of the file, so if by chance system crashes while writing into the file, using the checksum we find that the control file is hosed then we  use the mirror file, which will have the control data written at last check point. see comment at beginning of file for log control file format. <P> MT- synchronized by caller When changing this code, also update the comment at the beginning of this class, the ControlFileReader of DERBY-5195, and the description on the web page in http://db.apache.org/derby/papers/logformats.html
--------------------------------------------------------------------------- Obtain a set of Properties for the client data source. -----------------------------transient state-------------------------------- Jdbc 1 ---------------------- 3-way tracing connects ----------------------------- Including protocol manager levels, and driver configuration Jdbc 2 Specialized by NetLogWriter.traceConnectsExit() ---------------------- tracing connects ----------------------------------- ---------------------------tracing exceptions and warnings----------------- -------------------------tracing driver configuration----------------------- --------------------------- method entry tracing -------------------------- --------------------------- method exit tracing -------------------------- ------------- API entry and exit trace methods ---------------------------- Entry and exit are be traced separately because input arguments need to be traced before any potential exception can occur. Exit tracing is only performed on methods that return values. Entry tracing is only performed on methods that update state, so entry tracing is not performed on simple getter methods. We could decide in the future to restrict entry tracing only to methods with input arguments. ------------------------ meta data tracing -------------------------------- ------------------------ tracepoint api ----------------------------------- properties.toString() will print out passwords, so this method was written to escape the password property value. printWriter_ synchronized by caller.
Apply the change indicated by this operation and optional data. <B>If this method fail, the system will be shut down because the log record has already been written to disk.  Moreover, the log record will be replayed during recovery and this doMe method will be called on the same page again, so if it fails again, recovery will fail and the database cannot be started.  So it is very important to make sure that every resource you need, such as disk space, has been acquired before the logAndDo method is called! </B> <BR>This method cannot acquire any resource (like latching of a page) since it is called underneath the logging system, ie., the log record has already been written to the log stream. <P> The available() method of in indicates how much data can be read, i.e. how much was originally written. The log operations are responsible to create the ByteArray, and the log operations should write out any optional data for the change to the ByteArray. The ByteArray can be prepared when the log operation is constructed, or it can be prepared when getPreparedLog() is called. Called by the log manager to allow the log operation to pass the buffer which contains optional data that will be available in to doMe() methods. Get the loggable's group value Determine if the operation should be reapplied in recovery redo. If redo is needed, acquire any resource that is necessary for the loggable's doMe method.  These need to be released in the releaseResource method. <P> The sequence of events in recovery redo of a Loggable operation is: <NL> <LI> Get the loggable operation.  If loggable.needsRedo is false, then no need to redo this operation. <LI> If loggable.needsRedo is true, all the resources necessary for applying the doMe is acquired in needsRedo. <LI> If the loggable is actually a compensation operation, then the logging system will find the undoable operation that needs to be undone, call compensation.setUndoOp with the undoable operation. <LI> The recovery system then calls loggable.doMe, which re-applies the loggable operation, or re-applies the compensation operation <LI> The recovery system then calls loggable.releaseResource. </NL> Release any resource that was acquired for doMe for rollback or recovery redo. This resource is acquired in either generateUndo (if this is a compensation operation during run time rollback or recovery rollback) or in needsRedo (if this is during recovery redo).  The run time transaction context should have all the resource already acquird for run time roll forward, so there is no need to releaseResource during run time roll forward. This method must be safe to be called multiple times.

Set the allocation status of pageNumber to doStatus.  To undo this operation, set the allocation status of pageNumber to undoStatus Chain one allocation page to the next. Compress free pages. <p> Compress the free pages at the end of the range maintained by this allocation page.  All pages being compressed should be FREE. Only pages in the last allocation page can be compressed. <p>
Flush all unwritten log record up to the log instance indicated to disk. Flush all unwritten log to disk Log the loggable operation under the context of the transaction and then apply the operation to the RawStore. <BR> Before you call this method, make sure that the Loggable's doMe method will succeed.  This method will go ahead and send the log record to disk, and once it does that, then doMe cannot fail or the system will be shut down and recovery may fail.  So it is <B> very important </B> to make sure that every resource you need for the loggable's doMe method, such as disk space, has be acquired or accounted for before calling logAndDo. Log the compensation operation under the context of the transaction and then apply the undo to the RawStore. <BR> Before you call this method, make sure that the Compensation's doMe method will succeed.  This method will go ahead and send the log record to disk, and once it does that, then doMe cannot fail or the system will be shut down and recovery may fail.  So it is <B> very important </B> to make sure that every resource you need for the Compensation's doMe method, such as disk space, has be acquired or accounted for before calling logAndUnDo. During recovery re-prepare a transaction. <p> After redo() and undo(), this routine is called on all outstanding in-doubt (prepared) transactions.  This routine re-acquires all logical write locks for operations in the xact, and then modifies the transaction table entry to make the transaction look as if it had just been prepared following startup after recovery. <p> Undo transaction.
//////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.0 IN JAVA 6  //////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.2 IN JAVA 8  //////////////////////////////////////////////////////////////////
--------------------------- helper methods -------------------------------- Verifies that there is an underlying physical connection for this logical connection. <p> If the physical connection has been nulled out it means that this logical connection has been closed. ------------------------ logical connection close ------------------------- All methods are simply forwarded to the physical connection, except for close() and isClosed(). //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.0 IN JAVA 6  ////////////////////////////////////////////////////////////////// ---------------------- wrapped public entry points ------------------------ All methods are forwarded to the physical connection in a standard way  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  <code>getClientInfo</code> forwards to <code>physicalConnection_</code>. <code>getClientInfo</code> always returns an empty <code>Properties</code> object since Derby doesn't support ClientInfoProperties. <code>getClientInfo</code> forwards to <code>physicalConnection_</code>. Always returns a <code>null String</code> since Derby does not support ClientInfoProperties. Retrieves a {@code DatabaseMetaData} object that contains metadata about the database to which this {@code Connection} object represents a connection. <p> The database metadata object is logical in the sense that it has the same lifetime as the logical connection. If the logical connection is closed, the underlying physical connection will not be accessed to obtain metadata, even if it is still open. Also, the reference to the logical connection instead of the underlying physical connection will be returned by {@link LogicalDatabaseMetaData#getConnection}. Returns the real underlying database metadata object. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// Get the name of the current schema. ---------------------------------------------------------------------------- Returns the client-side transaction id from am.Connection. <p> <em>NOTE:</em> This method was added for testing purposes. Avoid use in production code if possible. Checks if the connection has not been closed and is still valid. The validity is checked by running a simple query against the database. Returns a newly created logical database metadata object. <p> Subclasses should override this method to return an instance of the correct implementation class of the logical metadata object. Notifies listeners about exceptions of session level severity or higher. <p> The exception, even if the severity is sufficiently high, is ignored if the underlying physical connection has been nulled out. Otherwise a {@code connectionErrorOccurred}-event is sent to all the registered listeners. Used by ClientPooledConnection close when it disassociates itself from the LogicalConnection <code>setClientInfo</code> forwards to <code>physicalConnection_</code>. <code>setClientInfo</code> forwards to <code>physicalConnection_</code>. Set the default schema for the Connection.
JDBC 4.0 methods ///////////////////////////////////////////////////////////////////////  JDBC 4.1 - New public methods  /////////////////////////////////////////////////////////////////////// See DatabaseMetaData javadoc ///////////////////////////////////////////////////////////////////////  JDBC 4.2 - New public methods  /////////////////////////////////////////////////////////////////////// See DatabaseMetaData javadoc See DatabaseMetaData javadoc. Empty ResultSet because Derby does not support pseudo columns. Returns the real metadata object if appropriate. <p> This is just a convenience wrapper method.
method specific to this class Find the page that the rollback operation should be applied to. <P>The actual logical log operation is expected to implement Undoable.generateUndo.  This utility function findLogicalPage is provided for the common case scenario of using a LogicalUndo interface to find the undo page.  The subclass that implements Undoable.generateUndo can use this function to find the logical page with its LogicalUndo callback function. This method can be used with the default releaseResource(). <P>During recovery redo, the logging system is page oriented and will use the pageID stored in the PageUndoOperation to find the page.  The page will be latched and released using the default findpage and releaseResource - this.releaseResource() will still be called so it has to know not to release any resource it did not acquire. Undoable method Generate a Compensation (PageUndoOperation) that will rollback the changes of this page operation. If this Page operation cannot or need not be rolled back (redo only), overwrite this function to return null. LogicalUndoable methods These methods are called by undo.findUndo to extract information out of the log record for the purpose of logical undo. Return the container handle where the log operated on Return the record handle that correspond to the record that was changed during roll forward.  This is used as a hint by logical undo as a good place to look for the record to apply the roll back. Read this in ************************************************************************ Public Methods of RePreparable Interface: ************************************************************************* reclaim locks associated with the changes in this log record. <p> After the logical undo logic figures out where the real record that needs roll back is, reset this log operation to refer to that record Undo the change indicated by this log operation and optional data. The undoPage and undoRecordId is the page, record the undo should apply to. The undoRecorId differs from the roll forward recordId if the undoPage differs from the page the roll forward operation was applied to, in other words, the record moved to another page and the recordId changed. <BR>A logical operation can at most deal with 1 record. <P> The available() method of in indicates how much data can be read, i.e. how much was originally written. <BR><B>In this RawStore implementation, should only only be called via CompOp.doMe</B>. Formatable methods
//////////////////////////////////////////////////////////////  ADDED BY JDBC 4.2  ////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////  ADDED BY JDBC 4.0  //////////////////////////////////////////////////////////////

Close the logical statement. //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.1 IN JAVA 7  ////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////  INTRODUCED BY JDBC 4.2 IN JAVA 8  ////////////////////////////////////////////////////////////////// Returns the associated physical callable statement. Returns the associated physical prepared statement. Returns the associated physical statement. Tells if the logical entity is closed. <p> If this method is used to avoid the possibility of raising an exception because the logical statement has been closed and then invoke a method on the physical statement, one must synchronize on this instance in the calling code. JDBC 4.0 java.sql.Wrapper interface methods Check whether this instance wraps an object that implements the interface specified by {@code iface}. Returns an instance of the specified interface if this instance is a wrapper for the interface.
Find the page and record to undo.  If no logical undo is necessary, i.e., row has not moved, then just return the latched page where undo should go.  If the record has moved, it has a new recordId on the new page, this routine needs to call pageOp.resetRecord with the new RecordHandle so that the logging system can update the compensation Operation with the new location.
Loggable methods Apply the undo operation, in this implementation of the RawStore, it can only call the undoMe method of undoOp Undo operation is a COMPENSATION log operation Read this in make sure resource found in undoOp is released Compensation methods Set up a LogicalOperation during recovery redo. DEBUG: Print self. Write this out.
Return the containerHandle used by this log operation.  Logical cannot change container identity between roll forward and roll back.  This method should only be called by LogicalUndo to extract information from the log record. Return the recordHandle stored in the log operation that correspond to the record that was changed in the rollforward.  This method should only be called by LogicalUndo to extract information from the log record. If the row has moved, reset the record handle that the undo should be applied on. Restore the row stored in the log operation.   This method should only be called by LogicalUndo to extract information from the log record.
********************************************** Close output streams and, if at least one message was printed to the log file, let the user know. @return true if all streams were closed successfully; false otherwise. ** ********************************************** Prints the received exception to the log file and, if the use has specified "verbose", the screen as well. @param e The exception to be printed. ** ********************************************** Prints the message for the received key to the log log file and, if the use has specified "verbose", the screen as well. @param key Key for the message to be printed. @param value Value to be substituted into the message. ** ********************************************** Prints the message for the received key to the log log file and, if the use has specified "verbose", the screen as well. @param key Key for the message to be printed. @param value Value to be substituted into the message. ** ********************************************** initLogs: Prepare output streams and initialize state for handling dblook output. @param logFileName File for errors/warnings. @param ddlFileName File for generated DDL. @param appendLogs Whether or not to append to existing log and ddl files. @param doVerbose verbose mode @param endOfStmt Statement delimiter. @return true if all state is initialized successfully; false otherwise. ** ********************************************** Method to report status info to the end-user. This information will be printed as SQL script comments, which means the messages must be preceded by a "--".  If the user specified a DDL file, then the message will be printed to that file; otherwise, it will be printed to the console. @param msg the information to print out. ** ********************************************** Report a localized message to output. @param key Key for the message to report. ** ********************************************** Report a localized message to output, substituting the received value where appropriate. @param key Key for the message to report. @param value Value to be inserted into the message at the {0} marker. ** ********************************************** Report a localized message to output, substituting the received values where appropriate. @param key Key for the message to report. @param values Array of Value to be inserted into the message at the {0}, {1}, etc markers. ** ********************************************** Report a specific string to output. @param str The string to report. ** ********************************************** Recursive method to unroll a chains of SQL exceptions. @param sqlE The SQL exception to unroll. @return A string representing the unrolled exception is returned. ** ********************************************** Write a newline character to the output DDL file, followed by a newline. ** ********************************************** Write the user-given statement delimiter to the output DDL file, followed by a newline. ** ********************************************** Write a string (piece of an SQL command) to the output DDL file. @param sql The string to write. **


Return remaining characters in the stream. Close the reader. Ensure reader is open. Fill array with blanks (SPACE). Fill internal buffer of character sequence. Reset the stream.
Return remaining bytes in the stream. Fill internal buffer of bytes (from character sequence). Reset the stream. Resetable interface
Return true if a low memory water mark has been set and the current free memory is lower than it. Otherwise return false. Set a low memory watermark where the owner of this object just hit an OutOfMemoryError. The caller is assumed it has just freed up any references it obtained during the operation, so that the freeMemory call as best as it can reflects the memory before the action that caused the OutOfMemoryError, not part way through the action.
Get the Analyzer used to create index terms Get the names of the fields which are created when text is indexed. These fields can be mentioned later on when querying the index. Get the QueryParser used to parse query text
get the string value of a property from the row properties columns: 1 == id 2 == schema 3 == table 4 == column name 5 == last modified get the properties of the current row Get the timestamp value of the 1-based column id List files Read the index properties file Fill in the schema, table, and column names
Be sure to close the Lucene IndexReader  This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.  Handle boolean columns Handle bytecolumns Handle byte columns Handle Date columns Handle double columns Handle float columns Invoke a static method (possibly supplied by the user) to instantiate an index descriptor. The method has no arguments. Returns a Lucene IndexReader, which reads from the indicated Lucene index. Handle integer columns Handle long columns ///////////////////////////////////////////////////////////////////  StringColumnVTI BEHAVIOR  /////////////////////////////////////////////////////////////////// columns: 1 ... $_maxKeyID == key columns $_maxKeyID + 1 == lucene docId $_maxKeyID + 2 == lucene score Handle short columns Handle Time columns Handle Timestamp columns ///////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////// Initialize the metadata and scan Return true if the 1-based column ID is the ID of a key column Read the index properties file Read the index properties file <p> Make sure that the index wasn't created with a Lucene version from the future. </p> <p> Make sure that the user has SELECT privilege on the text column and on all the key columns of the underlying table. </p>
Return the boolean value of a system property With Lucene versions up to 4.7, the Lucene plugin doesn't work on platforms without JMX (in particular: Java SE 8 Compact Profile 2). See DERBY-6650.
Add a document to a Lucene index wrier. Add the field to the document so that it can be read by LuceneQueryVTI. May raise an exception if the type is not supported. 1-based Raise an error if an argument is being given a null value Close an IndexWriter. ///////////////////////////////////////////////////////////////////  CREATE INDEX  /////////////////////////////////////////////////////////////////// Create a Lucene index on the specified column. Add a document to a Lucene index wrier. Create or re-create a Lucene index on the specified column. Decompose a function name of the form $table__$column into $table and $column Really delete a file Double quote an identifier in order to preserver casing ///////////////////////////////////////////////////////////////////  DROP INDEX  /////////////////////////////////////////////////////////////////// Drop a Lucene index. This removes the Lucene index directory from the filesystem. <p> Drop the Lucene directories which support an index. </p> Execute a DDL statement Return true if the file exists Privileged service lookup. Must be private so that user code can't call this entry point. Privileged startup. Must be private so that user code can't call this entry point. Forbid invalid character ///////////////////////////////////////////////////////////////////  SQL/JDBC SUPPORT  /////////////////////////////////////////////////////////////////// Raise an error if the connection is readonly. Get a binary value to add to the document read by LuceneQueryVTI. 1-based Get the ClassFactory to use with this database. Get the DataFactory of the connected database Get a Date value to add to the document read by LuceneQueryVTI. 1-based Get a connection to the database ///////////////////////////////////////////////////////////////////  DERBY STORE  /////////////////////////////////////////////////////////////////// <p> Get the handle on the Lucene directory inside the database. </p> Get a double value to add to the document read by LuceneQueryVTI. 1-based Get a float value to add to the document read by LuceneQueryVTI. 1-based Invoke a static method (possibly supplied by the user) to instantiate an index descriptor. The method has no arguments. Invoke a static method (possibly supplied by the user) to instantiate an index descriptor. The method has no arguments. ///////////////////////////////////////////////////////////////////  MANAGE THE INDEX PROPERTIES FILE  /////////////////////////////////////////////////////////////////// <p> Get the handle on the file holding the index properties. </p> <p> Get the handle on the file holding the index properties. </p> ///////////////////////////////////////////////////////////////////  LUCENE SUPPORT  /////////////////////////////////////////////////////////////////// Returns a Lucene IndexWriter, that writes inside the lucene directory inside the database directory. Get an integer value to add to the document read by LuceneQueryVTI. 1-based Return the key columns for an existing LuceneQueryVTI table function. Return column information for a proposed set of keys. Get an long value to add to the document read by LuceneQueryVTI. 1-based Return the primary key columns for a table, sorted by key position. Get the StorageFactory of the connected database Get a string value to add to the document read by LuceneQueryVTI. 1-based Get a Time value to add to the document read by LuceneQueryVTI. 1-based Get a Timestamp value to add to the document read by LuceneQueryVTI. 1-based Grant permissions to use the newly loaded LuceneSupport routines. ///////////////////////////////////////////////////////////////////  FILE MANAGEMENT  /////////////////////////////////////////////////////////////////// Return true if the directory is empty ///////////////////////////////////////////////////////////////////  LIST INDEXES  /////////////////////////////////////////////////////////////////// Return a list of Lucene indexes for this database. Filter by schema and table, if given. Load the procedures and functions for Lucene support: In the LuceneSupport schema, these are: listIndexes, createIndex, dropIndex, updateIndex. ///////////////////////////////////////////////////////////////////  LUCENE QUERY  /////////////////////////////////////////////////////////////////// Query a Lucene index created by createIndex Return true if the LuceneSupport schema exists already Return the qualified name of the table function Return the qualified name of the table. Make the unqualified name of a querying table function <p> Get the type of an external database's column as a Derby type name. </p> ///////////////////////////////////////////////////////////////////  TYPE HANDLING  /////////////////////////////////////////////////////////////////// Get the SQL type name for a key column <p> Build a precision and scale designator. </p> <p> Turns precision into a length designator. </p> Read the index properties file Raise an exception if a field has the same name as a key or if two fields have the same name. Return true if the table function exists Removes the functions and procedures loaded by loadTool and created by createIndex. Drop the LuceneSupport schema. Drop the lucene subdirectory. ///////////////////////////////////////////////////////////////////  UPDATE INDEX  /////////////////////////////////////////////////////////////////// Update a document in a Lucene index. Drops and recreates the Lucene index but does not touch the query function specific to the index. ///////////////////////////////////////////////////////////////////  NAMESPACE  /////////////////////////////////////////////////////////////////// A Lucene query table function already has system-supplied columns named documentID and score. These can't be the names of the key or text columns supplied by the user. Verify that the schema, table, and column names aren't null Raise an exception if the text column doesn't exist or isn't a String datatype. Write the index properties file
///////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  /////////////////////////////////////////////////////////////// Get the version of the Lucene library on the classpath. <p> Get the default Analyzer associated with the database Locale. </p> <p> Get the default index descriptor. This has a single field named TEXT, a defaultAnalyzer() and a defaultQueryParser(). </p> <p> Get the default, classic QueryParser. </p> <p> Get the Analyzer associated with the given Locale. </p> <p> Get the language code for a Lucene Analyzer. Each of the Analyzers lives in a package whose last leg is the language code. </p> <p> Get the StandardAnalyzer for parsing text. </p> ///////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////// Store an Analyzer class in the HashMap of Analyzers, keyed by language code

Get the right Main (according to the JDBC version. Get the right utilMain (according to the JDBC version. Get the right utilMain (according to the JDBC version. This overload allows the choice of whether the system properties will be used or not. Give a shortcut to go on the utilInstance so we don't expose utilMain. ij can be used directly on a shell command line through its main program.
reset the system properties to prevent confusion when running with java threads
Return the system identifier that this MBean is managing. Privileged module lookup. Must be private so that user code can't call this entry point. Return state of Derby's JMX management. Start Derby's MBeans. Stop Derby's MBeans.
Get the system identifier that this MBean is managing. The system identifier is a runtime value to disambiguate multiple Derby systems in the same virtual machine but different class loaders. Is Derby's JMX management active. If active then Derby has registered MBeans relevant to its current state. Inform Derby to start its JMX management by registering MBeans relevant to its current state. If Derby is not booted then no action is taken. <P> Require <code>SystemPermission("jmx", "control")</code> if a security manager is installed. Inform Derby to stop its JMX management by unregistering its MBeans. If Derby is not booted then no action is taken. <P> Require <code>SystemPermission("jmx", "control")</code> if a security manager is installed.
Quote an MBean key property value, so that it is safe to pass to {@link #registerMBean} even if it potentially contains special characters. Registers an MBean with the MBean server. The mbean will be unregistered automatically when Derby shuts down. Unregister a mbean previously registered with registerMBean.
* The following methods return all of the java primitive types and * their wrapper classes, plus all of the types corresponding to the * built-in SQL types. * Some methods for testing interface resolution Methods with more than one parameter * The following methods are for testing null arguments.  These methods * return Strings with the names of the parameter types, so we can be * sure the right method was called. * The following methods are for testing signature matching.  Each method * takes a single parameter.  The parameter types vary by method.  All * of the Java primitive types are covered as well as their wrapper classes. * All of the Java classes corresponding to the currently supported SQL * types are covered. Methods for negative testing Static methods "Cast to sub class"

Append a chunk of log records to the log buffer. The method is not threadsafe; only one thread should access this method at a time. ////////////////////////////////////////////////////////// Implementation of methods from interface ModuleControl // ////////////////////////////////////////////////////////// Used by Monitor.bootServiceModule to start the service. Currently only used to set up the replication mode. ////////////////////////////////////////////////////////////// Implementation of methods from interface ModuleSupportable // ////////////////////////////////////////////////////////////// Used by Monitor.bootServiceModule to check if this class is usable for replication. To be usable, we require that asynchronous replication is specified in startParams by checking that a property with key MasterFactory.REPLICATION_MODE has the value MasterFactory.ASYNCHRONOUS_MODE. Used by the LogFactory to notify the replication master controller that the log records up to this instant have been flushed to disk. The master controller takes action according to the current replication strategy when this method is called. When the asynchronous replication strategy is used, the method does not force log shipping to the slave; the log records may be shipped now or later at the MasterController's discretion. However, if another strategy like 2-safe replication is implemented in the future, a call to this method may force log shipment before returning control to the caller. Currently, only asynchronous replication is supported. Not implemented yet <p> Returns a name of a database associated with this master controller. </p> <p> Note: The only purpose of the method as of now is to give a meaningful name to a log shipper thread. The log shipper thread name should contain a name of a corresponding master database, and this method is used to access it. </p> Used to return the host name of the slave being connected to. Load relevant system property: replication log buffer size Used to return the port number of the slave being connected to. Used to handle the exceptions (IOException and StandardException) from the log shipper. used to handle the case when an attempt to failover the database fails. used to print the error stack for the given exception and stop the master. Connects to the slave being replicated to.  ////////////////////////////////////////////////////////// Implementation of methods from interface MasterFactory // ////////////////////////////////////////////////////////// Will perform all the work that is needed to set up replication. Will stop the replication master service. Will perform all work that is needed to shut down replication. Stop log shipping, notify slave that replication is stopped and tear down network connection with slave. Used to notify the log shipper that a log buffer element is full.
Append a chunk of log records to the log buffer. Used by the LogFactory to notify the replication master controller that the log records up to this instant have been flushed to disk. The master controller takes action according to the current replication strategy when this method is called. When the asynchronous replication strategy is used, the method does not force log shipping to the slave; the log records may be shipped now or later at the MasterFactory's discretion. However, if another strategy like 2-safe replication is implemented in the future, a call to this method may force log shipment before returning control to the caller. Currently, only asynchronous replication is supported. Will perform all work needed to failover Methods Will perform all the work that is needed to set up replication Will perform all work that is needed to shut down replication. Used to notify the log shipper that a log buffer element is full.
Return true if the passed in object matches this object.
/////////////////////////////////////////////////////////////////////////////////  CONSTRUCT ROWS TO PUT INTO THE TEMPORARY TABLE  ///////////////////////////////////////////////////////////////////////////////// <p> Construct and buffer a row for the INSERT/UPDATE/DELETE action corresponding to this [ NOT ] MATCHED clause. </p> <p> Construct and buffer a row for the INSERT/UPDATE/DELETE action corresponding to this [ NOT ] MATCHED clause. The buffered row is built from columns in the passed-in row. The passed-in row is the SELECT list of the MERGE statement's driving left join. </p> /////////////////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////////////////// Get the clause type: WHEN_NOT_MATCHED_THEN_INSERT, WHEN_MATCHED_THEN_UPDATE, WHEN_MATCHED_THEN_DELETE <p> Release resources at the end. </p> <p> Create the temporary table for holding the rows which are buffered up for bulk-processing after the driving left join completes. </p> <p> Run the matching refinement clause associated with this WHEN [ NOT ] MATCHED clause. The refinement is a boolean expression. Return the boolean value it resolves to. A boolean NULL is treated as false. If there is no refinement clause, then this method evaluates to true. </p> /////////////////////////////////////////////////////////////////////////////////  ConstantAction BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Get the formatID which corresponds to this class. /////////////////////////////////////////////////////////////////////////////////  OTHER PACKAGE VISIBLE BEHAVIOR, CALLED BY MergeResultSet  ///////////////////////////////////////////////////////////////////////////////// <p> Initialize this constant action, nulling out any transient state left over from a previous use. </p> /////////////////////////////////////////////////////////////////////////////////  Formatable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Read this object from a stream of stored objects. Write this object to a stream of stored objects.
/////////////////////////////////////////////////////////////////////////////////  Visitable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Accept the visitor for all visitable children of this node. <p> Point the column references in the matching refinement at the corresponding columns returned by the driving left join. </p> <p> Point the column references in the temporary row at the corresponding columns returned by the driving left join. </p> /////////////////////////////////////////////////////////////////////////////////  bind() BEHAVIOR CALLED BY MergeNode  ///////////////////////////////////////////////////////////////////////////////// Bind this WHEN [ NOT ] MATCHED clause against the parent MergeNode //////////////  BIND DELETE  ////////////// Bind a WHEN MATCHED ... THEN DELETE clause ///////////////  BIND MINIONS  /////////////// Boilerplate for binding a list of ResultColumns against a FromList //////////////  BIND INSERT  ////////////// Bind a WHEN NOT MATCHED ... THEN INSERT clause Bind the values in the INSERT list Bind the optional refinement condition in the MATCHED clause Bind the SET clauses of an UPDATE action //////////////  BIND UPDATE  ////////////// Bind a WHEN MATCHED ... THEN UPDATE clause <p> Build the full column list for a table. </p> <p> Build the signature of the row which will go into the temporary table. </p> <p> Construct the signature of the temporary table which drives the INSERT/UPDATE/DELETE action. </p> <p> Construct the row in the temporary table which drives an INSERT action. Unlike a DELETE, whose temporary row is just a list of copied columns, the temporary row for INSERT may contain complex expressions which must be code-generated later on. </p> <p> Construct the row in the temporary table which drives an UPDATE action. Unlike a DELETE, whose temporary row is just a list of copied columns, the temporary row for UPDATE may contain complex expressions which must be code-generated later on. </p> <p> Forbid subqueries in WHEN [ NOT ] MATCHED clauses. </p> <p> Generate a method to invoke the INSERT/UPDATE/DELETE action. This method will be called at runtime by MatchingClauseConstantAction.executeConstantAction(). </p> <p> Generate a method to build a row for the temporary table for INSERT/UPDATE actions. The method stuffs each column in the row with the result of the corresponding expression built out of columns in the current row of the driving left join. The method returns the stuffed row. </p> <p> Adds a field to the generated class to hold the ResultSet of "then" rows which drive the INSERT/UPDATE/DELETE action. Generates code to push the contents of that field onto the stack. </p> <p> Get the bound SELECT node under the dummy UPDATE node. This may not be the source result set of the UPDATE node. That is because a ProjectRestrictNode may have been inserted on top of it by DEFAULT handling. This method exists to make the UPDATE actions of MERGE statements behave like ordinary UPDATE statements in this situation. The behavior is actually wrong. See DERBY-6414. Depending on how that bug is addressed, we may be able to remove this method eventually. </p> Get the names of the columns explicitly changed by SET clauses <p> Get the names of the generated columns which are changed by the UPDATE statement. These are the generated columns which match one of the following conditions: </p> <ul> <li>Are explicitly mentioned on the left side of a SET clause.</li> <li>Are built from other columns which are explicitly mentioned on the left side of a SET clause.</li> </ul> columns which are explicitly mentioned on the left side of a SET clause /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// Get a list of column references in an expression Collect the columns mentioned by expressions in this MATCHED clause Find the MERGE table id of the indicated column <p> Find a column reference in the SELECT list of the driving left join and return its 1-based offset into that list.  Returns -1 if the column can't be found. </p> Return the list of columns which form the rows of the ResultSet which drive the INSERT/UPDATE/DELETE actions. Return true if this is a WHEN MATCHED ... DELETE clause Return true if this is a WHEN NOT MATCHED ... INSERT clause Return true if the ResultColumn represents a RowLocation /////////////////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////////////////// Return true if this is a WHEN MATCHED ... UPDATE clause <p> Make a ResultColumn for an identity column which is being set to the DEFAULT value. This special ResultColumn will make it through code generation so that it will be calculated when the INSERT/UPDATE action is run. </p> /////////////////////////////////////////////////////////////////////////////////  generate() BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Make a WHEN MATCHED ... THEN DELETE clause Make a WHEN NOT MATCHED ... THEN INSERT clause Make a WHEN MATCHED ... THEN UPDATE clause /////////////////////////////////////////////////////////////////////////////////  optimize() BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Optimize the INSERT/UPDATE/DELETE action. </p> Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. <p> Due to discrepancies in how names are resolved by SELECT and UPDATE, we have to force the left side of SET clauses to use the same table identifiers as the right sides of the SET clauses. </p> <p> Re-map ColumnReferences in constraints to point into the row from the temporary table. This is where the row will be stored when constraints are being evaluated. </p> Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. <p> Point a node's ColumnReferences into the row returned by the driving left join. </p>
------------------------ abstract box car and callback methods --------------------------------
The sql parameter is supplied in the read method for drivers that process all commands on the "read-side" and do little/nothing on the "write-side". Drivers that follow the write/read paradigm (e.g. NET) will likely ignore the sql parameter. Used for re-prepares across commit and other places as well
Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.

If the result set has been opened, close the open scan. Gets information from last getNextRow call. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant)  Get the next row from the source ResultSet tree and insert into the temp table Get the next Row from the temp table  CursorResultSet interface  Gets information from its source. We might want to have this take a CursorResultSet in its constructor some day, instead of doing a cast here? Return the total amount of time spent in this ResultSet  ResultSet interface (leftover from NoPutResultSet)  open a scan on the source. scan parameters are evaluated at each open, so there is probably some way of altering their values... reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
create the tables that are used by this test. Insert some rows into the table. update some rows in the table. verify the rows in the table.
Set system property
Determines the result datatype.  Accept NumberDataValues only. <P> <I>Note</I>: In the future you should be able to do a sum user data types.  One option would be to run sum on anything that implements divide(). Return if the aggregator class is for min/max. This is set by the parser.
Accumulate Get the formatID which corresponds to this class.    ///////////////////////////////////////////////////////////  FORMATABLE INTERFACE  Formatable implementations usually invoke the super() version of readExternal or writeExternal first, then do the additional actions here. However, since the superclass of this class requires that its externalized data must be the last data in the external stream, we invoke the superclass's read/writeExternal method last, not first. See DERBY-3219 for more discussion. ///////////////////////////////////////////////////////////
Create a string representation of an internal buffer of bytes. This is useful during debugging. Get the next buffer for reading bytes. Get the next buffer for writing bytes. Do sanity checking when getting the next write buffer Initialize a buffer for writing Return the number of bytes that have been saved to this byte holder. This result is different from available() as it is unaffected by the current read position on the ByteHolder.        Produce a string describing the state of this ByteHolder. This is mainly for debugging.


Cleans up database resources by closing known statements and connection, and deleting known in-memory databases. Creates a new database and keeps track of it to delete it when the clean up is invoked. <p> If the database already exists, a connection to the existing database is returned. Creates a new database and keeps track of it to delete it when the clean up is invoked. <p> If the database already exists, a connection to the existing database is returned. Creates a new statement from the given connection and keeps track of it and closes it when the clean up is invoked. Drops the specified database. <p> Note that the specified URL will be appended to a fixed JDBC protcol prefix. Creates a new connection to the specified database (url). <p> Note that the specified URL will be appended to a fixed JDBC protcol prefix. Returns a shared manager instance. Creates a new prepared statement from the given connection and keeps track of it and closes it when the clean up is invoked.
/////////////////////////////////////////////////////////////////////////////////  ConstantAction BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Get the ith (0-based) matching clause Get the formatID which corresponds to this class. /////////////////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////////////////// Get the number of matching clauses /////////////////////////////////////////////////////////////////////////////////  Formatable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Read this object from a stream of stored objects. Write this object to a stream of stored objects.
Called when the caller has completed inserting rows into the sorter. Methods of MergeInserter.  Arranged alphabetically. Return SortInfo object which contains information about the current sort. <p> Initialize this inserter. Methods of SortController Insert a row into the sort.
If the result set has been opened, close the open scan. ////////////////////////////////////////////////////////////////////  ResultSet interface (leftover from NoPutResultSet)  //////////////////////////////////////////////////////////////////// Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction parameters are evaluated for each row. ////////////////////////////////////////////////////////////////  SERVILE METHODS  //////////////////////////////////////////////////////////////// Return the total amount of time spent in this ResultSet
/////////////////////////////////////////////////////////////////////////////////  Visitable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Accept the visitor for all visitable children of this node. Add a column to the evolving map of referenced columns <p> Add SELECT privilege on the indicated column. </p> <p> Add to an evolving select list the columns from the indicated table. </p> /////////////////////////  PRIVILEGE MANAGEMENT  ///////////////////////// <p> Add the privileges required by the ON clause. </p> <p> Add EXECUTE privilege on the indicated routine. </p> Add the target table's row location to the left join's select list <p> Associate a column with the SOURCE or TARGET table. This is part of the special name resolution machinery which smooths over the differences between name resolution for SELECTs and UPDATEs. </p> Boilerplate for binding an expression against a FromList /////////////////////////  BINDING THE LEFT JOIN  ///////////////////////// Bind the driving left join select. Stuffs the left join SelectNode into the resultSet variable. /////////////////////////////////////////////////////////////////////////////////  bind() BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////  BUILD THE SELECT LIST FOR THE DRIVING LEFT JOIN.  ///////////////////////////// Build the select list for the left join //////////////////////////  CLONING THE FROM LIST  ////////////////////////// Create a FromList for binding a WHEN [ NOT ] MATCHED clause Clone a FromTable to avoid binding the original <p> Because of name resolution complexities, we do not allow derived column lists on source or target tables. These lists arise in queries like the following: </p> <pre> merge into t1 r( x ) using t2 on r.x = t2.a when matched then delete; merge into t1 using t2 r( x ) on t1.a = r.x when matched then delete; </pre> Neither the source nor the target table may be a synonym /////////////////////////////////////////////////////////////////////////////////  generate() BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Get a list of CastNodes in an expression Get a list of column references in an expression Get the column names from the table with the given table number, in sorted order Add a list of columns to the the evolving map <p> Add a list of columns to the the evolving map. This is called when we're building the SELECT list for the driving left join. </p> <p> Add the columns in the matchingRefinement clause to the evolving map. This is called when we're building the SELECT list for the driving left join. </p> ///////////////////////////////////  TABLE AND CORRELATION CHECKS  /////////////////////////////////// Get the exposed name of a FromTable Get a list of routines in an expression /////////////////////////////////////////////////////////////////////////////////  BIND-TIME ENTRY POINTS CALLED BY MatchingClauseNode  ///////////////////////////////////////////////////////////////////////////////// Get the target table for the MERGE statement Make a HashMap key for a column in the driving column map of the LEFT JOIN Throw a "not base table" exception /////////////////////////////////////////////////////////////////////////////////  optimize() BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Return true if the source table is a base table, view, or table function Return true if the target table is a base table
<p> Add another subject row id to the evolving hashtable of affected target rows. The concept of a subject row is defined by the 2011 SQL Standard, part 2, section 14.12 (merge statement), general rule 6. A row in the target table is a subject row if it joins to the source table on the main search condition and if the joined row satisfies the matching refinement condition for some WHEN MATCHED clause. A row in the target table may only be a subject row once. That is, a given target row may only qualify for UPDATE or DELETE processing once. If it qualifies for more than one UPDATE or DELETE action, then the Standard requires us to raise a cardinality violation. </p> Clean up resources and call close on data members. <p> Loop through the rows in the driving left join. </p> <p> Create a BackingStoreHashtable to hold the ids of subject rows. </p> /////////////////////////////////////////////////////////////////////////////////  BEHAVIOR  /////////////////////////////////////////////////////////////////////////////////
Close the scan. Close the scan. Methods of MergeScan Initialize the scan, returning false if there was some error. Insert rows while we keep getting duplicates from the merge run whose scan is in the open scan array entry indexed by scanindex. Methods of MergeSortScan Move to the next position in the scan.
Close the row source - implemented by MergeScan already Private/Protected methods of This class: Public Methods of This class: Public Methods of RowSource class: All columns are always set from a sorter   Disable illegal and dangerous scan controller interface call @exception StandardException This is an illegal operation
Methods of MergeSort.  Arranged alphabetically. Check the column ordering against the template, making sure that each column is present in the template, is not mentioned more than once, and that the columns isn't {@code null}. <p> Intended to be called as part of a sanity check. All columns are orderable, since {@code DataValueDescriptor} extends {@code Orderable}. Check that the columns in the row agree with the columns in the template, both in number and in type. <p> XXX (nat) Currently checks that the classes implementing each column are the same -- is this right? Remove all the rows from the sort buffer and store them in a temporary conglomerate.  The temporary conglomerate is a "merge run".  Returns the container id of the merge run. An inserter is closing. Drop the sort. Get rid of the merge runs, if there are any. Must not cause any errors because it's called during error processing. Go from the CLOSED to the INITIALIZED state. DEBUG (nat) void printRunInfo(TransactionController tran) throws StandardException { java.util.Enumeration e = mergeRuns.elements(); while (e.hasMoreElements()) { long conglomid = ((Long) e.nextElement()).longValue(); ScanController sc = tran.openScan(conglomid, false, false, null, null, 0, null, null, 0); System.out.println("Merge run: conglomid=" + conglomid); while (sc.next()) { sc.fetch(template); System.out.println(template); } sc.close(); } } Methods of Sort Open a sort controller. <p> This implementation only supports a single sort controller per sort. Open a row source to get rows out of the sorter. Open a scan controller.
Return all information gathered about the sort. <p> This routine returns a list of properties which contains all information gathered about the sort.  If a Property is passed in, then that property list is appended to, otherwise a new property object is created and returned. <p> Not all sorts may support all properties, if the property is not supported then it will not be returned.  The following is a list of properties that may be returned: sortType - type of the sort being performed: internal external numRowsInput - the number of rows input to the sort.  This number includes duplicates. numRowsOutput - the number of rows to be output by the sort.  This number may be different from numRowsInput since duplicates may not be output. numMergeRuns - the number of merge runs for the sort. Applicable to external sorts only. Note: when a SortController is closed, numMergeRuns may increase by 1, to reflect the additional merge run that may be created for any data still in the sort buffer. mergeRunsSize - the size (number of rows) of each merge run for the sort. Applicable to external sorts only. e.g. [3,3,2] indicates 3 merge runs, where the first two runs have 3 rows each, and the last run has 2 rows. Note: when a SortController is closed, this vector may get an additional element, to reflect the additional merge run that may be created for any data still in the sort buffer. NOTE - this list will be expanded as more information about the sort is gathered and returned.
<p> Count the substitutable arguments in an internationalized message string. These arguments have the form {n} where n is a number. </p> ///////////////////////////////////////////////////////////////////////  GENERALLY USEFUL MINIONS  /////////////////////////////////////////////////////////////////////// <p> Echo a message to the console. </p> <p> Replace newlines with the escape sequence needed by properties files. Also, replace single quotes with two single quotes. </p> <p> Replace single quotes with two single quotes. Only needed when there are parameters with quotes. </p> <p> Read the xml message descriptors and output messages_en.properties and the dita source for the SQLState table in the Derby Reference Guide. After setting up arguments using the above setter methods, Ant calls this method in order to run this custom task. </p> <p> Flush and close file writers. </p> //////////////////////////////////////////////////////  XML MINIONS  ////////////////////////////////////////////////////// <p> Get some optional sub-elements. </p> <p> Convert a message handle into a SQLState, stripping off trailing encodings as necessary. </p> <p> Plug arg values into parameter slots in an internationalizable message string. </p> <p> Read a family of message descriptors </p> <p> Read and process a message. </p> ///////////////////////////////////////////////////////////////////////  MINIONS TO PROCESS MESSAGE DESCRIPTORS  /////////////////////////////////////////////////////////////////////// <p> Loop through descriptors and write appropriate output to the properties and dita files. </p> <p> Read a section from the message descriptor file. </p> <p> Loop through sections in the message descriptor file.. </p> Replace a substring with some equivalent. For example, we would like to replace "&lt;" with "&amp;lt;" in the error messages. Add any substrings you would like to replace in the code below. Be aware that the first parameter to the replaceAll() method is interpreted as a regular expression. <p>Let Ant set the file name for the SQLState dita file we will write.</p> <p>Let Ant set the file name for the message property file we will write.</p> ///////////////////////////////////////////////////////////////////////  Task BEHAVIOR  /////////////////////////////////////////////////////////////////////// <p>Let Ant set the input file name.</p> <p> Squeeze the text out of an Element. </p>

class implementation  Method to use instead of ResourceBundle.getBundle(). This method acts like ResourceBundle.getBundle() but if the resource is not available in the requested locale, default locale or base class the one for en_US is returned.  Transform the message from messageID to the actual error, warning, or info message using the correct locale. <P> The arguments to the messages are passed via an object array, the objects in the array WILL be changed by this class. The caller should NOT get the object back from this array. Hash function to split messages into 50 files based upon the message identifier or SQLState. We don't use String.hashCode() as it varies between releases and doesn't provide an even distribution across the 50 files.
Compose a default message so that the user at least gets *something* useful rather than just a MissingResourceException, which is particularly unhelpful Count the number of substituation parameters in the message Format a message given a resource bundle and a message id. <p> The arguments to the messages are passed via an object array. The objects in the array WILL be changed by this class. The caller should NOT get the object back from this array. Instance method to get the complete message, using the provided resource bundle name as specified when this instance was constructed If for some reason the message could not be found, we return a default message using the message arguments This is a wrapper for the getCompleteMessage workhorse routine using some obvious defaults, particularly for non-engine subsystems that only ever use the default locale. Get a message using the default locale.  If the message is not found with the default locale, use the US locale.   Do this both for the common bundle and the parent bundle. If the message is not found in common or in the parent resource bundle, return a default message composed of the message arguments. Generic routine to get a message with any number of arguments. Looks in the provided resource bundle for the message, using the specified locale and then the US locale. Get a message with default locale.
Build the SQLERRMC for a {@code java.sql.DataTruncation} warning. Serialize all the fields of the {@code DataTruncation} instance in the order in which they appear in the parameter list of the constructor. Build preformatted SQLException text for severe exceptions or SQLExceptions that are not Derby exceptions. Just send the message text localized to the server locale. Get messageId and arguments, messageId is necessary for us to look up localized message from property file.  messageId was sent as the last token in the sqlerrmc. The last element of the returned array contains the MessageId field, that must be extracted by the caller funtion, to get an array of arguments.  Method to use instead of ResourceBundle.getBundle(). This method acts like ResourceBundle. getBundle() but if the resource is not available in the requested locale, default locale or base class the one for en_US is returned. Method used by Derby Network Server to get localized message Method used by Derby Network Server to get localized message Hash function to split messages into 50 files based upon the message identifier or SQLState. We don't use String.hashCode() as it varies between releases and doesn't provide an even distribution across the 50 files.
Check that single-quote characters are doubled, as required by {@code java.text.MessageFormat}. Raise an error otherwise. Check that a message format specifier is valid. Raise an error if it is not. <p> Check all the message translations in the specified directories for common problems. Assume that all properties files in the directories are message translations. </p> <p> If a problem is found, an error will be raised. </p> Vet the messages in this file. An error will be raised if an ill-formatted message is found. Vet a specific message. Raise an error if it is not well-formed.

AliasInfo methods   Get the formatID which corresponds to this class. Formatable methods Read this object from a stream of stored objects.  Write this object to a stream of stored objects.
Declare the method throws an exception. Must be called before any code is added to the method. Call a method previously described by describeMethod(). <PRE> static methods Stack ...,value* =&gt; [numArgs number of values will be popped] ...,return_value [void methods will not push a value] non-static methods Stack ...,ref,value* =&gt; [numArgs number of values will be popped] ...,return_value [void methods will not push a value] </PRE> Call a method. The instance (receiver or reference) for non-static methods must be pushed by the caller. The instance (for non-static) and the arguments are popped of the stack, and the return value (if any) is pushed onto the stack. <BR> The type needs to be one of: <UL> <LI> VMOpcode.INVOKESTATIC - call a static method <LI> VMOpcode.INVOKEVIRTUAL - call method declared in the class or super-class. <LI> VMOpcode.INVOKEINTERFACE - call a method declared in an interface </UL> <PRE> static methods Stack ...,value* =&gt; [numArgs number of values will be popped] ...,return_value [void methods will not push a value] non-static methods Stack ...,ref,value* =&gt; [numArgs number of values will be popped] ...,return_value [void methods will not push a value] </PRE> <BR> The type of the arguments to the methods must exactly match the declared types of the parameters to the methods. If a argument is of the incorrect type the caller must up cast it or down cast it. Call super(). Caller must only add this to a constructor. <PRE> Stack ... =&gt; ... </PRE> Cast the top stack value. Correctly down-casts a reference or casts a primitive type (e.g. int to short). <PRE> Stack ...,value =&gt; ...,cast_value </PRE> Indicate the method is complete. Once this call has been made the caller must discard the reference to this object. Complete a conditional which completes the false code path. Initiate a conditional sequence. The top value on the stack must be a boolean and will be popped. If it is true then the code following this call until the startElseCode() will be executed at runtime, otherwise the code following startElseCode() until the completeConditional() is called. See conditionalIfNull() for example and restrictions. <PRE> Stack ...,boolean_value =&gt; ... </PRE>. Initiate a conditional sequence. The top value on the stack (a reference) is popped and compared to 'null'. If the value is null then the code following this call until the startElseCode() will be executed at runtime, otherwise the code following startElseCode() until the completeConditional() is called. <BR> E.g. <PRE> mb.callMethod(...); // pushes an object onto the stack mb.conditionalIfNull(); mb.push(3); mb.startElseCode(); mb.push(5); mb.completeConditional(); // at this point 3 or 5 will be on the stack </PRE> Each path through the ?: statement must leave the stack at the same depth as the other. <BR> If the if or else code pops values from the stack that were before the conditional value, then they must use the same number of values from the stack. <PRE> Stack ...,ref =&gt; ... </PRE>. Return an object that efficiently (to the implementation) describes a zero-argument method and can be used with the single argument callMethod(). Descriptions for the parameters to this method are the same as the five argument callMethod(). This allows the caller to cache frequently used methods. The returned object is only valid for use by this MethodBuilder. <BR> This call does not affect the Stack. Duplicate the top value on the stack. <PRE> Stack ...,value =&gt; ...,value,value </PRE> End a statement. Pops the top-word of the stack, if any. Must only be called if zero or one item exists on the stack. <PRE> Stack value =&gt; :empty: or Stack :empty: =&gt; :empty: </PRE>. Pop an array refrence off the stack and push an element from that array. <PRE> Stack ...,array_ref =&gt; ...,value </PRE> Push the contents of the described field onto the stack. This call requires the instance (reference) to be pushed by the caller. <PRE> Stack ...,field_ref  =&gt; ...,field_value </PRE> Push the contents of the local field onto the stack. This call pushes the this instance required to access the field itself. <PRE> Stack ...  =&gt; ...,field_value </PRE> return the name of the method. Push a parameter value. <PRE> Stack ...  =&gt; ...,param_value </PRE> Push the contents of the described static field onto the stack. <PRE> Stack ...  =&gt; ...,field_value </PRE> Pop the top stack value and push a boolean that is the result of an instanceof check on the popped reference. <PRE> Stack ...,ref =&gt; ...,boolean_value </PRE>. Return from a method, optionally with a value. Must only be called if zero or one item exists on the stack. If the stack contains a single value then that is popped and used as the returned value. <PRE> Stack value =&gt; :empty: or Stack :empty: =&gt; :empty: </PRE>. Pop the top value off the stack <PRE> Stack ..., value =&gt; ... </PRE>. Push a boolean constant onto the stack <PRE> Stack ...  =&gt; ...,boolean_value </PRE> Push a byte constant onto the stack <PRE> Stack ...  =&gt; ...,byte_value </PRE> Push a double constant onto the stack <PRE> Stack ...  =&gt; ...,double_value </PRE> Push a float constant onto the stack <PRE> Stack ...  =&gt; ...,float_value </PRE> Push a int constant onto the stack <PRE> Stack ...  =&gt; ...,int_value </PRE> Push a String constant onto the stack <PRE> Stack ...  =&gt; ...,String_value </PRE> Push a long constant onto the stack <PRE> Stack ...  =&gt; ...,long_value </PRE> Push a short constant onto the stack <PRE> Stack ...  =&gt; ...,short_value </PRE> Create an instance of an array and push it onto the stack. <PRE> Stack ...  =&gt; ...,array_ref </PRE> Complete the sequence that was started with pushNewStart(). Pop the arguments to the constructor and push the reference to the newly created object. <PRE> Stack ...,value* =&gt; [numArgs number of values will be popped] ...,new_ref </PRE> Initiate a sequence that calls a constructor, equivalent to the new operator in Java. After this call, the caller must push any arguments and then complete the construction with a call to pushNewComplete(). Only arguments to the constructor can be pushed onto the stack between the pushNewStart() and pushNewComplete() method calls. <PRE> Stack ... =&gt; [unchanged] ... </PRE> Push a typed null onto the stack <PRE> Stack ...  =&gt; ...,null </PRE> Push this onto the stack. <PRE> Stack ...  =&gt; ...,this_ref </PRE> Pop the top stack value and store it in the instance field of this class. This call pushes the this instance required to access the field itself. Like the Java language 'field = value', this leaves the value on the stack. <PRE> Stack ...,value  =&gt; ...,value </PRE> Pop the top stack value and store it in the field. This call requires the instance to be pushed by the caller. Like the Java language 'field = value', this leaves the value on the stack. <PRE> Stack ...,field_ref,value  =&gt; ...,value </PRE> Pop the top stack value and store it in the local field. This call pushes the this instance required to access the field itself. Like the Java language 'field = value', this leaves the value on the stack. <PRE> Stack ...,value  =&gt; ...,value </PRE> Pop an array reference off the stack, store a value in the array at the passed in offset. <PRE> Stack ...,array_ref, value =&gt; ... </PRE> Pop the top stack value and store it in the local field. This call pushes the this instance required to access the field itself. This call does not leave any value on the stack. <PRE> Stack ...,value  =&gt; ... </PRE> Complete the true code path of a conditional. Tell if statement number in this method builder hits limit.  This method builder keeps a counter of how many statements are added to it. Caller should call this function every time it tries to add a statement to this method builder (counter is increased by 1), then the function returns whether the accumulated statement number hits a limit. The reason of doing this is that Java compiler has a limit of 64K code size for each method.  We might hit this limit if an extremely long insert statement is issued, for example (see beetle 4293).  Counting statement number is an approximation without too much overhead. Swap the top two values on the stack. <PRE> Stack ...,valueA,valueB =&gt; ...,valueB,valueA </PRE> Upcast the top stack value. This is used for correct method resolution by upcasting method parameters. It does not put any casting code into the byte code stream. Can only be used for refrences. <PRE> Stack ...,ref =&gt; ...,ref </PRE>
Accept the visitor for all visitable children of this node. Add the parameter list Return whether or not all of the parameters to this node are QUERY_INVARIANT or CONSTANT.  This is useful for VTIs - a VTI is a candidate for materialization if all of its parameters are QUERY_INVARIANT or CONSTANT Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. <p> Generate and cast one parameter, pushing the result onto the stack. </p> Generate one parameter to the given method call. This method is overriden by RepStaticMethodCallNode. Generate the parameters to the given method call <p> Generate the trailing routine arguments into a varargs array and push that array onto the stack. </p> Build a JBitSet of all of the tables that we are correlated with. Override method in ancestor. Get the index of the first vararg if this is a varargs method <p> Get the schema-qualified name of the the routine. Is non-null only for StaticMethodCallNodes. </p> Build an array of booleans denoting whether or not a given method parameter is a ?.  Get the resolved Classes of our parameters ///////////////////////////////////////////////////////////////////  ACCESSORS  /////////////////////////////////////////////////////////////////// Get the method parameters. Build an array of names of the argument types. These types are biased toward Java objects. That is, if an argument is of SQLType, then we map it to the corresponding Java synonym class (e.g., SQLINT is mapped to 'java.lang.Integer'). Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions)  <p> Get the offset into the routine arguments corresponding to the index of the invocation parameter. The two indexes may be different in the case of varargs methods. There may be more invocation args than declared routine args. For a varargs routine, all of the trailing invocation parameters correspond to the last argument declared by the CREATE FUNCTION/PROCEDURE statement. </p> Get the details on the invoked routines. Turn an array type name into the corresponding vararg type name Return true if the routine has varargs Return true if the parameter is a vararg Parse the user supplied signature for a method and validate it, need to match the number of parameters passed in and match the valid types for the parameter. Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set the appropriate type information for a null passed as a parameter. This method is called after method resolution, when a signature was successfully matched. Return true if some parameters are null, false otherwise. Strip the trailing [] from a type name Build parameters for error message and throw the exception when there is no matching signature found. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Return the default properties for this access method. Return the primary format that this access method supports. Although an access method may support more than one format, this is the usual one.  the access manager will put the primary format in a hash table for fast access to the appropriate method. Return the primary implementation type for this access method. Although an access method may implement more than one implementation type, this is the expected one.  The access manager will put the primary implementation type in a hash table for fast access. Return whether this access method supports the format supplied in the argument. Return whether this access method implements the implementation type given in the argument string.
Does not override finish() (no action required) No action is required, but not implemented in any base class Opens a MiscResultSet, executes the Activation's ConstantAction, and then immediately closes the MiscResultSet.
Generic generate code for all Misc statements that need activations. Returns whether or not this Statement requires a set/clear savepoint around its execution.  The following statement "types" do not require them: Cursor	- unnecessary and won't work in a read only environment Xact	- savepoint will get blown away underneath us during commit/rollback

Check the validity of the default, if any, for this node. Check if the the column can be modified, and throw error if not. If the type of a column is being changed (for instance if the length of the column is being increased) then make sure that this does not violate any key constraints; the column being altered is 1. part of foreign key constraint ==&gt; ERROR. This references a Primary Key constraint and the type and lengths of the pkey/fkey must match exactly. 2. part of a unique/primary key constraint ==&gt; OK if no fkey references this constraint. ==&gt; ERROR if any fkey in the system references this constraint. Check the validity of a user type.  Checks that 1. the column type is either varchar, .... 2. is the same type after the alter. 3. length is greater than the old length. Get the action associated with this node. Get the column position for the column. Get the UUID of the old column default. If the column being modified is of character string type, then it should get its collation from the corresponding column in the TableDescriptor. This will ensure that at alter table time, the existing character string type columns do not loose their collation type. If the alter table is doing a drop column, then we do not need to worry about collation info. check the validity of autoincrement values in the case that we are modifying an existing column (includes checking if autoincrement is set when making a column nullable)

Boot this module with the given properties. Creates a module instance that can be found using the findModule() methods of Monitor. The module can only be found using one of these findModule() methods once this method has returned. <P> An implementation's boot method can throw StandardException. If it is thrown the module is not registered by the monitor and therefore cannot be found through a findModule(). In this case the module's stop() method is not called, thus throwing this exception must free up any resources. <P> When create is true the contents of the properties object will be written to the service.properties of the persistent service. Thus any code that requires an entry in service.properties must <B>explicitly</B> place the value in this properties set using the put method. <BR> Typically the properties object contains one or more default properties sets, which are not written out to service.properties. These default sets are how callers modify the create process. In a JDBC connection database create the first set of defaults is a properties object that contains the attributes that were set on the jdbc:derby: URL. This attributes properties set has the second default properties set as its default. This set (which could be null) contains the properties that the user set on their DriverManager.getConnection() call, and are thus not owned by Derby code, and thus must not be modified by Derby code. <P> When create is false the properties object contains all the properties set in the service.properties file plus a <B>limited</B> number of attributes from the JDBC URL attributes or connection properties set. This avoids properties set by the user compromising the boot process. An example of a property passed in from the JDBC world is the bootPassword for encrypted databases. <P> Code should not hold onto the passed in properties reference after boot time as its contents may change underneath it. At least after the complete boot is completed, the links to all the default sets will be removed. Stop the module. The module may be found via a findModule() method until some time after this method returns. Therefore the factory must be prepared to reject requests to it once it has been stopped. In addition other modules may cache a reference to the module and make requests of it after it has been stopped, these requests should be rejected as well.
Obtain a class that supports the given identifier. Create a persistent service. <BR> <B>Do not call directly - use Monitor.startPersistentService()</B> Find the module in the system with the given module protocol, protocolVersion and identifier. Find a service. <BR> <B>Do not call directly - use Monitor.findService()</B> Return the application set of properties which correspond to the set of properties in the file derby.properties. Canonicalize a service name, mapping different user-specifications of a database name onto a single, standard name. Get a newly created background thread. The thread is set to be a daemon but is not started. Return the environment object that this system was booted in. This is a free form object that is set by the method the system is booted. For example when running in a Marimba system it is set to the maribma application context. In most environments it will be set to a java.io.File object representing the system home directory. Code that call this method usualy have predefined knowledge of the type of the returned object, e.g. Marimba store code knows that this will be set to a marimba application context. Return a property from the JVM's system set. In a Java2 environment this will be executed as a privileged block if and only if the property starts with 'derby.'. If a SecurityException occurs, null is returned. Return the locale of the service that the passed in module lives in. Will return null if no-locale has been defined. Translate a string of the form ll[_CC[_variant]] to a Locale. This is in the Monitor because we want this translation to be in only one place in the code. Return an array of the service identifiers that are running and implement the passed in protocol (java interface class name). This list is a snapshot of the current running systesm, once the call returns the service may have been shutdown or new ones added. Return the name of the service that the passed in module lives in. Return the PersistentService for a subsubprotocol. Return the PersistentService object for a service. Will return null if the service does not exist. Get the defined default system streams object. Get the Timer factory for the system. The Timer factory provides access to Timer objects for various purposes. Get the UUID factory for the system.  The UUID factory provides methods to create and recreate database unique identifiers. Check if a thread is a daemon thread created by {@link #getDaemonThread}. Obtain an new instance of a class that supports the given identifier. Set the locale for the service *outside* of boot time. Set the locale for the service at boot time. The passed-in properties must be the one passed to the boot method. Shut down the complete system that was started by this Monitor. Will cause the stop() method to be called on each loaded module. Shut down a service that was started by this Monitor. Will cause the stop() method to be called on each loaded module. Requires that a context stack exist. Start a module. <BR> <B>Do not call directly - use Monitor.startSystemModule() or Monitor.bootServiceModule()</B> Start a non-persistent service. <BR> <B>Do not call directly - use Monitor.startNonPersistentService()</B> Start a persistent service. <BR> <B>Do not call directly - use Monitor.startPersistentService()</B> <P> The poperty set passed in is for boot options for the modules required to start the service. It does not support defining different or new modules implementations. Start all services identified by derby.service.* in the property set. If bootAll is true the services that are persistent will be booted.
Check whether booting of the module has completed. Set a flag that indicates that booting of the module has completed.
See if this implementation can support any attributes that are listed in properties. This call may be made on a newly created instance before the boot() method has been called, or after the boot method has been called for a running module. <P> The module can check for attributes in the properties to see if it can fulfill the required behaviour. E.g. the raw store may define an attribute called RawStore.Recoverable. If a temporary raw store is required the property RawStore.recoverable=false would be added to the properties before calling bootServiceModule. If a raw store cannot support this attribute its canSupport method would return null. Also see the Monitor class's prologue to see how the identifier is used in looking up properties. <BR><B>Actually a better way maybe to have properties of the form RawStore.Attributes.mandatory=recoverable,smallfootprint and RawStore.Attributes.requested=oltp,fast </B>
Boot or find a identified module within a service. This call allows modules to start or create any modules they explicitly require to exist within their service. If no module matching the criteria is found (see this class's prologue for details) then an instance will be created (see prologue) and booted as follows. <PRE> ((ModuleControl) instance).boot(create, identifer, properties); </PRE> <BR> The service is defined by the service that the module serviceModule lives in, typically this call is made from the boot method of a module and thus 'this' is passed in for serviceModule. Boot or find a unidentified module within a service. This call allows modules to start or create any modules they explicitly require to exist within their service. If no module matching the criteria is found (see this class's prologue for details) then an instance will be created (see prologue) and booted as follows. <PRE> ((ModuleControl) instance).boot(create, (String) null, properties); </PRE> <BR> The service is defined by the service that the module serviceModule lives in, typically this call is made from the boot method of a module and thus 'this' is passed in for serviceModule. Obtain the class object for a class that supports the given identifier. If no class has been registered for the identifier then a StandardException is thrown with no attached java.lang exception (nextException). If a problem loading or accessing the class is obtained then a StandardException is thrown with the real java.lang exception attached. Create a named service that implements the java interface (or class) fully qualified by factoryInterface. The Properties object specifies create time parameters to be used by the modules within the service. Any module created by this service may add or remove parameters within the properties object in their ModuleControl.boot() method. The properties set will be saved by the Monitor for later use when the monitor is started. <P><B>Context</B><BR> A context manager will be created and installed at the start of this method and destroyed just before this method returns. return a StandardException to indicate that an exception caused starting the module to fail. Find an identified module within a service. <BR> The service is defined by the service that the module serviceModule lives in. public static Object findServiceModule(Object serviceModule, String factoryInterface, String identifier) { return monitor.findModule(serviceModule, factoryInterface, identifier); } Find a service. Find an unidentified module within a service. <BR> The service is defined by the service that the module serviceModule lives in. Find a module in the system service.  Translate a localeDescription of the form ll[_CC[_variant]] to a Locale object. Get the monitor. Return the name of the service that the passed in module lives in. Return a system module. If it cannot be found or the monitor is not running then null is returned. Return true if the properties set provided contains database creation attributes for a database of the correct type Is engineType a match for desiredType. A match exists if the bit intersect of the two values is no zero.  Single point for checking if an upgrade is allowed. Logs the stack trace of the specified throwable object. return a StandardException to indicate a missing implementation. * Static methods for startup type exceptions. return a StandardException to indicate that a module failed to start because it could not obtain the version of a required product. Obtain an new instance of a class that supports the given identifier. If no class has been registered for the identifier then a StandardException is thrown with no attached java.lang exception (getNestedException). If a problem loading or accessing the class or creating the object is obtained then a StandardException is thrown with the real java.lang exception attached. Initialize this class, must only be called by an implementation of the monitor (ModuleFactory). Start a Monitor based software system. This method will execute the following steps. <OL> <LI> Create an instance of a module (monitor) of the required implementation. <LI> Start the monitor which will in turn start any requested services <LI> Execute the run() method of startCode (if startCode was not null). <LI> Return. </OL> <P> If MonitorBoot.start() is called more then once then subsequent calls have no effect. Start a non-persistent service. <P><B>Context</B><BR> A context manager will be created and installed at the start of this method and destroyed just before this method returns. Start a persistent service. The name of the service can include a service type, in the form 'type:serviceName'. <BR> Note that the return type only indicates if the service can be handled by the monitor. It does not indicate the service was started successfully. The cases are <OL> <LI> Service type not handled - false returned. <LI> Service type handled, service does not exist, true returned. <LI> Service type handled, service exists and booted OK, true returned. <LI> Service type handled, service exists and failed to boot, exception thrown. </OL> If true is returned then findService should be used to see if the service exists or not. <P> The poperty set passed in is for boot options for the modules required to start the service. It does not support defining different or new modules implementations. Start or find a module in the system service. This call allows modules to explictly start services they require. If no module matching the criteria is found (see this class's prologue for details) then an instance will be created (see prologue) and booted as follows. <PRE> ((ModuleControl) instance).boot(false, (String) null, (Properties) null); </PRE>
Privileged Monitor lookup. Must be private so that user code can't call this entry point.
Return the next non-duplicate value from the probe list. Assumption is that the list is sorted so that duplicates appear next to each other, and that probeValIndex is the index of the next value. If we've exhausted the probe list then just return null. Return the next row (if any) from the scan (if open). More specifically we do the following: 1 - See if we have a row to read from the current scan position. If so, return that row (done). 2 - If there are no more rows to read from the current scan position AND if there are more probe values to look at, then a) reposition the scan using the next probe value as the start/stop key and b) go back to step 1.  Otherwise proceed to step 3. 3 - Return null (no more rows). Note that step 1 is important for cases where multiple rows in this table match a single probe value.  In such a scenario we have to be sure that we do *not* move on to the next probe value until we have returned all of the rows for the _current_ probe value. Initialize the start key and the stop key used in the scan. Both keys will be set to the probe value. If no new probe value was found (the probe list was exhausted), the flag skipNextScan will be {@code true} when the method returns to prevent a new scan from being reopened with a missing or incorrect probe value. Figure out whether or not we can (re-)position the scan controller based on the next value in probeValues.  This will return false when we have exhausted the probe list (i.e. when we've gone through all of the values).   There are two scenarios for which we reopen this kind of scan: A - The first is for join processing.  In this case we have a(nother) row from some outer table and we want to reopen this scan to look for rows matching the new outer row. B - The second is for multi-probing.  Here we want to reopen the scan on this table to look for rows matching the next value in the probe list. If we are reopening the scan for scenario A (join processing) then we need to reset our position within the probe list. If we are reopening the scan for scenario B then we do *not* want to reset our position within the probe list because that position tells us where to find the next probe value. That said, this method does the work of reopenCore() using the received boolean to determine which of the two scenarios we are in.  Note that if our current position (i.e. the value of probeValIndex) is beyond the length of the probe list then we know that we are reopening the scan for scenario A.  Or put another away, we should never get here for scenario B if probeValIndex is greater than or equal to the length of the probe list.  The reason is that the call to reopenCore() for scenario B will only ever happen when moreInListVals() returns true--and in that case we know that probeValIndex will be less than the length of the probeValues.  But the opposite is not true: i.e. it is *not* safe to say that a probeValIndex which is less than the length of probe list is always for scenario B.  That's not true because it's possible that the join to which this scan belongs is a "oneRowRightSide" join, meaning that this, the "right" side scan, will be "interrupted" after we return a single row for the current outer row.  If we then come back with a new outer row we need to reset our position-- even though probeValIndex will be less than probeValues.length in that case.  DERBY-3603. Reopen the scan controller Check if the scan should be skipped. It should be skipped if (1) {@link #initStartAndStopKey()} exhausted the probe list, or (2) the scan should return no results because of nulls in the start key or stop key. See {@link NoPutResultSetImpl#skipScan(ExecIndexRow,ExecIndexRow)} for details about (2).
Execute count transactions per submitter using a newly created thread for each submitter. In total (count*submitter.length) transactions will be executed. The time returned will be the time to execute all the transactions. Each submitter will have its clearTransactionCount called before the run. Return a thread that will run count transactions using a submitter.
Bind this NOP statement.  This throws an exception, because NOP statements by definition stop after parsing.
Implementation of the run() method to start the server
Checks wether the we trust the client. Since this trust manager is just for the Derby clients, this routine is actually never called, but need to be here when we implement X509TrustManager. Checks wether the we trust the server, which we allways will. Return an array of certificate authority certificates which are trusted for authenticating peers. Not relevant for this trust manager. Generate a socket factory with this trust manager. Derby Utility routine which is not part of the X509TrustManager interface.
Cacheable interface
/////////////////////////////////////////////////////////////////////////////////  AUTHENTICATE LOCALLY  ///////////////////////////////////////////////////////////////////////////////// Authenticate the passed-in credentials against the local database. /////////////////////////////////////////////////////////////////////////////////  AUTHENTICATE REMOTELY  ///////////////////////////////////////////////////////////////////////////////// Authenticate the passed-in credentials against another Derby database. This is done by getting a connection to the credentials database using the supplied username and password. If the connection attempts succeeds, then authentication succeeds. Override behavior in superclass Authenticate the passed-in user's credentials. <p> Return true if we are authenticating in this database. </p> <p> Return true if we are authenticating in this service. </p>  /////////////////////////////////////////////////////////////////////////////////  ModuleControl BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Check if we should activate this authentication service. Get the canonical name of the current database service Turn a service name into its normalized, standard form Privileged Monitor lookup. Must be private so that user code can't call this entry point. Privileged Monitor lookup. Must be private so that user code can't call this entry point. /////////////////////////////////////////////////////////////////////////////////  UserAuthenticator BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Override behavior in superclass <p> Return true if the passed in service is the credentials database. </p> <p> Parse the specification of NATIVE authentication. It can take 3 forms: </p> <ul> <li><i>NATIVE:$credentialsDB</i> - Here $credentialsDB is the name of a Derby database. This means that all authentication should take place in $credentialsDB.</li> <li><i>NATIVE:$credentialsDB:LOCAL</i>- This means that system-wide operations (like engine shutdown) are authenticated in $credentialsDB but connections to existing databases are authenticated in those databases.</li> <li><i>NATIVE::LOCAL</i> - This means that connections to a given database are authenticated in that database.</li> </ul> <p> Return true if AUTHENTICATION_PROVIDER_PARAMETER was well formatted. The property must have designated some database as the authentication authority. </p>
ResultSet interface (leftover from NoPutResultSet)  Clear any private state that changes during scans. This includes things like the last row seen, etc. THis does not include immutable things that are typically set up in the constructor. <p> This method is called on open()/close() and reopen() <p> WARNING: this should be implemented in every sub class and it should always call super.clearScanState(). If the result set has been opened, close the open scan. Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction parameters are evaluated for each row. Return the total amount of time spent in this ResultSet
@see JoinStrategy#estimateCost              Can this join strategy be used on the outermost table of a join.
Clear any private state that changes during scans. This includes things like the last row seen, etc. THis does not include immutable things that are typically set up in the constructor. <p> This method is called on open()/close() and reopen() <p> WARNING: this should be implemented in every sub class and it should always call super.clearScanState().  ResultSet interface (leftover from NoPutResultSet)  Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction parameters are evaluated for each row.
Close socket and its streams. Returns the current timeout value that is set on the socket. Note that the support for timeout on sockets is dependent on the OS implementation. For the same reason we ignore any exceptions thrown by the call to the socket layer. ----------------------- call-down methods --------------------------------- Marks the agent's write chain as dirty. A write chain is dirty when data from it has been sent to the server. A dirty write chain cannot be reset and reused for another request until the remaining data has been sent to the server and the write chain properly ended. Resetting a dirty chain will cause the new request to be appended to the unfinished request already at the server, which will likely lead to cryptic syntax errors. Specifies the maximum blocking time that should be used when sending and receiving messages. The timeout is implemented by using the the underlying socket implementation's timeout support. Note that the support for timeout on sockets is dependent on the OS implementation. For the same reason we ignore any exceptions thrown by the call to the socket layer. Switches the current CCSID manager to EBCDIC Switches the current CCSID manager to UTF-8
-----------------------------state------------------------------------------ ---------------------constructors/finalizer---------------------------------

Check if the connection can be closed when there are uncommitted operations. Driver-specific determination if local COMMIT/ROLLBACK is allowed; Allow local COMMIT/ROLLBACK only if we are not in an XA transaction -------------------private helper methods-------------------------------- Determine if a security mechanism is supported by the security manager used for the connection. An exception is thrown if the security mechanism is not supported by the secmgr. Close the connection and release its resources. Construct the correlation token. The crrtkn has the following format.  <Almost IP address>.<local port number><current time in millis> |                   | |               ||                  | +----+--------------+ +-----+---------++---------+--------+ |                      |                | 8 bytes               4 bytes         6 bytes Total lengtho of 19 bytes.  1 char for each 1/2 byte in the IP address. If the first character of the <IP address> or <port number> starts with '0' thru '9', it will be mapped to 'G' thru 'P'. Reason for mapping the IP address is in order to use the crrtkn as the LUWID when using SNA in a hop site. JDBC 4.0 methods --------------------------------flow methods-------------------------------- The User ID and Strong Password Substitute mechanism (USRSSBPWD) authenticates the user like the user ID and password mechanism, but the password does not flow. A password substitute is generated instead using the SHA-1 algorithm, and is sent to the application server. The application server generates a password substitute using the same algorithm and compares it with the application requester's password substitute. If equal, the user is authenticated. The SECTKN parameter is used to flow the client and server encryption seeds on the ACCSEC and ACCSECRD commands. More information in DRDA, V3, Volume 3 standard - PWDSSB (page 650) <code>getClientInfo</code> always returns an empty <code>Properties</code> object since Derby doesn't support ClientInfoProperties. <code>getClientInfo</code> always returns a <code>null String</code> since Derby doesn't support ClientInfoProperties. Methods to get the manager levels for Regression harness only. Handle socket timeouts during connection attempts SECMEC_USRSSBPWD security mechanism - Generate a source (client) seed to send to the target (application) server.  Returns if a transaction is in process Checks if the connection has not been closed and is still valid. The validity is checked by running a simple query against the database. The timeout specified by the caller is implemented as follows: On the server: uses the queryTimeout functionality to make the query time out on the server in case the server has problems or is highly loaded. On the client: uses a timeout on the socket to make sure that the client is not blocked forever in the cases where the server is "hanging" or not sending the reply. Returns false unless <code>interfaces</code> is implemented If secchkcd is not 0, map to SqlException according to the secchkcd received. -------------------Abstract object factories-------------------------------- Invokes readCommit on NetXAConnection Invokes writeRollback on NetXAConnection preferably without password in the method signature. We can probally get rid of flowReconnect method. Checks whether the server supports locators for large objects. Check whether the server has full support for the QRYCLSIMP parameter in OPNQRY. Return true if the server supports nanoseconds in timestamps Check whether the server supports UDTs Check whether the server supports the UTF-8 Ccsid Manager secmecList is always required and will not be null. secchkcd has an implied severity of error. it will be returned if an error is detected. if no errors and security mechanism requires a sectkn, then <code>setClientInfo</code> will always throw a <code>SQLClientInfoException</code> since Derby does not support any properties. <code>setClientInfo</code> will throw a <code>SQLClientInfoException</code> unless the <code>properties</code> parameter is empty, since Derby does not support any properties. All the property keys in the <code>properties</code> parameter are added to failedProperties of the exception thrown, with REASON_UNKNOWN_PROPERTY as the value.  -------------------parse callback methods-------------------------------- Check whether the server supports session data caching Returns <code>this</code> if this class implements the interface Invokes write commit on NetXAConnection Invokes writeRollback on NetXAConnection
----------------------non-parsing computational helper methods-------------- Messages SQLSTATE : 58010 Execution failed due to a distribution protocol error that will affect the successful execution of subsequent DDM commands or SQL statements. SQLCODE : -30021 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Manager <manager> at Level <level> not supported.  A system erro occurred that prevented successful connection of the application to the remote database.  This message (SQLCODE) is producted for SQL CONNECT statement. These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. The client can detect that a conversational protocol error has occurred. This can also be detected at the server in which case a PRCCNVRM is returned. The Conversation Protocol Error Code, PRCCNVRM, describes the various errors.  Note: Not all of these may be valid at the client.  See descriptions for which ones make sense for client side errors/checks. Conversation Error Code                  Description of Error -----------------------                  -------------------- 0x01                                     RPYDSS received by target communications manager. 0x02                                     Multiple DSSs sent without chaining or multiple DSS chains sent. 0x03                                     OBJDSS sent when not allowed. 0x04                                     Request correlation identifier of an RQSDSS is less than or equal to the previous RQSDSS's request correlatio identifier in the chain. 0x05                                     Request correlation identifier of an OBJDSS does not equal the request correlation identifier of the preceding RQSDSS. 0x06                                     EXCSAT was not the first command after the connection was established. 0x10                                     ACCSEC or SECCHK command sent in wrong state. 0x11                                     SYNCCTL or SYNCRSY command is used incorrectly. 0x12                                     RDBNAM mismatch between ACCSEC, SECCHK, and ACCRDB. 0x13                                     A command follows one that returned EXTDTAs as reply object.  When the client detects these errors, it will be handled as if a PRCCNVRM is returned from the server.  In this PRCCNVRM case, PROTOCOL architects an SQLSTATE of 58008 or 58009 depening of the SVRCOD.  In this case, a 58009 will always be returned. Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database. Also called by NetStatementReply and others Also called by NetStatementReply Abnormal End Unit of Work Condition Reply Message indicates that the current unit of work ended abnormally because of some action at the target server.  This can be caused by a deadlock resolution, operator intervention, or some similar situation that caused the relational database to rollback the current unit of work.  This reply message is returned only if an SQLAM issues the command.  Whenever an ABNUOWRM is returned in response to a command, an SQLCARD object must also be returned following the ABNUOWRM.  The SQLSTATE is returned in the SQLCARD.  Returned from Server: SVRCOD - required (8 - ERROR) RDBNAM - required  Called by all the NET*Reply classes. Access to RDB Completed (ACRDBRM) Reply Message specifies that an instance of the SQL application manager has been created and is bound to the specified relation database (RDB).  Returned from Server: SVRCOD - required  (0 - INFO, 4 - WARNING) PRDID - required TYPDEFNAM - required (MINLVL 4) (QTDSQLJVM) TYPDEFOVR - required RDBINTTKN - optional CRRTKN - optional USRID - optional SRVLST - optional (MINLVL 5) Parse the reply for the Access RDB Command. This method handles the parsing of all command replies and reply data for the accrdb command. The Access Security Reply Data (ACSECRD) Collection Object contains the security information from a target server's security manager. this method returns the security check code received from the server (if the server does not return a security check code, this method will return 0).  it is up to the caller to check the value of this return code and take the appropriate action.  Returned from Server: SECMEC - required SECTKN - optional (MINLVL 6) SECCHKCD - optional Parse the reply for the Access Security Command. This method handles the parsing of all command replies and reply data for the accsec command. Perform necessary actions for parsing of a ABNUOWRM message. Perform necessary actions for parsing of a ABNUOWRM message. Perform necessary actions for parsing of a ABNUOWRM message. CCSID for Double-Byte Characters specifies a coded character set identifier for double-byte characters. CCSID for Mixed-Byte Characters specifies a coded character set identifier for mixed-byte characters. CCSID for Single-Byte Characters specifies a coded character set identifier for single-byte characters. Command Check Reply Message indicates that the requested command encountered an unarchitected and implementation-specific condition for which there is no architected message.  If the severity code value is ERROR or greater, the command has failed.  The message can be accompanied by other messages that help to identify the specific condition. The CMDCHKRM should not be used as a general catch-all in place of product-defined messages when using product extensions to DDM. PROTOCOL architects the SQLSTATE value depending on SVRCOD SVRCOD 0 -> SQLSTATE is not returned SVRCOD 8 -> SQLSTATE of 58008 or 58009 SVRCOD 16,32,64,128 -> SQLSTATE of 58009  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.   Returned from Server: SVRCOD - required  (0 - INFO, 4 - WARNING, 8 - ERROR, 16 - SEVERE, 32 - ACCDMG, 64 - PRMDMG, 128 - SESDMG)) RDBNAM - optional (MINLVL 3) RECCNT - optional (MINVAL 0, MINLVL 3)  Called by all the Reply classesCMDCHKRM Command Not Supported Reply Message indicates that the specified command is not recognized or not supported for the specified target.  The reply message can be returned only in accordance with the architected rules for DDM subsetting. PROTOCOL architects an SQLSTATE of 58014.  Messages SQLSTATE : 58014 The DDM command is not supported. SQLCODE : -30070 <command-identifier> Command is not supported. The current transaction is rolled back and the application is disconnected from the remote database. The statement cannot be processed.   Returned from Server: SVRCOD - required  (4 - WARNING, 8 - ERROR) (MINLVL 2) CODPNT - required RDBNAM - optional (MINLVL 3)  The Code Point Data specifies a scalar value that is an architected code point. Code Point Data Representation specifies the data representation of a dictionary codepoint.  Code points are hexadecimal aliases for DDM named terms. Correlation Token specifies a token that is conveyed between source and target servers for correlating the processing between servers. -----------------------------parse DDM Reply Messages----------------------- Called by all the NET*Reply classes. Must make a version that does not change state in the associated connection Parse the reply for the Exchange Server Attributes Command (Dummy) This method handles the parsing of all command replies and reply data for the excsat command. The End Unit of Work Condition (ENDUOWRM) Reply Mesage specifies that the unit of work has ended as a result of the last command.  Returned from Server: SVRCOD - required  (4 WARNING) UOWDSP - required RDBNAM - optional --------------------- parse DDM Reply Data-------------------------------------- The Server Attributes Reply Data (EXCSATRD) returns the following information in response to an EXCSAT command: - the target server's class name - the target server's support level for each class of manager the source requests - the target server's product release level - the target server's external name - the target server's name  Returned from Server: EXTNAM - optional MGRLVLLS - optional SRVCLSNM - optional SRVNAM - optional SRVRLSLV - optional Parse the reply for the Exchange Server Attributes Command. This method handles the parsing of all command replies and reply data for the excsat command. The External Name is the name of the job, task, or process on a system for which a DDM server is active.  On a source DDM server, the external name is the name of the job that is requesting access to remote resources.  For a target DDM server, the external name is the name of the job the system creates or activates to run the DDM server. No semantic meaning is assigned to external names in DDM. External names are transmitted to aid in problem determination. this is duplicated in parseColumnMetaData, but different DAGroup under NETColumnMetaData requires a lot more stuffs including precsion, scale and other stuffs Parse the initial PBSD - PiggyBackedSessionData code point. <p> If sent by the server, it contains a PBSD_ISO code point followed by a byte representing the JDBC isolation level, and a PBSD_SCHEMA code point followed by the name of the current schema as an UTF-8 String. Manager-Level List. Specifies a list of code points and support levels for the classes of managers a server supports. The target server must not provide information for any target managers unless the source explicitly requests it. For each manager class, if the target server's support level is greater than or equal to the source server's level, then the source server's level is returned for that class if the target server can operate at the source's level; otherwise a level 0 is returned.  If the target server's support level is less than the source server's level, the target server's level is returned for that class.  If the target server does not recognize the code point of a manager class or does not support that class, it returns a level of 0.  The target server then waits for the next command or for the source server to terminate communications. When the source server receives EXCSATRD, it must compare each of the entries in the mgrlvlls parameter it received to the corresponding entries in the mgrlvlls parameter it sent.  If any level mismatches, the source server must decide whether it can use or adjust to the lower level of target support for that manager class.  There are no architectural criteria for making this decision. The source server can terminate communications or continue at the target servers level of support.  It can also attempt to use whatever commands its user requests while receiving eror reply messages for real functional mismatches. The manager levels the source server specifies or the target server returns must be compatible with the manager-level dependencies of the specified manangers.  Incompatible manager levels cannot be specified. After this method successfully returns, the targetXXXX values (where XXXX represents a manager name.  example targetAgent) contain the negotiated manager levels for this particular connection. Manager-Level Number Attribute Binary Integer Number specifies the level of a defined DDM manager. Manager-Level Conflict (MGRLVLRM) Reply Message indicates that the manager levels specified in the MGRLVLLS conflict amoung themselves or with previously specified manager levels. - The manager-level dependencies of one specified manager violates another specified maanger level. - The manager- level specified attempts to respecify a manager level that previously EXCSAT command specified. PROTOCOL architects an SQLSTATE of 58010.  Messages SQLSTATE : 58010 Execution failed due to a distributed protocol error that will affect the successful execution of subsequent DDM commands or SQL statements. SQLCODE : -30021 Execution failed due to a distribution protocol error that will affect the successful execution of subsequent commands and SQL statements: Manager <manager> at Level <level> not supported.  A system error occurred that prevented successful connection of the application to the remote database.   Returned from Server: SVRCOD - required  (8 - ERROR) MGRLVLLS - required  Object Not Supported Reply Message indicates that the target server does not recognize or support the object specified as data in an OBJDSS for the command associated with the object. The OBJNSPRM is also returned if an object is found in a valid collection in an OBJDSS (such as RECAL collection) that that is not valid for that collection. PROTOCOL Architects an SQLSTATE of 58015.  Messages SQLSTATE : 58015 The DDM object is not supported. SQLCODE : -30071 <object-identifier> Object is not supported. The current transaction is rolled back and the application is disconnected from the remote database. The command cannot be processed.   Returned from Server: SVRCOD - required  (8 - ERROR, 16 - SEVERE) CODPNT - required RECCNT - optional (MINVAL 0)  (will not be returned - should be ignored) RDBNAM - optional (MINLVL 3)  Also called by NetPackageReply and NetStatementReply Parse a PBSD - PiggyBackedSessionData code point. Can contain one or both of, a PBSD_ISO code point followed by a byte representing the jdbc isolation level, and a PBSD_SCHEMA code point followed by the name of the current schema as an UTF-8 String. Conversational Protocol Error Code specifies the condition for which the PRCCNVRm was returned. Conversational Protocol Error Reply Message indicates that a conversational protocol error occurred. PROTOCOL architects the SQLSTATE value depending on SVRCOD SVRCOD 8 -> SQLSTATE of 58008 or 58009 SVRCOD 16,128 -> SQLSTATE of 58009  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.   Returned from Server: SVRCOD - required  (8 - ERROR, 16 - SEVERE, 128 - SESDMG) PRCCNVCD - required RECCNT - optional (MINVAL 0, MINLVL 3) RDBNAM - optional (NINLVL 3)  Product specific Identifier specifies the product release level of a DDM server. RDB Currently Accessed Reply Message inidcates that the ACCRDB command cannot be issued because the requester has access to a relational database. PROTOCOL architects an SQLSTATE of 58008 or 58009.  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.   Returned from Server: SVRCOD - required  (8 - ERROR) RDBNAM - required  RDB Access Failed Reply Message specifies that the relational database failed the attempted connection. An SQLCARD object must also be returned, following the RDBAFLRM, to explain why the RDB failed the connection. In addition, the target SQLAM instance is destroyed. The SQLSTATE is returned in the SQLCARD.  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.   Returned from Server: SVRCOD - required  (8 - ERROR) RDBNAM - required  Not Authorized to RDB Reply Message specifies that the requester is not authorized to access the specified relational database. PROTOCOL architects an SQLSTATE of 08004  Messages SQLSTATE : 8004 Authorization ID <authorization-ID> attempted to perform the specified <operation> without having been granted the proper authorization to do so. SQLCODE : -30060 <authorization-ID> does not have the privilege to perform operation <operation>.   Returned from Server: SVRCOD - required  (8 - ERROR) RDBNAM - required  ------------------parse reply for specific command-------------------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. Parse the reply for the RDB Commit Unit of Work Command. This method handles the parsing of all command replies and reply data for the rdbcmm command. RDB Not Accessed Reply Message indicates that the access relational database command (ACCRDB) was not issued prior to a command requesting the RDB Services. PROTOCOL Architects an SQLSTATE of 58008 or 58009.  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.   Returned from Server: SVRCOD - required  (8 - ERROR) RDBNAM - required  Called by all the NET*Reply classes. Relational Database Name specifies the name of a relational database of the server.  A server can have more than one RDB. RDB Not Found Reply Message indicates that the target server cannot find the specified relational database. PROTOCOL architects an SQLSTATE of 08004.  Messages SQLSTATE : 8004 The application server rejected establishment of the connection. SQLCODE : -30061 The database alias or database name <name> was not found at the remote node. The statement cannot be processed.   Returned from Server: SVRCOD - required  (8 - ERROR) RDBNAM - required  Parse the reply for the RDB Rollback Unit of Work Command. This method handles the parsing of all command replies and reply data for the rdbrllbck command. The Security Check Code String codifies the security information and condition for the SECCHKRM. The Security Check (SECCHKRM) Reply Message indicates the acceptability of the security information. this method returns the security check code. it is up to the caller to check the value of this return code and take the appropriate action.  Returned from Server: SVRCOD - required  (0 - INFO, 8 - ERROR, 16 -SEVERE) SECCHKCD - required SECTKN - optional, ignorable SVCERRNO - optional Parse the reply for the Security Check Command. This method handles the parsing of all command replies and reply data for the secchk command. Security Mechanims. The Security Token Byte String is information provided and used by the various security mechanisms. SQLCAGRP : FDOCA EARLY GROUP SQL Communcations Area Group Description  FORMAT FOR SQLAM <= 6 SQLCODE; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; PROTOCOL TYPE N-GDA; ENVLID 0x52; Length Override 0  FORMAT FOR SQLAM >= 7 SQLCODE; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; PROTOCOL TYPE N-GDA; ENVLID 0x52; Length Override 0 SQLDIAGGRP; PROTOCOL TYPE N-GDA; ENVLID 0x56; Length Override 0 --------------------------parse FDOCA objects------------------------ SQLCARD : FDOCA EARLY ROW SQL Communications Area Row Description  FORMAT FOR ALL SQLAM LEVELS SQLCAGRP; GROUP LID 0x54; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLCAXGRP : EARLY FDOCA GROUP SQL Communications Area Exceptions Group Description  FORMAT FOR SQLAM <= 6 SQLRDBNME; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 18 SQLERRD1; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD2; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD3; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD4; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD5; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD6; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLWARN0; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN1; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN2; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN3; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN4; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN5; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN6; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN7; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN8; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN9; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARNA; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLERRMSG_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 70 SQLERRMSG_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 70  FORMAT FOR SQLAM >= 7 SQLERRD1; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD2; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD3; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD4; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD5; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD6; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLWARN0; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN1; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN2; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN3; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN4; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN5; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN6; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN7; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN8; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN9; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARNA; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLRDBNAME; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 1024 SQLERRMSG_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 70 SQLERRMSG_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 70 SQL Diagnostics Connection Group Description - Identity 0xD6 Nullable  SQLCNSTATE; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLCNSTATUS; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLCNATYPE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLCNETYPE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLCNPRDID; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 8 SQLCNRDB; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 1024 SQLCNCLASS; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLCNAUTHID; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQL Diagnostics Connection Row - Identity 0xE6 SQLCNGRP; GROUP LID 0xD6; ELEMENT TAKEN 0(all); REP FACTOR 1 SQL Diagnostics Condition Group Description  SQLDCCODE; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCSTATE; PROTOCOL TYPE FCS; ENVLID Ox30; Lengeh Override 5 SQLDCREASON; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCLINEN; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCROWN; PROTOCOL TYPE FD; ENVLID 0x0E; Lengeh Override 31 SQLDCER01; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER02; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER03; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER04; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCPART; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCPPOP; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCMSGID; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 10 SQLDCMDE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 8 SQLDCPMOD; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 5 SQLDCRDB; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLDCTOKS; PROTOCOL TYPE N-RLO; ENVLID 0xF7; Length Override 0 SQLDCMSG_m; PROTOCOL TYPE NVMC; ENVLID 0x3F; Length Override 32672 SQLDCMSG_S; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 32672 SQLDCCOLN_m; PROTOCOL TYPE NVCM ; ENVLID 0x3F; Length Override 255 SQLDCCOLN_s; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCCURN_m; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCCURN_s; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCPNAM_m; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCPNAM_s; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXGRP; PROTOCOL TYPE N-GDA; ENVLID 0xD3; Length Override 1 SQL Diagnostics Condition Row - Identity 0xE5 SQLDCGRP; GROUP LID 0xD5; ELEMENT TAKEN 0(all); REP FACTOR 1 SQL Diagnostics Condition Token Array - Identity 0xF7 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLTOKROW; ROW LID 0xE7; ELEMENT TAKEN 0(all); REP FACTOR 0(all) SQL Diagnostics Extended Names Group Description - Identity 0xD5 Nullable  SQLDCXRDB_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 1024 SQLDCXSCH_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXNAM_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXTBLN_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXRDB_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 1024 SQLDCXSCH_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXNAM_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXTBLN_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255  SQLDCXCRDB_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 1024 SQLDCXCSCH_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXCNAM_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXCRDB_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 1024 SQLDCXCSCH_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXCNAM_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255  SQLDCXRRDB_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 1024 SQLDCXRSCH_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXRNAM_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXRRDB_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 1024 SQLDCXRSCH_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXRNAM_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255  SQLDCXTRDB_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 1024 SQLDCXTSCH_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXTNAM_m ; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCXTRDB_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 1024 SQLDCXTSCH_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXTNAM_s ; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQL Diagnostics Condition Information Array - Identity 0xF5 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDCIROW; ROW LID 0xE5; ELEMENT TAKEN 0(all); REP FACTOR 0(all) SQL Diagnostics Connection Array - Identity 0xF6 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLCNROW;  ROW LID 0xE6; ELEMENT TAKEN 0(all); REP FACTOR 0(all) SQLDIAGGRP : FDOCA EARLY GROUP SQL Diagnostics Group Description - Identity 0xD1 Nullable Group SQLDIAGSTT; PROTOCOL TYPE N-GDA; ENVLID 0xD3; Length Override 0 SQLDIAGCN;  DRFA TYPE N-RLO; ENVLID 0xF6; Length Override 0 SQLDIAGCI;  PROTOCOL TYPE N-RLO; ENVLID 0xF5; Length Override 0 SQL Diagnostics Statement Group Description - Identity 0xD3 Nullable Group SQLDSFCOD; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDSCOST; PROTOCOL TYPE I4; ENVLID 0X02; Length Override 4 SQLDSLROW; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDSNPM; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDSNRS; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDSRNS; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDSDCOD; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDSROWC; PROTOCOL TYPE FD; ENVLID 0x0E; Length Override 31 SQLDSNROW; PROTOCOL TYPE FD; ENVLID 0x0E; Length Override 31 SQLDSROWCS; PROTOCOL TYPE FD; ENVLID 0x0E; Length Override 31 SQLDSACON; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLDSACRH; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLDSACRS; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLDSACSL; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLDSACSE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLDSACTY; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLDSCERR; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLDSMORE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLNUMGRP : FDOCA EARLY GROUP SQL Number of Elements Group Description  FORMAT FOR ALL SQLAM LEVELS SQLNUM; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLNUMROW : FDOCA EARLY ROW SQL Number of Elements Row Description  FORMAT FOR SQLAM LEVELS SQLNUMGRP; GROUP LID 0x58; ELEMENT TAKEN 0(all); REP FACTOR 1 check on SQLTOKGRP format SQL Diagnostics Token Row - Identity 0xE7 SQLTOKGRP; GROUP LID 0xD7; ELEMENT TAKEN 0(all); REP FACTOR 1 Server Class name specifies the name of a class of DDM servers. Server class names are assigned for each product involved in PROTOCOL. Server Name is the name of the DDM server. No semantic meaning is assigned to server names in DDM, but it is recommended that the server names are transmitted for problem determination. Server Product Release Level String specifies the product release level of a DDM server.  The contents of the parameter are unarchitected.  Up to 255 bytes can be sent. SRVRLSLV should not be used in place of product-defined extensions to carry information not related to the products release level. Severity Code is an indicator of the severity of a condition detected during the execution of a command. The SYNCCRD Reply Mesage  Returned from Server: XARETVAL - required Called by the XA commit and rollback parse reply methods. This method handles the parsing of all command replies and reply data for the SYNCCTL command. Process XA return value Syntax Error Code String specifies the condition that caused termination of data stream parsing. Data Stream Syntax Error Reply Message indicates that the data sent to the target agent does not structurally conform to the requirements of the DDM architecture.  The target agent terminated paring of the DSS when the condition SYNERRCD specified was detected. PROTOCOL architects an SQLSTATE of 58008 or 58009.  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.   Returned from Server: SVRCOD - required  (8 - ERROR) SYNERRCD - required RECCNT - optional (MINVAL 0, MINLVL 3) (will not be returned - should be ignored) CODPNT - optional (MINLVL 3) RDBNAM - optional (MINLVL 3)  Unit of Work Disposition Scalar Object specifies the disposition of the last unit of work. The User Id specifies an end-user name. Parameter Value Not Supported Reply Message indicates that the parameter value specified is either not recognized or not supported for the specified parameter. The VALNSPRM can only be specified in accordance with the rules specified for DDM subsetting. The code point of the command parameter in error is returned as a parameter in this message. PROTOCOL Architects an SQLSTATE of 58017.  if codepoint is 0x119C,0x119D, or 0x119E then SQLSTATE 58017, SQLCODE -332 else SQLSTATE 58017, SQLCODE -30073  Messages SQLSTATE : 58017 The DDM parameter value is not supported. SQLCODE : -332 There is no available conversion for the source code page <code page> to the target code page <code page>. Reason code <reason-code>. The reason codes are as follows: 1 source and target code page combination is not supported by the database manager. 2 source and target code page combination is either not supported by the database manager or by the operating system character conversion utility on the client node. 3 source and target code page combination is either not supported by the database manager or by the operating system character conversion utility on the server node.  SQLSTATE : 58017 The DDM parameter value is not supported. SQLCODE : -30073 <parameter-identifier> Parameter value <value> is not supported. Some possible parameter identifiers include: 002F  The target server does not support the data type requested by the application requester. The target server does not support the CCSID requested by the application requester. Ensure the CCSID used by the requester is supported by the server. 119C - Verify the single-byte CCSID. 119D - Verify the double-byte CCSID. 119E - Verify the mixed-byte CCSID.  The current environment command or SQL statement cannot be processed successfully, nor can any subsequent commands or SQL statements.  The current transaction is rolled back and the application is disconnected from the remote database. The command cannot be processed.  Returned from Server: SVRCOD - required  (8 - ERROR) CODPNT - required RECCNT - optional (MINLVL 3, MINVAL 0) (will not be returned - should be ignored) RDBNAM - optional (MINLVL 3)  Process XA return value NET only entry point NET only entry point NET only entry point NET only entry point
The Access RDB (ACCRDB) command makes a named relational database (RDB) available to a requester by creating an instance of an SQL application manager.  The access RDB command then binds the created instance to the target agent and to the RDB. The RDB remains available (accessed) until the communications conversation is terminate. build the Exchange Server Attributes Command. This command sends the following information to the server. - this driver's server class name - this driver's level of each of the manager's it supports - this driver's product release level - this driver's external name - this driver's server name The External Name is the name of the job, task, or process on a system for which a DDM server is active. ----------------------helper methods---------------------------------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. RDB Commit Unit of Work (RDBCMM) Command commits all work performed for the current unit of work.  The Relational Database Name (RDBNAM) is an optional parameter which will not be sent by this command to reduce size, building, and parsing. Relational Database Name specifies the name of a relational database of the server. if length of RDB name &lt;= 18 characters, there is not change to the format of the RDB name.  The length of the RDBNAM remains fixed at 18 which includes any right bland padding if necessary. if length of the RDB name is &gt; 18 characters, the length of the RDB name is identical to the length of the RDB name.  No right blank padding is required. RDB Rollback Unit of Work(RDBRLLBCK) Command rolls back all work performed for the current unit of work.  The Relational Database Name (RDBNAM) is an optional parameter which will not be sent by this command to reduce size, building, and parsing. Precondition: valid secmec is assumed. Server Name is the name of the DDM server. Server Product Release Level String specifies the product release level of a DDM server. ----------------------------- entry points --------------------------------- Build the SYNNCTL commit command Build the SYNNCTL rollback command
Adjust column offsets after fetching the next part of a split row. -----------------------------parsing the data buffer------------------------ Calculate the column offsets for a row. <p> Pseudo-code: <ol> <li>parse thru the current row in dataBuffer computing column offsets</li> <li>if (we hit the super.lastValidBytePosition, ie. encounter partial row) <ol> <li>shift partial row bytes to beginning of dataBuffer (this.shiftPartialRowToBeginning())</li> <li>reset current row position (also done by this.shiftPartialRowToBeginning())</li> <li>send and recv continue-query into commBuffer (rs.flowContinueQuery())</li> <li>parse commBuffer up to QRYDTA (rs.flowContinueQuery())</li> <li>copy query data from reply's commBuffer to our dataBuffer (this.copyQrydta())</li> </ol> </ol> Calculates the column index for Lob objects constructed from EXTDTA data. Describe information isn't sufficient because we have to check for trivial values (nulls or zero-length) and exclude them. Need also to check whether locator was returned since in that case there will be no EXTDTA data for the LOB column. It is possible for the driver to have received an QRYDTA(with incomplete row)+ENDQRYRM+SQLCARD. This means some error has occurred on the server and the server is terminating the query. Before sending a CNTQRY to retrieve the rest of the split row, check if an ENDQRYRM has already been received.  If so, do not send CNTQRY because the cursor is already closed on the server. Instead, throw a SqlException.  Since we did not receive a complete row, it is not safe to allow the application to continue to access the ResultSet, so we close it. Check if the data we want crosses a row split, and fetch more data if necessary. This method is not for column data; use {@link #checkForSplitRowAndComplete(int, int)} for that. Check if the data we want crosses a row split, and fetch more data if necessary. Fetch more data for a row that has been split up.   Check whether QRYCLSIMP is enabled on this cursor. this is really an event-callback from NetStatementReply.parseSQLDTARDarray() prereq: the base data for the cursor has been processed for offsets and lengths Get locator for LOB of the designated column <p> Note that this method cannot be invoked on a LOB column that is NULL. memory leak fix SQLCAGRP : FDOCA EARLY GROUP SQL Communcations Area Group Description  FORMAT FOR SQLAM <= 6 SQLCODE; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; PROTOCOL TYPE N-GDA; ENVLID 0x52; Length Override 0  FORMAT FOR SQLAM >= 7 SQLCODE; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLSTATE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 5 SQLERRPROC; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 8 SQLCAXGRP; PROTOCOL TYPE N-GDA; ENVLID 0x52; Length Override 0 SQLDIAGGRP; PROTOCOL TYPE N-GDA; ENVLID 0x56; Length Override 0 SQLCARD : FDOCA EARLY ROW SQL Communications Area Row Description  FORMAT FOR ALL SQLAM LEVELS SQLCAGRP; GROUP LID 0x54; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLCAXGRP : EARLY FDOCA GROUP SQL Communications Area Exceptions Group Description  FORMAT FOR SQLAM <= 6 SQLRDBNME; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 18 SQLERRD1; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD2; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD3; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD4; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD5; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD6; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLWARN0; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN1; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN2; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN3; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN4; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN5; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN6; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN7; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN8; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN9; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARNA; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLERRMSG_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 70 SQLERRMSG_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 70  FORMAT FOR SQLAM >= 7 SQLERRD1; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD2; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD3; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD4; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD5; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLERRD6; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLWARN0; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN1; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN2; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN3; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN4; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN5; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN6; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN7; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN8; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARN9; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLWARNA; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 1 SQLRDBNAME; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 1024 SQLERRMSG_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 70 SQLERRMSG_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 70 SQL Diagnostics Condition Group Description  SQLDCCODE; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCSTATE; PROTOCOL TYPE FCS; ENVLID Ox30; Lengeh Override 5 SQLDCREASON; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCLINEN; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCROWN; PROTOCOL TYPE I8; ENVLID 0x16; Lengeh Override 8 SQLDCER01; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER02; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER03; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCER04; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCPART; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCPPOP; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLDCMSGID; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 10 SQLDCMDE; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 8 SQLDCPMOD; PROTOCOL TYPE FCS; ENVLID 0x30; Length Override 5 SQLDCRDB; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 1024 SQLDCTOKS; PROTOCOL TYPE N-RLO; ENVLID 0xF7; Length Override 0 SQLDCMSG_m; PROTOCOL TYPE NVMC; ENVLID 0x3F; Length Override 32672 SQLDCMSG_S; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 32672 SQLDCCOLN_m; PROTOCOL TYPE NVCM ; ENVLID 0x3F; Length Override 255 SQLDCCOLN_s; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCCURN_m; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCCURN_s; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCPNAM_m; PROTOCOL TYPE NVCM; ENVLID 0x3F; Length Override 255 SQLDCPNAM_s; PROTOCOL TYPE NVCS; ENVLID 0x33; Length Override 255 SQLDCXGRP; PROTOCOL TYPE N-GDA; ENVLID 0xD3; Length Override 1 SQL Diagnostics Condition Row - Identity 0xE5 SQLDCGRP; GROUP LID 0xD5; ELEMENT TAKEN 0(all); REP FACTOR 1 SQL Diagnostics Condition Token Array - Identity 0xF7 NULLDATA will be received for now SQL Diagnostics Extended Names Group Description - Identity 0xD5 NULLDATA will be received for now SQL Diagnostics Condition Information Array - Identity 0xF5 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDCIROW; ROW LID 0xE5; ELEMENT TAKEN 0(all); REP FACTOR 0(all) SQL Diagnostics Connection Array - Identity 0xF6 NULLDATA will be received for now SQLDIAGGRP : FDOCA EARLY GROUP SQL Diagnostics Statement Group Description - Identity 0xD3 NULLDATA will be received for now Reads <i>length</i> number of bytes from the dataBuffer starting from the current position.  Returns a new byte array which contains the bytes read. If current position plus length goes past the lastValidBytePosition, send CNTQRY to get more data. Reads 1-byte from the dataBuffer from the current position. If position is already at the end of the buffer, send CNTQRY to get more data. Reads 1-byte from the dataBuffer from the current position. If position is already at the end of the buffer, send CNTQRY to get more data. This is not used for column data. Reads 2-bytes from the dataBuffer starting from the current position, and returns an integer constructed from the 2-bytes.  If current position plus 2 bytes goes past the lastValidBytePosition, send CNTQRY to get more data. Scan the data buffer to see if end of data (SQL state 02000) has been received. This method should only be called when the cursor is being closed since the pointer to the current row can be modified. Set the value of value of allRowsReceivedFromServer_. Set a flag indicating whether QRYCLSIMP is enabled. Shift partial row bytes to beginning of dataBuffer, and resets current row position, and lastValidBytePosition. When we shift partial row, we'll have to recalculate column offsets up to this column. Check if position plus length goes past the lastValidBytePosition. If so, send CNTQRY to get more data. length - number of bytes to skip returns the number of bytes skipped
---------------------------call-down methods--------------------------------
Gets the int value of the two byte unsigned codepoint. synchonized so only one thread can initialize the table ------------------------------entry points---------------------------------- Specialization of LogWriter.traceConnectsExit() Pass the connection handle and print it in the header What exactly is supposed to be passed,  assume one complete DSS packet Write the communication buffer data to the trace. The data is passed in via a byte array.  The start and length of the data is given. The type is needed to indicate if the data is part of the send or receive buffer. The class name, method name, and trcPt number are also written to the trace. Not much checking is performed on the parameters.  This is done to help performance.
Also called by NetStatementReply RDB Update Reply Message indicates that a DDM command resulted in an update at the target relational database.  If a command generated multiple reply messages including an RDBUPDRM, then the RDBUPDRM must be the first reply message for the command. For each target server, the RDBUPDRM  must be returned the first time an update is made to the target RDB within a unit of work. The target server may optionally return the RDBUPDRM after subsequent updates within the UOW.  If multiple target RDBs are involved with the current UOW and updates are made with any of them, then the RDBUPDRM must be returned in response to the first update at each of them. SQL Error Condition Reply Message indicates that an SQL error has occurred.  It may be sent even though no reply message precedes the SQLCARD object that is the normal response to a command when an exception occurs. The SQLERRM is also used when a BNDSQLSTT command is terminated by an INTRRDBRQS command. This reply message must precede an SQLCARD object. The SQLSTATE is returned in the SQLCARD.  Returned from Server: SVRCOD - required  (8 - ERROR) RDBNAM - optional  Also called by NetResultSetReply and NetStatementReply
this specifies the fully qualified package name, consistency token, and section number within the package being used to execute the SQL.  If the connection supports reusing the previous package information and this information is the same except for the section number then only the section number needs to be sent to the server. SQLSTT : FDOCA EARLY ROW SQL Statement Row Description  FORMAT FOR ALL SQLAM LEVELS SQLSTTGRP; GROUP LID 0x5C; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLSTTGRP : FDOCA EARLY GROUP SQL Statement Group Description  FORMAT FOR SQLAM <= 6 SQLSTATEMENT_m; PROTOCOL TYPE LVCM; ENVLID 0x40; Length Override 32767 SQLSTATEMENT_s; PROTOCOL TYPE LVCS; ENVLID 0x34; Length Override 32767  FORMAT FOR SQLAM >= 7 SQLSTATEMENT_m; PROTOCOL TYPE NOCM; ENVLID 0xCF; Length Override 4 SQLSTATEMENT_s; PROTOCOL TYPE NOCS; ENVLID 0xCB; Length Override 4 throws an exception if lengths exceed the maximum. returns a boolean indicating if SLCDTALEN is required.
super.readOpenQuery()
Analyze the error handling here, and whether or not can be pushed to common can we make this the common layer fetch method.  Called by the read/skip Fdoca bytes methods in the net whenever data reads exhaust the internal buffer used by this reply. -----------------------------helper methods--------------------------------- -------------------------------flow methods--------------------------------- Go through the QRYDTA's received, and calculate the column offsets for each row. Method that is invoked by <code>closeX()</code> before the result set is actually being closed. If QRYCLSIMP is enabled on the cursor, scan data buffer for end of data (SQL state 02000). If end of data is received, the result set is closed on the server. think about splitting out the position cursor stuff from the fetch stuff use commented out abstract position cursor methods above ------------------------------- abstract box car methods --------------------------------------

----------------------helper methods---------------------------------------- ------------------parse reply for specific command-------------------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. Parse the reply for the Close Query Command. This method handles the parsing of all command replies and reply data for the clsqry command. Parse the reply for the Continue Query Command. This method handles the parsing of all command replies and reply data for the cntqry command. If doCopyQrydta==false, then there is no data, and we're only parsing out the sqlca to get the row count. -----------------------------parse DDM Reply Messages----------------------- Query Not Opened Reply Message is issued if a CNTQRY or CLSQRY command is issued for a query that is not open.  A previous ENDQRYRM, ENDUOWRM, or ABNUOWRM reply message might have terminated the command. PROTOCOL architects the SQLSTATE value depending on SVRCOD SVRCOD 4 -> SQLSTATE is 24501 SVRCOD 8 -> SQLSTATE of 58008 or 58009  if SVRCOD is 4 then SQLSTATE 24501, SQLCODE -501 else SQLSTATE 58009, SQLCODE -30020  Messages SQLSTATE : 24501 The identified cursor is not open. SQLCODE : -501 The cursor specified in a FETCH or CLOSE statement is not open. The statement cannot be processed. SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.  Returned from Server: SVRCOD - required  (4 - WARNING, 8 - ERROR) RDBNAM - required PKGNAMCSN - required  ----------------------------- entry points ---------------------------------
----------------------helper methods---------------------------------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. buildCoreCntqry builds the common parameters Send CTNQRY to reposition the cursor on the target server. Send CNTQRY to get a new rowset from the target server. ----------------------non-parsing computational helper methods-------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. Called by NetResultSetRequest.writeScrollableFetch() ----------------------------- entry points ---------------------------------
Get Framework Info stop the Server
Display the current Network server status Get the form of NetServlet. Provides buttons and forms to control the Network server. Get the form of NetServlet. Provides a buttons and form to control the Network server Escapes potentially dangerous characters in data written to the browser. <p> <em>NOTE</em>: This is a poor mans implementation - it doesn't protect against all kinds of attacks, and it cannot be used in all contexts. If the received string has one or more single quotes in it, replace each one with the HTML escape-code for a single quote (apostrophe) so that the string can be properly displayed on a submit button. Fix the language code, as some browsers send then in a bad format (for instance, Firefox sends en-us instead of en_US). Determine the locale file needed for this browsers preferences Defaults to the settings for derby.locale and derby.codeset if set English otherwise if browsers preferences can't be found get an HTML labelled message from the resource bundle file, according to the given key. Get an integer parameter Get locale string from language which may have qvalue set get UTF8 parameter value and decode international characters Get the currrent server status by using test connection Initialize the servlet. Configuration parameters: <UL> <LI><PRE>portNumber</PRE> - Port number</LI> <LI><PRE>host</PRE> - Host name</LI> <LI><PRE>traceDirectory</PRE> - location of trace directory</LI> <LI><PRE>startNetworkServerOnInit</PRE> - start the server on initialization</LI> </UL> Turn logging of connections on Print the received string as a header. Print Derby Network Server banner Display an error form Display an error form Start the network server and attempt to connect to it before returning Set defaults for logging and tracing (both off) Set Network server parameters Shutdown the network server Change tracing for all sessions Set trace directory Change tracing for a given session Check if the required translation is available


NOTE: NET processing does not require parameters supplied on the "read-side" so parameter sql is ignored. ------------------------abstract box car methods-----------------------------------------------
----------------------non-parsing computational helper methods-------------- Possible errors to detect include: DSCERRCD_01 - FDOCA triplet is not used in PROTOCOL descriptors or type code is invalid DSCERRCD_02 - FDOCA triplet sequence error DSCERRCD_03 - An array description is required and this is not one (too many or too few RLO triplets) DSCERRCD_04 - A row description is required and this is not one (too many or too few RLO triplets) DSCERRCD_05 - Late Environmental Descriptor just received not supported DSCERRCD_06 - Malformed triplet, required parameter is missing DSCERRCD_07 - Parameter value is not acceptable DSCERRCD_11 - MDD present is not recognized as an SQL descriptor DSCERRCD_12 - MDD class is not recognized as a valid SQL class DSCERRCD_13 - MDD type not recognized as a valid SQL type DSCERRCD_21 - Representation is incompatible with SQL type (in prior MDD) DSCERRCD_22 - CCSID is not supported DSCERRCD_32 - GDA references a local identifier which is not an SDA or GDA DSCERRCD_33 - GDA length override exceeds limits DSCERRCD_34 - GDA precision exceeds limits DSCERRCD_35 - GDA scale greater than precision or scale negative DSCERRCD_36 - GDA length override missing or incompatible with data type DSCERRCD_41 - RLO references a LID which is not an RLO or GDA. DSCERRCD_42 - RLO fails to reference a required GDA or RLO. Parse the reply for the Describe SQL Statement Command. This method handles the parsing of all command replies and reply data for the dscsqlstt command. 0 is output, else input Also called by NetResultSetReply subclass. The End of Query Reply Message indicates that the query process has terminated in such a manner that the query or result set is now closed. It cannot be resumed with the CNTQRY command or closed with the CLSQRY command. The ENDQRYRM is always followed by an SQLCARD. Parse the reply for the Execute Immediate SQL Statement Command. This method handles the parsing of all command replies and reply data for the excsqlimm command. Called by NETSetClientPiggybackCommand.read() Parse the reply for the Execute SQL Statement Command. This method handles the parsing of all command replies and reply data for the excsqlstt command. Also called by ClientCallableStatement.readExecuteCall() this is duplicated in parseColumnMetaData, but different DAGroup under NETColumnMetaData requires a lot more stuffs including precsion, scale and other stuffs Open Query Failure (OPNQFLRM) Reply Message indicates that the OPNQRY command failed to open the query.  The reason that the target relational database was unable to open the query is reported in an SQLCARD reply data object. Whenever an OPNQFLRM is returned, an SQLCARD object must also be returned following the OPNQFLRM.  Returned from Server: SVRCOD - required (8 - ERROR) RDBNAM - required SRVDGN - optional -----------------------------parse DDM Reply Messages----------------------- Open Query Complete Reply Message indicates to the requester that an OPNQRY or EXCSQLSTT command completed normally and that the query process has been initiated.  It also indicates the type of query protocol and cursor used for the query. <p> When an EXCSQLSTT contains an SQL statement that invokes a stored procedure, and the procedure completes, an OPNQRYRM is returned for each answer set. Parse the reply for the Open Query Command. This method handles the parsing of all command replies and reply data for the opnqry command. will be replaced by parseOPNQRYreply (see parseOPNQRYreplyProto) ------------------------parse DDM Scalars----------------------------- RDB Package name, consistency token, and section number specifies the fully qualified name of a relational database package, its consistency token, and a specific section within a package.  Only called for generated secctions from a callable statement.  RDB Package Namce, Consistency Token, and Section Number List specifies a list of fully qualified names of specific sections within one or more packages. ----------------------helper methods---------------------------------------- ------------------parse reply for specific command-------------------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. Query Previously Opened Reply Message is issued when an OPNQRY command is issued for a query that is already open. A previous OPNQRY command might have opened the query which may not be closed. PROTOCOL Architects an SQLSTATE of 58008 or 58009.  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database.   Returned from Server: SVRCOD - required  (8 - ERROR) RDBNAM - required PKGNAMCSN - required SRVDGN - optional  RDB Result Set Reply Message (RSLSETRM) indicates that an EXCSQLSTT command invoked a stored procedure, that the execution of the stored procedure generated one or more result sets, and additional information aobut these result sets follows the SQLCARD and SQLDTARD in the reply data of the response  Returned from Server: SVRCOD - required  (0 INFO) PKGSNLST - required SRVDGN - optional Parse the Result Set component of the reply for a stored procedure call which returns result sets. The Result Set component consists of an Open Query Reply Message followed by an optional SQLCARD, followed by an optional SQL Column Information Reply data object, followed by a Query Descriptor. There may also be Query Data or an End of Query Reply Message. SQLCINRD : FDOCA EARLY ARRAY SQL Result Set Column Array Description  FORMAT FOR SQLAM <= 6 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLCIROW; ROW LID 0x6B; ELEMENT TAKEN 0(all); REP FACTOR 0(all)  FORMAT FOR SQLAM >= 7 SQLDHROW; ROW LID 0xE0; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDAROW; ROW LID 0x60; ELEMENT TAKEN 0(all); REP FACTOR 0(all)  SQL Result Set Column Information Reply Data (SQLCINRD) is a byte string that specifies information about columns for a result set returned as reply data in the response to an EXCSQLSTT command that invodes a stored procedure These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. SQLDAGRP : EARLY FDOCA GROUP SQL Data Area Group Description  FORMAT FOR SQLAM <= 6 SQLPRECISION; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLSCALE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLLENGTH; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLTYPE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLCCSID; PROTOCOL TYPE FB; ENVLID 0x26; Length Override 2 SQLNAME_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 30 SQLNAME_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 30 SQLLABEL_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 30 SQLLABEL_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 30 SQLCOMMENTS_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 254 SQLCOMMENTS_m; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 254  FORMAT FOR SQLAM == 6 SQLPRECISION; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLSCALE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLLENGTH; PROTOCOL TYPE I8; ENVLID 0x16; Length Override 8 SQLTYPE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLCCSID; PROTOCOL TYPE FB; ENVLID 0x26; Length Override 2 SQLNAME_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 30 SQLNAME_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 30 SQLLABEL_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 30 SQLLABEL_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 30 SQLCOMMENTS_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 254 SQLCOMMENTS_m; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 254 SQLUDTGRP; PROTOCOL TYPE N-GDA; ENVLID 0x51; Length Override 0  FORMAT FOR SQLAM >= 7 SQLPRECISION; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLSCALE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLLENGTH; PROTOCOL TYPE I8; ENVLID 0x16; Length Override 8 SQLTYPE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLCCSID; PROTOCOL TYPE FB; ENVLID 0x26; Length Override 2 SQLDOPTGRP; PROTOCOL TYPE N-GDA; ENVLID 0xD2; Length Override 0 --------------------------parse FDOCA objects------------------------ SQLDARD : FDOCA EARLY ARRAY SQL Descriptor Area Row Description with SQL Communications Area  FORMAT FOR SQLAM <= 6 SQLCARD; ROW LID 0x64; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDAROW; ROW LID 0x60; ELEMENT TAKEN 0(all); REP FACTOR 0(all)  FORMAT FOR SQLAM >= 7 SQLCARD; ROW LID 0x64; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDHROW; ROW LID 0xE0; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDAROW; ROW LID 0x60; ELEMENT TAKEN 0(all); REP FACTOR 0(all) SQLDAROW : FDOCA EARLY ROW SQL Data Area Row Description  FORMAT FOR ALL SQLAM LEVELS SQLDAGRP; GROUP LID 0x50; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDHGRP : EARLY FDOCA GROUP SQL Descriptor Header Group Description  FORMAT FOR SQLAM >= 7 SQLDHOLD; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLDRETURN; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLDSCROLL; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLDSENSITIVE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLDFCODE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLDKEYTYPE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLDRDBNAM; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 1024 SQLDSCHEMA_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLDSCHEMA_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLDHROW : FDOCA EARLY ROW SQL Descriptor Header Row Description  FORMAT FOR SQLAM >= 7 SQLDHGRP;  GROUP LID 0xD0; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLDOPTGRP : EARLY FDOCA GROUP SQL Descriptor Optional Group Description  FORMAT FOR SQLAM >= 7 SQLUNNAMED; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLNAME_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLNAME_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLLABEL_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLLABEL_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLCOMMENTS_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLCOMMENTS_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLUDTGRP; PROTOCOL TYPE N-GDA; ENVLID 0x5B; Length Override 0 SQLDXGRP; PROTOCOL TYPE N-GDA; ENVLID 0xD4; Length Override 0 --------------------- parse DDM Reply Data-------------------------------------- SQL Data Reply Data consists of output data from the relational database (RDB) processing of an SQL statement.  It also includes a description of the data.  Returned from Server: FDODSC - required FDODTA - required SQLDXGRP : EARLY FDOCA GROUP SQL Descriptor Extended Group Description  FORMAT FOR SQLAM >=7 SQLXKEYMEM; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLXUPDATEABLE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLXGENERATED; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLXPARMMODE; PROTOCOL TYPE I2; ENVLID 0x04; Length Override 2 SQLXRDBNAM; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 1024 SQLXCORNAME_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLXCORNAME_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLXBASENAME_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLXBASENAME_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLXSCHEMA_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLXSCHEMA_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLXNAME_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLXNAME_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLRSGRP : EARLY FDOCA GROUP SQL Result Set Group Description  FORMAT FOR SQLAM >= 7 SQLRSLOCATOR; PROTOCOL TYPE RSL; ENVLID 0x14; Length Override 4 SQLRSNAME_m; PROTOCOL TYPE VCM; ENVLID 0x3E; Length Override 255 SQLRSNAME_s; PROTOCOL TYPE VCS; ENVLID 0x32; Length Override 255 SQLRSNUMROWS; PROTOCOL TYPE I4; ENVLID 0x02; Length Override 4 SQLRSLRD : FDOCA EARLY ARRAY Data Array of a Result Set  FORMAT FOR ALL SQLAM LEVELS SQLNUMROW; ROW LID 0x68; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLRSROW; ROW LID 0x6F; ELEMENT TAKEN 0(all); REP FACTOR 0(all)  SQL Result Set Reply Data (SQLRSLRD) is a byte string that specifies information about result sets returned as reply data in the response to an EXCSQLSTT command that invokes a stored procedure SQLRSROW : FDOCA EARLY ROW SQL Row Description for One Result Set Row  FORMAT FOR ALL SQLAM LEVELS SQLRSGRP; GROUP LID 0x5F; ELEMENT TAKEN 0(all); REP FACTOR 1 SQLUDTGRP : EARLY FDOCA GROUP SQL User-Defined Data Group Description  For an explanation of the format, see the header comment on DRDAConnThread.writeSQLUDTGRP().  ----------------------------- entry points ---------------------------------
Build the Describe SQL Statement command.  preconditions: the sqlam and/or prdid must support command and parameters passed to this method, method will not validate against the connection's level of support Build the Execute Immediate SQL Statement Command to execute a non-cursor SQL statement sent as command data.  precondtions: Build the command to execute an SQL SET Statement. Called by NETSetClientPiggybackCommand.write()  preconditions: the sqlam and/or prdid must support command and parameters passed to this method, method will not validate against the connection's level of support Build the Execute SQL Statement (EXCSQLSTT) Command to execute a non-cursor SQL statement previously bound into a named package of a relational database (RDB).  The SQL statement can optionally include references to input variables, output variables, or both.  At SQLAM >= 7 we can get a DA back on this, are there times that we want to request it If so, we probably want to pass a parameter indicating the sqldaLevel requested.  preconditions: the sqlam and/or prdid must support command and parameters passed to this method, method will not validate against the connection's level of support Here is the preferred codepoint ordering: PKGNAMCSN RDBCMTOK OUTEXP QRYBLKSZ MAXBLKEXT MAXRSLCNT RSLSETFLG QRYROWSET RTNSQLDA TYPSQLDA NBRROW ATMIND PRCNAM OUTOVROPT RDBNAM preconditions: Build the FDOCA Data Descriptor Scalar whose value is a FDOCA Descriptor or a segment of an FDOCA Descriptor.  preconditions: Maximum Number of Extra Blocks specifies a limit on the number of extra blocks of answer set data per result set that the requester is capable of receiveing. this value must be able to be contained in a two byte signed number. there is a minimum value of 0. a zero indicates that the requester is not capable of receiving extra query blocks of answer set data. there is a SPCVAL of -1. a value of -1 indicates that the requester is capable of receiving the entire result set.  preconditions: sqlam must support this parameter on the command, method will not check. Maximum Result Set Count specifies a limit on the number of result sets the requester is capable of receiving as reply data in response to an ECSQLSTT command that invokes a stored procedure.  If the stored procedure generates more than MAXRSLCNT result sets, then the target system returns at most, the first MAXRSLCNT of these result sets.  The stored procedure defines the order in which the target system returns result sets. this is s two byte signed binary number. it has a min value of 0 which indicates the requester is not capable of receiving result sets as reply data in response to the command. a special value, -1 (CodePoint.MAXRSLCNT_NOLIMIT = 0xffff), indicates the requester is capable of receiving all result sets in response the EXCSQLSTT.  preconditions: sqlam must support this parameter for the command, method will not check. the value must be in correct range (-1 to 32767), method will not check. Write the message to execute an SQL Set Statement. public void writeSetGenericSQLSetInfo ( SetGenericSQLSetPiggybackCommand setGenericSQLSetPiggybackCommand, JDBCSection section) throws SqlException { buildEXCSQLSET (section); List sqlsttList = setGenericSQLSetPiggybackCommand.getList(); for (int i = 0; i < sqlsttList.size(); i++) { String sql = (String)sqlsttList.get(i); // Build SQLSTT only for the SET statement that coming from the server after RLSCONV buildSQLSTTcommandData (sql); } } ----------------------helper methods---------------------------------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. Build the Open Query Command to open a query to a relational database. At SQLAM >= 7 we can request the return of a DA, are there scenarios where this should currently be done (it is not supported now)  preconditions: the sqlam and/or prdid must support command and parameters passed to this method, method will not validate against the connection's level of support  Output Expected indicates wheterh the requester expects the target SQLAM to return output with an SQLDTARD reply data object as a result of the execution of the referenced SQL statement. this is a single byte. there are two possible enumerated values: 0x'F1' (CodePoint.TRUE) - for true indicating the requester expects output 0x'F0' (CodePoint.FALSE) - for false indicating the requeser does not expect output 0x'F0' is the default.  preconditions: sqlam must support this parameter on the command, method will not check. ///////// perf end The Procedure Name. The default value of PRCNAM is the procedure name value contained within the section identified by the pkgnamcsn parameter.  If that value is null, then the prcnam parameter must be specified. it has a max length of 255. the prcnam is required on commands if the procedure name is specified by a host variable. the default value is the procedure name contained in the section specified by the pkgnamcsn parameter on the EXCSQLSTT command.  preconditions: sqlam must support this parameter for the command, method will not check. prcnam can not be null, SQLException will be thrown prcnam can not be 0 length or > 255 length, SQLException will be thrown. Build the Prepare SQL Statement Command to dynamically binds an SQL statement to a section in an existing relational database (RDB) package.  preconditions: the sqlam and/or prdid must support command and parameters passed to this method, method will not validate against the connection's level of support Query Block Size specifies the query block size for the reply data objects and the reply messages being returned from this command. this is a 4 byte unsigned binary number. the sqlam 6 min value is 512 and max value is 32767. this value was increased in later sqlam levels. until the code is ready to support larger query block sizes, it will always use DssConstants.MAX_DSS_LEN which is 32767.  preconditions: sqlam must support this parameter for the command, method will not check. Build QRYCLSIMP (Query Close Implicit). Query Close Implicit controls whether the target server implicitly closes a non-scrollable query upon end of data (SQLSTATE 02000). preconditions: RDB Commit Allowed specifies whether an RDB should allow the processing of any commit or rollback operations that occure during execution of a statement. True allow the processing of commits and rollbacks Result Set Flags is a single byte where each bit it a boolean flag. It specifies wheter the requester desires the server to return name, label and comment information for the columns of result sets generated by the command. The default is b'00000000'. columnNamesRequired false means the requester does not desire column names returned. true means the requester desires column names returned. columnLabelsRequired false means the requester does not desire column labels returned. true means the requester desires column labels returned. columnCommentsRequired false means the requester does not desire column comments returned. true means the requester desired column comments returned. cantProcessAnswerSetData false means that for each result set, the requester expects the command to return an FDOCA description of the answer set data and to possibly return answer set data.  the block containing the end of the description may be completed if room exists with answer set data.  additional blocks of answer set data may also be chained to the block containing the end of the FDOCA description.  up to the maximum number of extra blocks of answer set data per result set specified in the MAXBLKEXT parameter. true means that for each result set, the requester expects the command to return an FDOCA description of the answer set data but does not expect the command to return any answer set data. at SQLAM 7, new flags are supported which can be used to return standard, extended, and light sqlda  preconditions: sqlam must support this parameter, method will not check. Return SQL Descriptor Area controls whether to return an SQL descriptor area that applies to the SQL statement this command identifies.  The target SQLAM obtains the SQL descriptor area by performing an SQL DESCRIBE function on the statement after the statement has been prepared. The value TRUE, X'F1' (CodePoint.TRUE), indicates an SQLDA is returned The value FALSE, X'F0' (CodePoint.FALSE), default, indicates an SQLDA is not returned.  preconditions: sqlam must support this parameter for the command, method will not check. Build the FDOCA SQLDTA Late Row Descriptor.  preconditions: Build the FDOCA SQLDTAGRP Late Group Descriptor. preconditions: Build the SQL Program Variable Data Command Data Object. This object contains the input data to an SQL statement that an RDB is executing.  preconditions: Type of SQL Descriptor Area. This is a single byte signed number that specifies the type of sqlda to return for the command. below sqlam 7 there were two possible enumerated values for this parameter. 0 (CodePoint.TYPSQLDA_STD_OUTPUT)- the default, indicates return the output sqlda. 1 (CodePoint.TYPSQLDA_STD_INPUT) - indicates return the input sqlda. the typsqlda was enhanced at sqlam 7 to support extened describe. at sqlam 7 the following enumerated values are supported. 0 (CodePoint.TYPSQLDA_STD_OUTPUT) - the default, standard output sqlda. 1 (CodePoint.TYPSQLDA_STD_INPUT) - standard input sqlda. 2 (CodePoint.TYPSQLDA_LIGHT_OUTPUT) - light output sqlda. 3 (CodePoint.TYPSQLDA_LIGHT_INPUT) - light input sqlda. 4 (CodePoint.TYPSQLDA_X_OUTPUT) - extended output sqlda. 5 (CodePoint.TYPSQLDA_X_INPUT) - extended input sqlda.  preconditions: sqlam or prdid must support this, method will not check. valid enumerated type must be passed to method, method will not check. Consider refacctor so that this does not even have to look at the actual object data, and only uses tags from the meta data only have to call this once, rather than calling this for every input row Comment: I don't think that it is possible to send decimal values without looking at the data for precision and scale (Kathey Marsden 10/11) backburner: after refactoring this, later on, think about replacing case statements with table lookups -------------------------helper methods------------------------------------- returns the a promototedParameter object for index or null if it does not exist helper method to buildFDODTA to build the actual data length Write the message to peform a describe input.  Write the message to peform a describe output.  preconditions: Write the message to execute  prepared sql statement.  preconditions: chained flag for blobs only  //dupqry Write the message to execute a stored procedure.  preconditions: chain is for blobs ----------------------------- entry points --------------------------------- Write the message to perform an execute immediate. The SQL statement sent as command data cannot contain references to either input variables or output variables.  preconditions: Write the message to open a bound or prepared query with input parameters. Check this -> For open query with input parameters  preconditions: Write the message to open a bound or prepared query without input parameters. Check this-> For open query without input parameters Write the message to perform a reprepare.  preconditions: Write the message to preform a prepare into. Once the SQL statement has been prepared, it is executed until the unit of work, in which the PRPSQLSTT command was issued, ends.  An exception to this is if Keep Dynamic is being used.  preconditions:

Creates NetConnection for the supported version of jdbc. This method can be overwritten to return NetConnection of the supported jdbc version. Returns underlying net connection
------------------------parse DDM Scalars----------------------------- ----------------------helper methods---------------------------------------- --------------------- parse DDM Reply Data-------------------------------------- The SYNCCRD Reply Mesage  Returned from Server: XARETVAL - required This method handles the parsing of all command replies and reply data for the SYNNCTL command. Process XA return value Process XA return value ----------------------------- entry points ---------------------------------
----------------------helper methods---------------------------------------- These methods are "private protected", which is not a recognized java privilege, but means that these methods are private to this class and to subclasses, and should not be used as package-wide friendly methods. ----------------------------- entry points --------------------------------- Build the SYNNCTL commit command Build the SYNNCTL rollback command
Ends the work performed on behalf of a transaction branch. The resource manager dissociates the XA resource from the transaction branch specified and let the transaction be completed. <p/> If TMSUSPEND is specified in flags, the transaction branch is temporarily suspended in incomplete state. The transaction context is in suspened state and must be resumed via start with TMRESUME specified. <p/> If TMFAIL is specified, the portion of work has failed. The resource manager may mark the transaction as rollback-only <p/> If TMSUCCESS is specified, the portion of work has completed successfully. Tell the resource manager to forget about a heuristically (MANUALLY) completed transaction branch. Get XAException.errorCode from SqlException For disconnect exception, return XAER_RMFAIL For other exceptions return XAER_RMERR For server side SQLExceptions during XA operations the errorCode has already been determined and wrapped in an XAException for return to the client. see EmbedXAResource.wrapInXAException Obtain the current transaction timeout value set for this XAResource instance. If XAResource.setTransactionTimeout was not use prior to invoking this method, the return value is 0; otherwise, the value used in the previous setTransactionTimeout call is returned. Ask the resource manager to prepare for a transaction commit of the transaction specified in xid. Obtain a list of prepared transaction branches from a resource manager. The transaction manager calls this method during recovery to obtain the list of transaction branches that are currently in prepared or heuristically completed states. Inform the resource manager to roll back work done on behalf of a transaction branch Set the current transaction timeout value for this XAResource instance. Once set, this timeout value is effective until setTransactionTimeout is invoked again with a different value. To reset the timeout value to the default value used by the resource manager, set the value to zero. If the timeout operation is performed successfully, the method returns true; otherwise false. If a resource manager does not support transaction timeout value to be set explicitly, this method returns false. Reset the transaction branch association state  to XA_T0_NOT_ASSOCIATED for XAER_RM* and XA_RB* Exceptions. All other exceptions leave the state unchanged Start work on behalf of a transaction branch specified in xid
<p> Find the url of the library directory which holds derby.jar and derbynet.jar. The Basic policy assumes that both jar files live in the same directory. </p> Get current Network server properties Get the hostname as a value suitable for substituting into the default server policy file. The special wildcard valuse "0.0.0.0" and "::" are forced to be "*" since that is the wildcard hostname understood by SocketPermission. SocketPermission does not understand the "0.0.0.0" and "::" wildcards. IPV6 addresses are enclosed in square brackets. This logic arose from two JIRAs: DERBY-2811 and DERBY-2874. Returns the current maxThreads setting for the running Network Server <p> Get the URL of the policy file. Typically, this will be some pointer into derbynet.jar. </p> Return detailed session runtime information about sessions, prepared statements, and memory usage for the running Network Server. Return classpath and version information about the running Network Server. Return the current timeSlice setting for the running Network Server return true if the two hostnames are equivalent Install a SecurityManager governed by the Basic startup policy. See DERBY-2196. return true if the host address is an IPV6 address Turn logging connections on or off. When logging is turned on a message is written to the Derby error log each time a connection is made. main routine for NetworkServerControl Return true if we need to install a Security Manager. All of the following must apply. See DERBY-2196. <ul> <li>The VM was booted with NetworkServerContro.main() as the entry point. This is handled by the fact that this method is only called by main().</li> <li>The VM isn't already running a SecurityManager.</li> <li>The command must be "start".</li> <li>The customer didn't specify the -noSecurityManager flag on the startup command line.</li> </ul> Check if the Network Server is started. Excecutes and returns without error if the server has started Protected methods ** * set the client locale. Used by servlet for localization Set Network Server maxthread parameter.  This is the maximum number of threads that will be used for JDBC client connections.   setTimeSlice should also be set so that clients will yield appropriately. Set Network Server connection time slice parameter. This should be set and is only relevant if setMaxThreads &gt; 0. Set directory for trace files. The directory must be on the machine where the server is running. Shutdown a Network Server. Shuts down the Network Server listening on the port and InetAddress specified in the constructor for this NetworkServerControl object. ******************************************************************** Public NetworkServerControl  commands The server commands throw exceptions for errors, so that users can handle them themselves. *********************************************************************** Start a Network Server. This method will launch a separate thread and start a Network Server. This method  may return before the server is ready to accept connections. This will also install a security manager with a default security policy. Use the ping method to verify that the server has started. <P> Note: an alternate method to starting the Network Server with the API, is to use the derby.drda.startNetworkServer property in derby.properties. </P> Turn tracing on or off for the specified connection on the Network Server. Turn tracing on or off for all connections on the Network Server. Verify that all prerequisites are met before bringing up a security manager. See DERBY-2196. If prerequisites aren't met, raise an exception which explains how to get up and running. At one point, we were going to require that authentication be enabled before bringing up a security manager. This, however, gave rise to incompatibilities. See DERBY-2757. Currently, this method is a nop.
Add a session - for use by <code>ClientThread</code>. Put the session into the session table and the run queue. Start a new <code>DRDAConnThread</code> if there are more sessions waiting than there are free threads, and the maximum number of threads is not exceeded. <p><code>addSession()</code> should only be called from one thread at a time. Return the att_extnam server attribute Return the att_srvclsnm server attribute Return the att_srvrlslv server attribute Start a network server Build local address list to allow admin commands. Authenticates the user and checks for shutdown System Privileges. No Network communication needed. To perform this check the following policy grant is required <ul> <li> to run the encapsulated test: permission javax.security.auth.AuthPermission "doAsPrivileged"; </ul> or a SQLException will be raised detailing the cause. <p> In addition, for the test to succeed <ul> <li> the given user needs to be covered by a grant: principal org.apache.derby.authentication.SystemPrincipal "..." {} <li> that lists a shutdown permission: permission org.apache.derby.security.SystemPermission "shutdown"; </ul> or it will fail with a SQLException detailing the cause.  Stream error writing to client socket Close the resources associated with the opened socket. Write an error message to console output stream and throw an exception for this error Write an exception to console output stream, but only if debugOutput is true. Write an exception (with trace) to console output stream. Write a message to console output stream Print the passed exception on the console and ignore it after that Put property message on console Put property message on console Put property message on console Handle console error message - display on console and if it is a user error, display usage - if user error or severe error, throw exception with message key and message Print trace change message to console Create the right kind of server socket Return the debug state Shutdown the server directly (If you have the original object) No Network communication needed. Shutdown the server directly (If you have the original object) No Network communication needed. Ensure the reply buffer is large enough to hold all the data; don't just rely on OS level defaults Execute the command given on the command line Fill the reply buffer with the reply allocates a reply buffer if one doesn't exist Go through the arguments and find the command and save the dash arguments and arguments to the command.  Only one command is allowed in the argument list. Privileged service lookup. Must be private so that user code can't call this entry point. Get the stored application requester or store if we haven't seen it yet Get Derby information Get current properties DERBY-6768(List the enabled protocols in derby.log for network server configuration) Get the enabled protocols so we can list them in the log file Get the host where we listen for connections. Get integer property values Get the current value of keepAlive to configure how long the server should keep the socket alive for a disconnected client Get the current value of logging connections Get the server manager level for a given manager Get the current value of maximum number of threads to create Determine type of message Get the current value of minimum number of threads to create at start Retrieve product version information We need to make sure that this method gets the stream and passes it to ProductVersionHolder, because it lives in the Network Server jar and won't be readily available to ProductVersionHolder when running under security manager. Get Net Server information Get the next session for the thread to work on Called from DRDAConnThread after session completes or timeslice exceeded. If there is a waiting session, pick it up and put currentSession at the back of the queue if there is one. Get the port where we listen for connections. Initialize fields from system properties <p> Constructs an object containing network server related properties and their values. Some properties are only included if set. Some other properties are included with a default value if not set.</p> <p> This method is accessing the local JVM in which the network server instance is actually running (i.e. no networking).</p> <p> This method is package private to allow access from relevant MBean implementations in the same package.</p>  Get the string value of the SSL-mode. This is the inverse of getSSLModeValue. Get the SSL-mode from a string. Retrieve the SECMEC integer value from the user friendly security mechanism name get the security mechanism (secmec value) that the server will accept connections from. Retrieve the string name for the integer secmec value Privileged module lookup. Must be private so that user code can't call this entry point. Get the current value of the time slice Get the current value of whether to trace all the sessions Get the current value of trace directory Get a thread name that is both meaningful and unique (primarily for debugging purposes). Is this the command protocol Determine whether string is a property key or not property keys start with DRDA_MSG_PREFIX Is string "on" or "off"  Convenience routine so that NetworkServerControl can localize messages. Localize a message given a particular AppUI Turn logging connections on or off. When logging is turned on a message is written to derby.log each time a connection is made. Record a change to the connection logging mode Get the log writer we're using Connect to  network server and set connection maxthread parameter Set network server connection timeslice parameter Parse the command-line arguments. As a side-effect, fills in various instance fields. This method was carved out of executeWork() so that NetworkServerControl can figure out whether to install a security manager before the server actually comes up. This is part of the work for DERBY-2196. Ping opening an new socket and close it. Ping the server using the client socket that is already open. Return the product id Return the bytes of the product id processCommands reads and processes NetworkServerControlImpl commands sent to the network server over the socket.  The protocol used is 4 bytes     - String CMD: 2 bytes     - Protocol version 1 byte      - length of locale (0 for default) n bytes - locale 1 byte      - length of codeset (0 for default) n bytes - codeset 1 byte      - command n bytes     - parameters for the command The server returns 4 bytes     - String RPY: for most commands 1 byte      - command result, 0 - OK, 1 - warning, 2 - error if warning or error 1 bytes     - length of message key n bytes     - message key 1 byte      - number of parameters to message {2 bytes        - length of parameter n bytes     - parameter} for each parameter for sysinfo 1 byte      - command result, 0 - OK, 1 - warning, 2 - error if OK 2 bytes     - length of sysinfo n bytes     - sysinfo Note, the 3rd byte of the command must not be 'D0' to distinquish it from DSS structures. The protocol for the parameters for each command follows: Command: trace <connection id> {on | off} Protocol: 4 bytes     - connection id - connection id of 0 means all sessions 1 byte      - 0 off, 1 on Command: logConnections {on | off} Protocol: 1 byte      - 0 off, 1 on Command: shutdown // DERBY-2109: transmit user credentials for System Privileges check 2 bytes     - length of user name n bytes     - user name 2 bytes     - length of password n bytes     - password Command: sysinfo No parameters Command: dbstart Protocol: 2 bytes     - length of database name n bytes     - database name 2 bytes     - length of boot password n bytes     - boot password 2 bytes     - length of encryption algorithm n bytes     - encryption algorithm 2 bytes     - length of encryption provider n bytes     - encryption provider 2 bytes     - length of user name n bytes     - user name 2 bytes     - length of password n bytes     - password Command: dbshutdown Protocol: 2 bytes     - length of database name n bytes     - database name 2 bytes     - length of user name n bytes     - user name 2 bytes     - length of password n bytes     - password Command: connpool Protocol: 2 bytes     - length of database name, if 0, default for all databases is set n bytes     - database name 2 bytes     - minimum number of connections, if 0, connection pool not used if value is -1 use default 2 bytes     - maximum number of connections, if 0, connections are created as needed, if value is -1 use default Command: maxthreads Protocol: 2 bytes     - maximum number of threads Command: timeslice Protocol: 4 bytes     - timeslice value Command: tracedirectory Protocol: 2 bytes     - length of directory name n bytes     - directory name Command: test connection Protocol: 2 bytes     - length of database name if 0, just the connection to the network server is tested and user name and password aren't sent n bytes     - database name 2 bytes     - length of user name (optional) n bytes     - user name 2 bytes     - length of password  (optional) n bytes     - password The calling routine is synchronized so that multiple threads don't clobber each other. This means that configuration commands will be serialized. This shouldn't be a problem since they should be fairly rare. Get the dash argument. Optional arguments are formated as -x value. Read Bytes reply Read the command reply header from the server Read int from buffer Read length delimited bytes from a buffer Read length delimited string from a buffer Read result from sending client message to server Read short from buffer Read String reply **************************************************************************** Protected methods **************************************************************************** Remove session from session table DERBY-6764(analyze impact of poodle security alert on Derby client - server ssl support) Remove SSLv3 and SSLv2Hello protocols from list of enabled protocols Remove a thread from the thread list. Should be called when a <code>DRDAConnThread</code> has been closed. Add session to the run queue Return true if the customer forcibly overrode our decision to install a default SecurityManager.  Send client message to server Send Error or Warning from server to client after processing a command Send OK from server to client after processing a command Send OK and int value Send property information from server to client Send RuntimeInfo information from server to client Send SQL Exception from server to client after processing a command  Send SysInfo information from server to client Set the current value of logging connections ****************************************************************************** Implementation of NetworkServerControl API The server commands throw exceptions for errors, so that users can handle them themselves in addition to having the errors written to the console and possibly derby.log.  To turn off logging the errors to the console, set the output writer to null. ****************************************************************************** Set the output stream for console messages If this is set to null, no messages will be written to the console Set the current value of maximum number of threads to create Set the current value of minimum number of threads to create at start Set the security mechanism for derby.drda.securityMechanism If this property is set, server will only allow connections from client with this security mechanism. This method will map the user friendly string representing the security mechanism to the corresponding drda secmec value Set the current value of  time slice Set the trace on/off for all sessions, or one session, depending on whether we got -s argument. Set the current value of whether to trace all the sessions Set the current value of trace directory Set up client socket to send a command to the network server Shutdown a network server Start a network server.  Launches a separate thread with DRDAServerStarter.  Want to use Monitor.startModule, so it can all get shutdown when Derby shuts down, but can't get it working right now. Load Derby and save driver for future use. We can't call Driver Manager when the client connects, because they might be holding the DriverManager lock. Check whether a CCSID code page is supported This method returns whether EUSRIDPWD security mechanism is supported or not. See class static block for more info.  Throw a SQL Exception which was sent over by a server Format of the msg is SQLSTATE:localized message\nSQLSTATE:next localized message Throw a SQL Warning which was sent over by a server Format of the msg is SQLSTATE:localized message\nSQLSTATE:next localized message Print a trace for the (unexpected) exception received, then throw a generic exception indicating that 1) an unexpected exception was thrown, and 2) we've already printed the trace (so don't do it again). Turn tracing on or off for all sessions Turn tracing on or off for one session or all sessions Display usage information Wrap SQL Error - display to console and raise exception Wrap SQL Warning - display to console and raise exception Write byte Routines for writing commands for NetworkServerControlImpl being used as a client to a server Write command header consisting of command header string and default protocol version and command. At this point, all the commands except shutdown with username/passwrod use default protocol version. Write command header consisting of command header string and passed protocol version and command. At this point, all the commands except shutdown with username/passwrod use default protocol version. **************************************************************************** Private methods **************************************************************************** Write Command reply Write length delimited string string Write short Write string
Helper method that invokes a method returning {@code void}. Method forwarding / invocation follows below //
<p> Gets the accumulated number of connections. This includes all active and waiting connections since the Network Server was started. This number will not decrease as long as the Network Server is running.</p> <p> Require <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Gets the number of currently active connections. All connections are active if the DrdaMaxThreads attribute (<code>derby.drda.maxThreads</code> property) is 0.</p> <p> If DrdaMaxThreads is &gt; 0 and DrdaTimeSlice is 0, connections remain active until they are closed. If there are more than DrdaMaxThreads connections, inactive connections will be waiting for some active connection to close. The connection request will return when the connection becomes active.</p> <p> If DrdaMaxThreads is &gt; 0 and DrdaTimeSlice &gt; 0, connections will be alternating beetween active and waiting according to Derby's time slicing algorithm.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Gets the total number of bytes read by the server since it was started. </p> <p> Require <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Gets the number of bytes received per second by the Network Server. This number is calculated by taking into account the number of bytes received since the last calculation (or since MBean startup if it is the first time this attibute is being read).</p> <p> The shortest interval measured is 1 second. This means that a new value will not be calculated unless there has been at least 1 second since the last calculation.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Gets the total number of bytes written by the server since it was started.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Gets the number of bytes sent per second by the Network Server. This number is calculated by taking into account the number of bytes sent since the last calculation (or since MBean startup if it is the first time this attibute is being read).</p> <p> The shortest interval measured is 1 second. This means that a new value will not be calculated unless there has been at least 1 second since the last calculation.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> public void setDrdaTraceDirectory(String dir) throws Exception; <p> Gets the total number of current connections (waiting or active) to the Network Server.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Get the size of the connection thread pool. If DrdaMaxThreads (<code>derby.drda.maxThreads</code>) is set to a non-zero value, the size of the thread pool will not exceed this value.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> --- ----------------- MBean attributes ------------------------------------ --- Commented setters because: No attribute setting yet due to security concerns, see DERBY-1387. <p> Gets the network interface address on which the Network Server is listening. This corresponds to the value of the <code>derby.drda.host</code> property.</p> <p> For example, the value "<code>localhost</code>" means that the Network Server is listening on the local loopback interface only. <p> The special value "<code>0.0.0.0</code>" (IPv4 environments only) represents the "unspecified address" - also known as the anylocal or wildcard address.  In this context this means that the server is listening on all network interfaces (and may thus be able to see connections from both the local host as well as remote hosts, depending on which network interfaces are available).</p> <p> Requires <code>SystemPermission("server", "control")</code> if a security manager is installed.</p> <p> Reports whether or not the Derby Network Server will send keep-alive probes and attempt to clean up connections for disconnected clients (the value of the {@code derby.drda.keepAlive} property).</p> <p> If {@code true}, a keep-alive probe is sent to the client if a "long time" (by default, more than two hours) passes with no other data being sent or received. This will detect and clean up connections for clients on powered-off machines or clients that have disconnected unexpectedly. </p> <p> If {@code false}, Derby will not attempt to clean up connections from disconnected clients, and will not send keep-alive probes.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> See also the documentation for the property {@code derby.drda.keepAlive} in the <em>Derby Server and Administration Guide</em>, section <em>Managing the Derby Network Server</em>, subsection <em>Setting Network Server Properties</em>, subsubsection <em>derby.drda.keepAlive property</em>. </p> <p> Reports the maximum number of client connection threads the Network Server will allocate at any given time. This corresponds to the <code>derby.drda.maxThreads</code> property.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> public void setDrdaMaxThreads(int max) throws Exception; <p> Gets the port number on which the Network Server is listening for client connections. This corresponds to the value of the <code>derby.drda.portNumber</code> Network Server setting.</p> <p> Requires <code>SystemPermission("server", "control")</code> if a security manager is installed.</p> <p> The Derby security mechanism required by the Network Server for all client connections. This corresponds to the value of the <code>derby.drda.securityMechanism</code> property on the server.</p> <p> If not set, the empty String will be returned, which means that the Network Server accepts any connection which uses a valid security mechanism.</p> <p> For a list of valid security mechanisms, refer to the documentation for the <code>derby.drda.securityMechanism</code> property in the <i>Derby Server and Administration Guide</i>.</p> <p> Requires <code>SystemPermission("server", "control")</code> if a security manager is installed.</p> <p> Reports whether client connections must be encrypted using Secure Sockets Layer (SSL), and whether certificate based peer authentication is enabled. Refers to the <code>derby.drda.sslMode</code> property.</p> <p> Peer authentication means that the other side of the SSL connection is authenticated based on a trusted certificate installed locally.</p> <p> The value returned is one of "<code>off</code>" (no SSL encryption), "<code>basic</code>" (SSL encryption, no peer authentication) and "<code>peerAuthentication</code>" (SSL encryption and peer authentication). Refer to the <i>Derby Server and Administration Guide</i> for more details.</p> <p> Requires <code>SystemPermission("server", "control")</code> if a security manager is installed.</p> <p> The size of the buffer used for streaming BLOB and CLOB from server to client. Refers to the <code>derby.drda.streamOutBufferSize</code> property.</p> <p> This setting may improve streaming performance when the default sizes of packets being sent are significantly smaller than the maximum allowed packet size in the network.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> If the server property <code>derby.drda.maxThreads</code> is set to a non-zero value, this is the number of milliseconds that each client connection will actively use in the Network Server before yielding to another connection. If this value is 0, a waiting connection will become active once a currently active connection is closed.</p> <p> Refers to the <code>derby.drda.timeSlice</code> server property.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> public void setDrdaTimeSlice(int timeSlice) throws Exception; <p> Whether server-side tracing is enabled for all client connections (sessions). Refers to the <code>derby.drda.traceAll</code> server property.</p> <p> Tracing may for example be useful when providing technical support information. The Network Server also supports tracing for individual connections (sessions), see the <i>Derby Server and Administration Guide</i> ("Controlling tracing by using the trace facility") for details.</p> <p> When tracing is enabled, tracing information from each client connection will be written to a separate trace file. </p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> public void setDrdaTraceAll(boolean on) throws Exception; <p> Indicates the location of tracing files on the server host, if server tracing has been enabled.</p> <p> If the server setting <code>derby.drda.traceDirectory</code> is set, its value will be returned. Otherwise, the Network Server's default values will be taken into account when producing the result.</p> <p> Requires <code>SystemPermission("server", "control")</code> if a security manager is installed.</p> <p> Gets the start time of the network server. The time is reported as the number of milliseconds (ms) since Unix epoch (1970-01-01 00:00:00 UTC), and corresponds to the value of <code>java.lang.System#currentTimeMillis()</code> at the time the Network Server was started.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Gets the time (in milliseconds) the Network Server has been running. In other words, the time passed since the server was started.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> <p> Gets the number of currently waiting connections. This number will always be 0 if DrdaMaxThreads is 0. Otherwise, if the total number of connections is less than or equal to DrdaMaxThreads, then no connections are waiting.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p> --- ----------------- MBean operations ------------------------------------ --- <p> Executes the network server's <code>ping</code> command. Returns without errors if the server was successfully pinged.</p> <p> Note that the <code>ping</code> command itself will be executed from the network server instance that is actually running the server, and that the result will be transferred via JMX to the JMX client invoking this operation. This means that this operation will test network server connectivity from the same host (machine) as the network server, as opposed to when the <code>ping</code> command (or method) of <code>NetworkServerControl</code> is executed from a remote machine.</p> <p> This operation requires the following permission to be granted to the network server code base if a Java security manager is installed in the server JVM:</p> <codeblock> <code> permission java.net.SocketPermission "*", "connect,resolve"; </code> </codeblock> <p>The value <code>"*"</code> will allow connections from the network server to any host and any port, and may be replaced with a more specific value if so desired. The required value will depend on the value of the <code>-h</code> (or <code>derby.drda.host</code>) (host) and <code>-p</code> (or <code>derby.drda.portNumber</code>) (port) settings of the Network Server.</p> <p> Requires <code>SystemPermission("server", "monitor")</code> if a security manager is installed.</p>
Ensure the caller has permission to control the network server. Ensure the caller has permission to monitor the network server. public void setDrdaTraceDirectory(String dir) throws Exception { try { server.sendSetTraceDirectory(dir); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } } public String getSysInfo() throws Exception { String sysInfo = ""; try { sysInfo = server.sysinfo(); // remove information that is also given in the DerbySystemMBean return sysInfo.substring(sysInfo.indexOf("DRDA"),sysInfo.indexOf("-- list")); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } } Some of the code is disabled (commented out) due to security concerns, see DERBY-1387 for details.  ------------------------- MBEAN ATTRIBUTES  ----------------------------  public void setDrdaMaxThreads(int max) throws Exception { try { server.netSetMaxThreads(max); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } } public void setDrdaTimeSlice(int timeSlice) throws Exception { try { server.netSetTimeSlice(timeSlice); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } } public void setDrdaTraceAll(boolean on) throws Exception { try { server.trace(on); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } } public String traceConnection(int connection, boolean on) throws Exception { String feedback; if(on){ feedback = "Tracing enabled for connection " + connection + ". \n (0 = all connections)"; } else{ feedback = "Tracing disabled for connection " + connection + ". \n (0 = all connections)"; } try { server.trace(connection, on); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } return feedback; } public String enableConnectionLogging() throws Exception { String feedback = "Connection logging enabled."; try { server.logConnections(true); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } return feedback; } public String disableConnectionLogging() throws Exception { String feedback = "Connection logging disabled."; try { server.logConnections(false); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } return feedback; } public void shutdown() throws Exception { try { server.shutdown(); } catch (Exception ex) { Monitor.logThrowable(ex); throw ex; } } ------------------------- UTILITY METHODS  ---------------------------- Gets the value of a specific network server setting (property). Most server-related property keys have the prefix <code>derby.drda.</code> and may be found in the org.apache.derby.iapi.reference.Property class. Return start time. Return time server has been running. ------------------------- MBEAN OPERATIONS  ---------------------------- Pings the Network Server.
Shutdown the NetworkServer Start Derby Network server Try to test for a connection Throws exception if unable to get a connection trace utility of server
Is this class assignable to the specified class? This is useful for the VTI interface where we want to see if the class implements java.sql.ResultSet. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. Is this class have a public method with the specified signiture This is useful for the VTI interface where we want to see if the class has the option static method for returning the ResultSetMetaData. Do code generation for this method call Report whether this node represents a builtin VTI. Mark this node as only needing to to a single instantiation.  (We can reuse the object after newing it.)
Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Dummy implementation to return a constant. Will be replaced with actual NEXT VALUE logic. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Return the suite that runs the NIST SQL scripts.




Get the estimated row count from this result set. Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction and projection parameters are evaluated for each row. Return the point of attachment for this subquery. (Only meaningful for Any and Once ResultSets, which can and will only be at the top of a ResultSet for a subquery.) Return the isolation level of the scan in the result set. Only expected to be called for those ResultSets that contain a scan. Is this ResultSet or it's source result set for update Mark the ResultSet as the topmost one in the ResultSet tree. Useful for closing down the ResultSet on an error. Marks the resultSet's currentRow as deleted after a delete has been issued by either by using positioned delete or JDBC's deleteRow method. open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... <p> openCore() can only be called on a closed result set.  see reopenCore if you want to reuse an open result set. <p> For NoPutResultSet open() must only be called on the top ResultSet. Opening of NoPutResultSet's below the top result set are implemented by calling openCore. Positions the cursor in the specified rowLocation. Used for scrollable insensitive result sets in order to position the cursor back to a row that has already be visited. reopen the scan.  behaves like openCore() but is optimized where appropriate (e.g. where scanController has special logic for us). <p> used by joiners <p> scan parameters are evaluated at each open, so there is probably some way of altering their values... Do we need to relock the row when going to the heap. Get the number of this ResultSet, which is guaranteed to be unique within a statement. Set the current row to the row passed in. Set that we are acting on behalf of an insert result set that has deferrable check constraints Set whether or not the NPRS need the row location when acting as a row source.  (The target result set determines this.) Notify a NPRS that it is the source for the specified TargetResultSet.  This is useful when doing bulk insert. New methods for supporting detectability of own changes for for updates and deletes when using ResultSets of type TYPE_SCROLL_INSENSITIVE and concurrency CONCUR_UPDATABLE. Updates the resultSet's current row with it's new values after an update has been issued either using positioned update or JDBC's udpateRow method.
Clear the current row class implementation Clear the Orderable cache for each qualifier. (This should be done each time a scan/conglomerate with qualifiers is reopened.) Close needs to invalidate any dependent statements, if this is a cursor. Must be called by any subclasses that override close().  Return my cursor name for JDBC. Can be null.  NoPutResultSet interface Returns the description of the table's rows RowSource interface  Is this ResultSet or it's source result set for update This method will be overriden in the inherited Classes if it is true  RowLocationRetRowSource interface    Return a 2-d array of Qualifiers as a String   Set the current row to the row passed in.   Return true if we should skip the scan due to nulls in the row when the start or stop positioners on the columns containing null do not implement ordered null semantics. Return true if we should skip the scan due to nulls in the start or stop position when the predicate on the column(s) in question do not implement ordered null semantics. beetle 4464, we also compact the areNullsOrdered flags into checkNullCols here. Get all of the columns out of a value stored in a BackingStoreHashtable.
Determine if the cursor is before the first row in the result set. Clear the current row. This is done after a commit on holdable result sets. This is a no-op on result set which do not provide rows. Dump the stat if not already done so. Close all of the open subqueries. Does this ResultSet cause a commit or rollback. Compute the generation clauses on the current row in order to fill in computed columns. Returns the row at the absolute position from the query, and returns NULL when there is no such position. (Negative position means from the end of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: An exception will be thrown on 0.  Get the Timestamp for the beginning of execution. class implementation Return the current time in milliseconds, if DEBUG and RunTimeStats is on, else return 0.  (Only pay price of system call if need to.) Return the cursor name, null in this case. Get the Timestamp for the end of execution. Get the execution time in milliseconds. Returns the first row from the query, and returns NULL when there are no rows. Returns the last row from the query, and returns NULL when there are no rows. No rows to return, so throw an exception. Returns the previous row from the query, and returns NULL when there are no more previous rows. RESOLVE - This method will go away once it is overloaded in all subclasses. Return the query plan as a String. Returns the row at the relative position from the current cursor position, and returns NULL when there is no such position. (Negative position means toward the beginning of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: 0 is valid. NOTE: An exception is thrown if the cursor is not currently positioned on a row. Returns null. Returns the row number of the current row.  Row numbers start from 1 and go to 'n'.  Corresponds to row numbering used to position current row in the result set (as per JDBC).  Return the total amount of time spent in this ResultSet Find out if the <code>ResultSet</code> is closed. Returns zero. Returns FALSE Sets the current position to after the last row and returns NULL because there is no current row. Sets the current position to before the first row and returns NULL because there is no current row. Set up the result set for use. Should always be called from <code>open()</code>. Construct support for normalizing generated columns. This figures out which columns in the target row have generation clauses which need to be run.


Return a node to the allocator. Expand the node allocator's capacity by certain percent. Initialize the allocator with default values for initial and maximum size.  Returns false if sufficient memory could not be allocated. Initialize the allocator with default values for initial size and the provided maximum size. Returns false if sufficient	memory could not be allocated. Initialize the allocator with the given initial and maximum sizes.  This method does not check, but assumes that the value of initSize is less than the value of maxSize, and that they are both powers of two. Returns false if sufficient memory could not be allocated. Clear all nodes that this allocator has allocated. The allocator must already have been initialized.
Accept the visitor for all visitable children of this node. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. Do code generation for this method call Generate the expression that evaluates to the receiver. This is for the case where a java expression is being returned to the SQL domain, and we need to check whether the receiver is null (if so, the SQL value should be set to null, and this Java expression not evaluated). Instance method calls and field references have receivers, while class method calls and calls to constructors do not. If this Java expression does not have a receiver, this method returns null. Only generate the receiver once and cache it in a field. This is because there will be two references to the receiver, and we want to evaluate it only once. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression.
* UserAuthenticator Authenticate the passed-in user's credentials.   ModuleControl implementation (overriden)  Check if we should activate this authentication service.
If the result set has been opened, close the open scan. <p> Compute the start column for an update/insert. <p> Fetch the result datatypes out of the activation. </p> Get a cached data value descriptor that can receive the normalized value of the specified column. Gets information from last getNextRow call. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Get a data type descriptor that describes the desired type for the specified column.   CursorResultSet interface  Gets information from its source. We might want to have this take a CursorResultSet in its constructor some day, instead of doing a cast here? Return the total amount of time spent in this ResultSet  Normalize a column.  For now, this means calling constructors through the type services to normalize a type to itself.  For example, if you're putting a char(30) value into a char(15) column, it calls a SQLChar constructor with the char(30) value, and the constructor truncates the value and makes sure that no non-blank characters are truncated.  class implementation  Normalize a row.  ResultSet interface (leftover from NoPutResultSet)  open a scan on the source. scan parameters are evaluated at each open, so there is probably some way of altering their values... reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
Push through the offset and fetch first parameters, if any, to the child result set. Push the order by list down from InsertNode into its child result set so that the optimizer has all of the information that it needs to consider sort avoidance. set the Information gathered from the parent table that is required to perform a referential action on dependent table.
Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. Do code generation for the NOT operator.

Create necessary schema if schema not already created end of method checkAndCreateSchema() close resources close connection Delete row from table deleteRow is the value of the t_key of the row to be deleted return number of rows deleted Perform an insert or an update or delete operation end of method doIUDOperation() executing a select and retrieving the results select the row with t_key value as 'selectWhat' Opens a connection and executes DML (insert, select, update, delete) operations end of method doNsSampleWork() gets a database connection If the dbUrl is trying to connect to the Derby NetNsSampleWork server using JCC then the jcc driver must be already loaded before calling this method, else there will be an error return jcc connection if no error, else null Generates random values and performs the inserts into the database Loads schema , inserts 'rowsToInsert' number of rows into the table end of method loadSchema() prepare required sql statements set the connection to this isolation level update a row in the table updateWhere	is the value of the t_key row which needs to be updated return  number of rows updated
Discard the data. Discard the data. * Methods of OutputStream Discard the data.
Numbers check for isNegative first and negate it if negative.   This method implements the / operator for TINYINT, SMALLINT and INTEGER. Specialized methods are not required for TINYINT and SMALLINT as the Java virtual machine always executes byte and int division as integer. Suitable for integral types that ignore scale. Privileged lookup of a Context. Must be private so that user code can't call this entry point. The isNegative abstract method.  Checks to see if this.value is negative. To be implemented by each NumberDataType. This method implements the - operator for TINYINT, SMALLINT and INTEGER. normalizeDOUBLE normalizes the value, so that negative zero (-0.0) becomes positive. normalizeREAL normalizes the value, so that negative zero (-0.0) becomes positive. <p> The reason for having normalizeREAL with two signatures is to avoid that normalizeREAL is accidentally called with a casted {@code (float)<double value>} since this can introduce an undetected underflow values to 0.0f. normalizeREAL normalizes the value, so that negative zero (-0.0) becomes positive. This method implements the + operator for TINYINT,SMALLINT,INT. setValue for integral exact numerics. Converts the BigDecimal to a long to preserve precision Set the value from a correctly typed Integer object. Used for TINYINT, SMALLINT, INTEGER. Common code to handle converting a byte to this value by using the int to this value conversion. Simply calls setValue(int). Common code to handle java.lang.Integer as a Number, used for TINYINT, SMALLINT, INTEGER Common code to handle converting a short to this value by using the int to this value conversion. Simply calls setValue(int). This is the sqrt method. Compare this (not null) to a non-null value. Implementation for integral types. Convert to a BigDecimal using long Controls use of old DB2 limits (DERBY-3398).
The SQL ABSOLUTE operator.  Absolute value of this NumberDataValue. The SQL / operator. The SQL / operator. The SQL unary - operator.  Negates this NumberDataValue. The SQL - operator. The SQL mod operator. The SQL + operator. Set the value of this NumberDataValue to the given value. This is only intended to be called when mapping values from the Java space into the SQL space, e.g. parameters and return types from procedures and functions. Each specific type is only expected to handle the explicit type according the JDBC. <UL> <LI> SMALLINT from java.lang.Integer <LI> INTEGER from java.lang.Integer <LI> LONG from java.lang.Long <LI> FLOAT from java.lang.Float <LI> DOUBLE from java.lang.Double <LI> DECIMAL from java.math.BigDecimal </UL> The SQL SQRT operator.  Sqrt value of this NumberDataValue. The SQL * operator.
This generates the proper constant.  It is implemented by every specific constant node (e.g. IntConstantNode). Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null. This is useful for bind time resolution of VTIs. RESOLVE: What do we do for primitives?
Tell whether this type (numeric) is compatible with the given type.  Return the method name to get a Derby DataValueDescriptor object of the correct type. This implementation returns {@code "getDataValue"}, unless the type is {@code DECIMAL}, in which case {@code "getDecimalDataValue"} is returned.   Get the precision of the operation involving two of the same types.  Only meaningful for decimals, which override this. Get the method name for getting out the corresponding primitive Java type. Get the scale of the operation involving two of the same types.  Since we don't really have a good way to pass the resultant scale and precision around at execution time, we will model that BigDecimal does by default. This is good in most cases, though we would probably like to use something more sophisticated for division.
addHelperColsToSubquery For some of the metadata queries, the ODBC version needs to access values that are only available in the JDBC subquery.  In such cases, we want to add those values as additional "helper" columns to the subquery result set, so that they can be referenced from the new ODBC outer query (without requiring a join).  For example, assume we have 2 tables T1(int i, int j) and T2 (int a), and a subquery "SELECT T1.i, T1.j + T2.a from T1, T2)". Then we have an outer query that, instead of returning "T1.j + T2.a", needs to return the value of "2 * T2.a": SELECT VT.i, 2 * T2.a FROM (SELECT T1.i, T1.j + T2.a FROM T1, T2) VT The above statement WON'T work, because the outer query can't see the value "T2.a".  So in such a a case, this method will add "T2.a" to the list of columns returned by the subquery, so that the outer query can then access it: SELECT VT.i, 2 * VT.a FROM (SELECT T1.i, T1.j + T2.a, T2.a FROM T1, T2) VT Which specific columns are added to the subquery depends on the query in question. addNewColumnsForODBC Adds new columns to the ODBC version of a metadata query (the ODBC version is at this point being built up in newQueryText).  Before this method was called, a dummy placeholder should have been placed in the newQueryText buffer (by a call to "markNewColPosition").  This method simply replaces that dummy placeholder with the SQL text for the new columns. changeColValueToODBC Searches for the received column name in the received String buffer and replaces it with an ODBC-compliant value. changeValuesForODBC Searches for a JDBC column name in the received String buffer and replaces the first occurrence with an ODBC- compliant value.  This method determines what specific columns need updated values for a given query, and then makes the appropriate call for each column. changeWhereClause Substitutes patterns in the WHERE clause extractColName Takes a single column definition from a SELECT clause and returns only the unqualified name of the column. Assumption here is that any column definition we see here will either 1) end with an "AS <COLUMN_NAME>" clause, or 2) consist of ONLY a column name, such as "A" or "A.B".  At the time of writing, these assumptions were true for all relevant metadata queries. Ex. If colDef is "A", this method will return "A". If colDef is "A.B", this method will return "B". If colDef is "<bunch of SQL> AS C", this method will return "C". fragSubstitution Replaces a single occurrence of the received fragment key with the text corresponding to that key. generateODBCQueries: Reads the existing (JDBC) metadata queries from metadata.properties and, for each one, makes a call to generate an ODBC-compliant version. generateODBCQuery Takes a specific JDBC query, writes it to the output file, and then creates an ODBC-compliant version of that query (if needed) and writes that to the output file, as well. generateSELECTClause Generates an outer SELECT clause that is then wrapped around a JDBC query to change the types and/or values of the JDBC result set.  The JDBC query thus becomes a subquery. Ex. if we have a JDBC query "SELECT A, B FROM T1" and ODBC requires that "A" be a smallint, this method will generate a select clause "SELECT CAST (T2.A AS SMALLINT), T2.B FROM" that is then used to wrap the JDBC query, as follows: SELECT CAST (T2.A AS SMALLINT), T2.B FROM (SELECT A, B FROM T1) T2 getCastInfoForCol Returns the target type for a result set column that needs to be cast into an ODBC type.  This is usually for casting integers to "SMALLINT". getFragment Looks up an SQL fragment and returns the value as a String. getSelectColDefinitions Parses the SELECT clause of a JDBC metadata SQL query and returns a list of the columns being selected.  For example, if the received statement was "SELECT A, B AS C, D * 2 FROM T1", this method will return an ArrayList with three string elements: 1) "A", 2) "B AS C", and 3) "D * 2". initChanges Create a listing of the types of changes that need to be made for each metadata query to be ODBC-compliant. If a metadata query has no entry in this map, then it is left unchanged and no ODBC-version will be created. Having this mapping allows us to skip over String parsing (which can be slow) when it's not required. For details on the changes, see the appropriate methods below. Return true if the column is a BOOLEAN column which should be coerced to an INTEGER. **** main: Open the metadata.properties file (the copy that is in the build directory, NOT the one in the source directory), figure out what changes are needed for the various metadata queries, and then generate the ODBC-compliant versions where needed. @param args Ignored. @return ODBC-compliant metadata statements have been generated and written out to "odbc_metadata.properties" in the running directory. markNewColPosition In effect, "marks" the position at which additional columns are to be added for ODBC compliance.  This is accomplished by adding a dummy column name to the list of SELECT columns.  Later, in the method that actually adds the columns, we'll do a find- replace on this dummy value. renameColForODBC Searches for the old column name in the received String buffer and replaces it with the new column name.  Note that we only replace the old column name where it is preceded by "AS", because this is the instance that determines the column name in the final result set. renameColsForODBC Renames any columns in the received query so that they are ODBC-compliant. stmtNeedsChange Returns whether or not a specific metadata statement requires the received type of change.  This is determined based on the info stored in the "changeMaps" mapping. Replaces a single occurrence of the received old pattern with the text in the new pattern trimIgnorable Removes all 'ignorable' chars that immediately precede or follow (depending on the direction) the character at the received index.  "Ignorable" here means whitespace OR a single backslash ("\"), which is used in the metadata.properties file to indicate line continuation. trimIgnorable Same as trimIgnorable above, except with String argument instead of char[].
Return suite of tests that OEChecks the row counts for all the tables in the Order Entry bechmark. Check if number of rows in table is as expected Check if number of rows in table is within one percent of expected value Return the number of rows in the table. A simple select count(*) from tableName Test cardinality of CUSTOMER table Test cardinality of DISTRICT table Test cardinality of HISTORY table Test cardinality of ITEM table Test cardinality of NEWORDERS table Test cardinality of ORDERLINE table Test cardinality of ORDERS table Test cardinality of STOCK table Test cardinality of WAREHOUSE table
Section 2.1.6 of TPC-C specification for CID NURand(A, x, y) = (((random(0, A) | random(x, y)) + C) % (y - x + 1)) + x NURand(1023, 1,3000) Section 2.1.6 of TPC-C specification, for C_LAST NURand(A, x, y) = (((random(0, A) | random(x, y)) + C) % (y - x + 1)) + x NURand(255,0,999) Section 2.1.6 of TPC-C specification, for OL_I_ID NURand(A, x, y) = (((random(0, A) | random(x, y)) + C) % (y - x + 1)) + x C is a run-time constant randomly chosen within [0 .. A] NURand(8191, 1,100000) Return a random carrier [1..10] Return a random district [1..10] Payment amount between 1.00 and 5,000.00 tpcc 4.3.2.2 (random a string) Section 4.3.2.2(and comments 1 and 2). The notation random a-string [x .. y] (respectively, n-string [x ..y]) represents a string of random alphanumeric (respectively, numeric)characters of a random length of minimum x, maximum y, and mean (y+x)/2. Clause 4.3.2.3 of the TPC-C specification Clause 4.3.3.1 of TPC-C spec. random a-string [26 .. 50]. For 10% of the rows, selected at random, the string "ORIGINAL" must be held by 8 consecutive characters starting at a random position within the string tpcc 4.3.2.5 Implements random within [x .. y ] for int tpcc 4.3.2.2 (random n string) Generate the zipcode value return zipcode value according to the requirements specified in Clause 4.3.2.7 of TPC-C spec Return a random threshold for the stock level [10..20]
Cacheable interface


<p> Get an array of versions supported by this platform. </p> <p> Squeeze the supported versions out of any array of candidate versions. </p>
If the result set has been opened, close the open scan. Return the requested value computed from the next row.  Return the total amount of time spent in this ResultSet  ResultSet interface (leftover from NoPutResultSet)  open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
Create a new database from the backup copy taken earlier. Check if backup is running ? Backup the database Restore the  database from the backup copy taken earlier. implementation of run() method in the Runnable interface, which is invoked when a thread is started using this class object. Performs online backup. Wait for the backup to start. Wait for the backup to finish.

************************************************************************ Public Methods of ConglomerateController interface: ************************************************************************* Check consistency of a btree. <p> Read in root and check consistency of entire tree.  Currently raises sanity check errors. <p> RESOLVE (mikem) if this is to be supported in non-sanity servers what should it do? Close the open conglomerate. Dump information about tree into the log. <p> Traverse the tree dumping info about tree into the log. return column Sort order information Return the container handle. <p> ************************************************************************ Public Methods of RowCountable class: ************************************************************************* Get the total estimated number of rows in the container. <p> The number is a rough estimate and may be grossly off.  In general the server will cache the row count and then occasionally write the count unlogged to a backing store.  If the system happens to shutdown before the store gets a chance to update the row count it may wander from reality. <p> This call is currently only supported on Heap conglomerates, it will throw an exception if called on btree conglomerates. get height of the tree. <p> Read in root and return the height (number of levels) of the tree. The level of a tree is 0 in the leaf and increases by 1 for each level of the tree as you go up the tree. ************************************************************************ Public Accessors of This class: ************************************************************************* * Methods of OpenBTree Initialize the open conglomerate. If container is null, open the container, otherwise use the container passed in. Initialize the open conglomerate. <p> If container is null, open the container, otherwise use the container passed in.  The container is always opened with no locking, it is up to the caller to make the appropriate container locking call. <p> Check if all the columns are Indexable and Storable.  Eventually this routine could check whether all the types were right also. ************************************************************************ Public Methods of ScanController interface: ************************************************************************* is the open btree table locked? Open the container after it has been closed previously. <p> Open the container, obtaining necessary locks.  Most work is actually done by RawStore.openContainer().  Will only reopen() if the container is not already open. Set the total estimated number of rows in the container. <p> Often, after a scan, the client of RawStore has a much better estimate of the number of rows in the container than what store has.  For instance if we implement some sort of update statistics command, or just after a create index a complete scan will have been done of the table.  In this case this interface allows the client to set the estimated row count for the container, and store will use that number for all future references. <p> This call is currently only supported on Heap conglomerates, it will throw an exception if called on btree conglomerates. Testing infrastructure to cause unusual paths through the code. <p> Through the use of debug flags allow test code to cause otherwise hard to cause paths through the code. <p>
************************************************************************ Public Methods implementing some ConglomerateController Interfaces: ************************************************************************* Check consistency of a conglomerate. <p> Checks the consistency of the data within a given conglomerate, does not check consistency external to the conglomerate (ie. does not check that base table row pointed at by a secondary index actually exists). <p> There is no checking in the default implementation, you must override to get conglomerate specific consistency checking. Close the container. <p> Handles being closed more than once. ************************************************************************ Public Methods implementing ConglomPropertyQueryable Interface: ************************************************************************* Request set of properties associated with a table. <p> Returns a property object containing all properties that the store knows about, which are stored persistently by the store.  This set of properties may vary from implementation to implementation of the store. <p> This call is meant to be used only for internal query of the properties by jbms, for instance by language during bulk insert so that it can create a new conglomerate which exactly matches the properties that the original container was created with.  This call should not be used by the user interface to present properties to users as it may contain properties that are meant to be internal to jbms.  Some properties are meant only to be specified by jbms code and not by users on the command line. <p> Note that not all properties passed into createConglomerate() are stored persistently, and that set may vary by store implementation. Get information about space used by the conglomerate. Request the system properties associated with a table. <p> Request the value of properties that are associated with a table.  The following properties can be requested: derby.storage.pageSize derby.storage.pageReservedSpace derby.storage.minimumRecordSize derby.storage.initialPages <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(ConglomerateController cc) { Properties prop = new Properties(); prop.put("derby.storage.pageSize", ""); cc.getTableProperties(prop); System.out.println( "table's page size = " + prop.getProperty("derby.storage.pageSize"); } ************************************************************************ Public Accessors of This class: ************************************************************************* ************************************************************************ Public Methods of this class: ************************************************************************* Open the container. <p> Open the container, obtaining necessary locks.  Most work is actually done by RawStore.openContainer(). is the open btree table locked? <p> Latch the page containing the current RowPosition. </p> <p> This implementation also automatically updates the RowPosition to point at the slot containing the current RowPosition.  This slot value is only valid while the latch is held. </p> <p> If the row pointed to by {@code pos} does not exist (including the case where the page itself does not exist), the page will not be latched, and {@code pos.current_page} will be set to {@code null}. </p> ************************************************************************ Public Methods implementing standard store row locking interfaces: latchPage(RowPosition) latchPageAndRepositionScan(RowPosition) lockPositionForRead(RowPosition, aux_pos, moveForwardIfRowDisappears) lockPositionForWrite(RowPosition, forInsert, wait) unlockPositionAfterRead(RowPosition) ************************************************************************* Latch the page containing the current RowPosition, and reposition scan. <p> Upon return the scan will hold a latch on the page to continue the scan on.  The scan will positioned on the record, just before the next record to return. Note that for both hold cursor and read uncommitted support this routine handles all cases of either the current position "dissappearing" (either the row and/or page).  The row and/or page can disappear by deleted space being reclaimed post commit of that delete, and for some reason the code requesting the reposition does not have locks which prevented the space reclamation.  Both hold cursor and read uncommitted scans are examples of ways the caller will not prevent space reclamation from claiming the position. This implementation also automatically updates the RowPosition to point at the slot containing the current RowPosition.  This slot value is only valid while the latch is held. <p> Lock row at given row position for read. </p> <p> This routine requests a row lock NOWAIT on the row located at the given RowPosition.  If the lock is granted NOWAIT the routine will return true.  If the lock cannot be granted NOWAIT, then the routine will release the latch on "page" and then it will request a WAIT lock on the row. </p> <p> This implementation: Assumes latch held on current_page. If the current_rh field of RowPosition is non-null it is assumed that we want to lock that record handle and that we don't have a slot number. If the current_rh field of RowPosition is null, it is assumed the we want to lock the indicated current_slot.  Upon return current_rh will point to the record handle associated with current_slot. </p> <p> After waiting and getting the lock on the row, this routine will fix up RowPosition to point at the row locked.  This means it will get the page latch again, and it will fix the current_slot to point at the waited for record handle - it may have moved while waiting on the lock. </p> <p> When this method returns, the page holding the row pointed to by the {@code RowLocation} is latched. This is however not the case if {@code moveForwardIfRowDisappears} is {@code false} and the row has disappeared. Then the latch will be released before the method returns, and {@code pos.current_page} will be set to {@code null}. </p> <p> Lock the row at the given position for write. </p> <p> The page pointed to by the {@code RowPosition} is assumed to be latched when this method is called. If the lock cannot be obtained without waiting, the latch will be released and re-obtained when the lock has been acquired. </p> <p> If the latch was released while waiting for the lock, and the row does not exist after the lock is obtained, the latch will be released again before the method returns, and {@code pos.current_page} will be set to {@code null}. </p> ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Private methods for This class: ************************************************************************* ************************************************************************ abstract methods of This class: ************************************************************************* Return an "empty" row location object of the correct type. <p> Open the container. <p> Open the container, obtaining necessary locks.  Most work is actually done by RawStore.openContainer().  Will only reopen() if the container is not already open. Unlock the record after a previous request to lock it. <p> Unlock the record after a previous call to lockRecordForRead().  It is expected that RowPosition contains information used to lock the record, Thus it is important if using a single RowPosition to track a scan to call unlock before you move the position forward to the next record. <p> Note that this routine assumes that the row was locked forUpdate if the OpenConglomerate is forUpdate, else it assumes the record was locked for read.
************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* Return an empty template (possibly partial) row to be given back to a client. <p> The main use of this is for fetchSet() and fetchNextGroup() which allocate rows and then give them back entirely to the caller. <p> Return an empty template (possibly partial) row to be used and reused internally for processing. <p> The main use of this is for qualifying rows where a row has not been provided by the client.  This routine cache's a single row for reuse by the caller, if the caller needs 2 concurrent scratch rows, some other mechanism must be used. <p> Return a scratch RowPosition. <p> Used by GenericConglomerateController.delete() and GenericConglomerateController.replace().  It may be reused so callers must insure that object no longer needed before next possible call to get it again. <p> Return a complete empty row. <p> The main use of this is for searching a tree where a complete copy of the row is needed for searching. <p> Tells if there is at least one column with a collation different than UCS BASIC in the conglomerate.
************************************************************************ Fields of the class ************************************************************************* ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* Return an "empty" row location object of the correct type. <p>

Release any resources. Execute a single delivery from the FIFO queue. Processing a delivery request is described in clause 2.7.4. Execute new order. New order is described in clause 2.4. <P> Assumption is that items.length == quanties.length == supplyW.length. Execute order status by customer identifer. Order status is described in clause 2.6. Execute order status by last name. Order status is described in clause 2.6. Execute payment by customer identifer. Payment is described in clause 2.5. Execute payment by last name. Payment is described in clause 2.5. Queue a delivery request. Queuing of delivery requests is described in clause 2.7.2. <P> The implementation of Operations is responsible for managing the FIFO queue of requests, which could be in a flat file, the database or memory etc. Execute stock level. Stock level is described in clause 2.8.
Operate on an input object
<p> Generate code that pushes an SqlXmlUtil instance onto the stack. The instance will be created and cached in the activation's constructor, so that we don't need to create a new instance for every row. </p> <p> If the {@code xmlQuery} parameter is non-null, there will also be code that compiles the query when the SqlXmlUtil instance is created. </p>
Print the trace so far. Say that we're adding an unordered optimizable. Say that we're considering a different access path for a table. Report the cost of the cheapest plan so far. Say that we have a complete join order. Report the selectivity calculated from SYSSTATISTICS. Report that we are considering a conglomerate for a table. Say that we're considering a particular join strategy on a particular table. Say that we're setting the lock mode to MODE_RECORD because the start and stop positions are all constant. Report the cost based on selectivities coming out of SYSSTATISTICS. Report the cost of a scan given the selectivity of an extra first column. Report the cost if we include an extra non-qualifier. Report the cost if we include an extra qualifier. Report the cost if we include an extra start/stop. Report the cost based on index statistics. Report the cost of a conglomerate scan. Report the cost of scanning a table a certain number of times Report the cost of using a non-covering index. Report the cost with sort avoidance. Report the cost without a sort. Say that the current plan avoids a sort. End tracing the optimization of a query block. Report the cost of using a particular conglomerate to scan a table. Report the columns being traced Say that the user specified an impossible join order. Say that we're considering a join order. Say that we're looking for an index specified by optimizer hints. Say that we're modifying access paths. Report that we are advancing to the next access path for the table. Say that we couldn't find a best plan. Say that we have exhausted the conglomerate possibilities for a table. Say that we're setting the lock mode to MODE_TABLE because there is no start/stop position. Say that we're considering a non-covering index. Say that we're starting to optimize a join node Report that we've found a best access path. Report the best access path for the table so far. Say that we are remembering the current plan as the best join order so far. Report the best sort-avoiding access path for this table so far. Report an optimizer failure, e.g., while optimizing an outer join Say that we've found a new best join strategy for the table. Say that we're considering scanning a heap even though we have a unique key match. Say that we short-circuited a join order. Report the cost of a scan which will match exactly one row. Say we won't consider a hash join because there are no hash key columns. Say that we won't consider a hash join because the result can't be materialized Say that we are skipping a plan because it consumes too much memory. Say that we're skipping the join order starting with the next optimizable. Report the sort cost. Report that this plan needs a sort Start optimizer tracing for a query block. //////////////////////////////////////////////////////////////////////  CONSTANTS  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  BEHAVIOR  ////////////////////////////////////////////////////////////////////// Start the start of tracing a statement. Say that the optimizer ran out of time. Say that we have optimized the user-specified join order. Say that there's nothing to optimizer.
//////////////////////////////////////////////////////////////////////  MINIONS  ////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  OptionalTool BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Takes the following arguments: </p> <ul> <li>fileURL - The url of the file holding the xml trace. E.g.: "file:///Users/me/mainline/z.txt"</li> </ul> <p> ...and creates the following schema objects for viewing the xml trace of the optimizer: </p> <ul> <li>ArrayList - A user-defined type wrapping java.util.ArrayList.</li> <li>asList - A factory function for creating ArrayLists.</li> <li>planCost - An XmlVTI for viewing xml traces.</li> <li>planCost - A view which passes the file name to the XmlVTI.</li> </ul> <p> Drop the schema objects which were created for viewing the xml file containing the optimizer trace. </p> Wrap an exception in a SQLException
Check whether this optimizable's sort avoidance path should be considered. Convert an absolute to a relative 0-based column position. This is useful when generating qualifiers for partial rows from the store. Estimate the cost of scanning this Optimizable using the given predicate list with the given conglomerate.  It is assumed that the predicate list has already been classified.  This cost estimate is just for one scan, not for the life of the query. Is the current proposed join strategy for this optimizable feasible given the predicate list? Return true if this is the target table of an update Get the table name of this Optimizable.  Only base tables have table names (by the time we use this method, all views will have been resolved). Get the best access path for this Optimizable. Get the best sort-avoidance path for this Optimizable. Get the current access path under consideration for this Optimizable Get the DataDictionary from this Optimizable.  This is useful for code generation because we need to get the constraint name if scanning a back index so that RunTimeStatistics can display the correct info. Get the (exposed) name of this Optimizable Get the number of the number of columns returned by this Optimizable. Get the optimizer tracer, if any Get the Properties list, if any, associated with this optimizable. Get the map of referenced tables for this Optimizable. Get this Optimizable's result set number Get the table descriptor for this table (if any).  Only base tables have table descriptors - for the rest of the optimizables, this method returns null. Get this Optimizable's table number Get the best access path overall for this Optimizable. Tell whether this Optimizable has any large object (LOB) columns. Return true if this Optimizable has a table number Return the hash key column numbers, for hash join strategy Init the access paths for this optimizable. Return the initial capacity of the hash table, for hash join strategy Tell whether this Optimizable represents a base table Return whether or not this is a covering index.  We expect to call this during generation, after access path selection is complete. Tell whether this Optimizable is materializable Will the optimizable return at most 1 row per scan? Is the optimizable the target table of an update or delete? Can this Optimizable appear at the current location in the join order. In other words, have the Optimizable's dependencies been satisfied? Return the load factor of the hash table, for hash join strategy Return the maximum capacity of the hash table, for hash join strategy  Modify the access path for this Optimizable, as necessary.  This includes things like adding a result set to translate from index rows to base rows Choose the next access path to evaluate for this Optimizable. Choose the best access path for this Optimizable. Report whether optimizer tracing is on Pull all the OptimizablePredicates from this Optimizable and put them in the given OptimizablePredicateList. Push an OptimizablePredicate down, if this node accepts it. Remember the current access path as the best one (so far). Remember the current join strategy as the best one so far in this join order. Mark this optimizable so that its sort avoidance path will be considered. Set the hash key column numbers, for hash join strategy Set the Properties list for this optimizalbe. Begin the optimization process for this Optimizable.  This can be called many times for an Optimizable while optimizing a query - it will typically be called every time the Optimizable is placed in a potential join order. Tell whether this Optimizable can be instantiated multiple times Does this optimizable have a uniqueness condition on the given predicate list, and if so, how many unique keys will be returned per scan. When remembering "truly the best" access path for an Optimizable, we have to keep track of which OptimizerImpl the "truly the best" access is for.  In most queries there will only be one OptimizerImpl in question, but in cases where there are nested subqueries, there will be one OptimizerImpl for every level of nesting, and each OptimizerImpl might have its own idea of what this Optimizable's "truly the best path" access path really is.  In addition, there could be Optimizables above this Optimizable that might need to override the best path chosen during optimization.  So whenever we save a "truly the best" path, we take note of which Optimizer/Optimizable told us to do so.  Then as each level of subquery finishes optimization, the corresponding OptimizerImpl/Optimizable can load its preferred access path into this Optimizable's trulyTheBestAccessPath field and pass it up the tree, until eventually the outer-most OptimizerImpl can choose to either use the best path that it received from below (by calling "rememberAsBest()") or else use the path that it found to be "best" for itself. This method is what allows us to keep track of which OptimizerImpl or Optimizable saved which "best plan", and allows us to load the appropriate plans after each round of optimization. Verify that the Properties list with optimizer overrides, if specified, is valid
Return the nth Optimizable in the list. Init the access paths for these optimizables. Tell whether the join order is legal. Tell whether the join order should be optimized. Set the join order for this list of optimizables.  The join order is represented as an array of integers - each entry in the array stands for the order of the corresponding element in the list.  For example, a joinOrder of {2, 0, 1} means that the 3rd Optimizable in the list (element 2, since we are zero-based) is the first one in the join order, followed by the 1st element in the list, and finally by the 2nd element in the list. This method shuffles this OptimizableList to match the join order. Obviously, the size of the array must equal the number of elements in the array, and the values in the array must be between 0 and the number of elements in the array minus 1, and the values in the array must be unique. Set the nth Optimizable to the specified Optimizable. Return the number of Optimizables in the list. user can specify that s/he doesn't want statistics to be considered when optimizing the query. Verify that the Properties list with optimizer overrides, if specified, is valid
Is this predicate a comparison with a known constant value? Is this predicate an equality comparison with a constant expression? (IS NULL is considered to be an = comparison with a constant expression). Get an Object representing the known constant value that the given Optimizable is being compared to. Get the position of the index column that this predicate restricts. NOTE: This assumes that this predicate is part of an OptimizablePredicateList, and that classify() has been called on the OptimizablePredicateList. Get the map of referenced tables for this OptimizablePredicate. Returns if the predicate involves an equal operator on one of the columns specified in the baseColumnPositions. Return whether or not an OptimizablePredicate contains a method call. Return whether or not an OptimizablePredicate contains a subquery. Is this predicate a qualifier? Is this predicate a start key? Is this predicate a stop key? Tell the predicate that it is to be used as a qualifier in an index scan. Tell the predicate that it is to be used as a column in the start key value for an index scan. Tell the predicate that it is to be used as a column in the stop key value for an index scan. Get a (crude) estimate of the selectivity of this predicate. This is to be used when no better technique is available for estimating the selectivity - this method's estimate is a hard- wired number based on the type of predicate and the datatype (the selectivity of boolean is always 50%).
Add the given OptimizablePredicate to the end of this list. Walk through the predicates in this list and make any adjustments that are required to allow for proper handling of an ORDER BY clause. Classify the predicates in this list according to the given table and conglomerate.  Each predicate can be a start key, stop key, and/or qualifier, or it can be none of the above.  This method also orders the predicates to match the order of the columns in a keyed conglomerate.  No ordering is done for heaps. Non-destructive copy of all of the predicates from this list to the other list. This is useful when splitting out a set of predicates from a larger set, like when generating a HashScanResultSet. Generate the qualifiers for a scan.  This method generates an array of Qualifiers, and fills them in with calls to the factory method for generating Qualifiers in the constructor for the activation. It stores the array of Qualifiers in a field in the activation, and returns a reference to that field. If there are no qualifiers, it initializes the array of Qualifiers to null. Generate the start key for a heap or index scan. Generate the stop key for a heap or index scan. Return the nth OptimizablePredicate in the list. Check into the predicate list if the passed column has an equijoin predicate on it. Is there an optimizable equality predicate on the specified column? Is there an optimizable equijoin on the specified column? Return whether or not the specified entry in the list is a redundant predicate. This is useful for selectivity calculations because we do not want redundant predicates included in the selectivity calculation. Mark all of the predicates as Qualifiers and set the numberOfQualifiers to reflect this.  This is useful for hash joins where all of the predicates in the list to be evaluated during the probe into the hash table on a next are qualifiers. Determine which predicates in this list are useful for limiting the scan on the given table using its best conglomerate.  Remove those predicates from this list and push them down to the given Optimizable table.  The predicates are pushed down in the order of the index columns that they qualify.  Also, the predicates are "marked" as start predicates, stop predicates, or qualifier predicates.  Finally, the start and stop operators are set in the given Optimizable. Find the optimizable equality predicate on the specified column and make it the first predicate in this list.  This is useful for hash joins where Qualifier[0] is assumed to be on the hash key. Remove the OptimizablePredicate at the specified index (0-based) from the list. Can we use the same key for both the start and stop key. This is possible when doing an exact match on an index where there are no other sargable predicates. calculate the selectivity for a set of predicates. If statistics exist for the predicates this method uses the statistics. If statistics do not exist, then simply call selectivity for each of the predicates and return the result. Sets the given list to have the same elements as this one, and the same properties as this one (number of qualifiers and start and stop predicates. Return the number of OptimizablePredicates in the list. Get the start operator for the given Optimizable for a heap or index scan. Get the stop operator for the given Optimizable for a heap or index scan. Transfer all the predicates from this list to the given list. Transfer the predicates whose referenced set is contained by the specified referencedTableMap from this list to the other list. This is useful when splitting out a set of predicates from a larger set, like when generating a HashScanResultSet. Return true if this predicate list is useful for limiting the scan on the given table using the given conglomerate.
Consider the cost of the given optimizable.  This method is like costOptimizable, above, but it is used when the Optimizable does not need help from the optimizer in costing the Optimizable (in practice, all Optimizables except FromBaseTable use this method. Caller is responsible for pushing all predicates which can be evaluated prior to costing. Cost the current Optimizable with the specified OPL. Caller is responsible for pushing all predicates which can be evaluated prior to costing. Cost the current permutation. Caller is responsible for pushing all predicates which can be evaluated prior to costing. Return the DataDictionary that the Optimizer is using. This is useful when an Optimizable needs to call optimize() on a child ResultSetNode. Get the final estimated cost of the optimized query.  This should be the cost that corresponds to the best overall join order chosen by the optimizer, and thus this method should only be called after optimization is complete (i.e. when modifying access paths). Gets a join strategy by number (zero-based). Gets a join strategy by name.  Returns null if not found. The look-up is case-insensitive. Get the level of this optimizer.  Iterate through the "decorated permutations", returning false when they are exhausted. NOTE - Implementers are responsible for hiding tree pruning of access methods behind this method call. Iterate through the permutations, returning false when the permutations are exhausted. NOTE - Implementers are responsible for hiding tree pruning of permutations behind this method call. Get the number of join strategies supported by this optimizer. Get the ith (0-based) Optimizable being considered by this Optimizer. Get the number of optimizables being considered by this Optimizer. Get the estimated cost of the optimized query Modify the access path for each Optimizable, as necessary.  This includes things like adding result sets to translate from index rows to base rows. Prepare for another round of optimization. This method is called before every "round" of optimization, where we define a "round" to be the period between the last time a call to getOptimizer() (on either a ResultSetNode or an OptimizerFactory) returned _this_ Optimizer and the time a call to this Optimizer's getNextPermutation() method returns FALSE.  Any re-initialization of state that is required before each round should be done in this method. Set the estimated number of outer rows - good for optimizing nested optimizables like subqueries and join nodes. Get the maximum number of estimated rows touched in a table before we decide to open the table with table locking (as opposed to row locking. Tells whether any of the tables outer to the current one has a uniqueness condition on the given predicate list, and if so, how many times each unique key can be seen by the current table. Process (i.e. add, load, or remove) current best join order as the best one for some outer query or ancestor node, represented by another Optimizer or an instance of FromTable, respectively. Then iterate through our optimizableList and tell each Optimizable to do the same.  See Optimizable.updateBestPlan() for more on why this is necessary. If statistics should be considered by the optimizer while optimizing a query. The user may disable the use of statistics by setting the property derby.optimizer.useStatistics or by using the property useStatistics in a query.
Tell whether to do join order optimization. Return a new CostEstimate. Return the maxMemoryPerTable setting, this is used in optimizer, as well as subquery materialization at run time. Only one optimizer level should exist in the database, however, the connection may have multiple instances of that optimizer at a given time. Return whether or not the optimizer associated with this factory supports optimizer trace.
ModuleControl interface     OptimizerFactory interface
Add scoped predicates to this optimizer's predicateList. This method is intended for use during the modifyAccessPath() phase of compilation, as it allows nodes (esp. SelectNodes) to add to the list of predicates available for the final "push" before code generation.  Just as the constructor for this class allows a caller to specify a predicate list to use during the optimization phase, this method allows a caller to specify a predicate list to use during the modify-access-paths phase. Before adding the received predicates, this method also clears out any scoped predicates that might be sitting in OptimizerImpl's list from the last round of optimizing. This method should be in the Optimizer interface, but it relies on an argument type (PredicateList) which lives in an impl package. This is the version of costOptimizable for non-base-tables. This method decides whether the given conglomerate descriptor is cheapest based on cost, rather than based on rules.  It compares the cost of using the given ConglomerateDescriptor with the cost of using the best ConglomerateDescriptor so far.   Do any work that needs to be done after the current round of optimization has completed.  For now this just means walking the subtrees for each optimizable and removing the "bestPlan" that we saved (w.r.t to this OptimizerImpl) from all of the nodes.  If we don't do this post-optimization cleanup we can end up consuming a huge amount of memory for deeply- nested queries, which can lead to OOM errors.  DERBY-1315. Estimate the total cost of doing a join with the given optimizable.        Get the number of join strategies supported by this optimizer.  Get the unique tuple descriptor of the current access path for an Optimizable. Determine if we want to try "jumping" permutations with this OptimizerImpl, and (re-)initialize the permuteState field accordingly. Return true if the optimizable is a table function Check to see if the optimizable corresponding to the received optNumber can legally be placed within the current join order. More specifically, if the optimizable has any dependencies, check to see if those dependencies are satisified by the table map representing the current join order.  This method is called before every "round" of optimization, where we define a "round" to be the period between the last time a call to getOptimizer() (on either a ResultSetNode or an OptimizerFactory) returned _this_ OptimizerImpl and the time a call to this OptimizerImpl's getNextPermutation() method returns FALSE.  Any re-initialization of state that is required before each round should be done in this method. Pull whatever optimizable is at joinPosition in the proposed join order from the join order, and update all corresponding state accordingly. * Push predicates from this optimizer's list to the given optimizable, * as appropriate given the outer tables. * * @param curTable	The Optimizable to push predicates down to * @param outerTables	A bit map of outer tables * * @exception StandardException		Thrown on error Iterate through all optimizables in the current proposedJoinOrder and find the accumulated sum of their estimated costs.  This method is used to 'recover' cost estimate sums that have been lost due to the addition/subtraction of the cost estimate for the Optimizable at position "joinPosition".  Ex. If the total cost for Optimizables at positions &lt; joinPosition is 1500, and then the Optimizable at joinPosition has an estimated cost of 3.14E40, adding those two numbers effectively "loses" the 1500. When we later subtract 3.14E40 from the total cost estimate (as part of "pull" processing), we'll end up with 0 as the result--which is wrong. This method allows us to recover the "1500" that we lost in the process of adding and subtracting 3.14E40. Is the cost of this join order lower than the best one we've found so far?  If so, remember it. NOTE: If the user has specified a join order, it will be the only join order the optimizer considers, so it is OK to use costing to decide that it is the "best" join order. This method decides whether the given conglomerate descriptor is cheapest based on rules, rather than based on cost estimates. The rules are: Covering matching indexes are preferred above all Non-covering matching indexes are next in order of preference Covering non-matching indexes are next in order of preference Heap scans are next in order of preference Non-covering, non-matching indexes are last in order of preference. In the current language architecture, there will always be a heap, so a non-covering, non-matching index scan will never be chosen.  However, the optimizer may see a non-covering, non-matching index first, in which case it will choose it temporarily as the best conglomerate seen so far. NOTE: This method sets the cost in the optimizable, even though it doesn't use the cost to determine which access path to choose.  There are two reasons for this: the cost might be needed to determine join order, and the cost information is copied to the query plan.   Get the trace machinery  Process (i.e. add, load, or remove) current best join order as the best one for some outer query or ancestor node, represented by another OptimizerImpl or an instance of FromTable, respectively. Then iterate through our optimizableList and tell each Optimizable to do the same.  See Optimizable.updateBestPlan() for more on why this is necessary.
//////////////////////////////////////////////////////////////////////  ABSTRACT BEHAVIOR  ////////////////////////////////////////////////////////////////////// <p> Bind the conglomerate and table function names in this plan. </p> <p> Count the number of leaf nodes under (and including) this node. </p> <p> Return true if this the schema and RowSource names have been resolved. </p> <p> Return true if this plan is a (left) leading prefix of the other plan. </p> <p> Get the leftmost leaf node in this plan. </p> //////////////////////////////////////////////////////////////////////  CONSTANTS  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  FACTORY METHODS  ////////////////////////////////////////////////////////////////////// <p> Make a RowSource corresponding to the given tuple descriptor. </p>
Get the optimizer trace output for the last optimized query as a String. Get the current optimizer tracer, if any. Turn default optimizer tracing on or off. Install an optimizer tracer (to enable tracing) or uninstall the current optimizer tracer (to disable tracing).
Privileged lookup of a Context. Must be private so that user code can't call this entry point. /////////////////////////////////////////////////////////////////////////////////  OptionalTool BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Turns on optimizer tracing. May take optional parameters: </p> <ul> <li>xml - If the first arg is the "xml" literal, then trace output will be formatted as xml.</li> <li>custom, $class - If the first arg is the "custom" literal, then the next arg must be the name of a class which implements org.apache.derby.iapi.sql.compile.OptTrace and which has a 0-arg constructor. The 0-arg constructor is called and the resulting OptTrace object is plugged in to trace the optimizer.</li> </ul> <p> Print the optimizer trace and turn off tracing. Takes optional parameters: </p> <ul> <li><b>fileName</b> - Where to write the optimizer trace. If omitted, the trace is written to System.out.</li> </ul> //////////////////////////////////////////////////////////////////////  MINIONS  ////////////////////////////////////////////////////////////////////// Wrap an exception in a SQLException
Load the tool, giving it optional configuration parameters Unload the tool, giving it optional configuration parameters
/////////////////////////////////////////////////////////////////////////////////  OptionalTool BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////////////////
Bind this logical operator.  All that has to be done for binding a logical operator is to bind the operands, check that both operands are BooleanDataValue, and set the result type to BooleanDataValue. Finish putting an expression into conjunctive normal form.  An expression tree in conjunctive normal form meets the following criteria: o  If the expression tree is not null, the top level will be a chain of AndNodes terminating in a true BooleanConstantNode. o  The left child of an AndNode will never be an AndNode. o  Any right-linked chain that includes an AndNode will be entirely composed of AndNodes terminated by a true BooleanConstantNode. o  The left child of an OrNode will never be an OrNode. o  Any right-linked chain that includes an OrNode will be entirely composed of OrNodes terminated by a false BooleanConstantNode. o  ValueNodes other than AndNodes and OrNodes are considered leaf nodes for purposes of expression normalization. In other words, we won't do any normalization under those nodes. In addition, we track whether or not we are under a top level AndNode. SubqueryNodes need to know this for subquery flattening. Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. Do bind() by hand for an AndNode that was generated after bind(), eg by putAndsOnTop(). (Set the data type and nullability info.) Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Mark this OrNode as the 1st OR in the OR chain. We will consider converting the chain to an IN list during preprocess() if all entries are of the form: ColumnReference = expression Verify that changeToCNF() did its job correctly.  Verify that: o  AndNode  - rightOperand is not instanceof OrNode leftOperand is not instanceof AndNode o  OrNode	- rightOperand is not instanceof AndNode leftOperand is not instanceof OrNode
Clear all information to allow object re-use.
Construct suite of tests Construct top level suite in this JUnit test Nested query expression body, with each level contributing to the set of ORDER BY and/or OFFSET/FETCH FIRST clauses. Cf. these productions in SQL 2011, section 7.11: <pre> <query expression> ::= [ <with clause> ] <query expression body> [ <order by clause> ] [ <result offset clause> ] [ <fetch first clause> ] <query expression body> ::= <query term> ... </pre> One of the productions of {@code <query expression body>}, is <pre> &lt;left paren&gt; &lt;query expression body&gt; [ <order by clause> ] [ <result offset clause> ] [ <fetch first clause> ] <right paren> </pre> so our clauses nests to arbitrary depth given enough parentheses, including ORDER BY and OFFSET/FETCH FIRST clauses. This nesting did not work correctly, cf. DERBY-6378. The corresponding productions in {@code sqlgrammar.jj} is {@code queryExpression} and {@code nonJoinQueryPrimary}. Test {@code INSERT INTO t SELECT .. FROM .. ORDER BY}. Same test as {@code testInsertSelectOrderBy} but with use of {@code OFFSET/FETCH FIRST}. <p/> Test {@code INSERT INTO t SELECT .. FROM .. ORDER BY} + {@code OFFSET FETCH} <p/> This test is a variant made my modifying {@code testInsertSelectOrderBy} with suitable {@code OFFSET/FETCH FIRST} clauses. Test JOIN with delimited subqueries Test nesting inside set operands, cf. this production in SQL 2011, section 7.12: <pre> <query primary> ::= <simple table> |  <left paren> <query expression body> [ <order by clause> ] [ <result offset clause> ] [ <fetch first clause> ] <right paren> </pre> The corresponding production in {@code sqlgrammar.jj} is {@code nonJoinQueryPrimary}. Cf. DERBY-6008. Prevent pushing of where predicates into selects with fetch and/or offset (DERBY-5911). Similarly, for windowed selects. {@code SELECT} subqueries with {@code ORDER BY} {@code SELECT} subqueries with {@code ORDER BY} and {@code OFFSET/FETCH}. <p/> This test is a variant made my modifying {@code testSelectSubqueriesOrderBy} with suitable {@code OFFSET/FETCH FIRST} clauses. {@code SELECT} subqueries with {@code ORDER BY} - negative tests {@code SELECT} subqueries with {@code ORDER BY} - check sort avoidance Test {@code ORDER BY} in a view definition Test {@code ORDER BY} + {@code FETCH/OFFSET} in a view definition <p/> This test is a variant made my modifying {@code testView} with suitable {@code OFFSET/FETCH FIRST} clauses.
Helper method that inserts a row into table1. Helper method that inserts a row into table2. Construct suite of tests Populate table1 and table2 with the rows needed for reproducing DERBY-3926. Construct top level suite in this JUnit test Some more tests for order by and sort avoidance logic Add a test case for DERBY-4240 where the rows were not ordered despite an order by clause. The fix for DERBY-3926 took care of the bug. Add a test case for DERBY-4331 where the rows were not ordered correctly for both ascending and descending order by clause. DERBY-6148. Verifying that permuted join order doesn't erroneously give sort avoidance under certain index access paths. Test for forcing a order of tables in the FROM list user optimizer overrides. This ordering of table is going to require us to do sorting. This forced sorting returns the correct result order. Test for forcing the index use using optimizer override. This will demonstrate the bug where we are returning the results in wrong order Following sql with no overrides also demonstrates the bug where we are returning the results in wrong order Helper method that updates a row in table2
Accept the visitor for all visitable children of this node. Bind this column. During binding, we may discover that this order by column was pulled up into the result column list, but is now a duplicate, because the actual result column was expanded into the result column list when "*" expressions were replaced with the list of the table's columns. In such a situation, we will end up calling back to the OrderByList to adjust the addedColumnOffset values of the columns; the "oblist" parameter exists to allow that callback to be performed. Reset addedColumnOffset to indicate that column is no longer added An added column is one which was artificially added to the result column list due to its presence in the ORDER BY clause, as opposed to having been explicitly selected by the user. Since * is not expanded until after the ORDER BY columns have been pulled up, we may add a column, then later decide it is a duplicate of an explicitly selected column. In that case, this method is called, and it does the following: - resets addedColumnOffset to -1 to indicate this is not an added col - calls back to the OrderByList to adjust any other added cols Adjust addedColumnOffset to reflect that a column has been removed This routine is called when a previously-added result column has been removed due to being detected as a duplicate. If that added column had a lower offset than our column, we decrement our offset to reflect that we have just been moved down one slot in the result column list. This method checks a ColumnReference of this OrderByColumn against the ColumnReferences of the select clause of the query. Is this OrderByColumn constant, according to the given predicate list? A constant column is one where all the column references it uses are compared equal to constants. Checks whether the whole expression (OrderByColumn) itself found in the select clause. Get the underlying expression, skipping over ResultColumns that are marked redundant. Get the underlying ResultColumn. Get the column order.  Overrides OrderedColumn.isAscending. Get the column NULL ordering. Overrides OrderedColumn.getIsNullsOrderedLow. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Pull up this orderby column if it doesn't appear in the resultset Remap all the column references under this OrderByColumn to their expressions. Order by columns now point to the PRN above the node of interest. We need them to point to the RCL under that one.  This is useful when combining sorts where we need to reorder the sorting columns. Assuming this OrderByColumn was "pulled" into the received target's ResultColumnList (because it wasn't there to begin with), use this.addedColumnOffset to figure out which of the target's result columns is the one corresponding to "this". The desired position is w.r.t. the original, user-specified result column list--which is what "visibleSize()" gives us.  I.e. To get this OrderByColumn's position in target's RCL, first subtract out all columns which were "pulled" into the RCL for GROUP BY or ORDER BY, then add "this.addedColumnOffset". As an example, if the query was: select sum(j) as s from t1 group by i, k order by k, sum(k) then we will internally add columns "K" and "SUM(K)" to the RCL for ORDER BY, *AND* we will add a generated column "I" to the RCL for GROUP BY.  Thus we end up with four result columns: (1)        (2)  (3)   (4) select sum(j) as s, K, SUM(K), I from t1 ... So when we get here and we want to find out which column "this" corresponds to, we begin by taking the total number of VISIBLE columns, which is 1 (i.e. 4 total columns minus 1 GROUP BY column minus 2 ORDER BY columns).  Then we add this.addedColumnOffset in order to find the target column position.  Since addedColumnOffset is 0-based, an addedColumnOffset value of "0" means we want the the first ORDER BY column added to target's RCL, "1" means we want the second ORDER BY column added, etc.  So if we assume that this.addedColumnOffset is "1" in this example then we add that to the RCL's "visible size". And finally, we add 1 more to account for fact that addedColumnOffset is 0-based while column positions are 1-based. This gives: position = 1 + 1 + 1 = 3 which points to SUM(K) in the RCL.  Thus an addedColumnOffset value of "1" resolves to column SUM(K) in target's RCL; similarly, an addedColumnOffset value of "0" resolves to "K". DERBY-3303. Mark the column as descending order Mark the column as ordered NULL values lower than non-NULL values. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Add a column to the list Are all columns in the list ascending. Bind the update columns by their names to the target resultset of the cursor specification. Adjust addedColumnOffset values due to removal of a duplicate column This routine is called by bind processing when it identifies and removes a column from the result column list which was pulled up due to its presence in the ORDER BY clause, but which was later found to be a duplicate. The OrderByColumn instance for the removed column has been adjusted to point to the true column in the result column list and its addedColumnOffset has been reset to -1. This routine finds any other OrderByColumn instances which had an offset greater than that of the column that has been deleted, and decrements their addedColumOffset to account for the deleted column's removal.  generate the sort result set operating over the source expression. Get a column from the list Get whether or not a sort is needed. Is this order by list an in order prefix of the specified RCL. This is useful when deciding if an order by list can be eliminated due to a sort from an underlying distinct or union.  Pull up Order By columns by their names to the target resultset of the cursor specification. Remap all ColumnReferences in this tree to be clones of the underlying expression. Remove any constant columns from this order by list. Constant columns are ones where all of the column references are equal to constant expressions according to the given predicate list. Remove any duplicate columns from this order by list. For example, one may "ORDER BY 1, 1, 2" can be reduced to "ORDER BY 1, 2". Beetle 5401. Build a new RCL with the same RCs as the passed in RCL but in an order that matches the ordering columns. Determine whether or not this RequiredRowOrdering has a DESCENDING requirement for the column referenced by the received ColumnReference. Order by columns now point to the PRN above the node of interest. We need them to point to the RCL under that one.  This is useful when combining sorts where we need to reorder the sorting columns.   RequiredRowOrdering interface
generate the distinct result set operating over the source result set. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.


Return the result of the operations that we have been performing.  Returns a DataValueDescriptor.    ///////////////////////////////////////////////////////////  EXTERNALIZABLE INTERFACE  /////////////////////////////////////////////////////////// Although we are not expected to be persistent per se, we may be written out by the sorter temporarily.  So we need to be able to write ourselves out and read ourselves back in.  We rely on formatable to handle situations where <I>value</I> is null. <p> Why would we be called to write ourselves out if we are null?  For scalar aggregates, we don't bother setting up the aggregator since we only need a single row.  So for a scalar aggregate that needs to go to disk, the aggregator might be null.
Get the position of this column Indicate whether this column is ascending or not. By default assume that all ordered columns are necessarily ascending.  If this class is inherited by someone that can be desceneded, they are expected to override this method. Indicate whether this column should be ordered NULLS low. By default we assume that all ordered columns are ordered with NULLS higher than non-null values. If this class is inherited by someone that can be specified to have NULLs ordered lower than non-null values, they are expected to override this method. Set the position of this column Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Get an array of ColumnOrderings to pass to the store

************************************************************************ Public Methods of CloneableStream Interface ************************************************************************ Clone this object. <p> Creates a deep copy of this object. The returned stream has its own working buffers and can be initialized, reset and read independently from this stream. <p> The cloned stream is set back to the beginning of stream, no matter where the current stream happens to be positioned. Close the Resetable stream. <p> Close the container associated with this stream.  (This will also free the associated IS table lock and the associated S row lock.) ************************************************************************ Public Methods of This class: ************************************************************************* If bytes remain in stream, insure the current buffer is not empty. <p> If there are bytes in current buffer than no more work necessary, else if there are no bytes available in current buffer and there are still more overflow segments then get the next buffer's worth of data. ************************************************************************ Public Methods of Resetable Interface ************************************************************************* Initialize a Resetable stream. <p> InitStream() must be called first before using any other of the Resetable interfaces. <p> Reopens the container.  This gets a separate intent shared locked on the table and a read lock on the appropriate row.  These locks remain until the enclosing blob/clob object is closed, or until the end of the transaction in which initStream() was first called.  This locking behavior protects the row while the stream is being accessed.  Otherwise for instance in the case of read committed the original row lock on the row would be released when the scan went to the next row, and there would be nothing to stop another transaction from deleting the row while the client read through the stream. Reset the stream back to beginning of the long column. <p> Also fills in the first buffer from the stream. <p> Throws exception if the underlying open container has been closed, for example automatically by a commit(). Set the next overflow page of the long column. <p> Used by StorePage.restorePortionLongColumn() as part of the call back process to save the state of the scan of the pieces of the long column. StorePage.restorePortionLongColumn() is called by fillByteHolder() to get the next page worth into a buffer, and in turn after those bytes are read the state of this stream is updated with then next overflow page. Set the next overflow page of the long column. <p> Used by StorePage.restorePortionLongColumn() as part of the call back process to save the state of the scan of the pieces of the long column. StorePage.restorePortionLongColumn() is called by fillByteHolder() to get the next page worth into a buffer, and in turn after those bytes are read the state of this stream is updated with then next overflow page.

Determines the equality of two <code>PackagePermission</code> objects. This method checks that specified package has the same package name and <code>PackagePermission</code> actions as this <code>PackagePermission</code> object. Returns the canonical string representation of the <code>PackagePermission</code> actions. <p> Always returns present <code>PackagePermission</code> actions in the following order: <code>EXPORT</code>,<code>IMPORT</code>. Returns the current action mask. <p> Used by the PackagePermissionCollection class. Parse action string into action mask. Returns the hash code value for this object. Determines if the specified permission is implied by this object. <p> This method checks that the package name of the target is implied by the package name of this object. The list of <code>PackagePermission</code> actions must either match or allow for the list of the target object to imply the target <code>PackagePermission</code> action. <p> The permission to export a package implies the permission to import the named package. <pre> x.y.*,&quot;export&quot; -&gt; x.y.z,&quot;export&quot; is true *,&quot;import&quot; -&gt; x.y, &quot;import&quot;      is true *,&quot;export&quot; -&gt; x.y, &quot;import&quot;      is true x.y,&quot;export&quot; -&gt; x.y.z, &quot;export&quot;  is false </pre> Called by constructors and when deserialized. Returns a new <code>PermissionCollection</code> object suitable for storing <code>PackagePermission</code> objects. readObject is called to restore the state of this permission from a stream. WriteObject is called to save the state of this permission object to a stream. The actions are serialized, and the superclass takes care of the name. Adds a permission to the <code>PackagePermission</code> objects. The key for the hash is the name. Returns an enumeration of all <code>PackagePermission</code> objects in the container. Determines if the specified permissions implies the permissions expressed in <code>permission</code>.
move rows from one page to another, purging in the process. <p> Move from this page slot[src_slot] to slot[src_slot+num_rows-1] to destPage slot[dest_slot] to slot[dest_slot + num_rows - 1], in that order. Both this page and destPage must be latched and from the same container with the same page and record format. <BR>Slot[src_slot] to slot[src_slot+numrows-1] will be purged from this page.  RecordId on the dest page will be brand new and not in any particular order or range.  RecordId of the purged rows in this page is never reused.  Deleted and undeleted rows are copied over just the same. Exception will be thrown if this page does not have all the rows in the moved over range. <BR><B>RESOLVE: reserve space now not copied over because in btree, a row never shrinks.  When this routine is called by heap or by some page which will have shrunken row, then we need to add that </B> <BR>DestPage must have at least dest_slot row occupying slot[0] to slot[dest_slot-1].  DestPage must have enough space to take the copied over data.  Rows that occupied slot number &gt; dest_slot will be moved up the slot (I.e., slot[dest_slot] -&gt; slot[dest_slot + num_rows]). <BR>If this operation rolls back, this page (the src page) will get the rows back and the dest page will purge the rows that were copied - this is as if the rows were inserted into the dest page with INSERT_UNDO_WITH_PURGE. <P> <B>Locking Policy</B> <P> Calls the lockRecordForWrite() method of the LockingPolicy object passed to the openContainer() call before the rows are copied over and bore the records are purged.  I.e, for num_rows moved, there will be 2*num_rows calls to lockRecordForWrite. <P> <P><B>Use with caution</B> <BR>As with a normal purge, no space is reserved on this page for rollback of the purge, so you must commit before inserting any rows onto this page - unless those inserts are INSERT_UNDO_WITH_PURGE. Return a time stamp that can be used to identify the page of this specific instance.  For pages that don't support timestamp, returns null. Mark the record at slot as deleted or undeleted according to delete flag. <p> <P> <B>Locking Policy</B> <P> Calls the lockRecordForWrite() method of the LockingPolicy object passed to the openContainer() call before the record is deleted.  If record already deleted, and an attempt is made to delete it, an exception is thrown.  If record not deleted, and an attempt is made to undelete it, an exception is thrown. <BR> MT - latched See if timeStamp for this page is the same as the current instance of the page.  Null timeStamp never equals the instance of the page. Fetch a single field from a deleted or non-deleted record. Fills in the passed in Object column with the field identified by fieldid if column is not null, otherwise the record is locked but not fetched. <BR> The fieldId of the first field is 0. If the fieldId is &gt;= the number of fields on the record, column is restored to null <P> <B>Locking Policy</B> <BR> No locks are obtained. It is up to the caller to obtain the correct locks. <BR> It is guaranteed that the page latch is not released by this method Fetch a record located in the passed in slot. <p> Fetch a record located in the passed in slot and fill-in the passed in StorebleRow and the Object columns contained within. If row is null then the record is locked but is not fetched. <BR> This interface allows the caller to either return a deleted row or not. If "ignoreDelete" is set to true, fetch the record regardless of whether it is deleted or not (same as above fetchFromSlot).  However, if "ignoreDelete" is set to false and the and the slot correspond to a deleted row, null is returned. <BR> If a non-null Qualifier list is provided then the qualifier array will be applied to the row and the row will only be returned if the row qualifies, otherwise null will be returned.  Values in the columns of row may or may not be altered while trying to apply the qualifiers, if null is returned the state of the columns is undefined.  If a null Qualifier list is provided then no qualification is applied. <BR> If a non-null record handle is passed in, it is assumed that the record handle corresponds to the record in the slot.  If record handle is null, a record handle will be manufactured and returned if the record is not deleted or if "ignoreDelete" is true.  This parameter is here for the case where the caller have already manufactured the record handle for locking or other purposes so it would make sense for the page to avoid creating a new record handle object if possible. Fetch the number of fields in a record. <p> <B>Locking Policy</B> <P> No locks are obtained. <BR> MT - latched Fetch the number of fields in a record. <p> <P> <B>Locking Policy</B> <P> No locks are obtained. <BR> It is guaranteed that the page latch is not released by this method Retrieve this page's aux object, returning null if there isn't one. The reference returned must only be used while the page is latched, once unlatch is called the reference to the aux object must be discarded. <BR> MT - latched ************************************************************************ Public Methods of This class: record handle interface. the following interfaces to page use the record Id or record handle (rather than the slot interface). ************************************************************************* Return an invalid record handle. <p> Find slot for record with an id greater than the passed in identifier. <p> Find the slot for the first record on the page with an id greater than the passed in identifier. <BR> Returns the slot of the first record on the page with an id greater than the one passed in.  Usefulness of this functionality depends on the client's use of the raw store interfaces.  If all "new" records are always inserted at the end of the page, and the raw store continues to guarantee that all record id's will be allocated in increasing order on a given page (assuming a PAGE_REUSABLE_RECORD_ID container), then a page is always sorted in record id order.  For instance current heap tables function this way.  If the client ever inserts at a particular slot number, rather than at the "end" then the record id's will not be sorted. <BR> In the case where all record id's are always sorted on a page, then this routine can be used by scan's which "lose" their position because the row they have as a position was purged.  They can reposition their scan at the "next" row after the row that is now missing from the table. <BR> This method returns the record regardless of its deleted status. <BR> MT - latched Return the page key of this page. <p> <BR> MT - Latched Return the page number of this page. <p> Page numbers are unique within a container and start at ContainerHandle.FIRST_PAGE_NUMBER and increment by 1 regardless of the page size. <p> <BR> MT - Latched Get the current version number of the page. Get a record handle from a previously stored record id. <p> Get a record handle from a previously stored record identifier that was obtained from a RecordHandle. <p> <BR> MT - Latched Get the record handle of row at slot. <p> Get the record handle of a record on a latched page using its slot number. <BR> MT - latched ************************************************************************ Public Methods of This class: slot interface. the following interfaces to page use the slot number (rather than the record handle interface). ************************************************************************* Get the slot number. <p> Get the slot number of a record on a latched page using its record handle. <P><B>Note</B> The slot number is only good for as long as the page is latched. <BR> MT - latched Insert a record anywhere on the page. <P> <B>Locking Policy</B> <BR> Calls the lockRecordForWrite() method of the LockingPolicy object passed to the openContainer() call before the record is inserted. <BR> MT - latched Insert a record at the specified slot. <p> All records that occupy FIRST_SLOT_NUMBER to (slot - 1) are not moved. <BR> All records that occupy slot to (recordCount() - 1) are moved up one slot. <BR> The new record is inserted at the specified slot. <BR> If slot == FIRST_SLOT_NUMBER, then the new record will be inserted at the first slot. <BR> If slot == recordCount(), then the record is inserted in a new slot, no records are moved. <BR> If slot is &gt; recordCount() or if slot &lt; FIRST_SLOT_NUMBER, an exception will be thrown. <P><B>Space Policy</B><BR> If the row will not fit on a page then: <UL> <LI> an exception is thrown if the page has no other rows, this is an indication that the row could never fit on a page in this container. <LI> null is returned if there are other rows on the page, this is an indication that the row can potentially be inserted successfully onto an empty page. </UL> <P> <B>Locking Policy</B> <BR> Calls the lockRecordForWrite() method of the LockingPolicy object passed to the openContainer() call before the record is inserted. <BR> MT - latched Test if a record is deleted. <p> <P> <B>Locking Policy</B> <BR> No locks are obtained. <BR> It is guaranteed that the page latch is not released by this method Check if this page has been changed in such a way that scans that are positioned on it will have to reposition. Only called on B-tree pages. Return a record handle for the given constant record id. <p> Return a record handle that doesn't represent a record but rather has a special meaning.  Used for special cases like creating a key specific to the page, but not specific to a row on the page. <p> See RecordHandle interface for a list of "special record handles." Move record to a page toward the beginning of the file. <p> As part of compressing the table records need to be moved from the end of the file toward the beginning of the file.  Only the contiguous set of free pages at the very end of the file can be given back to the OS.  This call is used to purge the row from the current page, insert it into a previous page, and return the new row location Mark the record identified by position as deleted. The record may be undeleted sometime later using undelete() by any transaction that sees the record. <p> The interface is optimized to work on a number of rows at a time, optimally processing all rows on the page at once.  The call will process either all rows on the page, or the number of slots in the input arrays - whichever is smaller. <B>Locking Policy</B> <P> MUST be called with table locked, not locks are requested.  Because it is called with table locks the call will go ahead and purge any row which is marked deleted.  It will also use purge rather than delete to remove the old row after it moves it to a new page.  This is ok since the table lock insures that no other transaction will use space on the table before this transaction commits. <BR> A page latch on the new page will be requested and released. Return the number of records on this page that are <B> not </B> marked as deleted. <BR> MT - latched Purge the row(s) from page. <p> Purge the row(s) from page, get rid of the row(s) and slot(s) - <B>USE WITH CAUTION</B>, please see entire description of this operation before attempting to use this. Starting from the specified slot, n rows will be purged. That is, rows that occupies from slot to slot+n-1 will be purged from the page. <P> <B>Locking Policy</B> <P> Calls the lockRecordForWrite() method of the LockingPolicy object passed to the openContainer() call before the records are purged. <P> <B>NOTE : CAVEAT</B><BR> This operation will physically get rid of the row from the page, so if a subsequent operation on this page uses a slot that has been purged, then the undo of this operation will fail.  It is only safe to use this operation if the caller knows that it has exclusive access to the page for the duration of the transaction, i.e, effectively holding a page lock on the page <P> <B>NOTE</B><BR> Outstanding handles to purged rows are no longer valid, accessing them will cause an exception to be thrown. <BR> <B>NOTE : Data Logging for Purges</B><BR> needDataLogged is used to specify whether data is required to be logged for purge operatios. Data Logging is required Only if the row can be reused or required for key search if a purge is rolled back;(rollback can occur if the system crashes in the middle of purges or some unexpected error condiditions  rolled back. For example: 1)Btree expects the data to be there if a purge is rolled back; needDataLogged=true 2)Heaps does not care if data exist because only operation that can occur on a row whose purge rolled back is purging again.(needDataLogged=false) MT - latched Return the number of records on the page. The returned count includes rows that are deleted, i.e. it is the same as the number of slots on the page. <BR> MT - latched does the record still exist on the page? <p> If "ignoreDelete" is true and the record handle represents a record on the page (either marked deleted or not) return true.  If "ignoreDelete" is false return true if the record handle represents a record on the page and the record is not marked as deleted.  Return false otherwise. <BR> MT - Latched Set the aux object for this page. To clear the auxObject in the page, pass in a null AuxObject. If the AuxObject has already been set, this method will call auxObjectInvalidated() on the old aux objkect and replace it with aux. <BR> MT - latched Returns true if the page is latched. Only intended to be used as a Sanity check. Callers must discard Page references once unlatch is called. <BR> MT - latched Set a hint in the page object to indicate that scans positioned on it need to reposition. Only called on B-tree pages. time stamp - for those implmentation that supports it Set the time stamp to what is on page at this instance.  No op if this page does not support time stamp. Is this page/deleted row a candidate for immediate reclaim space. <p> Used by access methods after executing a delete on "slot_just_deleted" to ask whether a post commit should be queued to try to reclaim space after the delete commits. <p> Will return true if the number of non-deleted rows on the page is &lt;= "num_non_deleted_rows".  For instance 0 means schedule reclaim only if all rows are deleted, 1 if all rows but one are deleted. <p> Will return true if the row just deleted is either a long row or long column.  In this case doing a reclaim space on the single row may reclaim multiple pages of free space, so better to do it now rather than wait for all rows on page to be deleted.  This case is to address the worst case scenario of all rows with long columns, but very short rows otherwise.  In this case there could be 1000's of rows on the main page with many gigabytes of data on overflow pages in deleted space that would not be reclaimed until all rows on the page were deleted. Is it likely that an insert will fit on this page? <p> Return true if there is a good chance an insert will fit on this page, false otherwise.  If this returns true then an insert may still fail by throwing an exception or by returning null, see insertAtSlot for details. It is very probable that this call is much faster than the version that takes a row. In situations where it is expected that the majority of times a row will fit on a page this method should be used and the null return handled from insert/insertAtSlot. <BR> MT - latched will insert of this row fit on this page? <p> Return true if this record is guaranteed to be inserted successfully using insert() or insertAtSlot(). This guarantee is only valid if the following conditions are fulfilled before an insert is called with t his row. <UL> <LI> The page is not unlatched <LI> The page is not modified in any way, ie. no updates or other inserts <LI> The row is not modified in such a way that would change its storage size </UL> <BR> MT - latched Page operations Unlatch me, the page is exclusivly latched by its current user until this method call is made. <BR> After using this method the caller must throw away the reference to the Page object, e.g. <PRE> ref.unlatch(); ref = null; </PRE> <BR> The page will be released automatically at the close of the container if this method is not called explictly. <BR> MT - latched Update the complete record identified by the slot. <P> <B>Locking Policy</B> <P> Calls the lockRecordForWrite() method of the LockingPolicy object passed to the openContainer() call before the record is undeleted. If record already deleted, an exception is thrown. <BR> It is guaranteed that the page latch is not released by this method Update a field within the record, replacing its current value with the stored representation of newValue. Record is identified by slot. If the field does not exist then it is added to the record, but only if (fieldId - 1) exists. <BR><B>RESOLVE</B> right now it throws an exception if fieldId is not already on the record, not add the next one as advertised. <P> <B>Locking Policy</B> <P> Calls the lockRecordForWrite() method of the LockingPolicy object passed to the openContainer() call before the record is updated. <BR> It is guaranteed that the page latch is not released by this method
Copy num_rows from srcPage into deestpage. <p> Longer descrption of routine. <p> Set the Delete status of the record at the given slot. <p> Subclass that implements this method has to call BasePage.setDeleteStatus to update the delete status on the in-memory slot table. <p> <BR> MT - latched, page is latched when this methods is called. Initialize the page due to allocation. <p> Initialize the page due to allocation - this page could be brand new or it could be being re-allocated. <p> Insert record at the given slot with this recordId. <p> <BR> MT - latched, page is latched when this methods is called. Invalidate the page due to deallocation. Short one line description of routine. <p> Invalidate the page due to deallocation - this is the action on the page that is being deallocated as opposed to the action on the allocation page. <p> Purge the record at the given slot. <p> Subclass that implements this method has to remove the slot from the base page in-memory slot table (removeAndShiftDown). <p> <BR> MT - latched, page is latched when this methods is called. Shrink the reserved space to the new value. <p> Shrink the reserved space to the new value.  This action is not undoable. <p> Update record at the given slot with this row. <p> <BR> MT - latched, page is latched when this methods is called. Update a field of the record at the given slot with this value. <p> <BR> MT - latched, page is latched when this methods is called.
Find the page the operation applies to and latch it, this only uses the segmentId, containerId, and pageId stored in this log record to find the page. Subclass (e.g., init page) that wishes to do something about missing pages in load tran should override this method to return the page the default for optional data is set to null.  If an operation has optional data, the operation need to prepare the optional data for this method. WARNING: If a log operation extends this class, and the operation has optional data, it MUST overwrite this method to return a ByteArray that contains the optional data. A page operation is a RAWSTORE log record Loggable methods Returns true if this op should be redone during recovery redo, if so, get and latched the page. ************************************************************************ Public Methods of RePreparable Interface: ************************************************************************* reclaim locks associated with the changes in this log record. <p> Release latched page and any other resources acquired during a previous findpage, safe to call multiple times. In this RawStore implementataion, resource is acquired by a log operation in one of two places <nl> <li> during runtime or recovery undo in PageOperation.generateUndo() <li> during recovery redo in PageBasicOperation.needsRedo() </nl> Methods specific to this class Reset the pageNumber Undo the change indicated by this log operation and optional data. The page the undo should apply to is the latched undoPage. The undoPage must be the same page as the doMe page and the undo operation must restore the before image of the row that changed. <BR> this can only be used under special circumstances: namely table level locking, and no internal or nested transaction, and all operations are rollec back with restoreMe instead of undoMe. <BR><B>This method is here to support BeforeImageLogging</B> Formatable methods

* Methods of object * Methods to read and write


Bind this expression.  A parameter can't figure out what its type is without knowing where it appears, so this method does nothing. It is up to the node that points to this parameter node to figure out the type of the parameter and set it, using the setType() method above.  //////////////////////////////////////////////////////////////////  CODE GENERATOR  ////////////////////////////////////////////////////////////////// For a ParameterNode, we generate for the return value: (<java type name>) ( (BaseActivation) this.getParameter(parameterNumber) ) End of generateExpression //////////////////////////////////////////////////////////////////  STATIC ROUTINES  ////////////////////////////////////////////////////////////////// Generate the code to create the ParameterValueSet, if necessary, when constructing the activation.  Also generate the code to call a method that will throw an exception if we try to execute without all the parameters being set. This generated code goes into the Activation's constructor early on. Get the default value for the parameter.  Parameters may get default values for optimization purposes. Get the JSQLType associated with this parameter. Again, part of method resolution for replicated work units. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Get the parameter number Return whether or not this expression tree represents a constant expression.   Is this as a return output parameter (e.g. ? = CALL myMethod())  Set the descriptor array //////////////////////////////////////////////////////////////////////  OVERRIDE METHODS IN VALUE NODE THAT ARE USED WHILE BINDING REPLICATED CALL WORK STATEMENTS.  In this scenario, a JSQLType was replicated along with this parameter. The JSQLType represents the bind() decision of the remote system, which we want to reproduce locally.  ////////////////////////////////////////////////////////////////////// Set the JSQLType of this parameter. This supports the unnamed parameters that we use for replicated work units. In a special circumstance, we want to consider parameters as constants.  For that situation, we allow a caller to temporarily set us to CONSTANT and then restore us. Mark this as a return output parameter (e.g. ? = CALL myMethod()) Set the DataTypeServices for this parameter Save the received ValueNode locally so that we can generate it (in place of "this") at generation time.  See the preprocess() method of InListOperatorNode for more on how this is used.
Tells whether all the parameters are set and ready for execution. OUT are not required to be set. Check that there are not output parameters defined by the parameter set. If there are unknown parameter types they are forced to input types. i.e. Derby static method calls with parameters that are array. ////////////////////////////////////////////////////////////////  MISC STATEMENT  //////////////////////////////////////////////////////////////// Sets all parameters to an uninitialized state. An exception will be thrown if the caller tries to execute a PreparedStatement when one or more parameters is uninitialized (i.e. has not had setParameterValue() called on it. Clone the ParameterValueSet and its contents. Returns the parameter at the given position. Returns the number of parameters in this set. Get the DataValueDescriptor for an INOUT or OUT parameter. Returns the parameter at the given position in order to set it. Setting via an unknown object type must use setParameterAsObject() to ensure correct typing. Return the mode of the parameter according to JDBC 3.0 ParameterMetaData Return the precision of the given parameter index in this pvs. Get the value of the return parameter in order to set it. Return the scale of the given parameter index in this pvs. Is there a return output parameter in this pvs.  A return parameter is from a CALL statement of the following syntax: ? = CALL myMethod().  Note that a return output parameter is NOT the same thing as an output parameter; it is a special type of output parameter. Initialize the parameter set by allocating DataValueDescriptor corresponding to the passed in type for each parameter. ////////////////////////////////////////////////////////////////  CALLABLE STATEMENT  //////////////////////////////////////////////////////////////// Mark the parameter as an output parameter. Set the value of this user defined parameter to the passed in Object. Set the mode of the parameter, called when setting up static method calls and stored procedures. Otherwise the parameter type will default to an IN parameter. Set the parameter values of the pvstarget to equal those set in this PVS. Used to transfer saved SPS parameters to the actual prepared statement parameters  once associated parameters have been established.  Assumes pvstarget is the same length as this. Validate the parameters.  This is done for situations where we cannot validate everything in the setXXX() calls.  In particular, before we do an execute() on a CallableStatement, we need to go through the parameters and make sure that all parameters are set up properly.  The motivator for this is that setXXX() can be called either before or after registerOutputParamter(), we cannot be sure we have the types correct until we get to execute().
Used to convert raw characters to their escaped version when these raw version cannot be used as part of an ASCII string literal. This method has the standard behavior when this object has been created using the standard constructors.  Otherwise, it uses "currentToken" and "expectedTokenSequences" to generate a parse error message and returns it.  If this object has been created due to a parse error, and you do not catch it (it gets thrown from the parser), then this method is called during the printing of the final stack trace, and hence the correct error message gets displayed.
Returns the current SQL text string that is being parsed. Parse an SQL fragment that represents a {@code <search condition>}. Parses the given statement and returns a query tree. The query tree at this point is a simple syntactic translation of the statement. No binding will have taken place, and no decisions will have been made regarding processing strategy.
new parser, appropriate for the ParserImpl object. Returns the current SQL text string that is being parsed. Returns a initialized (clean) TokenManager, paired w. the Parser in getParser, Appropriate for this ParserImpl object. Parse a statement and return a query tree.  Implements the Parser interface Parse a full SQL statement or a fragment that represents a {@code <search condition>}.

<p> Encodes the hashing algorithm in a string suitable for storing in SYSUSERS.HASHINGSCHEME. </p> <p> Hash a username/password pair and return an encoded representation suitable for storing as a BUILTIN password value in the PropertyConglomerate. </p> /////////////////////////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Produce a hashed password using a plaintext username and password. Turn it into a printable string. </p>
Returns the base name of the path. Returns the parent of the path. Joins the two paths by inserting the separator chararcter between them.


Get the provider's type.  ////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////// Return the name of this Provider.  (Useful for errors.) Get the protected object.

end of createIdentity Cacheable interface end of setIdentity
Build an index key row from a permission descriptor. A key row does not include the RowLocation column. Extract an internal authorization ID from a row. Or a set of permissions in with a row from this catalog table Remove a set of permissions from a row from this catalog table Set the uuid of the passed permission descriptor to the uuid of the row from the system table. DataDictionary will make this call before calling the dependency manager to send invalidation messages to the objects dependent on the permission descriptor's uuid.
This method checks if the passed authorization id is same as the owner of the object on which this permission is defined. This method gets called by create view/constraint/trigger to see if this permission needs to be saved in dependency system for the view/constraint/trigger. If the same user is the owner of the the object being accessed and the newly created object, then no need to keep this privilege dependency ----- getter functions for rowfactory ------ ////////////////////////////////////////////  PROVIDER INTERFACE  ////////////////////////////////////////////////////////////////// Get the provider's UUID Gets the UUID of the table. Is this provider persistent?  A stored dependency will be required if both the dependent and provider are persistent.   Sets the UUID of the table
Put a readme file in database directory which will caution users against touching any files in the directory. This file will be created at database creation time. Returns the canonical name of the service. Return an Enumeration of service names descriptors (Strings) that should be be started at boot time by the monitor. The monitor will boot the service if getServiceProperties() returns a Properties object and the properties does not indicate the service should not be auto-booted. <P> This method may return null if there are no services that need to be booted automatically at boot time. <P> The service name returned by the Enumeration must be in its canonical form. Convert a service name into its canonical form. Returns null if the name cannot be converted into a canonical form. For a service return its service properties, typically from the service.properties file. Get an initialized StorageFactoryInstance Return the type of this service. Return the user form of a service name. This name is only valid within this system. The separator character used must be '/'  Remove a service's root and its contents. Save to a backup file.
Get properties that can be stored in a java.util.Properties object. <p> Get the sub-set of stored properties that can be stored in a java.util.Properties object. That is all the properties that have a value of type java.lang.String.  Changes to this properties object are not reflected in any persisent storage. <p> Code must use the setProperty() method call. Gets a value for a stored property. The returned value will be: <OL> <LI> the de-serialized object associated with the key using setProperty if such a value is defined or <LI> the default de-serialized object associated with the key using setPropertyDefault if such a value is defined or <LI> null </OL> <p> The Store provides a transaction protected list of database properties. Higher levels of the system can store and retrieve these properties once Recovery has finished. Each property is a serializable object and is stored/retrieved using a String key. <p> Gets a default value for a stored property. The returned value will be: <OL> <LI> the default de-serialized object associated with the key using setPropertyDefault if such a value is defined or <LI> null </OL> <p> The Store provides a transaction protected list of database properties. Higher levels of the system can store and retrieve these properties once Recovery has finished. Each property is a serializable object and is stored/retrieved using a String key. <p> Return true if the default property is visible. A default is visible as long as the property is not set. Sets the Serializable object associated with a property key. <p> See the discussion of getProperty(). <p> The value stored may be a Formatable object or a Serializable object whose class name starts with java.*. This stops arbitary objects being stored in the database by class name, which will cause problems in obfuscated/non-obfuscated systems. Sets the Serializable object default value associated with a property key. <p> See the discussion of getProperty(). <p> The value stored may be a Formatable object or a Serializable object whose class name starts with java.*. This stops arbitary objects being stored in the database by class name, which will cause problems in obfuscated/non-obfuscated systems.
Clear some fields in ContextService to allow the engine to be garbage collected. This is a workaround for DERBY-23. Clear a static ThreadLocal field in TableDescriptor so that the engine classes can be garbage collected when they are no longer used. This is a workaround for DERBY-4895, which affects Derby 10.5 and 10.6. Clear a field that is possibly private or final. Make sure the JDBC driver in the class loader associated with this version is deregistered. This is a workaround for DERBY-2905, which affected Derby 10.2 - 10.7, and it is needed to make the old engine classes eligible for garbage collection. Get a handle to the ContextService in the old engine if the version is affected by DERBY-23. Set the phase and boot the database, creating it or upgrading it as required. The thread context class loader is changed to point to the old jar files if required for the phase. Shutdown the database engine and reset the class loader.
no fields, therefore no writeExternal or readExternal Undoable method Generate a Compensation (PageUndoOperation) that will rollback the changes of this page operation. If this Page operation cannot or need not be rolled back (redo only), overwrite this function to return null. <P><B>Note</B><BR> For operation that needs logical undo, use LogicalUndoOperation instead</B>  This implementation just finds the same page that the PageOperation was applied on - i.e., only works for undo on the same page. <P>During recovery redo, the logging system is page oriented and will use the pageID stored in the PageUndoOperation to find the page.  The page will be latched and released using the default findpage and releaseResource - this.releaseResource() will still be called so it has to know not to release any resource it did not acquire. Undo the change indicated by this log operation and optional data. The page the undo should apply to is the latched undoPage, the recordId is the same as the roll forward operation. <BR><B>In this RawStore implementation, should only only be called via CompOp.doMe</B>. <P> The available() method of in indicates how much data can be read, i.e. how much was originally written.
Loggable methods Apply the undo operation, in this implementation of the RawStore, it can only call the undoMe method of undoOp Undo operation is a COMPENSATION log operation make sure resource found in undoOp is released no fields, therefore no writeExternal or readExternal Compensation methods Set up a PageUndoOperation during recovery redo. DEBUG: Print self.
Get a reference (handle) to the PiggyBackedSessionData object. Null will be returned either if the conn argument is not valid, or if the createOnDemand argument is false and the existing argument is null.      Refresh with the latest session attribute values from the connection. Any changes will be reflected in the corresponding xModified() methods, until setUnmodified() is called. Clear the modified status. Called after session attributes have been sent to the client so that the xModified methods will return false.
Get PKGCNSTKN. Get PKGID. Get PKGSN. Get RDBCOLID. Get RDBNAM. Return an object which can be used as a key in a hash table when the value of the consistency token can be ignored. The object has <code>equals()</code> and <code>hashCode()</code> methods which consider other objects returned from <code>getStatementKey()</code> equal if RDBNAM, RDBCOLID, PKGID and PKGSN are equal. Return string representation.
Reading the user's option
User name ignored, only applicable for remote connections.
Initialize the load generator. Print average transaction injection rate and response times. Start steady-state phase. Start warmup phase. Stop the load generator.
Run OE load parse arguments. junit tests to do the OE load. test the initial database load
Closes the resettable stream. Returns the current position of the underlying store stream. Initialize the resettable stream for use. Reads a single byte from the underlying stream. Reads a number of bytes from the underlying stream and stores them in the specified byte array. Reads a number of bytes from the underlying stream and stores them in the specified byte array at the specified offset. Repositions the underlying store stream to the requested position. <p> Repositioning is required because there can be several uses of the store stream, which changes the position of it. If a class is dependent on the underlying stream not changing its position, it must call reposition with the position it expects before using the stream again. <p> If the repositioning fails because the stream is exhausted, most likely because of an invalid position specified by the user, the stream is reset to position zero and the {@code EOFException} is rethrown. Resets the resettable stream. Skips up to the specified number of bytes from the underlying stream.
Returns a reference to self as an {@code InputStream}. <p> This method is not allowed to return {@code null}. Returns the current byte position of the stream. Repositions the stream to the requested byte position. <p> If the repositioning fails because the stream is exhausted, most likely because of an invalid position specified by the user, the stream is reset to position zero and an {@code EOFException} is thrown.
Accept the visitor for all visitable children of this node. Get a string version of the column references for this predicate IF it's a binary relational operator.  We only print out the names of the operands if they are column references; otherwise we just print a dummy value.  This is for debugging purposes only--it's a convenient way to see what columns the predicate is referencing, especially when tracing through code and printing assert failure. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. Clear the start/stop position and qualifier flags Comparable interface  Copy all fields of this Predicate (except the two that are set from 'init').  Return the andNode.  Get the equivalenceClass for this predicate.  If this predicate's operator is a BinaryRelationalOperatorNode, then look at the operands and return a new, equivalent predicate that is "scoped" to the received ResultSetNode.  By "scoped" we mean that the operands, which shold be column references, have been mapped to the appropriate result columns in the received RSN. This is useful for pushing predicates from outer queries down into inner queries, in which case the column references need to be remapped. For example, let V1 represent select i,j from t1 UNION select i,j from t2 and V2 represent select a,b from t3 UNION select a,b from t4 Then assume we have the following query: select * from V1, V2 where V1.j = V2.b Let's further assume that this Predicate object represents the "V1.j = V2.b" operator and that the childRSN we received as a parameter represents one of the subqueries to which we want to push the predicate; let's say it's: select i,j from t1 Then this method will return a new predicate whose binary operator represents the expression "T1.j = V2.b" (that is, V1.j will be mapped to the corresponding column in T1).  For more on how that mapping is made, see the "getScopedOperand()" method in BinaryRelationalOperatorNode.java. ASSUMPTION: We should only get to this method if we know that at least one operand in this predicate can and should be mapped to the received childRSN.  For an example of where that check is made, see the pushOptPredicate() method in SetOperatorNode.java. Return the pushable. Optimizable interface  Return the referencedSet. Get the RelationalOperator on the left side of the AND node, if there is one.  If the left side is not a RelationalOperator, return null. If this predicate corresponds to an IN-list, return the underlying InListOperatorNode from which it was built.  There are two forms to check for: 1. This predicate is an IN-list "probe predicate", in which case the underlying InListOpNode is stored within the binary relational operator that is the left operand of this predicate's AND node. 2. This predicate corresponds to an IN-list that could _not_ be transformed into a "probe predicate" (i.e. the IN-list contains one or more non-parameter, non-constant values). In that case the underlying InListOpNode is simply the left operand of this predicate's AND node. If this predicate does not correspond to an IN-list in any way, this method will return null. Does the work of getSourceInList() above, but can also be called directly with an argument to indicate whether or not we should limit ourselves to probe predicates. Get the start operator for this predicate for a scan.   Return whether or not this predicate is an IN-list probe predicate. Is this predicate a join predicate?  In order to be so, it must be a binary relational operator node that has a column reference on both sides. Is this predicate an pushable OR list? <p> Does the predicate represent a AND'd list of OR term's, all of which are pushable.  To be pushable each of OR terms must be a legal qualifier, which is a column reference on one side of a Relational operator and a constant on the other.  Return whether or not this predicate corresponds to a legitimate relational operator. Return whether or not this predicate is a scoped copy of another predicate. Return true if this predicate is scoped AND the scoped operand is a ColumnReference that points to a source result set.  If the scoped operand is not a ColumnReference that points to a source result set then it must be pointing to some kind of expression, such as a literal (ex. 'strlit'), an aggregate value (ex. "count(*)"), or the result of a function (ex. "sin(i)") or operator (ex. "i+1"). This method is used when pushing predicates to determine how far down the query tree a scoped predicate needs to be pushed to allow for successful evaluation of the scoped operand.  If the scoped operand is not pointing to a source result set then it should not be pushed any further down tree.  The reason is that evaluation of the expression to which the operand is pointing may depend on other values from the current level in the tree (ex. "sin(i)" depends on the value of "i", which could be a column at the predicate's current level).  If we pushed the predicate further down, those values could become inaccessible, leading to execution-time errors. If, on the other hand, the scoped operand *is* pointing to a source result set, then we want to push it further down the tree until it reaches that result set, which allows evaluation of this predicate to occur as close to store as possible.  This method doesn't actually do the push, it just returns "true" and then the caller can push as appropriate.   Is this predicate a possible Qualifier for store? <p> Current 2 types of predicates can be pushed to store: 1) RelationalOperator - represented with by left operand as instance of RelationalOperator. 2) A single And'd term of a list of OR terms represented by left operand as instance of OrNode. More checking specific operator's terms to see if they are finally pushable to store.  In the final push at execution each term of the AND or OR must be a Relational operator with a column reference on one side and a constant on the other. Indicate that this predicate is a scoped copy of some other predicate (i.e. it was created as the result of a call to getPredScopedForResultSet() on some other predicate).    Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Determine whether or not this predicate is eligible for push-down into subqueries.  Right now the only predicates we consider to be eligible are those which 1) are Binary Relational operator nodes and 2) have a column reference on BOTH sides, each of which has a reference to a base table somewhere beneath it. When remapping a "normal" (i.e. non-scoped) predicate both of the predicate's operands are remapped and that's it. But when remapping a scoped predicate, things are slightly different.  This method handles remapping of scoped predicates. We know that, for a scoped predicate, exactly one operand has been scoped for a specific target result set; the other operand is pointing to some other instance of FromTable with which the target result set is to be joined (see getScopedOperand() in BinaryRelationalOperatorNode.java).  For every level of the query through which the scoped predicate is pushed, we have to perform a remap operation of the scoped operand.  We do *not*, however, remap the non-scoped operand.  The reason is that the non-scoped operand is already pointing to the result set against which it must be evaluated.  As the scoped predicate is pushed down the query tree, the non-scoped operand should not change where it's pointing and thus should not be remapped.  For example, assume we have a query whose tree has the following form: SELECT[0] /     \ PRN      PRN |        | SELECT[4]   UNION |           /   \ PRN     SELECT[1]  SELECT[2] |         |          | [FBT:T1]     PRN        PRN |          | SELECT[3]  [FromBaseTable:T2] | PRN | [FromBaseTable:T3] Assume also that we have some predicate "SELECT[4].i = <UNION>.j". If the optimizer decides to push the predicate to the UNION node, it (the predicate) will be scoped to the UNION's children, yielding something like "SELECT[4].i = SELECT[1].j" for the left child and "SELECT[4].i = SELECT[2].j" for the right child. These scoped predicates will then be pushed to the PRNs above SELECT[3] and T2, respectively.  As part of that pushing process a call to PRN.pushOptPredicate() will occur, which brings us to this method.  So let's assume we're here for the scoped predicate "SELECT[4].i = SELECT[1].j".  Then we want to remap the scoped operand, "SELECT[1].j", so that it will point to the correct column in "SELECT[3]".  We do NOT, however, want to remap the non-scoped operand "SELECT[4].i" because that operand is already pointing to the correct result set--namely, to a column in SELECT[4].  That non-scoped operand should not change regardless of how far down the UNION subtree the scoped predicate is pushed. If we did try to remap the non-scoped operand, it would end up pointing to result sets too low in the tree, which could lead to execution-time errors.  So when we remap a scoped predicate, we have to make sure we only remap the scoped operand.  That's what this method does.  Set the andNode. Set the equivalence class, if any, for this predicate. Set the position of the index column that this predicate restricts Set whether or not this predicate is pushable.  This method is intended for use when creating a copy of the predicate, ex for predicate pushdown.  We choose not to add this assignment to copyFields() because the comments for that method say that it should copy all fields _except_ the two specified at init time; "pushable" is one of the two specified at init time. Mark this predicate as having been used to add a new predicate of the specified type via transitive closure on search clauses. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Return whether or not this predicate has been used to add a new search clause of the specified type via transitive closure. NOTE: This can only be true if this is an equijoin between 2 column references.
Another flavor of addOptPredicate that inserts the given predicate at a given position.  This is not yet part of any interface. Add a Predicate to the list.  Check if all of the predicates in the list are pushable. Check if all the predicates reference a given {@code FromBaseTable}. assign a weight to each predicate-- the maximum weight that a predicate can have is numUsefulPredicates. If a predicate corresponds to the first index position then its weight is numUsefulPredicates. The weight of a pwlist is the sum of the weights of individual predicates. Categorize the predicates in the list.  Initially, this means building a bit map of the referenced tables for each predicate. Update the array of columns in = conditions with constants or correlation or join columns.  This is useful when doing subquery flattening on the basis of an equality condition. choose the statistic which has the maximum match with the predicates. value is returned in ret.  Class implementation  Decrement the level of any CRs from the subquery's FROM list that are interesting to transitive closure. Eliminate predicates of the form: AndNode /	   \ true BooleanConstantNode		true BooleanConstantNode This is useful when checking for a NOP PRN as the Like transformation on c1 like 'ASDF%' can leave one of these predicates in the list. Finish generating a start or stop key If there is an IN-list probe predicate in this list then generate the corresponding IN-list values as a DataValueDescriptor array, to be used for probing at execution time.  Also generate a boolean value indicating whether or not the values are already in sorted order. Assumption is that by the time we get here there is at most one IN-list probe predicate in this list. Generate the indexable row for a start key or stop key.  Generate the code to set the value from a predicate in an index column.   OptimizableList interface  Build a list of pushable predicates, if any, that satisfy the referencedTableMap.    Check if a node is representing a constant or a parameter.  Perform transitive closure on join clauses.  For each table in the query, we build a list of equijoin clauses of the form: ColumnReference relop ColumnReference Each join clause is put on 2 lists since it joins 2 tables. We then walk the array of lists.  We first walk it as the outer list. For each equijoin predicate, we assign an equivalence class if it does not yet have one.  We then walk the predicate list (as middle) for the other table, searching for other equijoins with the middle table number and column number.  All such predicates are assigned the same equivalence class. We then walk the predicate list (as inner) for the other side of the middle predicate to see if we can find an equijoin between outer and inner.  If so, then we simply assign it to the same equivalence class.  If not, then we add the new equijoin clause. Note that an equijoin predicate between two tables CANNOT be used for transitive closure, if either of the tables is in the fromlist for NOT EXISTS. In that case, the join predicate actually specifies that the rows from the indicated table must NOT exist, and therefore those non-existent rows cannot be transitively joined to the other matching tables. See DERBY-3033 for a description of a situation in which this actually arises.  Mark all of the RCs and the RCs in their RC/VCN chain referenced in the predicate list as referenced. Break apart the search clause into matching a PredicateList where each top level predicate is a separate element in the list. Build a bit map to represent the FromTables referenced within each top level predicate. NOTE: We want the rightOperand of every AndNode to be true, in order to simplify the algorithm for putting the predicates back into the tree. (As we put an AndNode back into the tree, we can ignore it's rightOperand.) Push all predicates, which can be pushed, into the underlying select. A predicate can be pushed into an underlying select if the source of every ColumnReference in the predicate is itself a ColumnReference. This is useful when attempting to push predicates into non-flattenable views or derived tables or into unions.   Remap all ColumnReferences in this tree to be clones of the underlying expression.  Another version of removeOptPredicate that takes the Predicate to be removed, rather than the position of the Predicate.  This is not part any interface (yet). Remove redundant predicates.  A redundant predicate has an equivalence class (!= -1) and there are other predicates in the same equivalence class after it in the list.  (Actually, we remove all of the predicates in the same equivalence class that appear after this one.) Rebuild a constant expression tree from the remaining constant predicates and delete those entries from the PredicateList. The rightOperand of every top level AndNode is always a true BooleanConstantNode, so we can blindly overwrite that pointer. Optimizations: We take this opportunity to eliminate: AndNode /		   \ true BooleanConstantNode	true BooleanConstantNode We remove the AndNode if the predicate list is a single AndNode: AndNode /	   \ LeftOperand			RightOperand becomes: LeftOperand If the leftOperand of any AndNode is False, then the entire expression will be False.  The expression simple becomes: false BooleanConstantNode Rebuild an expression tree from the remaining predicates and delete those entries from the PredicateList. The rightOperand of every top level AndNode is always a true BooleanConstantNode, so we can blindly overwrite that pointer. Optimizations: We take this opportunity to eliminate: AndNode /	   \ true BooleanConstantNode	true BooleanConstantNode We remove the AndNode if the predicate list is a single AndNode: AndNode /	   \ LeftOperand			RightOperand becomes: LeftOperand If the leftOperand of any AndNode is False, then the entire expression will be False.  The expression simple becomes: false BooleanConstantNode  Perform transitive closure on search clauses.  We build a list of search clauses of the form: <ColumnReference> <RelationalOperator> [<ConstantNode>] We also build a list of equijoin conditions of form: <ColumnReference1> = <ColumnReference2> where both columns are from different tables in the same query block. For each search clause in the list, we search the equijoin list to see if there is an equijoin clause on the same column.  If so, then we search the search clause list for a search condition on the column being joined against with the same relation operator and constant.  If a match is found, then there is no need to add a new predicate. Otherwise, we add a new search condition on the column being joined with.  In either case, if the relational operator in the search clause is an "=" then we mark the equijoin clause as being redundant. Redundant equijoin clauses will be removed at the end of the search as they are * unnecessary.  Compute selectivity the old fashioned way.     Transfer the non-qualifiers from this predicate list to the specified predicate list. This is useful for arbitrary hash join, where we need to separate the 2 as the qualifiers get applied when probing the hash table and the non-qualifiers get * applied afterwards.   XOR fromMap with the referenced table map in every remaining Predicate in the list.  This is useful when pushing down multi-table predicates.
Set the fixture up with a clean, standard table PED001. Create a suite of tests, one per statement in DDL. This test is for testing the embedded dependency system though possibly it could be used for testing in client as well. Tear-down the fixture by removing the table (if it still exists).
Execute the PreparedStatement and return results, used for top level statements (not substatements) in a connection. <p> There is no executeQuery() or executeUpdate(); a method is provided in ResultSet to tell whether to expect rows to be returned. Execute a statement as part of another statement (ithout a nested connection) and return results. <p> There is no executeQuery() or executeUpdate(); a method is provided in ResultSet to tell whether to expect rows to be returned. Execute a statement as part of another statement (without a nested connection) and return results. <p> Creates a new single use activation and executes it, but also passes rollbackParentContext parameter. PreparedStatements are re-entrant - that is, more than one execution can be active at a time for a single prepared statement. An Activation contains all the local state information to execute a prepared statement (as opposed to the constant information, such as literal values and code). Each Activation class contains the code specific to the prepared statement represented by an instance of this class (PreparedStatement). Get the timestamp for the beginning of compilation Get the bind time for the associated query in milliseconds. Get the total compile time for the associated query in milliseconds. Compile time can be divided into parse, bind, optimize and generate times. Return any compile time warnings. Null if no warnings exist. Get the timestamp for the end of compilation Get the generate time for the associated query in milliseconds. Get the optimize time for the associated query in milliseconds. Get the type of the parameter at the given (0-based) index. Raises an exception if the index is out of range. Get an array of DataTypeDescriptors describing the types of the parameters of this PreparedStatement. The Nth element of the array describes the Nth parameter. Get the parse time for the associated query in milliseconds. Get the ResultDescription for the statement.  The ResultDescription describes what the results look like: what are the rows and columns? <p> This is available here and on the ResultSet so that users can see the shape of the result before they execute. Return the SPS Name for this statement. Return the SQL string that this statement is for. Get the version counter. A change in the value indicates a recompile has happened. Returns whether or not this Statement requires should behave atomically -- i.e. whether a user is permitted to do a commit/rollback during the execution of this statement. Re-prepare the statement if it is not up to date or, if requested, simply not optimal. If there are open cursors using this prepared statement, then we will not be able to recompile the statement. Return true if the query node for this statement references SESSION schema tables. Checks whether this PreparedStatement is up to date. A PreparedStatement can become out of date if any of several things happen: A schema used by the statement is dropped A table used by the statement is dropped A table used by the statement, or a column in such a table, is altered in one of several ways: a column is dropped, a privilege is dropped, a constraint is added or dropped, an index is dropped. A view used by the statement is dropped. In general, anything that happened since the plan was generated that might cause the plan to fail, or to generate incorrect results, will cause this method to return FALSE.

methods to be registered as functions Externalizable implementation
getHeader
This method adds a warning if a revoke statement has not revoked any privileges from a grantee. Determines whether a user is the owner of an object (table, function, or procedure). Note that Database Owner can access database objects without needing to be their owner This is the guts of the Execution-time logic for GRANT/REVOKE
Bind this GrantNode. Resolve all table, column, and routine references. Register a dependency on the object of the privilege if it has not already been done end of bind  Report an unimplemented feature
Get the type of the object for storage in SYS.SYSPERMS
//////////////////////////////////////////////////////  STATIC BEHAVIOR  ////////////////////////////////////////////////////// <p> Score a test run and update TestTaking with the score. </p>

Wait till the myThread has finished its work or incase a timeout was set on this object, then to set a flag to indicate the myThread to leave at the end of the timeout period. Behavior is as follows: 1) If timeout is set to a valid value (&gt;0) - in this case, if myThread has not finished its work by the time this method was called, then it will wait till the timeout has elapsed or if the myThread has finished its work. 2)If timeout is not set ( &lt;= 0) - in this case, if myThread has not finished its work by the time this method was called, then it will wait till myThread has finished its work. If timeout is set to a valid value, and the timeout amount of time has elapsed, then the interrupted  flag is set to true to indicate that it is time for the myThread to stop its work and leave.


Return the build number for this product. Return the build number as an integer if possible, mapping from the SVN number. nnnnn -&gt; returns nnnnn nnnnnM -&gt; returns -nnnnn indicates a modified code base nnnnn:mmmmm -&gt; returns -nnnnn anything else -&gt; returns -1 Return the drda protocol maintenance version for this minor release. Starts at 0 for each minor release and only incremented when client behaviour changes based on the server version. Return the fix pack version from the maintenance encoding. Return the <B>encoded</B> maintainence version number. Return the major version number. Return the minor version number. Return the external product name. Return the product vendor name. Create a valid ProductVersionHolder. If any of the parameters provided is invalid, this returns null. Get a ProductVersionHolder based on the information in the Properties object provided. Load the version info from the already opened properties files. We need to do this because if the jar files (e.g. db2jtools and db2j) are in different security contexts (entries in the policy files) then we cannot load the version information for one of them correctly. This is because the this class will either have been loaded from only one of the jars and hence can only access the resource in its own jar. By making code specific to the jar open the resource we are guaranteed it will work. Get a ProductVersionHolder for a product of a given genus, that is available in the caller's environment. Even though this uses a priv bock, it may stil fail when the jar the version is being fetched from, is different to the one that loaded this class, AND the jars are in different security contexts. Return the feature version string, ie. major.minor. (e.g. 5.2) Returns a short-hand value for the product version string. Used by Sysinfo. Includes the optional <beta> designation Return true if this is a alpha product. Return true if this is a beta product. SECURITY PERMISSION - IP4 Parse a string containing a non-negative integer. Return a negative integer is the String is invalid. Convert a major and minor number with beta status into a string. Return  a string representation of this ProductVersion. The difference between this and createProductVersionString, is that this method retruns a String when this ProductVersionHolder holds invalid version information.
Accept the visitor for all visitable children of this node. Add a new predicate to the list.  This is useful when doing subquery transformations, when we build a new predicate with the left side of the subquery operator and the subquery's result column. Consider materialization for this ResultSet tree if it is valid and cost effective (It is not valid if incorrect results would be returned.)  Ensure that the top of the RSN tree has a PredicateList.  Evaluate whether or not the subquery in a FromSubquery is flattenable. Currently, a FSqry is flattenable if all of the following are true: o  Subquery is a SelectNode. o  It contains no top level subqueries.  (RESOLVE - we can relax this) o  It does not contain a group by or having clause o  It does not contain aggregates. For joins, the tree will be (nodes are left out if the clauses are empty): ProjectRestrictResultSet -- for the having and the select list SortResultSet -- for the group by list ProjectRestrictResultSet -- for the where and the select list (if no group or having) the result set for the fromList Logic shared by generate() and generateResultSet(). Bypass the generation of this No-Op ProjectRestrict, and just generate its child result set. General logic shared by Core compilation.   Get the CostEstimate for this ProjectRestrictNode.  Get the final CostEstimate for this ProjectRestrictNode. Determine whether or not the specified name is an exposed name in the current query block. Return the restriction list from this node.   Return the user specified join strategy, if any for this table. Is it possible to do a distinct scan on this ResultSet tree. (See SelectNode for the criteria.)  Mark the underlying scan as a distinct scan.  Optimizable interface  Determine whether this ProjectRestrict does anything.  If it doesn't filter out any rows or columns, it's a No-Op. Optimize this ProjectRestrictNode.  Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.  Push expressions down to the first ResultSetNode which can do expression evaluation and has the same referenced table map. RESOLVE - This means only pushing down single table expressions to ProjectRestrictNodes today.  Once we have a better understanding of how the optimizer will work, we can push down join clauses. Push down the offset and fetch first parameters, if any, to the underlying child result set.  Push the order by list down from InsertNode into its child result set so that the optimizer has all of the information that it needs to consider sort avoidance.   This method creates a HashTableNode between the PRN and it's child when the optimizer chooses hash join on an arbitrary (non-FBT) result set tree. We divide up the restriction list into 3 parts and distribute those parts as described below. set the Information gathered from the parent table that is required to perform a referential action on dependent table.   Get the lock mode for the target of an update statement (a delete or update).  The update mode will always be row for CurrentOfNodes.  It will be table if there is no where clause.
ResultSet interface If the result set has been opened, close the open scan. Do the projection against the sourceRow. If the source of the result set is of type ProjectRestrictResultSet, the projection by that result set will also be performed. Do the projection against the source row.  Use reflection where necessary, otherwise get the source column into our result row. Get projection mapping array. The array consist of indexes which maps the column in a row array to another position in the row array. If the value is projected out of the row, the value is negative. Gets last row returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> restriction and projection parameters are evaluated for each row.  CursorResultSet interface  Gets information from its source. We might want to have this take a CursorResultSet in its constructor some day, instead of doing a cast here? Return the total amount of time spent in this ResultSet Is this ResultSet or it's source result set for update   NoPutResultSet interface  open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...

Privileged startup. Must be private so that user code can't call this entry point. Fetch the set of properties as a Properties object. This means that only keys that have String values will be included. Gets the de-serialized object associated with a property key. <p> The Store provides a transaction protected list of database properties. Higher levels of the system can store and retrieve these properties once Recovery has finished. Each property is a serializable object and is stored/retrieved using a String key. <p> In this implementation a lookup is done on the PropertyConglomerate conglomerate, using a scan with "key" as the qualifier. <p> Get the default for a property. Return true if the caller holds the exclusive update lock on the property conglomerate. Lock the database properties for an update. Create a new empty PropertyConglomerate row, to fetch values into. Private/Protected methods of This class: Create a new PropertyConglomerate row, with values in it. Call the property set callbacks to map a proposed property value to a value to save. <P> The caller must run this in a block synchronized on this to serialize validations with changes to the set of property callbacks Open a scan on the properties conglomerate looking for "key". <p> Open a scan on the properties conglomerate qualified to find the row with value key in column 0.  Both column 0 and column 1 are included in the scan list. Read the database properties and add in the service set. Sets the Serializable object associated with a property key. <p> This implementation turns the setProperty into an insert into the PropertyConglomerate conglomerate. <p> See the discussion of getProperty(). <p> The value stored may be a Formatable object or a Serializable object whose class name starts with java.*. This stops arbitary objects being stored in the database by class name, which will cause problems in obfuscated/non-obfuscated systems. Package Methods of This class: Set a property in the conglomerate. Set the default for a property. Call the property set callbacks to validate a property change against the property set provided. <P> The caller must run this in a block synchronized on this to serialize validations with changes to the set of property callbacks
************************************************************************ methods that are Property related. ************************************************************************* Add a callback for a change in any property value. <BR> The callback is made in the context of the transaction making the change. Call the property set callbacks to map a proposed property value to a value to save. <P> The caller must run this in a block synchronized on this to serialize validations with changes to the set of property callbacks  validation a single property Validate a Property set. <p> Validate a Property set by calling all the registered property set notification functions with .
/////////////////////////////////////////////////////////////////////////////////  FlatFileVTI BEHAVIOR TO BE IMPLEMENTED BY SUBCLASSES  ///////////////////////////////////////////////////////////////////////////////// <p> Parse the next chunk of text, using readLine(), and return the next row. Returns null if the file is exhausted. </p> /////////////////////////////////////////////////////////////////////////////////  TABLE FUNCTION METHOD  ///////////////////////////////////////////////////////////////////////////////// <p> This is the method which is registered as a table function. </p>
Set or delete the value of a property of the database on the current connection. For security reasons (see DERBY-6616), this code is duplicated in SystemProcedures.

<p> Prompt for and set a property if it isn't already set. </p> ///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// <p> Prompt for and set the property. </p> <p> Prompt the user for a line of input. </p> <p>Let Ant set the prompt to be used in case the property isn't set.</p> <p> Set an ant property. </p> ///////////////////////////////////////////////////////////////////////  Task BEHAVIOR  /////////////////////////////////////////////////////////////////////// <p>Let Ant set the name of the property.</p>
Apply a property change. Will only be called after validate has been called and only if validate returned true. If this method is called then the new value is the value to be used, ie. the property is not set in the overriding JVM system set. Initialize the properties for this callback. Called when addPropertySetNotification() is called with a non-null transaction controller. This allows code to set read its initial property values at boot time. <P> Code within an init() method should use the 3 argument PropertyUtil method getPropertyFromSet() to obtain a property's value. Map a proposed new value for a property to an official value. Will only be called after apply() has been called. Validate a property change.
Parse and validate and return a boolean property value. If the value is invalid raise an exception. <P> The following are valid property values. <UL> <LI> null - returns defaultValue <LI> "true" - returns true (in any case without the quotes) <LI> "false" - return true (in any case without the quotes) </UL> Return {@code true} if {@code username} is defined as a built-in user i.e. there exists a property {@code derby.user.}&lt;userid&gt; in the database (or, possibly, in system properties if not forbidden by {@code derby.database.propertiesOnly}). Note that &lt;userid&gt; found in a property will be normalized to case normal form before comparison is performed against username, which is presumed normalized already. Get a property only looking in the Persistent Transactional (database) set. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Get a property from the passed in set. The passed in set is either: <UL> <LI> The properties object passed into ModuleControl.boot() after the database has been booted. This set will be a DoubleProperties object with the per-database transaction set as the read set and the service.properties as the write set. <LI> The Dictionary set returned/passed in by a method of BasicService.Properties. </UL> <BR> This method uses the same search order as the getService() calls. Get a service wide property as a boolean. Get a service wide property as a int. Get a service wide property as a int. The passed in Properties set overrides any system, applcation or per-database properties. Find a service wide property. The service is the persistent service associated with the current context stack. Find a service wide property with a default. Search order is The service is the persistent service associated with the current context stack. Get the list of properties which are normally stored in service.properties Get a system wide property as a boolean. Get a system wide property as a boolean. Get a system wide property as a int. s Get a system wide property as a int. Find a system wide property. Find a system wide property with a default. Search order is <OL> <LI> JVM property <LI> derby.properties </OL> <P> This method can be used by a system that is not running Derby, just to maintain the same lookup logic and security manager concerns for finding derby.properties and reading system properties. Parse an string as an int based property value. Parse, validate and return an integer property value. If the value is invalid raise an exception. If the value passed in is null return a default value. Return true iff the key is the name of a database property that is stored in services.properties. Return true if the passed-in properties specify NATIVE authentication using LOCAL credentials. Return true if NATIVE authentication is turned on for the passed-in value of Property.AUTHENTICATION_PROVIDER_PARAMETER. Return true if NATIVE authentication has been enabled in the passed-in properties. Return true if username is defined as a system property i.e. there exists a property {@code derby.user.}&lt;userid&gt; in the system properties. Note that &lt;userid&gt; will be normalized to case normal form before comparison is performed against username, which is presumed normalized already.
Call the property set callbacks to map a proposed property value to a value to save. <P> The caller must run this in a block synchronized on this to serialize validations with changes to the set of property callbacks
* Methods required to use this key *

Get the DependableFinder. Get the object id Get the provider's name.
Add a Provider to the list.
Generates a SQLException for signalling that the operation failed due to a database error.
Get a reference to the buffer array stored in the byte array output stream

Loggable methods Apply the purge operation to the page. methods to support prepared log the following two methods should not be called during recover Return my format identifier. Read this in PageBasicOperation restore the before image of the page DEBUG: Print self. PhysicalPageOperation methods Undo the purge operation on the page. Write out the purged record from the page.  Used for undo only.
Returns the name of this taglet purpose not expected to be used in constructor documentation. purpose not expected to be used in field documentation. purpose not expected to be used in method documentation. purpose can be used in overview documentation. purpose can be used in package documentation. purpose can be used in type documentation. purpose is not an inline tag. Register this Taglet. Embed the contents of the purpose tag as a row in the disk format table. Embed multiple purpose tags as cells in the disk format table.
Indicates the columns that must be returned by a read-write VTI's ResultSet. This method is called only during the runtime execution of the VTI, after it has been constructed and before the executeQuery() method is called. At compile time the VTI needs to describe the complete set of columns it can return. <BR> The column identifiers contained in projectedColumns map to the columns described by the VTI's PreparedStatement's ResultSetMetaData. The ResultSet returned by PreparedStatement.executeQuery() must contain these columns in the order given. Column 1 in this ResultSet maps the the column of the VTI identified by projectedColumns[0], column 2 maps to projectedColumns[1] etc. <BR> Any additional columns contained in the ResultSet are ignored by the database engine. The ResultSetMetaData returned by ResultSet.getMetaData() must match the ResultSet. <P> PreparedStatement's ResultSetMetaData column list {"id", "desc", "price", "tax", "brand"} <BR> projectedColumns = { 2, 3, 5} <BR> results in a ResultSet containing at least these 3 columns {"desc", "price", "brand"} The  JDBC column numbering scheme (1 based) ise used for projectedColumns.

Clear the DataValueDescriptor cache, if one exists. (The DataValueDescriptor can be 1 of 3 types: o  VARIANT		  - cannot be cached as its value can vary within a scan o  SCAN_INVARIANT - can be cached within a scan as its value will not change within a scan o  QUERY_INVARIANT- can be cached across the life of the query as its value will never change Get the (zero based) id of the column to be qualified. <p> This id is the column number of the column in the table, no matter whether a partial column set is being retrieved by the actual fetch. Note that the column being specified in the qualifier must appear in the column list being fetched. Get the operator to use in the comparison. Get the value that the column is to be compared to. Get the getOrderedNulls argument to use in the comparison. Get the getOrderedNulls argument to use in the comparison. Determine if the result from the compare operation should be negated. If true then only rows which fail the compare operation will qualify. This method reinitializes all the state of the Qualifier.  It is used to distinguish between resetting something that is query invariant and something that is constant over every execution of a query.  Basically, clearOrderableCache() will only clear out its cache if it is a VARIANT or SCAN_INVARIANT value.  However, each time a query is executed, the QUERY_INVARIANT qualifiers need to be reset.
Clear the DataValueDescriptor cache, if one exists. (The DataValueDescriptor can be 1 of 3 types: o  VARIANT		  - cannot be cached as its value can vary within a scan o  SCAN_INVARIANT - can be cached within a scan as its value will not change within a scan o  QUERY_INVARIANT- can be cached across the life of the query as its value will never change o  CONSTANT		  - can be cached across executions Qualifier interface: * Get the id of the column to be qualified. * Get the operator to use in the comparison. Get the value that the column is to be compared to. * Get the getOrderedNulls argument to use in the comparison. Get the getOrderedNulls argument to use in the comparison. Should the result of the compare be negated? This method reinitializes all the state of the Qualifier.  It is used to distinguish between resetting something that is query invariant and something that is constant over every execution of a query.  Basically, clearOrderableCache() will only clear out its cache if it is a VARIANT or SCAN_INVARIANT value.  However, each time a query is executed, the QUERY_INVARIANT qualifiers need to be reset.







Accept a visitor, and call {@code v.visit()} on child nodes as necessary. Sub-classes should not override this method, but instead override the {@link #acceptChildren(Visitor)} method. Accept a visitor on all child nodes. All sub-classes that add fields that should be visited, should override this method and call {@code accept(v)} on all visitable fields, as well as {@code super.acceptChildren(v)} to make sure all visitable fields defined by the super-class are accepted too. Add USAGE privilege for all UDTs mentioned in the indicated ValueNodes. Add USAGE privilege for a single UDT. Bind the parameters of OFFSET n ROWS and FETCH FIRST n ROWS ONLY, if any. Bind the UDTs in a table type. Bind user defined types as necessary Bind a UDT. This involves looking it up in the DataDictionary and filling in its class name. Bind time logic. Raises an error if this ValueNode, once compiled, returns unstable results AND if we're in a context where unstable results are forbidden. Called by children who may NOT appear in the WHERE subclauses of ADD TABLE clauses. Bind time logic. Raises an error if this ValueNode, once compiled, returns unstable results AND if we're in a context where unstable results are forbidden. Called by children who may NOT appear in the WHERE subclauses of ADD TABLE clauses. Translate a Default node into a default value, given a type descriptor. Copy the tags from another QueryTreeNode Declare a dependency on an ANSI UDT, identified by its AliasDescriptor, and check that you have privilege to use it. Declare a dependency on a type and check that you have privilege to use it. This is only used if the type is an ANSI UDT. Flush the debug stream out Print a String for debugging Triggers, constraints and views get executed with their definers' privileges and they can exist in the system only if their definers still have all the privileges to create them. Based on this, any time a trigger/view/constraint is executing, we do not need to waste time in checking if the definer still has the right set of privileges. At compile time, we will make sure that we do not collect the privilege requirement for objects accessed with definer privileges by calling the following method. Format a node that has been converted to a String for printing as part of a tree.  This method indents the String to the given depth by inserting tabs at the beginning of the string, and also after every newline. Do the code generation for this node.  This is a place-holder method - it should be over-ridden in the sub-classes. Add an authorization check into the passed in method. Gets the beginning offset of the SQL substring which this query node represents. Get the ClassFactory to use with this database. Get the CompilerContext Privileged lookup of a Context. Must be package protected so that user code can't call this entry point. Get the current ContextManager. Get the DataDictionary Gets the ending offset of the SQL substring which this query node represents. Gets the constant action factory for this database. Get the int value of a Property Gets the LanguageConnectionContext for this connection. Get the long value of a Property Get a ConstantNode to represent a typed null value. Get all child nodes of a specific type, and return them in the order in which they appear in the SQL text. Gets the NodeFactory for this database. Convenience method for finding the optimizer tracer Parameter info is stored in the compiler context. Hide this from the callers. Get the descriptor for the named schema. If the schemaName parameter is NULL, it gets the descriptor for the current compilation schema. QueryTreeNodes must obtain schemas using this method or the two argument version of it. This is to ensure that the correct default compliation schema is returned and to allow determination of if the statement being compiled depends on the current schema. Schema descriptors include authorization ids and schema ids. SQL92 allows a schema to specify a default character set - we will not support this.  Will check default schema for a match before scanning a system table. Get the descriptor for the named schema. If the schemaName parameter is NULL, it gets the descriptor for the current compilation schema. QueryTreeNodes must obtain schemas using this method or the single argument version of it. This is to ensure that the correct default compliation schema is returned and to allow determination of if the statement being compiled depends on the current schema. Return the type of statement, something from StatementType. Get the descriptor for the named table within the given schema. If the schema parameter is NULL, it looks for the table in the current (default) schema. Table descriptors include object ids, object types (table, view, etc.) If the schema is SESSION, then before looking into the data dictionary for persistent tables, it first looks into LCC for temporary tables. If no temporary table tableName found for the SESSION schema, then it goes and looks through the data dictionary for persistent table We added getTableDescriptor here so that we can look for non data dictionary tables(ie temp tables) here. Any calls to getTableDescriptor in data dictionary should be only for persistent tables Get the TypeCompiler associated with the given TypeId Get the AliasDescriptor of a UDT Return true from this method means that we need to collect privilege requirement for this node. For following cases, this method will return true. 1)execute view - collect privilege to access view but do not collect privilege requirements for objects accessed by actual view uqery 2)execute select - collect privilege requirements for objects accessed by select statement 3)create view -  collect privileges for select statement : the select statement for create view falls under 2) category above. Checks if the passed schema name is for SESSION schema Checks if the passed schema descriptor is for SESSION schema This creates a class that will do the work that's constant across all Executions of a PreparedStatement. It's up to our subclasses to override this method if they need to compile constant actions into PreparedStatements. Return header information for debug printing of this query tree node. Convenience method for checking whether optimizer tracing is on OR in more reliability bits and return the old reliability value. Parse an SQL fragment that represents a {@code <search condition>}. * Parse the a SQL statement from the body of another SQL statement. Pushes and pops a separate CompilerContext to perform the compilation. Parse a full SQL statement or a fragment representing a {@code <search condition>}. This is a worker method that contains common logic for {@link #parseStatement} and {@link #parseSearchCondition}. Print the given label at the given indentation depth. Print the sub-nodes of this node. Each sub-class of QueryTreeNode is expected to provide its own printSubNodes() method.  In each case, it calls super.printSubNodes(), passing along its depth, to get the sub-nodes of the super-class. Then it prints its own sub-nodes by calling treePrint() on each of its members that is a type of QueryTreeNode.  In each case where it calls treePrint(), it should pass "depth + 1" to indicate that the sub-node should be indented one more level when printing. Also, it should call printLabel() to print the name of each sub-node before calling treePrint() on the sub-node, so that the reader of the printed tree can tell what the sub-node is. This printSubNodes() exists in here merely to act as a backstop. In other words, the calls to printSubNodes() move up the type hierarchy, and in this node the calls stop. I would have liked to put the call to super.printSubNodes() in this super-class, but Java resolves "super" statically, so it wouldn't get to the right super-class. Return true if the node references SESSION schema tables (temporary or permanent) Resolve table/view reference to a synonym. May have to follow a synonym chain. Sets the beginning offset of the SQL substring which this query node represents. Sets the ending offset of the SQL substring which this query node represents. set the Information gathered from the parent table that is required to perform a referential action on dependent table. Print call stack for debug purposes Common code for the 2 checkReliability functions.  Always throws StandardException. Format this node as a string Each sub-class of QueryTreeNode should implement its own toString() method.  In each case, toString() should format the class members that are not sub-types of QueryTreeNode (printSubNodes() takes care of following the references to sub-nodes, and toString() takes care of all members that are not sub-nodes).  Newlines should be used liberally - one good way to do this is to have a newline at the end of each formatted member.  It's also a good idea to put the name of each member in front of the formatted value.  For example, the code might look like: "memberName: " + memberName + "\n" + ... Vector members containing subclasses of QueryTreeNode should subclass QueryTreeNodeVector. Such subclasses form a special case: These classes should not implement printSubNodes, since there is generic handling in QueryTreeNodeVector.  They should only implement toString if they contain additional members. Print this tree for debugging purposes.  This recurses through all the sub-nodes and prints them indented by their depth in the tree. Print this tree for debugging purposes.  This recurses through all the sub-nodes and prints them indented by their depth in the tree, starting with the given indentation. Verify that a java class exists, is accessible (public) and not a class representing a primitive type.
Accept the visitor for all visitable children of this node. Iterable interface Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.
Backup the  container. The container is written to the backup by reading  the pages through the page cache, and then writing into the backup container. If the container is dropped(commitetd drop), only container stub is copied to the  backup using simple file copy. MT - At any given time only one backup thread is allowed, but when backup in progress DML/DDL operations can run in parallel. Pages are latched while writing them to the backup to avoid copying partial changes to the pages. Online backup does not acquire any user level locks , so users can drop tables when backup is in progress. So it is possible that Container Removal request can come in when container backup is in progress. This case is handled by using the synchronization on this object monitor and using inRemove and inBackup flags. Conatiner removal checks if backup is in progress and wait for the backup to yield to continue the removal. Basic idea is to give preference to remove by stopping the backup of the container temporarily,  when the remove container is requested by another thread. Generally, it takes more  time to backup a regular container than the stub becuase  stub is just one page. After each page copy, a check is made to find  if a remove is requested and if it is then backup of the container is aborted and the backup thread puts itself into the wait state until remove  request thread notifies that the remove is complete. When remove request compeletes stub is copied into the backup. Compress is blocked when backup is in progesss, so truncation of the container can not happen when backup is in progess. No need to synchronize backup of the container with truncation. Clean the container. <p> Write out the container header and sync all dirty pages of this container to disk before returning. <p> checkpoint calls this interface through callbacks by telling the cache manager to clean all containers in the open container cache.  This sync of the file happens as part of writing and then syncing the container header in writeRAFHeader(). <p> Copy the contents of a {@code StorageFile} to a {@code java.io.File}. end of createContainer Creates encrypted or decrypted version of the container. Reads all the pages of the container from the original container through the page cache, then either encrypts page data with the new encryption mechanism or leaves the page data un-encrypted, and finally writes the data to the specified new container file. <p> The encryption and decryption engines used to carry out the cryptographic operation(s) are configured through the raw store, and accessed via the data factory. Note that the pages have already been decrypted before being put into the page cache. flush the cache to ensure all of my pages are written to disk Get a RandomAccessFile for accessing a file in read-write mode. * Methods overriding super-class Pad the file upto the passed in page offset. Returns true if the file needed padding. Preallocate some pages if need be end of privGetFileName end of privRemoveFile * Methods used solely by StoredPage Read a page into the supplied array. <BR> MT - thread safe * Container creation, opening, and closing Remove the container Remove a file. end of removeFile Only used by RAFContainer4 (NIO) to reopen RAF when its channel gets closed due to interrupts. PrivilegedExceptionAction method end of run Truncate pages of a container. <p> Truncate all pages from lastValidPagenum+1 through the end of the file. <p> Updates the page array with container header if the page is a first allocation page and encrypts the page data if the database is encrypted. Write a page from the supplied array. <BR> MT - thread safe Write the header of a random access file and sync it
Use when seeing an exception during IO and when another thread is presumably doing the recovery. <p/> If {@code stealthMode == false}, wait for another thread to recover the container after an interrupt. If {@code stealthMode == true}, throw internal exception {@code InterruptDetectedException} to do retry from higher in the stack. <p/> If {@code stealthMode == false}, maximum wait time for the container to become available again is determined by the product {@code InterruptStatus.MAX_INTERRUPT_RETRIES * InterruptStatus.INTERRUPT_RETRY_SLEEP}. There is a chance this thread will not see any recovery occuring (yet), in which case it waits for a bit and just returns, so the caller must retry IO until success. <p/> If for some reason the recovering thread has given up on resurrecting the container, cf {@code #giveUpIO}, the method throws {@code FILE_IO_INTERRUPTED}. override of RAFContainer#closeContainer override of RAFContainer#createContainer <p> Return the file channel for the current value of the {@code fileData} field. If {@code fileData} doesn't support file channels, return {@code null}. </p> <p> Callers of this method must synchronize on the container object since two shared fields ({@code fileData} and {@code ourChannel}) are accessed. </p> Return the {@code FileChannel} for the specified {@code StorageRandomAccessFile} if it is a {@code RandomAccessFile}. Otherwise, return {@code null}. Read an embryonic page (that is, a section of the first alloc page that is so large that we know all the borrowed space is included in it) from the specified offset in a {@code StorageRandomAccessFile}. <p/> override of FileContainer#getEmbryonicPage <p/> <p> This method handles what to do when, during a NIO operation we receive a {@code ClosedChannelException}. Note the specialization hierarchy: </p> <p> {@code ClosedChannelException} -&gt; {@code AsynchronousCloseException} -&gt; {@code ClosedByInterruptException} </p> <p> If {@code e} is a ClosedByInterruptException, we normally start container recovery, i.e. we need to reopen the random access file so we get get a new interruptible channel and continue IO. </p> <p> If {@code e} is a {@code AsynchronousCloseException} or a plain {@code ClosedChannelException}, the behavior depends of {@code stealthMode}: </p> <p> If {@code stealthMode == false}, the method will wait for another thread tp finish recovering the IO channel before returning. </p> <p> If {@code stealthMode == true}, the method throws {@code InterruptDetectedException}, allowing retry at a higher level in the code.  The reason for this is that we sometimes need to release monitors on objects needed by the recovery thread. </p> Wrapping methods that retrieve the FileChannel from RAFContainer's fileData after calling the real methods in RAFContainer. override of RAFContainer#openContainer Attempts to fill buf completely from start until it's full. <p/> FileChannel has no readFull() method, so we roll our own. <p/> These are the methods that were rewritten to use FileChannel. Read a page into the supplied array. <p/> override of RAFContainer#readPage <p/> <BR> MT - thread safe Read a page into the supplied array. <p/> override of RAFContainer#readPage <p/> <BR> MT - thread safe Use this when the thread has received a ClosedByInterruptException (or, prior to JDK 1.7 it may also be AsynchronousCloseException - a bug) exception during IO and its interruped flag is also set. This makes this thread a likely candicate to do container recovery, unless another thread started it already, cf. return value. When the existing channel ({@code ourChannel}) has been closed due to interrupt, we need to reopen the underlying RAF to get a fresh channel so we can resume IO. Write a sequence of bytes at the given offset in a file.  This method operates in <em>stealth mode</em>, see doc for {@link #handleClosedChannel handleClosedChannel}. This presumes that IO retry happens at a higher level, i.e. the caller(s) must be prepared to handle {@code InterruptDetectedException}. <p/> This method overrides FileContainer#writeAtOffset. <p/> Attempts to write buf completely from start until end, at the given position in the destination fileChannel. <p/> FileChannel has no writeFull() method, so we roll our own. <p/> Write a page from the supplied array. <p/> override of RAFContainer#writePage <p/> <BR> MT - thread safe
************************************************************************ Public Methods implementing ModuleControl Interface: ************************************************************************* Query property system to get the System lock level. <p> This routine will be called during boot after access has booted far enough, to allow access to the property conglomerate.  This routine will call the property system and set the value to be returned by getSystemLockLevel(). <p> Privileged startup. Must be private so that user code can't call this entry point. Privileged startup. Must be private so that user code can't call this entry point. Add a newly created conglomerate to the cache. <p> package Find a conglomerate by conglomid in the cache. <p> Look for a conglomerate given a conglomid.  If in cache return it, otherwise fault in an entry by asking the owning factory to produce an entry. <p> package ************************************************************************ Conglomerate Cache routines: ************************************************************************* ACCESSMANAGER CONGLOMERATE CACHE - <p> Every conglomerate in the system is described by an object which implements Conglomerate.  This object basically contains the parameters which describe the metadata about the conglomerate that store needs to know - like types of columns, number of keys, number of columns, ... <p> It is up to each conglomerate to maintain it's own description, and it's factory must be able to read this info from disk and return it from the ConglomerateFactory.readConglomerate() interface. <p> This cache simply maintains an in memory copy of these conglomerate objects, key'd by conglomerate id.  By caching, this avoids the cost of reading the conglomerate info from disk on each subsequent query which accesses the conglomerate. <p> The interfaces and internal routines which deal with this cache are: conglomCacheInit() - initializes the cache at boot time. Initialize the conglomerate cache. <p> Simply calls the cache manager to create the cache with some hard coded defaults for size. <p> Invalide the current Conglomerate Cache. <p> Abort of certain operations will invalidate the contents of the cache.  Longer term we could just invalidate those entries, but for now just invalidate the whole cache. <p> package Remove an entry from the cache. <p> package ************************************************************************ Public Methods implementing AccessFactory Interface: ************************************************************************* Database creation finished.  Tell RawStore. DERBY-5996(Create readme files (cautioning users against modifying database files) at database hard upgrade time) This gets called during hard upgrade. It will create 3 readme files one in database directory, one in "seg0" directory and one in log directory. These readme files warn users against touching any of files associated with derby database  Find an access method that implements a format type. Find an access method that implements an implementation type. Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of a Context. Must be private so that user code can't call this entry point. /////////////////////////////////////////////////////////////// Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. <p> Get the current transaction context. </p> <p> If there is an internal transaction on the context stack, return the internal transaction. Otherwise, if there is a nested user transaction on the context stack, return the nested transaction. Otherwise, return the current user transaction. </p> ************************************************************************ Private/Protected methods of This class: ************************************************************************* Return the default locking policy for this access manager. Bump the conglomid. <p> For some reason we have found that the give conglomid already exists in the directory so just bump the next conglomid to greater than this one.  The algorithm to store and retrieve the last conglomid is not transactional as we don't want to pay the overhead for such an algorithm on every ddl statement - so it is possible to "lose" an update to the counter if we crash at an inopportune moment.  In general the upper level store code will just handle the error from addContainer which says there already exists a conglom with that id, update the next conglomid and then try again. <p> currently not used, but this is one idea on how to handle non-transactional update of the nextid field, just handle the error if we try to create a conglom and find the container already exists. private void handleConglomidExists( long   conglomid) throws StandardException { synchronized (conglom_cache) { conglom_nextid = ((conglomid >> 4) + 1); } } Given a conglomid, return the factory which "owns" it. <p> A simple lookup on the boot time built table which maps the low order 4 bits into which factory owns the conglomerate. <p> Privileged Monitor lookup. Must be private so that user code can't call this entry point. Return next conglomid to try to add the container with. <p> The conglomerate number has 2 parts.  The low 4 bits are used to encode the factory which "owns" the conglomerate.  The high 60 bits are used as a normal unique id mechanism. <p> So for example if the next id to assign is 0x54 the following will be the conglomid: if a HEAP  (factory 0) - 0x540 if a BTREE (factory 1) - 0x541 And the next id assigned will be: if a HEAP  (factory 0) - 0x550 if a BTREE (factory 1) - 0x551 ************************************************************************* * Abstract Methods of RAMAccessManager, interfaces that control locking * level of the system. *************************************************************************** Return the locking level of the system. <p> This routine controls the lowest level of locking enabled for all locks for all tables accessed through this accessmanager.  The concrete implementation may set this value always to table level locking for a client configuration, or it may set it to row level locking for a server configuration. <p> If TransactionController.MODE_RECORD is returned table may either be locked at table or row locking depending on the type of access expected (ie. level 3 will require table locking for heap scans.) Return the XAResourceManager associated with this AccessFactory. <p> Returns an object which can be used to implement the "offline" 2 phase commit interaction between the accessfactory and outstanding transaction managers taking care of in-doubt transactions. XAResourceManager Methods of the PropertySetCallback interface This interface is implemented to ensure the user cannot change the encryption provider or algorithm. /////////////////////////////////////////////////////////////// * CacheableFactory interface Start the replication master role for this database. Privileged startup. Must be private so that user code can't call this entry point. Start a global transaction. <p> Get a transaction controller with which to manipulate data within the access manager.  Implicitly creates an access context. <p> Must only be called if no other transaction context exists in the current context manager.  If another transaction exists in the context an exception will be thrown. <p> The (format_id, global_id, branch_id) triplet is meant to come exactly from a javax.transaction.xa.Xid.  We don't use Xid so that the system can be delivered on a non-1.2 vm system and not require the javax classes in the path. XATransactionController Stop the replication master role for this database.
************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* Interface to be called when an undo of an insert is processed. <p> Implementer of this class provides interface to be called by the raw store when an undo of an insert is processed.  Initial implementation will be by Access layer to queue space reclaiming events if necessary when a rows is logically "deleted" as part of undo of the original insert.  This undo can happen a lot for many applications if they generate expected and handled duplicate key errors. <p> It may be useful at some time to include the recordId of the deleted row, but it is not used currently by those notified.  The post commit work ultimately processes all rows on the table while it has the latch which is more efficient than one row at time per latch. <p> It is expected that notifies only happen for pages that caller is interested in.  Currently only the following aborted inserts cause a notify: o must be on a non overflow page o if all "user" rows on page are deleted a notify happens (page 1 has a system row so on page one notifies happen if all but the first row is deleted). o if the aborted insert row has either an overflow row or column component then the notify is executed.
************************************************************************ Public Methods of TransactionController interface: ************************************************************************* Add a column to a conglomerate.  The conglomerate must not be open in the current transaction.  This also means that there must not be any active scans on it. The column can only be added at the spot just after the current set of columns. The template_column must be nullable. After this call has been made, all fetches of this column from rows that existed in the table prior to this call will return "null". Add to the list of post commit work. <p> Add to the list of post commit work that may be processed after this transaction commits.  If this transaction aborts, then the post commit work list will be thrown away.  No post commit work will be taken out on a rollback to save point. <p> This routine simply delegates the work to the Rawstore transaction. Check to see if a database has been upgraded to the required level in order to use a store feature. ************************************************************************ Private/Protected methods of This class: ************************************************************************* XXX (nat) currently closes all controllers. The ConglomerateController.close() method has been called on "conglom_control". <p> Take whatever cleanup action is appropriate to a closed conglomerateController.  It is likely this routine will remove references to the ConglomerateController object that it was maintaining for cleanup purposes. The SortController.close() method has been called on "sort_control". <p> Take whatever cleanup action is appropriate to a closed sortController.  It is likely this routine will remove references to the SortController object that it was maintaining for cleanup purposes. The ScanManager.close() method has been called on "scan". <p> Take whatever cleanup action is appropriate to a closed scan.  It is likely this routine will remove references to the scan object that it was maintaining for cleanup purposes. Return free space from the conglomerate back to the OS. <p> Returns free space from the conglomerate back to the OS.  Currently only the sequential free pages at the "end" of the conglomerate can be returned to the OS. <p> Report on the number of open conglomerates in the transaction. <p> There are 4 types of open "conglomerates" that can be tracked, those opened by each of the following: openConglomerate(), openScan(), openSort(), and openSortScan().  This routine can be used to either report on the number of all opens, or may be used to track one particular type of open. This routine is expected to be used for debugging only.  An implementation may only track this info under SanityManager.DEBUG mode. If the implementation does not track the info it will return -1 (so code using this call to verify that no congloms are open should check for return &lt;= 0 rather than == 0). The return value depends on the "which_to_count" parameter as follows: OPEN_CONGLOMERATE  - return # of openConglomerate() calls not close()'d. OPEN_SCAN          - return # of openScan() calls not close()'d. OPEN_CREATED_SORTS - return # of sorts created (createSort()) in current xact.  There is currently no way to get rid of these sorts before end of transaction. OPEN_SORT          - return # of openSort() calls not close()'d. OPEN_TOTAL         - return total # of all above calls not close()'d. - note an implementation may return -1 if it does not track the above information. Create a conglomerate and populate it with rows from rowSource. Create a BackingStoreHashtable which contains all rows that qualify for the described scan. Create a new conglomerate. <p>  Convert a local transaction to a global transaction. <p> Get a transaction controller with which to manipulate data within the access manager.  Tbis controller allows one to manipulate a global XA conforming transaction. <p> Must only be called a previous local transaction was created and exists in the context.  Can only be called if the current transaction is in the idle state.  Upon return from this call the old tc will be unusable, and all references to it should be dropped (it will have been implicitly destroy()'d by this call. <p> The (format_id, global_id, branch_id) triplet is meant to come exactly from a javax.transaction.xa.Xid.  We don't use Xid so that the system can be delivered on a non-1.2 vm system and not require the javax classes in the path. XATransactionController Return a string with debug information about opened congloms/scans/sorts. <p> Return a string with debugging information about current opened congloms/scans/sorts which have not been close()'d. Calls to this routine are only valid under code which is conditional on SanityManager.DEBUG. <p> Compress table in place. <p> Returns a GroupFetchScanController which can be used to move rows around in a table, creating a block of free pages at the end of the table.  The process will move rows from the end of the table toward the beginning.  The GroupFetchScanController will return the old row location, the new row location, and the actual data of any row moved.  Note that this scan only returns moved rows, not an entire set of rows, the scan is designed specifically to be used by either explicit user call of the SYSCS_ONLINE_COMPRESS_TABLE() procedure, or internal background calls to compress the table. The old and new row locations are returned so that the caller can update any indexes necessary. This scan always returns all collumns of the row. All inputs work exactly as in openScan().  The return is a GroupFetchScanController, which only allows fetches of groups of rows from the conglomerate. <p> Determine correct locking policy for a conglomerate open. <p> Determine from the following table whether to table or record lock the conglomerate we are opening. <p> System level override ------------------------------- user requests                       table locking    record locking -------------                       -------------    -------------- TransactionController.MODE_TABLE     TABLE             TABLE TransactionController.MODE_RECORD    TABLE             RECORD Drop a sort. <p> Drop a sort created by a call to createSort() within the current transaction (sorts are automatically "dropped" at the end of a transaction.  This call should only be made after all openSortScan()'s and openSort()'s have been closed. <p> Retrieve the maximum value row in an ordered conglomerate. <p> Returns true and fetches the rightmost row of an ordered conglomerate into "fetchRow" if there is at least one row in the conglomerate.  If there are no rows in the conglomerate it returns false. <p> Non-ordered conglomerates will not implement this interface, calls will generate a StandardException. <p> RESOLVE - this interface is temporary, long term equivalent (and more) functionality will be provided by the openBackwardScan() interface. ************************************************************************ Public Methods of TransactionManager interface: ************************************************************************* Return existing Conglomerate after doing lookup by ContainerKey <p> Throws exception if it can't find a matching conglomerate for the ContainerKey. Get reference to access factory which started this transaction. <p> Get string id of the transaction that would be when the Transaction is IN active state. Get the context manager that the transaction was created with. <p> Return dynamic information about the conglomerate to be dynamically reused in repeated execution of a statement. <p> The dynamic info is a set of variables to be used in a given ScanController or ConglomerateController.  It can only be used in one controller at a time.  It is up to the caller to insure the correct thread access to this info.  The type of info in this is a scratch template for btree traversal, other scratch variables for qualifier evaluation, ... <p> Get an Internal transaction. <p> Start an internal transaction.  An internal transaction is a completely separate transaction from the current user transaction.  All work done in the internal transaction must be physical (ie. it can be undone physically by the rawstore at the page level, rather than logically undone like btree insert/delete operations).  The rawstore guarantee's that in the case of a system failure all open Internal transactions are first undone in reverse order, and then other transactions are undone in reverse order. <p> Internal transactions are meant to implement operations which, if interupted before completion will cause logical operations like tree searches to fail.  This special undo order insures that the state of the tree is restored to a consistent state before any logical undo operation which may need to search the tree is performed. <p> Return an object that when used as the compatibility space, <strong>and</strong> the object returned when calling <code>getOwner()</code> on that object is used as group for a lock request, guarantees that the lock will be removed on a commit or an abort.    Get the Transaction from the Transaction manager. <p> Access methods often need direct access to the "Transaction" - ie. the raw store transaction, so give access to it. Return static information about the conglomerate to be included in a a compiled plan. <p> The static info would be valid until any ddl was executed on the conglomid, and would be up to the caller to throw away when that happened.  This ties in with what language already does for other invalidation of static info.  The type of info in this would be containerid and array of format id's from which templates can be created. The info in this object is read only and can be shared among as many threads as necessary. <p> Get string id of the transaction. <p> This transaction "name" will be the same id which is returned in the TransactionInfo information, used by the lock and transaction vti's to identify transactions. <p> Although implementation specific, the transaction id is usually a number which is bumped every time a commit or abort is issued. <p> For now return the toString() method, which does what we want.  Later if that is not good enough we can add public raw tran interfaces to get exactly what we want. A superset of properties that "users" can specify. <p> A superset of properties that "users" (ie. from sql) can specify.  Store may implement other properties which should not be specified by users. Layers above access may implement properties which are not known at all to Access. <p> This list is a superset, as some properties may not be implemented by certain types of conglomerates.  For instant an in-memory store may not implement a pageSize property.  Or some conglomerates may not support pre-allocation. <p> This interface is meant to be used by the SQL parser to do validation of properties passsed to the create table statement, and also by the various user interfaces which present table information back to the user. <p> Currently this routine returns the following list: derby.storage.initialPages derby.storage.minimumRecordSize derby.storage.pageReservedSpace derby.storage.pageSize ************************************************************************ Constructors for This class: ************************************************************************* Invalidate the conglomerate cache, if necessary.  If an alter table call has been made then invalidate the cache. Reveals whether the transaction is a global or local transaction. Reveals whether the transaction has ever read or written data. Reveals whether the transaction is currently pristine. Bulk load into the conglomerate.  Rows being loaded into the conglomerate are not logged. Use this for incremental load in the future. Log an operation and then action it in the context of this transaction. <p> This simply passes the operation to the RawStore which logs and does it. <p>  Return an open SortCostController. <p> Return an open SortCostController which can be used to ask about the estimated costs of SortController() operations. <p>   Return an open StoreCostController for the given conglomid. <p> Return an open StoreCostController which can be used to ask about the estimated row counts and costs of ScanController and ConglomerateController operations, on the given conglomerate. <p>  Purge all committed deleted rows from the conglomerate. <p> This call will purge committed deleted rows from the conglomerate, that space will be available for future inserts into the conglomerate. <p> recreate a conglomerate and populate it with rows from rowSource. {@inheritDoc } <p> For now, this only works if the transaction has its own compatibility space. If it has inherited the compatibility space from its parent, the request will be ignored (or cause a failure in debug builds).   Get an nested user transaction. <p> A nested user can be used exactly as any other TransactionController, except as follows.  For this discussion let the parent transaction be the transaction used to make the getNestedUserTransaction(), and let the child transaction be the transaction returned by the getNestedUserTransaction() call. <p> The nesting is limited to one level deep.  An exception will be thrown if a subsequent getNestedUserTransaction() is called on the child transaction. <p> The locks in the child transaction will be compatible with the locks of the parent transaction. <p> A commit in the child transaction will release locks associated with the child transaction only, work can continue in the parent transaction at this point. <p> Any abort of the child transaction will result in an abort of both the child transaction and parent transaction. <p> A TransactionController.destroy() call should be made on the child transaction once all child work is done, and the caller wishes to continue work in the parent transaction. <p> Nested internal transactions are meant to be used to implement system work necessary to commit as part of implementing a user's request, but where holding the lock for the duration of the user transaction is not acceptable.  2 examples of this are system catalog read locks accumulated while compiling a plan, and auto-increment. <p> ************************************************************************ Public Methods implementing the XATransactionController interface. ************************************************************************* This method is called to commit the current XA global transaction. <p> RESOLVE - how do we map to the "right" XAExceptions. <p> This method is called to ask the resource manager to prepare for a transaction commit of the transaction specified in xid. <p> rollback the current global transaction. <p> The given transaction is roll'ed back and it's history is not maintained in the transaction table or long term log. <p>
* Context methods (most are implemented by super-class). Handle cleanup processing for this context. The resources associated with a transaction are the open controllers. Cleanup involves closing them at the appropriate time. Rollback of the underlying transaction is handled by the raw store. package package
File deletion is a quick operation and typically releases substantial amount of space very quickly, this work should be done on the user thread.
* Returns true if any of the foreign keys are null * otherwise, false. Perform the check. If deferred constraint mode, the numbers of failed rows returned will be always be 0 (but any violating keys will have been saved for later checking). Use bulk fetch to get the next set of rows, or read the next out of our internal array. Use bulk fetch to get the next set of rows, or read the next out of our internal array.
clean up Check that everything in the row is ok, i.e. that there are no foreign keys in the passed in row that have invalid values. Check that there are no referenced primary keys in the passed in row.  So for each foreign key that references a primary key constraint, make sure that there is no row that matches the values in the passed in row. Execute the specific RI check on the passed in row. Do any work needed to reopen our ri checkers for another round of checks.  Must do a close() first.
/////////////////////////////////////////////////////////////////////////////////  TABLE FUNCTION  ///////////////////////////////////////////////////////////////////////////////// <p> This is the method which is registered as a table function. </p> /////////////////////////////////////////////////////////////////////////////////  IMPLEMENTATIONS OF ABSTRACT METHODS  /////////////////////////////////////////////////////////////////////////////////
If drop is true, drop the container.  if drop is false, un-drop the container Creates an encrypted or decrypted version of the container. Return a Page that represents any page - alloc page, valid page, free page, dealloced page etc. Return the status of the container - one of NORMAL, DROPPED, COMMITTED_DROP. Get the logged container version Log all information necessary to recreate the container during a load tran. The container is about to be modified. Loggable actions use this to make sure the container gets cleaned if a checkpoint is taken after any log record is sent to the log stream but before the container is actually dirtied. Backup restore support ReCreate a page for redo recovery. Used during redo recovery while trying to apply log records which are creating the page. Remove the container.
Return a "bad args" exception //////////////////////////////////////////////////////////////////////  LOADING MINIONS  ////////////////////////////////////////////////////////////////////// Create and populate the schema which holds control objects. Create a table function and view for a corrupt database table. Create user schemas. Create the table function and view for a single corrupt table. Add statements to the recovery script for siphoning data out of the corrupt database into the healthy database. Create table functions and views on corrupt user tables. These objects are created in the healthy database. Write the recovery script. //////////////////////////////////////////////////////////////////////  SQL MINIONS  ////////////////////////////////////////////////////////////////////// Drop the schema which holds the table functions and views on the corrupt core conglomerates. Drop the table function and view for a catalog in the corrupt database. Drop the now empty schemas which held the table functions and views on conglomerates in the corrupt database. //////////////////////////////////////////////////////////////////////  UNLOADING MINIONS  ////////////////////////////////////////////////////////////////////// Drop the table functions and views on the corrupt user tables. //////////////////////////////////////////////////////////////////////  Connection MANAGEMENT  ////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  OptionalTool BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Creates the following schema objects in a healthy database in order to siphon data out of a corrupt database: </p> <ul> <li>A control schema containing views on SYSSCHEMAS, SYSCONGLOMERATES, SYSTABLES and SYSCOLUMNS in the corrupt database.</li> <li>A schema for every user schema in the corrupt database.</li> <li>Table functions and views on every user table in the corrupt database.</li> </ul> <p> In addition, the tool creates a script for siphoning data out of the corrupt database. </p> <p> Takes the following arguments: </p> <ul> <li>recoveryScript (required) - Name of the recovery script file which the tool will write.</li> <li>controlSchema (required) - Name of a schema in which control objects will be created. These include the views on the corrupt database's SYSSCHEMAS, SYSCONGLOMERATES, SYSTABLES, and SYSCOLUMNS catalogs. May not be null or an empty string.</li> <li>schemaPrefix (required) - This prefix is prepended to the names of the schemas which are created in the healthy database. May not be null or empty.</li> <li>corruptDBLocation (required) - Absolute path to the corrupt database directory. That is the directory which contains service.properties. May not be null or empty.</li> <li>encryptionAttributes (required) - Encryption attributes which were used to connect to the corrupt database when it was bootable. May be null if encryption is not being used.</li> <li>dbo (required) - User name of the owner of the corrupt database. May be null if authentication is not being used.</li> <li>dboPassword (required) - Password for the owner of the corrupt database. May be null if authentication is not being used.</li> </ul> Make the name of a local schema from a prefix and the name of a corrupt schema Make the signature of a table from its column names and types Use dblook methods to normalize the name of a column Returns true if the text is null or empty Return "null" if string is null, otherwise single quote it <p> Removes the schemas, table functions, and views created by loadTool(). </p> <p> Takes the following arguments: </p> <ul> <li>controlSchema (required) - Name of the schema in which loadTool() created control objects in the healthy database. May not be null or an empty string.</li> <li>schemaPrefix (required) - This is the prefix which was prepended to the names of the schemas which loadTool() created in the healthy database. May not be null or empty.</li> </ul>

Configure the database for encryption, with the  specified encryption  properties. Basic idea is to encrypt all the containers with new password/key specified by the user and keep old versions of the data to rollback the database to the state before the configuration of database with new encryption attributes. Users can configure the database with new encryption  attributes at boot time only; advantage of this approach is that there will not be any concurrency issues to handle because no users will be modifying the data. First step is to encrypt the existing data with new encryption attributes  and then update the encryption properties for the database. Configuring  an un-encrypted database for encryption problem is a minor variation of  re-encrypting an encrypted database with new encryption key. The database reconfiguration with new encryption attributes is done under one transaction, if there is a crash/error before it is committed, then it  is rolled back and the database will be brought back to the state it was before the encryption. One trickey case in (re) encrypion of database is unlike standard protocol transaction  commit means all done, database (re) encryption process has to perform a checkpoint with a newly generated key then only database  (re) encrption is complete, Otherwise the problem  is recovery has to deal with transaction log that is encrypted with old encryption key and the new encryption key. This probelm is avoided  writing COMMIT and new  CHECKPOINT log record  to a new log file and encrypt the with a new key, if there is  crash before checkpoint records are updated , then on next boot the log file after the checkpoint is deleted before reovery,  which will be the one that is written with new encryption key and also contains COMMIT record, so the COMMIT record is also gone when  log file is deleted. Recovery will not see the commit , so it will  rollback the (re) encryption and revert all the containers to the original versions. Old container versions are deleted only when the check point with new encryption key is successful, not on post-commit. @param properties  properties related to this database. @exception StandardException Standard Derby Error Policy Backup the database to a backup directory. Backup the database. Online backup copies all the database files (log, seg0  ...Etc) to the specified backup location without blocking any user operation for the duration of the backup. Stable copy is made of each page using page level latches and in some cases with the help of monitors. Transaction log is also backed up, this is used to bring the database to the consistent state on restore. <P> MT- only one thread  is allowed to perform backup at any given time. Synchronized on this. Parallel backups are not supported. Backup the database to a backup directory and enable the log archive mode that will keep the archived log files required for roll-forward from this version backup. Privileged startup. Must be private so that user code can't call this entry point. * Methods of ModuleControl We use this RawStore for all databases. Check to see if a database has been upgraded to the required level in order to use a store feature. when the input debug flag is set, an expception is throw when run in the debug mode. Create a directory for backup.  checks if the database is in the right state to (re)encrypt it. Decrypt cleartext from ciphertext. Disable the log archive mode and delete the archived log files if requested. @param deleteOnlineArchivedLogFiles If true deletes online archived log files that exist before this backup, delete will occur  only after the backup is  complete. @exception StandardException thrown on error. Encrypt cleartext into ciphertext.  Privileged startup. Must be private so that user code can't call this entry point. * backup restore Freeze persistent store.  Reads can still happen, only cannot write. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Private so that user code can't call this entry point. Returns the encryption block size used by the algorithm at time of creation of an encrypted database Get the file path.  If the canonical path can be obtained then return the canonical path, otherwise just return the abstract path. Typically if there are no permission to read user.dir when  running under security manager canonical path can not be obtained. This method is used to a write a file path name to error/status log file, where it would be nice to print full paths but not esstential that the user grant permissions to read user.dir property. Get the file path. If the canonical path can be obtained then return the canonical path, otherwise just return the abstract path. Typically if there are no permission to read user.dir when  running under security manager canonical path can not be obtained. This method is used to a write path name to error/status log file, where it would be nice to print full paths but not esstential that the user grant permissions to read user.dir property. Return an id which can be used to create a container. <p> Return an id number with is greater than any existing container in the current database.  Caller will use this to allocate future container numbers - most likely caching the value and then incrementing it as it is used. <p> Privileged Monitor lookup. Must be private so that user code can't call this entry point. Get JBMS properties relevant to raw store Privileged service name lookup. Must be private so that user code can't call this entry point. class specific methods subclass can override this method to load different submodules Return the module providing XAresource interface to the transaction table. @exception StandardException Standard Derby exception policy. XAResourceManager Get the Transaction Factory to use with this store. Engine might have crashed during encryption of un-encrypted datbase or while re-encryptin an already encrypted database with a new key after all the containers or (re) encrypted. If crash has occured before all containers are encrypted, recovery wil un-do re-encryption using the transaction log, nothing to be done here. If crash has occured after database encryption status flag (RawStoreFactory.DB_ENCRYPTION_STATUS) is set, this method will do any cleanup necessary for the recovery to correctly perform the rollback if required. * Methods of RawStoreFactory Is the store read-only. Tells if the attribute/property has been specified. Tells if the attribute/property has been set to {@code true}. * These methods require Priv Blocks when run under a security manager. Removes properties related to encrypted databases. Restore any remaining files from backup that are not restored by the individual factories. 1) copy jar files from backup.. 2) copy backup history file. PrivilegedExceptionAction method end of run Register a handler class for insert undo events. <p> Register a class to be called when an undo of an insert is executed. When an undo of an event is executed by the raw store UndoHandler.insertUndoNotify() will be called, allowing upper level callers to execute code as necessary.  The initial need is for the access layer to be able to queue post commit reclaim space in the case of inserts which are aborted (including the normal case of inserts failed for duplicate key violations) (see DERBY-4057) <p> * data encryption/decryption support Setup encryption engines according to the user properties and the current database state. Start the replication master role for this database Privileged startup. Must be private so that user code can't call this entry point. Stop the replication master role for this database. Freeze persistent store.  Reads can still happen, only cannot write.
Backup the database to backupDir. <P>Please see Derby on line documentation on backup and restore. Backup the database to a backup directory and enable the log archive mode that will keep the archived log files required for roll-forward from this version backup. Change the boot password.  Return the encrypted form of the secret key. The new value must be a String of the form: oldBootPassword, newBootPassword Check to see if a database has been upgraded to the required level in order to use a store feature. Checkpoint the database. The raw store will wait for any current checkpoints to complete.  It will start a new checkpoint and not return until that checkpoint completes. Use the available storage factory handle to create a readme file in "seg0" directory warning users to not edit/delete any files in the directory to avoid database corruption. the database creation phase is finished @exception StandardException Standard Derby exception policy. Decrypt cleartext from ciphertext. disables the log archival process, i.e No old log files will be kept around for a roll-forward recovery. Encrypt cleartext into ciphertext. Start failover for this database. Find a user transaction in the context manager, which must be the current context manager.  If a user transaction does not already exist, then create one @see #startTransaction Freeze the database temporarily so a backup can be taken. <P>Please see Derby on line documentation on backup and restore. Backup / restore support Freeze the database from altering any persistent storage. If this raw store has a daemon that services its need, return the daemon.  If not, return null return the data factory module Returns the encryption block size used during creation of the encrypted database Get the LockFactory to use with this store. return the Log factory module Return an id which can be used to create a container. <p> Return an id number with is greater than any existing container in the current database.  Caller will use this to allocate future container numbers - most likely caching the value and then incrementing it as it is used. <p> Get JBMS properties relevant to raw store return the transaction factory module  Return the module providing XAresource interface to the transaction table. @exception StandardException Standard Derby exception policy. XAResourceManager Get the Transaction Factory to use with this store. Idle the raw store as much as possible. Is the store read-only. Get a flushed scan. Returns a secure random number for this raw store - if database is not encrypted, returns 0. Register a handler class for insert undo events. <P> Register a class to be called when an undo of an insert is executed.  When an undo of an event is executed by the raw store UndoHandler.insertUndoNotify() will be called, allowing upper level callers to execute code as necessary.  The initial need is for the access layer to be able to queue post commit reclaim space in the case of inserts which are aborted (including the normal case of inserts failed for duplicate key violations) (see DERBY-4057) Create a global user transaction, almost all work within the raw store is performed in the context of a transaction. <P> The (format_id, global_id, branch_id) triplet is meant to come exactly from a javax.transaction.xa.Xid.  We don't use Xid so that the system can be delivered on a non-1.2 vm system and not require the javax classes in the path. <P> Starting a transaction always performs the following steps. <OL> <LI>Create an raw store transaction context <LI>Create a new idle transaction and then link it to the context. </UL> Only one user transaction can be active in a context at any one time. After a commit the transaction may be re-used. <P> <B>Raw Store Transaction Context Behaviour</B> <BR> The cleanupOnError() method of this context behaves as follows: <UL> <LI> If error is an instance of StandardException that has a severity less than ExceptionSeverity.TRANSACTION_SEVERITY then no action is taken. <LI> If error is an instance of StandardException that has a severity equal to ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, and the transaction returned to the idle state. <LI> If error is an instance of StandardException that has a severity greater than  ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. <LI> If error is not an instance of StandardException then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. </UL> Create an internal transaction. <P> Starting an internal transaction always performs the following steps. <OL> <LI>Create an raw store internal transaction context <LI>Create a new idle internal transaction and then link it to the context. </UL> <P> AN internal transaction is identical to a user transaction with the exception that <UL> <LI> Logical operations are not supported <LI> Savepoints are not supported <LI> Containers are not closed when commit() is called. <LI> Pages are not unlatched (since containers are not closed) when commit() is called. <LI> During recovery time internal transactions are rolled back before user transactions. </UL> Only one internal transaction can be active in a context at any one time. After a commit the transaction may be re-used. <P> <B>Raw Store Internal Transaction Context Behaviour</B> <BR> The cleanupOnError() method of this context behaves as follows: <UL> <LI> If error is an instance of StandardException that has a severity less than ExceptionSeverity.TRANSACTION_SEVERITY then the internal transaction is aborted, the internal transaction is closed,        the context is popped off the stack, and an exception of severity Transaction exception is re-thrown. <LI> If error is an instance of StandardException that has a severity greater than or equal to ExceptionSeverity.TRANSACTION_SEVERITY then the context's internal transaction is aborted, the internal transaction is closed and the context is popped off the stack. <LI> If error is not an instance of StandardException then the context's internal transaction is aborted, the internal transaction is closed and the context is popped off the stack. </UL> Create a nested user transaction, almost all work within the raw store is performed in the context of a transaction. <P> A nested user transaction is exactly the same as a user transaction, except that one can specify a compatibility space to associate with the transaction. Starting a transaction always performs the following steps. <OL> <LI>Create an raw store transaction context <LI>Create a new idle transaction and then link it to the context. </UL> Only one user transaction and one nested user transaction can be active in a context at any one time. After a commit the transaction may be re-used. <P> <B>Raw Store Transaction Context Behaviour</B> <BR> The cleanupOnError() method of this context behaves as follows: <UL> <LI> If error is an instance of StandardException that has a severity less than ExceptionSeverity.TRANSACTION_SEVERITY then no action is taken. <LI> If error is an instance of StandardException that has a severity equal to ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, and the transaction returned to the idle state.  If a user transaction exists on the context stack then that transaction is aborted also. <LI> If error is an instance of StandardException that has a severity greater than  ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. <LI> If error is not an instance of StandardException then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. </UL> Create a nested user transaction, almost all work within the raw store is performed in the context of a transaction. <P> A nested user transaction is exactly the same as a user transaction, except that one can specify a compatibility space to associate with the transaction. Starting a transaction always performs the following steps. <OL> <LI>Create an raw store transaction context <LI>Create a new idle transaction and then link it to the context. </UL> Only one user transaction and one nested user transaction can be active in a context at any one time. After a commit the transaction may be re-used. <P> <B>Raw Store Transaction Context Behaviour</B> <BR> The cleanupOnError() method of this context behaves as follows: <UL> <LI> If error is an instance of StandardException that has a severity less than ExceptionSeverity.TRANSACTION_SEVERITY then no action is taken. <LI> If error is an instance of StandardException that has a severity equal to ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, and the transaction returned to the idle state.  If a user transaction exists on the context stack then that transaction is aborted also. <LI> If error is an instance of StandardException that has a severity greater than  ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. <LI> If error is not an instance of StandardException then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. </UL> Start the replication master role for this database Create a user transaction, almost all work within the raw store is performed in the context of a transaction. <P> Starting a transaction always performs the following steps. <OL> <LI>Create an raw store transaction context <LI>Create a new idle transaction and then link it to the context. </UL> Only one user transaction and one nested user transaction can be active in a context at any one time. After a commit the transaction may be re-used. <P> <B>Raw Store Transaction Context Behaviour</B> <BR> The cleanupOnError() method of this context behaves as follows: <UL> <LI> If error is an instance of StandardException that has a severity less than ExceptionSeverity.TRANSACTION_SEVERITY then no action is taken. <LI> If error is an instance of StandardException that has a severity equal to ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, and the transaction returned to the idle state. <LI> If error is an instance of StandardException that has a severity greater than  ExceptionSeverity.TRANSACTION_SEVERITY then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. <LI> If error is not an instance of StandardException then the context's transaction is aborted, the transaction closed, and the context is popped off the stack. </UL> Stop the replication master role for this database. Unfreeze the database after a backup has been taken. <P>Please see Derby on line documentation on backup and restore. Unfreeze the database, persistent storage can now be altered.
JDBC 3.0 (from tutorial book) requires that an input stream has the correct number of bytes in the stream. Read from the wrapped stream prepending the intial bytes if needed. If stream has been read, and eof reached, in that case any subsequent read will throw an EOFException Read from the wrapped stream prepending the intial bytes if needed. If stream has been read, and eof reached, in that case any subsequent read will throw an EOFException
Add this raw transaction on to the list of update transaction Make the transaction block the online backup. Check to see if a logical operation is allowed by this transaction, throws a TransactionExceotion if it isn't. This implementation allows logical operations. Transactions that need to disallow logical operations should hide this method. Redo a checkpoint during rollforward recovery. Get the data factory to be used during this transaction. Get the log instant for the first log record written by this transaction. Get the shortId of this transaction.  May return null if transactio has no ID. Get the shortId of this transaction.  May return null if transactio has no ID. Get the log instant for the last log record written by this transaction. If the transaction is unclear what its last log instant is, than it may return null. Get the lock factory to be used during this transaction. Get the log buffer to be used during this transaction. Get the log factory to be used during this transaction. Can this transaction handles post termination work Is the transaction in the middle of an abort. Retunrs true if the transaction is part of rollforward recovery Check if the transaction is blocking the backup ? Log a compensation operation and then action it in the context of this transaction. The CompensationOperation is logged in the transaction log file and then its doMe method is called to perform the required change.  This compensation operation will rollback the change that was done by the Loggable Operation at undoInstant. Allow my users to notigy my observers. Open a container that may be dropped - use only by logging and recovery. During recovery redo, a log record may refer to a container that has long been dropped.  This interface is provided so a dropped container may be opened. If the container has been dropped and is known to be committed, then even if we open the dropped container with forUpdate true, the container will be silently opened as read only.  Logging and recovery code always check for committed drop status.  Anybody else wanting to use this interface must keep this in mind. Change the state of transaction in table to prepare. Recreate a container during redo recovery. Used during redo recovery when processing log records trying to create a container, but no container is found in the db. Return true if this transaction should be rolled back first in recovery. This implementation returns false. Transactions that need to rollback first during recovery should hide this method. Make this transaction aware that it is being used by recovery Remove this raw transaction from the list of update transaction During recovery re-prepare a transaction. <p> After redo() and undo(), this routine is called on all outstanding in-doubt (prepared) transactions.  This routine re-acquires all logical write locks for operations in the xact, and then modifies the transaction table entry to make the transaction look as if it had just been prepared following startup after recovery. <p> Set the log instant for the first log record written by this transaction. Set the log instant for the last log record written by this transaction. Allow an Observer to indicate an exception to the transaction that is raised in its update() method. Methods to help logging and recovery Set the transactionId (Global and internal) of this transaction using a log record that contains the Global id Start a nested top transaction. A nested top transaction behaves exactly like a user transaction. Nested top transaction allow system type work to proceed in a separate transaction to the current user transaction and be committed independently of the user transaction (usually before the user transaction). Only one nested top transaction can be active in a context at any one time. After a commit the transaction may be re-used. A nested top transaction conflicts on the logical locks of its "parent" transaction. Status that needs to go into the begin transaction log record, if there is one, to help with recovery Status that needs to go into the end transaction log record, if there is one, to help with recovery
This method in java.lang.Object was deprecated as of build 167 of JDK 9. See DERBY-6932.
reclaim locks associated with the changes in this log record. <p>

Log backup is not started for for read only databases, no work to do here. * Methods of ModuleControl Check to see if a database has been upgraded to the required level in order to use a store feature. This method is generally used to prevent writes to data/log file by a particular store feature until the database is upgraded to the required version. In read-only database writes are not allowed, so nothing to do for this method in this implementation of the log factory. MT - not needed, no work is done Perform a checkpoint during rollforward recovery. Not applicable in readonly databases delete the log file after the checkpoint. <P>MT - synchronization provided by caller - RawStore boot, This method is called only if a crash occured while re-encrypting the database at boot time. Read-only database can not be re-encrypted, nothing to do in this case. this function is suppose to delete all the logs before this call that are not active logs. roll-forward recovery support routines Nothing to be done for read only databases There are no log files to backup for read only databases, nothing to be done here. @param toDir - location where the log files should be copied to. @exception StandardException Standard Derby error policy Backup restore - stop sending log record to the log stream  Get JBMS properties relevant to the log factory * Methods of Log Factory  Is the transaction in rollforward recovery Replication not applicable on readonly databases. find if the checkpoint is in the last log file. <P>MT - synchronization provided by caller - RawStore boot, This method is called only if a crash occured while re-encrypting the database at boot time. Read-only database can not be re-encrypted, nothing to do in this case. Backup restore - is the log being archived to some directory? if RawStore.LOG_ARCHIVAL_DIRECTORY is set to some value, that means the log is meant to be archived.  Else, log not archived.    MT - not needed, no work is done   Sets whether the database is encrypted. <p> Read-only database can not be re-encrypted, nothing to do in this case. Not applicable in readonly databases  truncation point support (not supported) There are no log files to backup for  read  only databases, nothing to be done here. @param toDir - location where the log files should be copied to. @exception StandardException Standard Derby error policy set up a new log file to start writing the log records into the new log file after this call. <P>MT - synchronization provided by caller - RawStore boot, This method is called while re-encrypting the database at databse boot time. Read-only database can not be reencrypted, nothing to do in this case. Replication not applicable on readonly databases Replication not applicable on readonly databases Backup restore - start sending log record to the log stream

Return an optimized version of bytes available to read from the stream. <p> Note, it is not exactly per {@code java.io.InputStream#available()}. Determine if trailing blank truncation is allowed. Validate the length of the stream, take corrective action if allowed. JDBC 3.0 (from tutorial book) requires that an input stream has the correct number of bytes in the stream. If the stream is too long, trailing blank truncation is attempted if allowed. If truncation fails, or is disallowed, an exception is thrown. return resources Fills the internal buffer with data read from the source stream. <p> The characters read from the source are converted to the modified UTF-8 encoding, used as the on-disk format by Derby. Marks the current position in the stream. <p> Note that this stream is not marked at position zero by default (i.e. in the constructor). Tests if this stream supports mark/reset. <p> The {@code markSupported} method of {@code ByteArrayInputStream} always returns {@code true}. Reads a byte from the stream. <p> Characters read from the source stream are converted to the UTF-8 Derby specific encoding. Reads up to {@code len} bytes from the stream. <p> Characters read from the source stream are converted to the UTF-8 Derby specific encoding. Repositions this stream to the position at the time the mark method was last called on this input stream. Attempt to truncate the stream by removing trailing blanks.
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Class implementation
Dump out the estimated cost information Class implementation Dump out the time information for run time stats. Get the objects to be displayed when this tree object is expanded. <P> The objects returned can be of any type, including addtional Inspectables. Return the time for all operations performed by the children of this node. Get the estimated row count for the number of rows returned by the associated query or statement. Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return the time for all operations performed by this node, but not the time for the children of this node. Return the time for all operations performed by this node, and the children of this node.  The times included open, next, and close.
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String.
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Class implementation
------------------------------------------------------ XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Formatable methods Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
ResultSetStatistics methods Class implementation
ResultSetStatistics methods Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Return the ResultSetStatistics for the child of this node. Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Class implementation
Class implementation Format for display, a name for this node.
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
Initialize the format info for run time statistics.
Dump out the time information for run time stats. Get the objects to be displayed when this tree object is expanded. <P> The objects returned can be of any type, including addtional Inspectables. Get the estimated row count for the number of rows returned by the associated query or statement. Format for display, a name for this node. Initialize the format info for run time statistics.
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
ExecutionFactory interface   ResultSetStatisticsFactory interface
Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Retrieves the children runtime statistics of this <code> RealSetOpResultSetStatistics</code> object Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a <code>String</code>. ResultSetStatistics methods Return the statement execution plan as a <code>String</code>. Class implementation Return the runtime statistics of this object in textual representation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Since this is a SQL operator name, it does not need to be internationalized. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. Since this is a SQL operator name, it does not need to be internationalized. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. Formatable methods Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- Format for display, a name for this node. If this node is on a database item (like a table or an index), then provide a string that describes the on item. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics methods Return the statement execution plan as a String. Class implementation
----------------------------------------------------- XPLAINable Implementation ----------------------------------------------------- RealBasicNoPutResultSetStatistics override. RealBasicNoPutResultSetStatistics override. RealBasicNoPutResultSetStatistics override. Return information on the scan nodes from the statement execution plan as a String. ResultSetStatistics interface Return the statement execution plan as a String. java.lang.Object override
class specific methods Serviceable methods @return true, if this work needs to be done on a user thread immediately debug
Open container shared no wait Reclaim space based on work.
Return the identity of my container. Obtain the page-unique identifier for this record. This id combined with a page number is guaranteed to be unique within a container. Return the identity of my Page. Obtain the page number this record lives on. What slot number might the record be at? <p> The raw store guarantees that the record handle of a record will not change, but it's slot number may.  An implementation of a record handle may provide a hint of the slot number, which may help routines like Page.getSlotNumber() perform better. <p> If an implementation does not track slot numbers at all the implementation should just always return Page.FIRST_SLOT_NUMBER.
*		Methods of Object Implement value equality. <BR> MT - Thread safe *	Methods of RecordHandle Get my record id. <BR> MT - thread safe Get my page number. <BR> MT - thread safe What slot number might the record be at? <p> The raw store guarantees that the record handle of a record will not change, but its slot number may.  When a RecordId is constructed the caller could have provided a slot number, if so return that slot number hint here.  If a hint was not provided then the default Page.FIRST_SLOT_NUMBER will be returned. Return a hashcode based on value. <BR> MT - thread safe This lockable wants to participate in the Virtual Lock table. * Methods of Lockable (from RecordHandle) Lock me. <BR> MT - Single thread required (methods of Lockable) Is a caller that holds a lock compatible with themselves? <p> Row locks held in the same transaction are always compatible with themselves. <BR> MT - Single thread required (methods of Lockable) Determine if this request can be granted. <p> Implements the grant/wait lock logic for row locks.  See the table in RowLock for more information. <BR> MT - Single thread required (methods of Lockable) Unlock me. <BR> MT - Single thread required (methods of Lockable)


Set the value of this RefDataValue.
Tell whether this type is compatible with the given type.
Reconstructs a Derby embedded-driver data source object from a JNDI data source reference. <p> The {@code getObjectInstance} method is passed a reference that corresponds to the object being retrieved as its first parameter. The other parameters are optional in the case of JDBC data source objects. The object factory should use the information contained in the reference to reconstruct the data source. If for some reason, a data source object cannot be reconstructed from the reference, a value of {@code null} may be returned. This allows other object factories that may be registered in JNDI to be tried. If an exception is thrown then no other object factories are tried. Set the Java bean properties for an object from its Reference. The Reference contains a set of StringRefAddr values with the key being the bean name and the value a String representation of the bean's value. This code looks for setXXX() method where the set method corresponds to the standard bean naming scheme and has a single parameter of type String, int, boolean or short.
Returns an array of 1-based column positions in the table that the check constraint is on. Returns an array of 1-based column positions in the trigger table. These columns are the ones referenced in the trigger action through the old/new transition variables.
TypedFormat interface Externalizable interface For triggers, 3 possible scenarios 1)referencedColumns is not null but referencedColumnsInTriggerAction is null - then following will get read referencedColumns.length individual elements from referencedColumns arrary eg create trigger tr1 after update of c1 on t1 for each row values(1); 2)referencedColumns is null but referencedColumnsInTriggerAction is not null - then following will get read -1 -1 referencedColumnsInTriggerAction.length individual elements from referencedColumnsInTriggerAction arrary eg create trigger tr1 after update on t1 referencing old as oldt for each row values(oldt.id); 3)referencedColumns and referencedColumnsInTriggerAction are not null - then following will get read -1 referencedColumns.length individual elements from referencedColumns arrary referencedColumnsInTriggerAction.length individual elements from referencedColumnsInTriggerAction arrary eg create trigger tr1 after update of c1 on t1 referencing old as oldt for each row values(oldt.id); Scenario 1 for triggers is possible for all different releases of dbs ie both pre-10.7 and 10.7(and higher). But scenarios 2 and 3 are only possible with database at 10.7 or higher releases. Prior to 10.7, we did not collect any trigger action column info and hence referencedColumnsInTriggerAction will always be null for triggers created prior to 10.7 release.  For triggers, 3 possible scenarios 1)referencedColumns is not null but referencedColumnsInTriggerAction is null - then following gets written referencedColumns.length individual elements from referencedColumns arrary eg create trigger tr1 after update of c1 on t1 for each row values(1); This can also happen for a trigger like following if the database is at pre-10.7 level. This is for backward compatibility reasons because pre-10.7 releases do not collect/work with trigger action column info in system table. That functionality has been added starting 10.7 release eg create trigger tr1 after update on t1 referencing old as oldt for each row values(oldt.id); 2)referencedColumns is null but referencedColumnsInTriggerAction is not null - then following gets written -1 -1 referencedColumnsInTriggerAction.length individual elements from referencedColumnsInTriggerAction arrary eg create trigger tr1 after update on t1 referencing old as oldt for each row values(oldt.id); 3)referencedColumns and referencedColumnsInTriggerAction are not null - then following gets written -1 referencedColumns.length individual elements from referencedColumns arrary referencedColumnsInTriggerAction.length individual elements from referencedColumnsInTriggerAction arrary eg create trigger tr1 after update of c1 on t1 referencing old as oldt for each row values(oldt.id);
Decrement the reference count by one. Get the referencing foreign key constraints Am I referenced by a FK on another table? Return the list of those foreign constraints. Get the number of enforced fks that reference this key. Am I referenced by a FK on the same table? Bump the reference count by one. Is this constraint referenced? Returns true if there are enforced fks that reference this constraint. Does this constraint need to fire on this type of DML?  For referenced keys, fire if referenced by a fk, and stmt is delete or bulk insert replace, or stmt is update and columns intersect.
Clean up all scan controllers and other resources Check that the row either has a null column(s), or has no corresponding foreign keys. <p> If a foreign key is found, an exception is thrown. If not, the scan is closed. Check that we have at least one more row in the referenced table table containing a key than the number of projected deletes of that key. Only used when the referenced constraint id deferred and with RESTRICT mode Remember the deletion of this key, it may cause a RESTRICT foreign key violation, cf. logic in @{link #postCheck}.
//////////////////////////////////////////////  CLASS INTERFACE  ////////////////////////////////////////////// No need to go below a Predicate or ResultSet. //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// Don't do anything unless we have a ColumnReference, Predicate or ResultSetNode node.


* Implementation specific methods * NOTE these are COPIED from ReflectLoader as the two classes cannot be made into a super/sub class pair. Because the Java2 one needs to call super(ClassLoader) that was added in Java2 and it needs to not implement loadClass() Load a generated class from the passed in class data.


Return the class name bound to an index into TwoByte Return the number of two-byte format ids
Return whether this operator compares the given Optimizable with a constant whose value is known at compile time. Return whether this operator is an equality comparison of the given optimizable with a constant expression. Generate the absolute column id for the ColumnReference that appears on one side of this RelationalOperator or the other, and that refers to the given table. (Absolute column id means column id within the row stored on disk.) Check whether this RelationalOperator is a comparison of the given column with an expression.  If so, generate the Expression for the ValueNode that the column is being compared to. Generate an expression that evaluates to true if the result of the comparison should be negated.  For example, col &gt; 1 generates a comparison operator of &lt;= and a negation of true, while col &lt; 1 generates a comparison operator of &lt; and a negation of false. Generate the comparison operator for this RelationalOperator. The operator can depend on which side of this operator the optimizable column is. Generate an expression that evaluates to true if this RelationalOperator uses ordered null semantics, false if it doesn't. Generate the method to evaluate a Qualifier.  The factory method for a Qualifier takes a GeneratedMethod that returns the Orderable that Qualifier.getOrderable() returns. Generate the relative column id for the ColumnReference that appears on one side of this RelationalOperator or the other, and that refers to the given table. (Relative column id means column id within the partial row returned by the store.) Get the ColumnReference for the given table on one side of this RelationalOperator.  This presumes it will be found only on one side.  If not found, it will return null. Check whether this RelationalOperator is a comparison of the given column with an expression.  If so, return the ColumnReference that corresponds to the given column, and that is on one side of this RelationalOperator or the other (this method copes with the column being on either side of the operator).  If the given column does not appear by itself on one side of the comparison, the method returns null. Return an Object representing the known value that this relational operator is comparing to a column in the given Optimizable. Check whether this RelationalOperator is a comparison of the given column with an expression.  If so, return the expression the column is being compared to. Find the operand (left or right) that points to the same table as the received ColumnReference, and then return either that operand or the "other" operand, depending on the value of otherSide. This presumes it will be found only on one side.  If not found, it will return null. Return the operator (as an int) for this RelationalOperator. Return the variant type for the Qualifier's Orderable. (Is the Orderable invariant within a scan or within a query?) Get the start operator for a scan (at the store level) for this RelationalOperator. Get the stop operator for a scan (at the store level) for this RelationalOperator. Return a relational operator which matches the current one but with the passed in ColumnReference as the (left) operand. Return true if this operator can be compiled into a Qualifier for the given Optimizable table.  This means that there is a column from that table on one side of this relop, and an expression that does not refer to the table on the other side of the relop. Note that this method has two uses: 1) see if this operator (or more specifically, the predicate to which this operator belongs) can be used as a join predicate (esp. for a hash join), and 2) see if this operator can be pushed to the target optTable.  We use the parameter "forPush" to distinguish between the two uses because in some cases (esp. situations where we have subqueries) the answer to "is this a qualifier?" can differ depending on whether or not we're pushing.  In particular, for binary ops that are join predicates, if we're just trying to find an equijoin predicate then this op qualifies if it references either the target table OR any of the base tables in the table's subtree. But if we're planning to push the predicate down to the target table, this op only qualifies if it references the target table directly.  This difference in behavior is required because in case 1 (searching for join predicates), the operator remains at its current level in the tree even if its operands reference nodes further down; in case 2, though, we'll end up pushing the operator down the tree to child node(s) and that requires additional logic, such as "scoping" consideration.  Until that logic is in place, we don't search a subtree if the intent is to push the predicate to which this operator belongs further down that subtree.  See BinaryRelationalOperatorNode for an example of where this comes into play. Return true if this operator uses ordered null semantics Check whether this RelationalOperator compares the given ColumnReference to any columns in the same table as the ColumnReference. Tell whether this relop is a useful start key for the given table. It has already been determined that the relop has a column from the given table on one side or the other. Tell whether this relop is a useful stop key for the given table. It has already been determined that the relop has a column from the given table on one side or the other.
<p> Forbid BLOCKQUOTEs for accessibility reasons. See http://www.w3.org/TR/WCAG10/#gl-structure-presentation </p> ///////////////////////////////////////////////////////////////////////  BEHAVIOR CALLED BY ReleaseNoteGenerator  /////////////////////////////////////////////////////////////////////// <p> Get the release note for an issue. </p> <p> Get the detail section for a release note </p> <p> Get the summary paragraph for a release note </p> ///////////////////////////////////////////////////////////////////////  MAIN  /////////////////////////////////////////////////////////////////////// The program entry point exercises all of the checks which would be performed by the ReleaseNoteGenerator on this particular release note. Takes one argument, the name of the file which holds the release note. //////////////////////////////////////////////////////  MISC MINIONS  //////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// Start the RELEASE_NOTES html docment. ////////////////////////////////  Build Environment SECTION  //////////////////////////////// Build the section describing the build environment. Build the Bug List section. ////////////////////////////////  New Features SECTION  //////////////////////////////// Build the New Features section. ////////////////////////////////  Overview SECTION  //////////////////////////////// Build the Overview section. ////////////////////////////////  Issues SECTION  //////////////////////////////// Build the Issues section. ////////////////////////////////  Release Verification SECTION  //////////////////////////////// Build the Release Verification section. ///////////////////////////////////////////////////////////////////////  ANT Task BEHAVIOR  /////////////////////////////////////////////////////////////////////// This is Ant's entry point into this task. Generate the release notes (for details on how to invoke this tool, see the header comment on this class). ////////////////////////////////  ARGUMENT MINIONS  //////////////////////////////// Returns true if arguments parse successfully, false otherwise. ////////////////////////////////  Bug List SECTION  //////////////////////////////// ////////////////////////////////  Print errors  //////////////////////////////// Print missing release notes
This is Ant's entry point into this task. <p> Find the header element with this given name. </p> ///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// Adjust input text to remove junk which confuses the xml parser and/or Forrest. Temporarily writes the adjusted text to the output file. <p> Post-process the output: </p> <ul> <li>Add preamble to the head of the file.</li> </ul> Print the generated output document to the output file. Print a line of text to the console. <p> Read a file and return the entire contents as a single String. </p> <p> Remove the blockquotes which hide text from Forrest. </p> <p> Remove the first list under an element. </p> <p> Remove the table of contents of the Issues section. This is the first list in that section. </p> <p> Remove the top level table of contents. This is the first list in the document. </p> Ant accessor to set the name of the cli.xconf file which pulls the download page into the built site. ///////////////////////////////////////////////////////////////////////  ANT Task BEHAVIOR  /////////////////////////////////////////////////////////////////////// Ant accessor to set the name of the input file, the original release notes. Ant accessor to set the name of the generated output file Ant accessor to set the release id. ///////////////////////////////////////////////////////////////////////  CORE BEHAVIOR  /////////////////////////////////////////////////////////////////////// <p> This is the guts of the processing. </p> <p> Wire the download page into the build instructions. </p> <p> Write a string into a file. </p>
///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// <p> Stuff the third and fourth numbers of a Derby release id into the encoded format expected by the Derby release machinery. </p> <p> Create the release properties file from the release id. Sets the property derby.release.id.new equal to the resulting release id. </p> <p> Flush and close file writers. </p> <p> Get the current year as an int. </p> <p> Read the DRDA maintenance id from the existing release properties. Returns 0 if the release properties file doesn't exist. </p> <p>Let Ant set our bumping behavior to true or false.</p> <p> Set an ant property. </p> ///////////////////////////////////////////////////////////////////////  Task BEHAVIOR  /////////////////////////////////////////////////////////////////////// <p>Let Ant set the Derby release id, a string of the form N.N.N.N.</p> <p>Let Ant set the output file name.</p>
Filters out distributions that cannot be run in the current environment for some reason. <p> The reason for getting filtered out is typically due to lacking functionality or a bug in a specific Derby distribution. Returns the list of distributions in the repository. Returns the release repository object. <p> The release repository will be built from a default directory, or from the directory specified by the system property {@code derbyTesting.oldReleasePath}. Prints a debug message if debugging is enabled. Prints a trace message if tracing is enabled.
No need to go below a SubqueryNode. //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// Don't do anything unless we have a ColumnReference node.
Return the number of bytes remains in the byteHolder for reading, without setting the write/read mode. Clear all the remembered bytes. This stream will remember any bytes read after this call. read len bytes from the input stream, and store it in the byte holder. Note, fillBuf does not return negative values, if there are no bytes to store in the byteholder, it will return 0 Get the byteHolder. Get an input stream for re-reading the remembered bytes. Return the number of bytes that have been saved to this byte holder. This result is different from available() as it is unaffected by the current read position on the ByteHolder. read len bytes from the byte holder, and write it to the output stream.   Return true iff this RememberBytesInputStream is in recording mode. Set the InputStream from which this reads. <P>Please note this does not clear remembered bytes. remove the remaining bytes in the byteHolder to the beginning set the position to start recording just after these bytes. returns how many bytes was transfered to the beginning.

(non-Javadoc) @see org.eclipse.ui.IActionDelegate#run(org.eclipse.jface.action.IAction) (non-Javadoc) @see org.eclipse.ui.IActionDelegate#selectionChanged(org.eclipse.jface.action.IAction, org.eclipse.jface.viewers.ISelection) (non-Javadoc) @see org.eclipse.ui.IObjectActionDelegate#setActivePart(org.eclipse.jface.action.IAction, org.eclipse.ui.IWorkbenchPart)
Loggable methods the default for prepared log is always null for all the operations that don't have optionalData.  If an operation has optional data, the operation need to prepare the optional data for this method. Space Operation has no optional data to write out Return my format identifier. A space operation is a RAWSTORE log record  Formatable methods
do necessary work for rename column at execute time. do necessary work for rename index at execute time. do necessary work for rename table at execute time. INTERFACE METHODS The guts of the Execution-time logic for RENAME TABLE/COLUMN/INDEX. Following is used for error handling by repSourceCompilerUtilities in it's method checkIfRenameDependency OBJECT METHODS
We inherit the generate() method from DDLStatementNode. Bind this node.  This means doing any static error checking that can be done before actually renaming the table/column/index. For a table rename: looking up the from table, verifying it exists verifying it's not a system table, verifying it's not view and looking up to table, verifying it doesn't exist. For a column rename: looking up the table, verifying it exists, verifying it's not a system table, verifying it's not view, verifying the from column exists, verifying the to column doesn't exist. For a index rename: looking up the table, verifying it exists, verifying it's not a system table, verifying it's not view, verifying the from index exists, verifying the to index doesn't exist. Create the Constant information that will drive the guts of Execution Return true if the node references SESSION schema tables (temporary or permanent) do any checking needs to be done at bind time for rename column do any checking needs to be done at bind time for rename table Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Don't visit children under the skipOverClass node, if it isn't null. //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// Don't do anything unless we have an aggregate node.

Don't visit children under the skipOverClass node, if it isn't null. Visitor override. Visitor override. //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// Don't do anything unless we have a window function node node. Visitor override. Visitor override.
Try to shrink the cache if it has exceeded its maximum size. It is not guaranteed that the cache will actually shrink. Insert an entry into the <code>ReplacementPolicy</code>'s data structure, possibly evicting another entry. The entry should be uninitialized when the method is called (that is, its <code>Cacheable</code> should be <code>null</code>), and it should be locked. When the method returns, the entry may have been initialized with a <code>Cacheable</code> which is ready to be reused. It is also possible that the <code>Cacheable</code> is still <code>null</code> when the method returns, in which case the caller must allocate one itself. The entry will be associated with a {@code Callback} object that it can use to communicate back to the replacement policy events (for instance, that it has been accessed or become invalid). Get the number of entries allocated in the data structure that holds cached objects. This number could include empty entries for objects that have been removed from the cache, if those entries are still kept in the data structure for reuse.
Append a chunk of log records to the log buffer. Returns a byte[] containing a chunk of serialized log records. Always returns the log that was oldest at the time next() was called last time. Use next() to move to the next chunk of log records. Used to calculate the Fill Information. The fill information is a indicator of how full the buffer is at any point of time fill information = (full buffers/Total Buffers)*100. The Fill information ranges between 0-100 (both 0 and 100 inclusive). Can be used so that only the necessary log records are sent when a flush(LogInstant flush_to_this) is called in the log factory. Returns the highest log instant in the chunk of log that can be read with getData().  Sets the output data to that of the next (oldest) buffer element in dirtyBuffers so that getData(), getLastInstant() and getSize() return values from the next oldest chunk of log. Used by the log consumer (the LogShipping service) to move to the next chunk of log in the buffer. Appends the currentDirtyBuffer to dirtyBuffers, and makes a fresh buffer element from freeBuffers the currentDirtyBuffer. Note: this method is not synchronized since all uses of it is inside synchronized(listLatch) code blocks. Method to determine whether or not the buffer had any log records the last time next() was called.
Method to get the data byte[] read by the last call to next(). Note that this byte[] contains both byte[] data and byte[] optional_data from LogAccessFile. To split this byte[] into data and optional_data, we would need to create a Loggable object from it because the log does not provide information on where to split. There is no need to split since this byte[] will only be written to the slave log anyway. If it was split, LogAccessFile would simply merge them when writing to file.   Used to determine whether or not the last call to next() was successful. Set all variables to default values, and makes logToScan the byte[] that will be scanned for log records. Used to determine whether the last call to next() found a log file switch Used to determine whether the last call to next() read a log record <p> Read the next log record in logToScan. The information in this log record can be read by using the getXXX methods. </p> <p> Side effects: <br> <br> On a successful read (return true): either...<br> <br> ... the scan read a log record indicating that a log file switch has taken place on the master, in which case isLogFileSwitch() returns true. In this case, getXXX will not contain valid data. Asserts handle calls to these methods when in sane mode. currentPosition is updated to point to the byte immediately following this log file switch log record.<br> <br> ... or the scan read a normal log record, in which case isLogRecord() returns true. Also sets currentInstant and currentData, and updates currentPosition to point to the byte immediatly following the log record. In this case, getXXX will return meaningful information about the log record. </p> <p> If there are no more log records in logToScan (returns false) or a problem occurs (throws StandardException): setting hasInfo = false </p> The retrieveXXX methods are used by next() to read a log record from byte[] logToScan. The methods should be changed to java.nio.ByteBuffer if it is later decided that replication does not have to support j2me. Copy length number of bytes from logToScan into readInto. Starts to copy from currentPosition. Also increments currentPosition by length. Read an int from logToScan. Also increments currentPosition by 4 (the number of bytes in an int). Read a long from logToScan. Also increments currentPosition by 8 (the number of bytes in a long).
Print error message and the stack trace of the throwable to the log (usually derby.log) provided that verbose is true. If verbose is false, nothing is logged. Print a text to the log (usually derby.log), provided that verbose is true.
Used to get the actual message that is wrapped inside the ReplicationMessage object. Used to get the type of this <code>ReplicationMessage</code>. Used to restore the contents of this object. Used to save the contents of this Object.
Verifies if the <code>SocketConnection</code> is valid. Used to create a <code>ServerSocket</code> for listening to connections from the master. Notify other replication peer that the message type was unexpected and throw a StandardException Used to create the server socket, listen on the socket for connections from the master and verify compatibility with the database version of the master. Check if the repliation network is working. Tries to send a ping message to the master and returns the network status based on the success or failure of sending this message and receiving a pong reply. MT: Currently, only one thread is allowed to check the network status at any time to keep the code complexity down. Used to parse the log instant initiator message from the master and check that the master and slave log files are in synch. Used to parse the initiator message from the master and check if the slave is compatible with the master by comparing the UID of the <code>ReplicationMessage</code> class of the master, that is wrapped in the initiator message, with the UID of the same class in the slave. Used to read a replication message sent by the master. This method would wait on the connection from the master until a message is received or a connection failure occurs. Replication network layer specific messages (i.e. ping/pong messages) are handled internally and are not returned. Used to send a replication message to the master. Used to close the <code>ServerSocket</code> and the resources associated with it.
Used to send initiator messages to the slave and receive information about the compatibility of the slave with the master. One message is used to check that the slave and master have the same software versions. A second message is used to check that the master and slave log files are in synch. Verifies if the <code>SocketConnection</code> is valid. Used to create a <code>Socket</code> connection to the slave and establish compatibility with the database version of the slave by comparing the UID's of the <code>ReplicationMessage</code> classes of the master and the slave. Used to send a replication message to the slave. Send a replication message to the slave and return the message received as a response. Will only wait DEFAULT_MESSAGE_RESPONSE_TIMEOUT millis for the response message. If not received when the wait times out, no message is returned. The method is synchronized to guarantee that only one thread will be waiting for a response message at any time. Tear down the network connection established with the other replication peer Used to parse a message received from the slave. If the message is an ack of the last shipped message, this method terminates quietly. Otherwise, it throws the exception received in the message from the slave describing why the last message could not be acked.
///////////////////////////////////////////////////////////////////////// Assert that the latest startSlave connection attempt got the expected SQLState. The method will wait for upto 5 seconds for the startSlave connection attemt to complete. If the connection attempt has not completed after 5 seconds it is assumed to have failed. ///////////////////////////////////////////////////////////////////////// Remove any servers or tests still running Close a connection. <p> Set up a data source. </p> //////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // b)  Running (positive and negative) tests at the various states //     of replication to test what is and is not accepted compared to //     the functional specification. // c)  Adding additional load on master and slave servers in //     different states of replication. // //////////////////////////////////////////////////////////// Template public void testReplication() throws Exception { util.DEBUG("WARNING: Define in subclass of ReplicationRun. " + "See ReplicationRun_Local for an example."); } Execute SQL on the master database through a Statement Execute SQL on the slave database through a Statement Get a connection to the master database. Get a connection to the slave database. private NetworkServerControl startServer_direct(String serverHost, String interfacesToListenOn, int serverPort, String fullDbDirPath, String securityOption) // FIXME? true/false? throws Exception { // Wotk in progress. Not currently used! Only partly tested! util.DEBUG("startServer_direct " + serverHost + " " + interfacesToListenOn +  " " + serverPort + " " + fullDbDirPath); assertTrue("Attempt to start server on non-localhost: " + serverHost, serverHost.equalsIgnoreCase("localhost")); System.setProperty("derby.system.home", fullDbDirPath); System.setProperty("user.dir", fullDbDirPath); NetworkServerControl server = new NetworkServerControl( InetAddress.getByName(interfacesToListenOn), serverPort); server.start(null); pingServer(serverHost, serverPort, 150); Properties sp = server.getCurrentProperties(); sp.setProperty("noSecurityManager", securityOption.equalsIgnoreCase("-noSecurityManager")?"true":"false"); // derby.log for both master and slave ends up in masters system! // Both are run in the same VM! Not a good idea? return server; } Register that a thread has been started so that we can wait for it to complete in {@link #tearDown()}. ?? The following should be moved to a separate class, subclass this and ?? CompatibilityCombinations Run the test. Extra logic in addition to BaseTestCase's similar logic, to save derby.log and database files for replication directories if a failure happens. Should allow: - Run load in separate thread. FIXME? Should we allow extra URL options? FIXME? Should we allow extra URL options? Run a Java command locally on the test host in a separate thread. The spawned process inherits the class path from the main test process. FIXME: NB NB Currently only invoked from startSlave_ij (others unused!) Run a Java command locally on the test host. The spawned process inherits the class path from the main test process. Parent super() Not yet used Verification code.. Set master db in replication master mode. slavePort) ////////////////////////////////////////////////////////////// Utilities.... Set slave db in replication slave mode Parent super()

When running in a distributed context, the environment is defined via the REPLICATIONTEST_PROPFILE. //////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // ////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // b)  Running (positive and negative) tests at the various states //     of replication to test what is and is not accepted compared to //     the functional specification. // c)  Adding additional load on master and slave servers in //     different states of replication. // ////////////////////////////////////////////////////////////
DERBY-3382: Test that start replication fails if master db is updated after copying the db to the slave location //////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // //////////////////////////////////////////////////////////// Test the "normal" replication scenario: Load on the master db while replicating to slave db, then verify that slave db is correct after failover.

Verify that indexes created on master before failover are available in slave database after failover.
//////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // ////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // ////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // //////////////////////////////////////////////////////////// no sleep between startMaster and stopMaster. Comment out fixture: Avoid attempt to call teardown! Hangs on Windows, but passes on Unixes... public void testReplication_Local_3_p2_StateTests_smallInsert_immediateStopMaster_DISABLED() throws Exception { // FIXME! ENABLE when DERBY-3617 is RESOLVED - otherwise hangs.... // ... Now gets connection instead of XRE07! // And then we experience hang again... // replication_Local_3_p2_StateTests(false, true); }
//////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // //////////////////////////////////////////////////////////// Verify that a second failover is not accepted.
//////////////////////////////////////////////////////////// // // The replication test framework (testReplication()): // a) "clean" replication run starting master and slave servers, //     preparing master and slave databases, //     starting and stopping replication and doing //     failover for a "normal"/"failure free" replication //     test run. // //////////////////////////////////////////////////////////// Test to verify that "internal_stopslave=true" is NOT accepted as connection attr. on the slave. "internal_stopslave=true" is for internal use only.
Test that DERBY-3924 fixed DERBY-3878.
ENABLE WHEN DERBY-3896 IS FIXED! public void testReplication_Local_3_p6_DERBY_3896() throws Exception { derby_3896(false); // Autocommit off during create table before starting replication } Test the DERBY-3896 scenario but with autocommit on which does not fail. DERBY-3896: Autocommit off during create table before starting replication causes uncommitted data not being replicated.
Test that a slave can wait a while for the master to connect without timing out. The startSlave command used to time out after one second before DERBY-4910.
Verify connection to the encrypted, replicated database: A) Database has not been shut down: further connects from the same JVM will succeed - connect without specifying encryption, - blank password, - incorrect password. B) After shutdown: Re-connects without correct password will fail - re-connecting as if un-encrypted, - blank password, - incorrect password. Do a simple test to verify replication can be performed on an encrypted database.


this will probably never be called. it is included here in the highly unlikely event that a reply object exceeds 32K.  for opimization purposes, we should consider removing this.  removing this should be ok since we handle most big stuff returned from the server (qrydta's for example) by copying out the data into some other storage.  any extended dss header info will be removed in the copying process. As part of parsing the reply, the client can detect that the data sent from the target agent does not structurally conform to the requirements of the DDM architecture.  These are the same checks performed by the target server on the messages it receives from the protocolj code.  Server side detected errors result in a SYNTAXRM being returned from the AS.  According to the DDM manual, parsing of the DSS is terminated when the error is detected.  The Syntax Error Code, SYNERRCD, describes the various errors.  Note: Not all of these may be valid at the client.  See descriptions for which ones make sense for client side errors/checks. Syntax Error Code                  Description of Error -----------------                  -------------------- 0x01                               Dss header Length is less than 6. 0x02                               Dss header Length does not match the number of bytes of data found. 0x03                               Dss header C-byte not D0. 0x04                               Dss header f-bytes either not recognized or not supported. 0x05                               DSS continuation specified but not found. For example, DSS continuation is specified on the last DSS, and the SNA LU 6.2 communication facility returned the SEND indicator. 0x06                               DSS chaining specified but no DSS found. For example, DSS chaining is specified on the last DSS, and the SNA LU 6.2 communication facility returned the SEND indicator. 0x07                               Object length less than four.  For example, a command parameter's length is specified as two, or a command's length is specified as three. 0x08                               Object length does not match the number of bytes of data found.  For example, a RQSDSS with a length of 150 contains a command whose length is 125 or a SRVDGN parameter specifies a length of 200 but there are only 50 bytes left in the DSS. 0x09                               Object length greater than maximum allowed. For example, a RECCNT parameter specifies a length of ten, but the parameter is defined to have a maximum length of eight. 0x0A                               Object length less than the minimum required. For example, a SVRCOD parameter specifies a length of five, but the parameter is defined to have a fixed length of six. 0x0B                               Object length not allowed.  For example, a FILEXDPT parameter is specified with a length of 11, but this would indicate that only half of the hours field is present instead of the complete hours field. 0x0C                               Incorrect large object extended length field (see description of DSS).  For example, an extended length field is present, but it is only three bytes long when it is defined to be a multiple of two bytes. 0x0D                               Object code point index not supported. For example, a code point of 8032 is encountered but x'8' is a reserved code point index. 0x0E                               Required object not found.  For example, a CLEAR command does not have a filnam parameter present, or a MODREC command is not followed by a RECORD command data object. 0x0F                               Too many command data objects sent.  For example, a MODREC command is followed by two RECORD command command data objects, or a DECREC command is followed by RECORD object. 0x10                               Mutually exclusive objects present. For example, a CRTDIRF command specifies both a DCLNAM and FILNAM parameters. 0x11                               Too few command data objects sent. For example, an INSRECEF command that specified RECCNT95) is followed by only 4 RECORD command data objects. 0x12                               Duplicate object present. For example, a LSTFAT command has tow FILNAM parameters specified. 0x13                               Invalid request correlator specified. Use PRCCNVRM with PRCCNVDC of 04 or 05 instead of this error code.  This error code is being retained for compatibility with Level 1 of the architecture. 0x14                               Required value not found. 0x15                               Reserved value not allowed.  For example, a INSRECEF command specified a RECCNT(0) parameter. 0x16                               DSS continuation less than or equal to two. For example, the length bytes of the DSS continuation have the value of one. 0x17                               Objects not in required order.  For example, a RECAL object contains a RECORD object followed by a RECNBR object with is not in the defined order. 0x18                               DSS chaining byt not b'1', but DSSFMT bit3 set to b'1'. 0x19                               Previous DSS indicated current DSS has the same request correlator, but the request correlators are not the same. 0x1A                               DSS cahining bit not b'1', but error continuation requested. 0x1B                               Mutually exclusive parameter values not specified. For example, an OPEN command specified PRPSHD(TRUE) and FILSHR(READER). 0x1D                               Code point not valid command.  For example, the first code point in RQSDSS either is not in the dictionary or is not a code point for a command.  When the client detects these errors, it will be handled as if a SYNTAXRM is returned from the server.  In this SYNTAXRM case, PROTOCOL architects an SQLSTATE of 58008 or 58009.  Messages SQLSTATE : 58009 Execution failed due to a distribution protocol error that caused deallocation of the conversation. SQLCODE : -30020 Execution failed because of a Distributed Protocol Error that will affect the successful execution of subsequent commands and SQL statements: Reason Code <reason-code>. Some possible reason codes include: 121C Indicates that the user is not authorized to perform the requested command. 1232 The command could not be completed because of a permanent error. In most cases, the server will be in the process of an abend. 220A The target server has received an invalid data description. If a user SQLDA is specified, ensure that the fields are initialized correctly. Also, ensure that the length does not exceed the maximum allowed length for the data type being used.  The command or statement cannot be processed.  The current transaction is rolled back and the application is disconnected from the remote database. Make sure a certain amount of Layer A data is in the buffer. The data will be in the buffer after this method is called. Now returns the total bytes read for decryption, use to return void. This method makes sure there is enough room in the buffer for a certain number of bytes.  This method will allocate a new buffer if needed and shift the bytes in the current buffer to make ensure space is available for a fill.  Right now this method will shift bytes as needed to make sure there is as much room as possible in the buffer before trying to do the read.  The idea is to try to have space to get as much data as possible if we need to do a read on the socket's stream. This method will attempt to read a minimum number of bytes from the underlying stream.  This method will keep trying to read bytes until it has obtained at least the minimum number. Now returns the total bytes read for decryption, use to return void. This will be the new and improved getData that handles all QRYDTA/EXTDTA Returns the stream so that the caller can cache it The only difference between this method and the original getData() method is this method is not doing an ensureALayerDataInBuffer This method is only used to match the codePoint for those class instance variables that are embedded in other reply messages. Read "length" number of bytes from the buffer into the byte array b starting from offset "offset".  The current offset in the buffer does not change. Pop the collection Length stack. pre:  The collection length stack must not be empty and the top value on the stack must be 0. post: The top 0 value on the stack will be popped. remove and return the top offset value from mark stack. the names of these methods start with a letter z. the z will be removed when they are finalized... reads a DSS continuation header prereq: pos_ is positioned on the first byte of the two-byte header post:   dssIsContinued_ is set to true if the continuation bit is on, false otherwise dssLength_ is set to DssConstants.MAX_DSS_LEN - 2 (don't count the header for the next read) helper method for getEXTDTAData This is a helper method which shifts the buffered bytes from wherever they are in the current buffer to the beginning of different buffer (note these buffers could be the same). State information is updated as needed after the shift.
experimental lob section insert 3 unsigned bytes into the buffer.  this was moved up from NetStatementRequest for performance helper method to calculate the minimum number of extended length bytes needed for a ddm.  a return value of 0 indicates no extended length needed. throws SqlException creates an request dss in the buffer to contain a ddm command object.  calling this method means any previous dss objects in the buffer are complete and their length and chaining bytes can be updated appropriately. creates an object dss in the buffer to contain a ddm command data object.  calling this method means any previous dss objects in the buffer are complete and their length and chaining bytes can be updated appropriately. method to determine if any data is in the request. this indicates there is a dss object already in the buffer. Encode a string and put it into the buffer. A larger buffer will be allocated if the current buffer is too small to hold the entire string. ensure length at the end of the buffer for a certain amount of data. if the buffer does not contain sufficient room for the data, the buffer will be expanded by the larger of (2 * current size) or (current size + length). the data from the previous buffer is copied into the larger buffer. Signal the completion of a DSS Layer A object. <p> The length of the DSS object will be calculated based on the difference between the start of the DSS, saved in the variable {@link #dssLengthLocation_}, and the current offset into the buffer which marks the end of the data. <p> In the event the length requires the use of continuation DSS headers, one for each 32k chunk of data, the data will be shifted and the continuation headers will be inserted with the correct values as needed. Note: In the future, we may try to optimize this approach in an attempt to avoid these shifts. experimental lob section - end used to finialize a dss which is already in the buffer before another dss is built.  this includes updating length bytes and chaining bits. write the request to the OutputStream and flush the OutputStream. trace the send if PROTOCOL trace is on. Writes out a scalar stream DSS segment, along with DSS continuation headers, if necessary. mark an offest into the buffer by placing the current offset value on a stack. mark the location of a two byte ddm length field in the buffer, skip the length bytes for later update, and insert a ddm codepoint into the buffer.  The value of the codepoint is not checked. this length will be automatically updated when construction of the ddm object is complete (see updateLengthBytes method). Note: this mechanism handles extended length ddms. insert the padByte into the buffer by length number of times. Pads a value with zeros until it has reached its defined length. <p> This functionality was introduced to handle the error situation where the actual length of the user stream is shorter than specified. To avoid DRDA protocol errors (or in this case a hang), we have to pad the data until the specified length has been reached. In a later increment the Derby-specific EXTDTA status flag was introduced to allow the client to inform the server that the value sent is invalid. remove and return the top offset value from mark stack. prepScalarStream does the following prep for writing stream data: 1.  Flushes an existing DSS segment, if necessary 2.  Determines if extended length bytes are needed 3.  Creates a new DSS/DDM header and a null byte indicator, if applicable Called to update the last ddm length bytes marked (lengths are updated in the reverse order that they are marked).  It is up to the caller to make sure length bytes were marked before calling this method. If the length requires ddm extended length bytes, the data will be shifted as needed and the extended length bytes will be automatically inserted. insert an unsigned single byte value into the buffer. perf end insert a big endian unsigned 2 byte value into the buffer. insert a big endian unsigned 4 byte value into the buffer. insert a java.math.BigDecimal into the buffer. insert a java boolean into the buffer.  the boolean is written as a signed byte having the value 0 or 1. insert a java byte into the buffer. copy length number of bytes starting at offset 0 of the byte array, buf, into the buffer.  it is up to the caller to make sure buf has at least length number of elements.  no checking will be done by this method. insert a pair of unsigned 2 byte values into the buffer. insert a java double into the buffer. Writes the Derby-specific EXTDTA status flag to the send buffer. <p> The existing buffer is flushed to make space for the flag if required. insert a java float into the buffer. insert a java int into the buffer. when writing Fdoca data. private helper method which should only be called by a Request method. must call ensureLength before calling this method. added for code reuse and helps perf by reducing ensureLength calls. ldSize and bytes.length may not be the same.  this is true when writing graphic ld strings. private helper method for writing just a subset of a byte array insert a 4 byte length/codepoint pair into the buffer. total of 4 bytes inserted in buffer. Note: the length value inserted in the buffer is the same as the value passed in as an argument (this value is NOT incremented by 4 before being inserted). if mdd overrides are not required, lids and lengths are copied straight into the buffer. otherwise, lookup the protocolType in the map.  if an entry exists, substitute the protocolType with the corresponding override lid. insert a java long into the buffer. Writes a long into the buffer, using six bytes. when writing Fdoca data. Writes a stream with unknown length onto the wire. <p> To avoid DRDA protocol exceptions, the data is truncated or padded as required to complete the transfer. This can be avoided by implementing the request abort mechanism specified by DRDA, but it is rather complex and may not be worth the trouble. <p> Also note that any exceptions generated while writing the stream will be accumulated and raised at a later time. <p> <em>Implementation note:</em> This method does not support sending values with a specified length using layer B streaming and at the same time applying length checking. For large values layer B streaming may be more efficient than using layer A streaming. Writes a stream with a known length onto the wire. <p> To avoid DRDA protocol exceptions, the data is truncated or padded as required to complete the transfer. This can be avoided by implementing the request abort mechanism specified by DRDA, but it is rather complex and may not be worth the trouble. <p> Also note that any exceptions generated while writing the stream will be accumulated and raised at a later time. insert a 4 byte length/codepoint pair and a 1 byte unsigned value into the buffer. total of 5 bytes inserted in buffer. insert a 4 byte length/codepoint pair and a 2 byte unsigned value into the buffer. total of 6 bytes inserted in buffer. insert a 4 byte length/codepoint pair and a 4 byte unsigned value into the buffer.  total of 8 bytes inserted in the buffer. insert a 4 byte length/codepoint pair and a 8 byte unsigned value into the buffer.  total of 12 bytes inserted in the buffer. this method writes a 4 byte length/codepoint pair plus the bytes contained in array buff to the buffer. the 2 length bytes in the llcp will contain the length of the data plus the length of the llcp.  This method does not handle scenarios which require extended length bytes. this method inserts a 4 byte length/codepoint pair plus length number of bytes from array buff starting at offset start. Note: no checking will be done on the values of start and length with respect the actual length of the byte array.  The caller must provide the correct values so an array index out of bounds exception does not occur. the length will contain the length of the data plus the length of the llcp. This method does not handle scenarios which require extended length bytes. this method inserts binary data into the buffer and pads the data with the padByte if the data length is less than the paddedLength. Not: this method is not to be used for truncation and buff.length must be <= paddedLength. Throw DataTruncation, instead of closing connection if input size mismatches An implication of this, is that we need to extend the chaining model for writes to accomodate chained write exceptoins Write string with no minimum or maximum limit. insert a 4 byte length/codepoint pair plus ddm character data into the buffer.  This method assumes that the String argument can be converted by the ccsid manager.  This should be fine because usually there are restrictions on the characters which can be used for ddm character data. The two byte length field will contain the length of the character data and the length of the 4 byte llcp.  This method does not handle scenarios which require extended length bytes. insert a java short into the buffer. -- The following are the write short/int/long in bigEndian byte ordering -- when writing Fdoca data. follows the TYPDEF rules (note: don't think ddm char data is ever length delimited) should this throw SqlException Will write a varchar mixed or single this was writeLDString should not be called if val is null
Estimate the cost of doing a sort for this row ordering, given the number of rows to be sorted.  This does not take into account whether the sort is really needed.  It also estimates the number of result rows. Indicate that a sort is necessary to fulfill this required ordering. This method may be called many times during a single optimization. Indicate that a sort is *NOT* necessary to fulfill this required ordering.  This method may be called many times during a single optimization. Tell whether sorting is required for this RequiredRowOrdering, given a RowOrdering. Tell whether sorting is required for this RequiredRowOrdering, given a RowOrdering representing a partial join order, and a bit map telling what tables are represented in the join order. This is useful for reducing the number of cases the optimizer has to consider.
Close. Free resources (such as open containers and locks) associated with the stream. Initialize. Needs to be called first, before a resetable stream can be used. Reset the stream to the beginning.
Cancel the XA transaction identified by the specified xid.  The method will atomically cancel any running statement on behalf of the transaction, end the transaction association with the XAResource instance, and rollback of the global transaction. If a run time global transaction exists, the resource adapter will find it and return a capsule of information so that a Connection can be attached to the transaction. XATransactionResource findTransaction(XAXactId xid); Start a run time global transaction.  Add this to the list of transactions managed by this resource adapter. boolean addTransaction(XATransactionResource tr); Terminates a run time global transction.  Remove this from the list of transactions managed by this resource adapter. void removeTransaction(XATransactionResource tr); Let a xaResource get the XAResourceManager to commit or rollback an in-doubt transaction. Get the context service factory. ContextService getContextServiceFactory(); Is the Resource Manager active
Module control  Resource Adapter methods Privileged startup. Must be private so that user code can't call this entry point. Return the XA Resource manager to the XA Connection
//////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  Connection MANAGEMENT  ////////////////////////////////////////////////////////////////////// <p> Scalar function to retrieve the last query generated by this machinery. </p> //////////////////////////////////////////////////////////////////////  RestrictedVTI BEHAVIOR  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  QUERY FACTORY  ////////////////////////////////////////////////////////////////////// <p> Build the query which will be sent to the nested connection. </p> //////////////////////////////////////////////////////////////////////  UTILITY METHODS  ////////////////////////////////////////////////////////////////////// <p> Map a 1-based Derby column number to a 1-based column number in the query. </p> //////////////////////////////////////////////////////////////////////  TABLE FUNCTIONS  ////////////////////////////////////////////////////////////////////// <p> Table function to read a table in Derby. </p>
<p> Initialize a scan of a ResultSet. This method is called once before the scan begins. It is called before any ResultSet method is called. This method performs two tasks: </p> <li><b>Column names</b> - Tells the ResultSet which columns need to be fetched.</li> <li><b>Limits</b> - Gives the ResultSet simple bounds to apply in order to limit which rows are returned. Note that the ResultSet does not have to enforce all of these bounds. Derby will redundantly enforce these limits on all rows returned by the ResultSet. That is, filtering not performed inside the ResultSet will still happen outside the ResultSet.</li> <p> The <i>columnNames</i> argument is an array of columns which need to be fetched.  This is an array of the column names declared in the Table Function's CREATE FUNCTION statement. Column names which were double-quoted in the CREATE FUNCTION statement appear case-sensitive in this array. Column names which were not double-quoted appear upper-cased. Derby asks the Table Function to fetch all columns mentioned in the query. This includes columns mentioned in the SELECT list as well as columns mentioned in the WHERE clause. Note that a column could be mentioned in the WHERE clause in a complex expression which could not be passed to the Table Function via the <i>restriction</i> argument. </p> <p> The array has one slot for each column declared in the CREATE FUNCTION statement. Slot 0 corresponds to the first column declared in the CREATE FUNCTION statement and so on. If a column does not need to be fetched, then the corresponding slot is null. If a column needs to be fetched, then the corresponding slot holds the column's name. </p> <p> Note that even though the array may have gaps, it is expected that columns in the ResultSet will occur at the positions declared in the CREATE FUNCTION statement. Consider the following declaration: </p> <blockquote><pre> create function foreignEmployeeTable() returns table ( id        int, birthDay  date, firstName varchar( 100 ), lastName  varchar( 100 ) ) ... </pre></blockquote> <p> and the following query: </p> <blockquote><pre> select lastName from table( foreignEmployeeTable() ) s </pre></blockquote> <p> In this example, the array passed to this method will have 4 slots. Slots 0, 1, and 2 will be null and slot 3 will hold the String "LASTNAME". Last names will be retrieved from the ResultSet by calls to getString( 4 )--remember that JDBC column ids are 1-based. </p> <p> The <i>restriction</i> argument is a simple expression which should be evaluated inside the Table Function in order to eliminate rows. The expression is a binary tree built out of ANDs, ORs, and column qualifiers. The column qualifiers are simple comparisons between constant values and columns in the Table Function. The Table Function only returns rows which satisfy the expression. The <i>restriction</i> is redundantly enforced by Derby on the rows returned by the ResultSet--this means that <i>restriction</i> gives the Table Function a hint about how to optimize its performance but the Table Function is not required to enforce the entire <i>restriction</i>. </p>
Utility method to parenthesize an expression Turn this Restriction into a string suitable for use in a WHERE clause.
Accept the visitor for all visitable children of this node. Adjust the virtualColumnId for this ResultColumn	by the specified amount Bind this expression.  This means binding the sub-expressions. In this case, we figure out what the result type of this result column is when we call one of the bindResultColumn*() methods. The reason is that there are different ways of binding the result columns depending on the statement type, and this is a standard interface that does not take the statement type as a parameter. Bind this result column by its name and set the VirtualColumnId. This is useful for update statements, and for INSERT statements like "insert into t (a, b, c) values (1, 2, 3)" where the user specified a column list. An exception is thrown when a columnDescriptor cannot be found for a given name.  (There is no column with that name.) NOTE: We must set the VirtualColumnId here because INSERT does not construct the ResultColumnList in the usual way. Bind this result column by ordinal position and set the VirtualColumnId. This is useful for INSERT statements like "insert into t values (1, 2, 3)", where the user did not specify a column list. If a columnDescriptor is not found for a given position, then the user has specified more values than the # of columns in the table and an exception is thrown. NOTE: We must set the VirtualColumnId here because INSERT does not construct the ResultColumnList in the usual way. Bind the result column to the expression that lives under it. All this does is copy the datatype information to this node. This is useful for SELECT statements, where the result type of each column is the type of the column's expression. This verifies that the expression is storable into the result column. It checks versus the expression under this ResultColumn. This method should not be called until the result column and expression both have a valid type, i.e. after they are bound appropriately. Its use is for statements like update, that need to verify if a given value can be stored into a column. This verifies that the expression is storable into the result column. It checks versus the given ResultColumn. This method should not be called until the result column and expression both have a valid type, i.e. after they are bound appropriately. Its use is for statements like insert, that need to verify if a given value can be stored into a column. Clear the table name for the underlying ColumnReference. See UpdateNode.scrubResultColumns() for full explaination. Make a copy of this ResultColumn in a new ResultColumn Adjust this virtualColumnId to account for the removal of a column This routine is called when bind processing finds and removes duplicate columns in the result list which were pulled up due to their presence in the ORDER BY clause, but were later found to be duplicate. If this column is a virtual column, and if this column's virtual column id is greater than the column id which is being removed, then we must logically shift this column to the left by decrementing its virtual column id. Return TRUE if this result column matches the provided column name. This function is used by ORDER BY column resolution. For the ORDER BY clause, Derby will prefer to match on the column's alias (_derivedColumnName), but will also successfully match on the underlying column name. Thus the following statements are treated equally: select name from person order by name; select name as person_name from person order by name; select name as person_name from person order by person_name; See DERBY-2351 for more discussion. Do code generation to return a Null of the appropriate type for the result column. Requires the getCOlumnExpress value pushed onto the stack PUSHCOMPILE void generateNulls(ExpressionClassBuilder acb, MethodBuilder mb, Expression getColumnExpress) throws StandardException { acb.pushDataValueFactory(mb); getTypeCompiler().generateNull(mb, acb.getBaseClassName()); mb.cast(ClassName.DataValueDescriptor); return eb.newCastExpression( ClassName.DataValueDescriptor, getTypeCompiler(). generateNull( eb, acb.getBaseClassName(), acb.getDataValueFactory(eb), getColumnExpress)); } * Check whether the column length and type of this result column * match the expression under the columns.  This is useful for * INSERT and UPDATE statements.  For SELECT statements this method * should always return true.  There is no need to call this for a * DELETE statement. * * @return	true means the column matches its expressions, *			false means it doesn't match. * The following methods implement the Comparable interface.  Do code generation for a result column.  This consists of doing the code generation for the underlying expression. Get the source BaseColumnNode for this result column. The BaseColumnNode cannot be found unless the ResultColumn is bound and is a simple reference to a column in a BaseFromTable. Get the column descriptor Get the expression in this ResultColumn. Returns a non-null value if the ResultColumn represents the join column which is part of the SELECT list of a RIGHT OUTER JOIN with USING/NATURAL. eg select c from t1 right join t2 using (c) The join column we are talking about is column c as in "select c" The return value of following method will show the association of this result column to the join resultset created for the RIGHT OUTER JOIN with USING/NATURAL. This information along with rightOuterJoinUsingClause will be used during the code generation time. Get the maximum size of the column The following methods implement the ResultColumnDescriptor interface.  See the Language Module Interface for details. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query CONSTANT				- constant Get the wrapped reference if any Get the resultSetNumber for this ResultColumn. Returns the underlying source column name, if this ResultColumn is a simple direct reference to a table column, or NULL otherwise.   If this ResultColumn is bound to a column in a table get the column descriptor for the column in the table. Otherwise return null. Search the tree beneath this ResultColumn until we find the number of the table to which this RC points, and return that table number.  If we can't determine which table this RC is for, then return -1. There are two places we can find the table number: 1) if our expression is a ColumnReference, then we can get the target table number from the ColumnReference and that's it; 2) if expression is a VirtualColumnNode, then if the VirtualColumnNode points to a FromBaseTable, we can get that FBT's table number; otherwise, we walk the VirtualColumnNode-ResultColumn chain and do a recursive search. Get non-null column name. This method is called during the bind phase to see if we are dealing with ResultColumn in the SELECT list that belongs to a RIGHT OUTER JOIN(NATURAL OR USING)'s join column. For a query like following, we want to use column name x and not the alias x1 when looking in the JoinNode for join column SELECT x x1 FROM derby4631_t2 NATURAL RIGHT OUTER JOIN derby4631_t1; For a query like following, getSourceColumnName() will return null because we are dealing with a function for the column. For this case, "name" will return the alias name cx SELECT coalesce(derby4631_t2.x, derby4631_t1.x) cx FROM derby4631_t2 NATURAL RIGHT OUTER JOIN derby4631_t1; For a query like following, getSourceColumnName() and name will return null and hence need to use the generated name SELECT ''dummy="'|| TRIM(CHAR(x))|| '"' FROM (derby4631_t2 NATURAL RIGHT OUTER JOIN derby4631_t1); Get the virtualColumnId for this ResultColumn Generate a unique (across the entire statement) column name for unnamed ResultColumns Return true if this result column represents a generated column. Returns true if this result column is a placeholder for a generated autoincrement value. Returns TRUE if the ResultColumn is standing in for a DEFAULT keyword in an insert/update statement. Is this a generated column? Is this columm generated for an unmatched column in an insert? Is the name for this ResultColumn generated? Is this a redundant ResultColumn? Is this a referenced column? Returns TRUE if the ResultColumn is join column for a RIGHT OUTER JOIN with USING/NATURAL. More comments at the top of this class where rightOuterJoinUsingClause is defined. Returns true if this column is updatable. This method is used for determining if updateRow and insertRow are allowed for this cursor (DERBY-1773). Since the updateRow and insertRow implementations dynamically build SQL statements on the fly, the critical issue here is whether we have a column that has been aliased, because if it has been aliased, the dynamic SQL generation logic won't be able to compute the proper true base column name when it needs to. Mark this RC and all RCs in the underlying RC/VCN chain as referenced. Mark this ResultColumn as a grouping column in the SELECT list Mark this a columm as a generated column Mark this a columm as generated for an unmatched column in an insert Mark this column as being updatable, so we can make sure it is in the "for update" list of a positioned update. Mark this column as being updated by an update statement. Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Mark this column as a referenced column if it is already marked as referenced or if any result column in its chain of virtual columns is marked as referenced. end of pullVirtualIsReferenced Look for and reject ?/-?/+? parameter under this ResultColumn.  This is called for SELECT statements. Set the column descriptor for this result column.  It also gets the data type services from the column descriptor and stores it in this result column: this is redundant, but we have to store the result type here for SELECT statements, and it is more orthogonal if the type can be found here regardless of what type of statement it is. Set the expression in this ResultColumn.  This is useful in those cases where you don't know the expression in advance, like for INSERT statements with column lists, where the column list and SELECT or VALUES clause are parsed separately, and then have to be hooked up. Set the expression to a null node of the correct type. This method gets called during the bind phase of a ResultColumn if it is determined that the ResultColumn represents the join column which is part of the SELECT list of a RIGHT OUTER JOIN with USING/NATURAL. eg select c from t1 right join t2 using (c) This case is talking about column c as in "select c" Set the name in this ResultColumn.  This is useful when you don't know the name at the time you create the ResultColumn, for example, in an insert-select statement, where you want the names of the result columns to match the table being inserted into, not the table they came from. Set that this result column name is generated. Mark this ResultColumn as redundant. Mark this column as a referenced column. Set the resultSetNumber for this ResultColumn.  This is the resultSetNumber for the ResultSet that we belong to.  This is useful for generate() and necessary since we do not have a back pointer to the RSN. Will be set to TRUE if this ResultColumn is join column for a RIGHT OUTER JOIN with USING/NATURAL. More comments at the top of this class where rightOuterJoinUsingClause is defined. 2 eg cases 1)select c from t1 right join t2 using (c) This case is talking about column c as in "select c" 2)select c from t1 right join t2 using (c) For "using(c)", a join predicate will be created as follows t1.c=t2.c This case is talking about column t2.c of the join predicate. This method gets called for Case 1) during the bind phase of ResultColumn(ResultColumn.bindExpression). This method gets called for Case 2) during the bind phase of JoinNode while we are going through the list of join columns for a NATURAL JOIN or user supplied list of join columns for USING clause(JoinNode.getMatchingColumn). Set the column source's schema name Set the column source's table name Mark this column as an unreferenced column. Set the virtualColumnId for this ResultColumn Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Change an untyped null to a typed null. Tell whether this column is updatable by a positioned update. Tell whether this column is being updated. Verify that this RC is orderable. Returns TRUE if the ResultColumn used to stand in for a DEFAULT keyword in an insert/update statement.
Get the position of the Column. NOTE - position is 1-based. Returns the name of the Column. Get the name of the schema for the Column's base table, if any. Following example queries will all return APP (assuming user is in schema APP) select t.a from t select b.a from t as b select app.t.a from t Get the name of the underlying(base) table this column comes from, if any. Following example queries will all return T select a from t select b.a from t as b select t.a from t Returns a DataTypeDescriptor for the column. This DataTypeDescriptor will not represent an actual value, it will only represent the type that all values in the column will have. Return true if this result column represents a generated column. Tell us if the column is an autoincrement column or not. Return true if the column is wirtable by a positioned update.
Add a column to the list given a table name, column name, and data type. Return the just-added column. Add an RC to the end of the list for the RID from an index. NOTE: RC.expression is a CurrentRowLocationNode.  This was previously only used for non-select DML.  We test for this node when generating the holder above and generate the expected code.  (We really should create yet another new node type with its own code generation.) Add a ResultColumn (at this point, ResultColumn or AllResultColumn) to the list Walk the list and adjust the virtualColumnIds in the ResultColumns by the specified amount.  If ResultColumn.expression is a VirtualColumnNode, then we adjust the columnId there as well. Determine if all of the RC.expressions are columns in the source result set. This is useful for determining if we need to do reflection at execution time. Return whether or not all of the RCs in the list whose expressions are ColumnReferences are from the same table.  One place this is useful for distinct elimination based on the existence of a uniqueness condition. Append a given ResultColumnList to this one, resetting the virtual column ids in the appended portion. Bind the expressions in this ResultColumnList.  This means binding the expression under each ResultColumn node. Bind the result columns by their names.  This is useful for GRANT and REVOKE statements like "GRANT SELECT ON t(c1,c1,c3) TO george", where the user specified a column list. This method does not check for duplicate column names. end of bindResultColumnsByName( TableDescriptor) Bind the result columns by their names.  This is useful for update, grant, and revoke statements, and for INSERT statements like "insert into t (a, b, c) values (1, 2, 3)" where the user specified a column list. If the statment is an insert or update verify that the result column list does not contain any duplicates. NOTE: We pass the ResultColumns position in the ResultColumnList so that the VirtualColumnId gets set. Bind the result columns by their names.  This is useful for update VTI statements, and for INSERT statements like "insert into new t() (a, b, c) values (1, 2, 3)" where the user specified a column list. Also, verify that the result column list does not contain any duplicates. NOTE: We pass the ResultColumns position in the ResultColumnList so that the VirtualColumnId gets set. Bind the result columns by ordinal position.  This is useful for INSERT statements like "insert into t values (1, 2, 3)", where the user did not specify a column list. Bind the result columns to the expressions that live under them. All this does is copy the datatype information to from each expression to each result column.  This is useful for SELECT statements, where the result type of each column is the type of the column's expression. Bind any untyped null nodes to the types in the given ResultColumnList. Nodes that don't know their type may pass down nulls to children nodes.  In the case of something like a union, it knows to try its right and left result sets against each other. But if a null reaches us, it means we have a null type that we don't know how to handle. Build an empty index row for the given conglomerate. Build an empty row with the size and shape of the ResultColumnList. Shorthand for {@code buildRowTemplate(null, false)}. Build an {@code ExecRowBuilder} instance that produces a row of the same shape as this result column list. Verify that all of the columns in the SET clause of a positioned update appear in the cursor's FOR UPDATE OF list. Walk the RCL and check for DEFAULTs.  DEFAULTs are invalid at the time that this method is called, so we throw an exception if found. NOTE: The grammar allows: VALUES DEFAULT; Verify that all the result columns have expressions that are storable for them.  Check versus the expressions under the ResultColumns. Verify that all the result columns have expressions that are storable for them.  Check versus the given ResultColumnList. Clear the column references from the RCL. (Restore RCL back to a state where none of the RCs are marked as referenced.) Adjust virtualColumnId values due to result column removal This method is called when a duplicate column has been detected and removed from the list. We iterate through each of the other columns in the list and notify them of the column removal so they can adjust their virtual column id if necessary. * Check whether the column lengths and types of the result columns * match the expressions under those columns.  This is useful for * INSERT and UPDATE statements.  For SELECT statements this method * should always return true.  There is no need to call this for a * DELETE statement. * NOTE: We skip over generated columns since they won't have a * column descriptor. * * @return	true means all the columns match their expressions, *		false means at least one column does not match its *		expression Return true if some columns in this list are updatable. dealingWithSelectResultColumnList true means we are dealing with ResultColumnList for a select sql. When dealing with ResultColumnList for select sql, it is possible that not all the updatable columns are projected in the select column list and hence it is possible that we may not find the column to be updated in the ResultColumnList and that is why special handling is required when dealingWithSelectResultColumnList is true. eg select c11, c13 from t1 for update of c11, c12 In the eg above, we will find updatable column c11 in the select column list but we will not find updatable column c12 in the select column list Create a new, compacted RCL based on the referenced RCs in this list.  If the RCL being compacted is for an updatable scan, then we simply return this. The caller tells us whether or not they want a new list if there is no compaction because all RCs are referenced. This is useful in the case where the caller needs a new RCL for existing RCs so that it can augment the new list. Return whether or not this RCL contains an AllResultColumn. This is useful when dealing with SELECT * views which reference tables that may have had columns added to them via ALTER TABLE since the view was created. Copy the RCs from this list to the supplied target list. Create a shallow copy of a ResultColumnList and its ResultColumns. (All other pointers are preserved.) Useful for building new ResultSetNodes during preprocessing. Copy the referenced RCs from this list to the supplied target list. Copy the result column names from the given ResultColumnList to this ResultColumnList.  This is useful for insert-select, where the columns being inserted into may be different from the columns being selected from.  The result column list for an insert is supposed to have the column names being inserted into. Count the number of RCs in the list that are referenced. Generate an RCL to match the contents of a ResultSetMetaData. This is useful when dealing with VTIs. Project out any unreferenced ResultColumns from the list and reset the virtual column ids in the referenced ResultColumns. If all ResultColumns are projected out, then the list is not empty. Expand any *'s in the ResultColumnList.  In addition, we will guarantee that each ResultColumn has a name.  (All generated names will be unique across the entire statement.) Expand this ResultColumnList by adding all columns from the given table that are not in this list.  The result is sorted by column position. Export the result column names to the passed in String[]. Given a ResultColumn at the next deepest level in the tree, search this RCL for its parent ResultColumn. For order by, get a ResultColumn that matches the specified columnName. This method is called during pull-up processing, at the very start of bind processing, as part of OrderByList.pullUpOrderByColumns. Its job is to figure out whether the provided column (from the ORDER BY list) already exists in the ResultColumnList or not. If the column does not exist in the RCL, we return NULL, which signifies that a new ResultColumn should be generated and added ("pulled up") to the RCL by our caller. Note that at this point in the processing, we should never find this column present in the RCL multiple times; if the column is already present in the RCL, then we don't need to, and won't, pull a new ResultColumn up into the RCL. If the caller specified "SELECT *", then the RCL at this point contains a special AllResultColumn object. This object will later be expanded and replaced by the actual set of columns in the table, but at this point we don't know what those columns are, so we may pull up an OrderByColumn which duplicates a column in the *-expansion; such duplicates will be removed at the end of bind processing by OrderByList.bindOrderByColumns. check if any autoincrement or generated columns exist in the result column list. called from insert or update where you cannot insert/update the value of a generated or autoincrement column. check if any autoincrement or generated columns exist in the result column list. called from insert or update where you cannot insert/update the value of a generated or autoincrement column. Generate the code to create an empty row in the constructor. Walk the list and replace ResultColumn.expression with a new VirtualColumnNode.  This is useful when propagating a ResultColumnList up the query tree. NOTE: This flavor marks all of the underlying RCs as referenced. Walk the list and replace ResultColumn.expression with a new VirtualColumnNode.  This is useful when propagating a ResultColumnList up the query tree. Generate the code to place the columns' values into a row variable named "r". This wrapper is here rather than in ResultColumn, because that class does not know about the position of the columns in the list. Generate the code to place the columns' values into a row variable named "r". This wrapper is here rather than in ResultColumn, because that class does not know about the position of the columns in the list. This is the method that does the work. <p> Generate the code for a method (userExprFun) which creates a row and, column by column, stuffs it with the evaluated expressions of our ResultColumns. The method returns the stuffed row. </p> This is the method that does the work. Generate the code to place the columns' values into a row variable named "r". This wrapper is here rather than in ResultColumn, because that class does not know about the position of the columns in the list. Get a ResultColumn that matches the specified columnName and mark the ResultColumn as being referenced. NOTE - this flavor enforces no ambiguity (at most 1 match) Only FromSubquery needs to call this flavor since it can have ambiguous references in its own list. Get an array of strings for all the columns in this RCL. Get an array of column positions (1-based) for all the columns in this RCL. Assumes that all the columns are in the passed-in table Get a FormatableBitSet of the columns referenced in this rcl Return whether or not a count mismatch is allowed between this RCL, as a derived column list, and an underlying RCL.  This is allowed for SELECT * views when an underlying table has had columns added to it via ALTER TABLE. return Whether or not a mismatch is allowed. Get the join columns from this list. This is useful for a join with a USING clause. (ANSI specifies that the join columns appear 1st.) Get a ResultColumn from a column position (1-based) in the list, null if out of range (for order by). For order by column bind, get a ResultColumn that matches the specified columnName. This method is called during bind processing, in the special "bind the order by" call that is made by CursorNode.bindStatement(). The OrderByList has a special set of bind processing routines that analyzes the columns in the ORDER BY list and verifies that each column is one of: - a direct reference to a column explicitly mentioned in the SELECT list - a direct reference to a column implicitly mentioned as "SELECT *" - a direct reference to a column "pulled up" into the result column list - or a valid and fully-bound expression ("c+2", "YEAR(hire_date)", etc.) At this point in the processing, it is possible that we'll find the column present in the RCL twice: once because it was pulled up during statement compilation, and once because it was added when "SELECT *" was expanded into the table's actual column list. If we find such a duplicated column, we can, and do, remove the pulled-up copy of the column and point the OrderByColumn to the actual ResultColumn from the *-expansion. Note that the association of the OrderByColumn with the corresponding ResultColumn in the RCL occurs in OrderByColumn.resolveAddedColumn. Get the position of first result column with the given name. Generate a FormatableBitSet representing the columns that are referenced in this RCL. The caller decides if they want this FormatableBitSet if every RC is referenced. Get a ResultColumn from a column position (1-based) in the list Return a result column, if any found, which contains in its expression/&#123;VCN,CR&#125; chain a result column with the given columnNumber from a FromTable with the given tableNumber. <p/> Used by the optimizer preprocess phase when it is flattening queries, which has to use the pair &#123;table number, column number&#125; to uniquely distinguish the column desired in situations where the same table may appear multiple times in the queries with separate correlation names, and/or column names from different tables may be the same (hence looking up by column name will not always work), cf DERBY-4679. <p/> {@code columnName} is used to assert that we find the right column. If we found a match on (tn, cn) but columnName is wrong, return null. Once we trust table numbers and column numbers to always be correct, cf. DERBY-4695, we could remove this parameter. Take a column position and a ResultSetNode and find the ResultColumn in this RCL whose source result set is the same as the received RSN and whose column position is the same as the received column position. Get a ResultColumn that matches the specified columnName and mark the ResultColumn as being referenced. Get a ResultColumn that matches the specified columnName. If requested to, mark the column as referenced. Get a ResultColumn that matches the specified columnName and mark the ResultColumn as being referenced. Return an array that contains references to the columns in this list sorted by position. Return an array holding the 0 based heap offsets of the StreamStorable columns in this ResultColumnList. This returns null if this list does not contain any StreamStorableColumns. The list this returns does not contain duplicates. This should only be used for a resultColumnList the refers to a single heap such as the target for an Insert, Update or Delete. Get the size of all the columns added together.  Does <B>NOT</B> include the column overhead that the store requires. Also, will be a very rough estimate for user types. Debugging methods Verify that all ResultColumns and their expressions have type information and that the type information between the respective RCs and expressions matches. Return whether or not this RCL can be flattened out of a tree. It can only be flattened if the expressions are all cloneable. Do the 2 RCLs have the same type and length. This is useful for UNIONs when deciding whether a NormalizeResultSet is required. Make a ResultDescription for use in a ResultSet. This is useful when generating/executing a NormalizeResultSet, since it can appear anywhere in the tree. Map the source columns to these columns.  Build an array to represent the mapping. For each RC, if the expression is simply a VCN or a CR then set the array element to be the virtual column number of the source RC.  Otherwise, set the array element to -1. This is useful for determining if we need to do reflection at execution time. <p/> Also build an array of boolean for columns that point to the same virtual column and have types that are streamable to be able to determine if cloning is needed at execution time. Walk the list and mark all RCs as unreferenced.  This is useful when recalculating which RCs are referenced at what level like when deciding which columns need to be returned from a non-matching index scan (as opposed to those returned from the base table). Mark all the columns in the select sql that this result column list represents as updatable if they match the columns in the given update column list. **** Take note of the size of this RCL _before_ we start processing/binding it.  This is so that, at bind time, we can tell if any columns in the RCL were added internally by us (i.e. they were not specified by the user and thus will not be returned to the user). Mark all the (base) columns in this list as updatable by a positioned update statement.  This is necessary for positioned update statements, because we expand the column list to include all the columns in the base table, and we need to be able to tell which ones the user is really trying to update so we can determine correctly whether all the updated columns are in the "for update" list. Mark as updatable all the columns in this result column list that match the columns in the given update column list Mark all the columns in this list as updated by an update statement. Mark as updatable all the columns in this result column list that match the columns in the given update column list. Generate (unique across the entire statement) column names for those ResultColumns in this list which are not named. Create a row location template of the right type for the source conglomerate. Determine whether this RCL is a No-Op projection of the given RCL. It only makes sense to do this if the given RCL is from the child result set of the ProjectRestrict that this RCL is from.  Build this ResultColumnList from a table description and an array of column IDs. Preprocess the expression trees under the RCL. We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Validate the derived column list (DCL) and propagate the info from the list to the final ResultColumnList. Or in any isReferenced booleans from the virtual column chain. That is the isReferenced bits on each ResultColumn on the list will be set if the ResultColumn is referenced or if any VirtualColumnNode in its expression chain refers to a referenced column. end of pullVirtualIsReferenced Record the top level ColumnReferences in the specified array and table map This is useful when checking for uniqueness conditions. NOTE: All top level CRs assumed to be from the same table. The size of the array is expected to be the # of columns in the table of interest + 1, so we use 1-base column #s. Record the column ids of the referenced columns in the specified array. Look for and reject ? parameters under ResultColumns.  This is done for SELECT statements. Check for (and reject) XML values directly under the ResultColumns. This is done for SELECT/VALUES statements.  We reject values in this case because JDBC does not define an XML type/binding and thus there's no standard way to pass such a type back to a JDBC application. Note that we DO allow an XML column in a top-level RCL IF that column was added to the RCL by _us_ instead of by the user.  For example, if we have a table: create table t1 (i int, x xml) and the user query is: select i from t1 order by x the "x" column will be added (internally) to the RCL as part of ORDER BY processing--and so we need to allow that XML column to be bound without throwing an error.  If, as in this case, the XML column reference is invalid (we can't use ORDER BY on an XML column because XML values aren't ordered), a more appropriate error message should be returned to the user in later processing. If we didn't allow for this, the user would get an error saying that XML columns are not valid as part of the result set--but as far as s/he knows, there isn't such a column: only "i" is supposed to be returned (the RC for "x" was added to the RCL by _us_ as part of ORDER BY processing). ASSUMPTION: Any RCs that are generated internally and added to this RCL (before this RCL is bound) are added at the _end_ of the list.  If that's true, then any RC with an index greater than the size of the initial (user-specified) list must have been added internally and will not be returned to the user. Remap all ColumnReferences in this tree to be clones of the underlying expression. Remove any generated columns from this RCL. Remove the columns which are join columns (in the joinColumns RCL) from this list.  This is useful for a JOIN with a USING clause. Remove any columns that may have been added for an order by clause. In a query like: <pre>select a from t order by b</pre> b is added to the select list However in the final projection, after the sort is complete, b will have to be removed. Replace any DEFAULTs with the associated tree for the default if allowed, or flag. Reset the virtual column ids for all of the underlying RCs.  (Virtual column ids are 1-based.) Return whether or not the same result row can be used for all rows returned by the associated ResultSet.  This is possible if all entries in the list are constants or AggregateNodes. Set the value of whether or not a count mismatch is allowed between this RCL, as a derived column list, and an underlying RCL.  This is allowed for SELECT * views when an underlying table has had columns added to it via ALTER TABLE. Set the default in a ResultColumn * Indicate that the conglomerate is an index, so we need to generate a * RowLocation as the last column of the result set. * * @param cid	The conglomerate id of the index Set the nullability of every ResultColumn in this list Mark all of the ResultColumns as redundant. Useful when chopping a ResultSetNode out of a tree when there are still references to its RCL. Set the resultSetNumber in all of the ResultColumns. Set up the result expressions for a UNION, INTERSECT, or EXCEPT: o Verify union type compatiblity o Get dominant type for result (type + max length + nullability) o Create a new ColumnReference with dominant type and name of from this RCL and make that the new expression. o Set the type info for in the ResultColumn to the dominant type NOTE - We are assuming that caller has generated a new RCL for the UNION with the same names as the left side's RCL and copies of the expressions. Return an array of all my column positions, sorted in ascending order. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Return true if the types of two expressions are union compatible. The rules for union compatibility are found in the SQL Standard, part 2, section 7.3 (<query expression>), syntax rule 20.b.ii. That in turn, refers you to section 9.3 (Result of data type combinations). See, for instance, <a href="https://issues.apache.org/jira/browse/DERBY-4692">DERBY-4692</a>. This logic may enforce only a weaker set of rules. Here is the original comment on the original logic: "We want to make sure that the types are assignable in either direction and they are comparable." We may need to revisit this code to make it conform to the Standard. Returns true if the given column position is for a column that will be or could be updated by the positioned update of a cursor. Does the column list contain any of the given column positions that are updated? Implements same named routine in UpdateList. Verify that all of the RCs in this list are comparable. Verify that all of the column names in this list are contained within the ColumnDefinitionNodes within the TableElementList. Check the uniqueness of the column names within a column list.
Return the position of the column matching the passed in names following the JDBC rules for ResultSet.getXXX and updateXXX. Rules are the matching is case insensitive and the insensitive name matches the first column found that matches (starting at postion 1). Returns the number of columns in the result set. Returns a ResultColumnDescriptor for the column, given the ordiinal position of the column. NOTE - position is 1-based. Return information about all the columns. Return the information about a single column (0-based indexing) Get the saved JDBC ResultSetMetaData. Will return null if setMetaData() has not been called on this object. The caller then should manufacture a ResultSetMetaData object and pass it into setMetaData. Returns an identifier that tells what type of statement has been executed. This can be used to determine what other methods to call to get the results back from a statement. For example, a SELECT statement returns rows and columns, while other statements don't, so you would only call getColumnCount() or getColumnType() for SELECT statements. Set the JDBC ResultSetMetaData for this ResultDescription. A ResultSetMetaData object can be saved in the statement plan using this method. This only works while the ResultSetMetaData api does not contain a getConnection() method or a close method. <BR> If this object already has a saved meta data object this call will do nothing. Due to synchronization the saved ResultSetMetaData object may not be the one passed in, ie. if two threads call this concurrently, only one will be saved. It is assumed the JDBC layer passes in a ResultSetMetaData object based upon this. Get a new result description that has been truncated from input column number.   If the input column is 5, then columns 5 to getColumnCount() are removed. The new ResultDescription points to the same ColumnDescriptors (this method performs a shallow copy. The saved JDBC ResultSetMetaData will not be copied.
Add a warning to this result set. Determine if the result set is at one of the positions according to the constants above (ISBEFOREFIRST etc). Only valid and called for scrollable cursors. Tells the system to clean up on an error. Clear the current row. The cursor keeps it current position, however it cannot be used for positioned updates or deletes until a fetch is done. This is done after a commit on holdable result sets. A fetch is achieved by calling one of the positioning methods: getLastRow(), getNextRow(), getPreviousRow(), getFirstRow(), getRelativeRow(..) or getAbsoluteRow(..). Tells the system that there will be no more calls to getNextRow() (until the next open() call), so it can free up the resources associated with the ResultSet. Tells the system that there will be no more access to any database information via this result set; in particular, no more calls to open(). Will close the result set if it is not already closed. Returns the row at the absolute position from the query, and returns NULL when there is no such position. (Negative position means from the end of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: An exception will be thrown on 0. ResultSet for rows inserted into the table (contains auto-generated keys columns only) Get the Timestamp for the beginning of execution. Returns the name of the cursor, if this is cursor statement of some type (declare, open, fetch, positioned update, positioned delete, close). Get the Timestamp for the end of execution. Get the execution time in milliseconds. Returns the first row from the query, and returns NULL when there are no rows. Returns the last row from the query, and returns NULL when there are no rows. Returns the next row from the query, and returns NULL when there are no more rows. Returns the previous row from the query, and returns NULL when there are no more previous rows. Returns the row at the relative position from the current cursor position, and returns NULL when there is no such position. (Negative position means toward the beginning of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: 0 is valid. NOTE: An exception is thrown if the cursor is not currently positioned on a row. Returns a ResultDescription object, which describes the results of the statement this ResultSet is in. This will *not* be a description of this particular ResultSet, if this is not the outermost ResultSet. Returns the row number of the current row.  Row numbers start from 1 and go to 'n'.  Corresponds to row numbering used to position current row in the result set (as per JDBC). Only valid and called for scrollable cursors. Get the subquery ResultSet tracking array from the top ResultSet. (Used for tracking open subqueries when closing down on an error.) Return the total amount of time spent in this ResultSet Return the set of warnings generated during the execution of this result set. The warnings are cleared once this call returns. Find out if the ResultSet is closed or not. Will report true for result sets that do not return rows. Returns the number of rows affected by the statement. Only valid of returnsRows() returns false. For other DML statements, it returns the number of rows modified by the statement. For statements that do not affect rows (like DDL statements), it returns zero. Needs to be called before the result set will do anything. Need to call before getNextRow(), or for a result set that doesn't return rows, this is the call that will cause all the work to be done. Returns TRUE if the statement returns rows (i.e. is a SELECT or FETCH statement), FALSE if it returns no rows. Sets the current position to after the last row and returns NULL because there is no current row. Sets the current position to before the first row and returns NULL because there is no current row. <p> Produce an xml image of this ResultSet and its descendant ResultSets. Appends an element to the parentNode and returns the appended element. </p>
Chain a warning on the result set object. The query was ended at the server because all rows have been retrieved.
An any result set iterates over its source, returning a row with all columns set to nulls if the source returns no rows. A table scan result set forms a result set on a scan of a table. The rows can be constructed as they are requested from the result set. <p> This form of the table scan operation is simple, and is to be used when there are no predicates to be passed down to the scan to limit its scope on the target table. A call statement result set simply reports that it completed. It does not return rows. A current of result set forms a result set on the current row of an open cursor. It is used to perform positioned operations such as positioned update and delete, using the result set paradigm.  DDL operations  Generic DDL result set creation. A delete Cascade result set simply reports that it completed, and the number of rows deleted.  It does not return rows. The delete has been completed once the delete result set is available. An update result set simply reports that it completed, and the number of rows updated.  It does not return rows. The update has been completed once the update result set is available. A delete result set simply reports that it completed, and the number of rows deleted.  It does not return rows. The delete has been completed once the delete result set is available. A delete VTI result set simply reports that it completed, and the number of rows deleted.  It does not return rows. The delete has been completed once the delete result set is available. A DistinctGroupedAggregateResultSet computes scalar aggregates when at least one of them is a distinct aggregate. It will compute the aggregates when open. A DistinctScalarAggregateResultSet computes scalar aggregates when at least one of them is a distinct aggregate. It will compute the aggregates when open. A distinct scan result set pushes duplicate elimination into the scan. <p> A GroupedAggregateResultSet computes non-distinct grouped aggregates. It will compute the aggregates when open. A hash join. A left outer join using a hash join. A hash result set forms a result set on a hash table built on a scan of a table. The rows are put into the hash table on the 1st open. <p> A hash table result set builds a hash table on its source, applying a list of predicates, if any, to the source, when building the hash table.  It then does a look up into the hash table on a probe. The rows can be constructed as they are requested from the result set. An index row to base row result set gets an index row from its source and uses the RowLocation in its last column to get the row from the base conglomerate. <p>  DML statement operations  An insert result set simply reports that it completed, and the number of rows inserted.  It does not return rows. The insert has been completed once the insert result set is available. An insert VTI result set simply reports that it completed, and the number of rows inserted.  It does not return rows. The insert has been completed once the insert result set is available.  Misc operations  A last index key result set returns the last row from the index in question.  It is used as an ajunct to max(). A ResultSet which materializes the underlying ResultSet tree into a temp table on the 1st open.  All subsequent "scans" of this ResultSet will return results from the temp table. A MERGE result set simply reports that it completed, and the number of rows that it INSERTed/UPDATEd/DELETEdd.  It does not return rows. The delete has been completed once the MERGE result set is available.  MISC operations  Generic Misc result set creation. A multi-probe result set, used for probing an index with one or more target values (probeValues) and returning the matching rows.  This type of result set is useful for IN lists as it allows us to avoid scannning an entire, potentially very large, index for a mere handful of rows (DERBY-47). All arguments are the same as for TableScanResultSet, plus the following: A nested loop left outer join result set forms a result set on top of 2 other result sets. The rows can be constructed as they are requested from the result set. <p> This form of the nested loop join operation is simple, and is to be used when there are no join predicates to be passed down to the join to limit its scope on the right ResultSet. A nested loop join result set forms a result set on top of 2 other result sets. The rows can be constructed as they are requested from the result set. <p> This form of the nested loop join operation is simple, and is to be used when there are no join predicates to be passed down to the join to limit its scope on the right ResultSet. REMIND: needs more description... A once result set iterates over its source, raising an error if the source returns &gt; 1 row and returning a row with all columns set to nulls if the source returns no rows.  Query expression operations  A project restrict result set iterates over its source, evaluating a restriction and when it is satisfied, constructing a row to return in its result set based on its projection. The rows can be constructed as they are requested from the result set. A Dependent table scan result set forms a result set on a scan of a dependent table for the rows that got materialized on the scan of its parent table and if the row being deleted on parent table has a reference in the dependent table. This result sets implements the filtering needed by <result offset clause> and <fetch first clause>. It is only ever generated if at least one of the two clauses is present. A row result set forms a result set on a single, known row value. It is used to turn constant rows into result sets for use in the result set paradigm. The row can be constructed when it is requested from the result set. A ScalarAggregateResultSet computes non-distinct scalar aggregates. It will compute the aggregates when open. A ResultSet which provides the insensitive scrolling functionality for the underlying result set by materializing the underlying ResultSet tree into a hash table while scrolling forward. The SetOpResultSet is used to implement an INTERSECT or EXCEPT operation. It selects rows from two ordered input result sets.  Transaction operations   A sort result set sorts its source and if requested removes duplicates.  It will generate the entire result when open, and then return it a row at a time. <p> If passed aggregates it will do scalar or vector aggregate processing.  A list of aggregator information is passed off of the PreparedStatement's savedObjects.  Aggregation and SELECT DISTINCT cannot be processed in the same sort. The Union interface is used to evaluate the union (all) of two ResultSets. (Any duplicate elimination is performed above this ResultSet.) Forms a ResultSet returning the union of the rows in two source ResultSets.  The column types in source1 and source2 are assumed to be the same. An update result set simply reports that it completed, and the number of rows updated.  It does not return rows. The update has been completed once the update result set is available.  A VTI result set wraps a user supplied result set. A table scan result set forms a result set on a scan of a table. The rows can be constructed as they are requested from the result set. <p> This form of the table scan operation is simple, and is to be used when there are no predicates to be passed down to the scan to limit its scope on the target table. A OLAP window on top of a regular result set. It is used to realize window functions. <p>
It may be we have a SELECT view underneath a LOJ. Return null for now.. we don't do any optimization. Accept the visitor for all visitable children of this node. Add a new predicate to the list.  This is useful when doing subquery transformations, when we build a new predicate with the left side of the subquery operator and the subquery's result column. Notify the underlying result set tree that the optimizer has chosen to "eliminate" a sort.  Sort elimination can happen as part of preprocessing (see esp. SelectNode.preprocess(...)) or it can happen if the optimizer chooses an access path that inherently returns the rows in the correct order (also known as a "sort avoidance" plan). In either case we drop the sort and rely on the underlying result set tree to return its rows in the correct order. For most types of ResultSetNodes we automatically get the rows in the correct order if the sort was eliminated. One exception to this rule, though, is the case of an IndexRowToBaseRowNode, for which we have to disable bulk fetching on the underlying base table.  Otherwise the index scan could return rows out of order if the base table is updated while the scan is "in progress" (i.e. while the result set is open). In order to account for this (and potentially other, similar issues in the future) this method exists to notify the result set node that it is expected to return rows in the correct order.  The result set can then take necessary action to satsify this requirement--such as disabling bulk fetch in the case of IndexRowToBaseRowNode. All of that said, any ResultSetNodes for which we could potentially eliminate sorts should override this method accordingly.  So we don't ever expect to get here. Same goal as adjustForSortElimination above, but this version takes a RequiredRowOrdering to allow nodes to adjust based on the ORDER BY clause, if needed. Assign the next resultSetNumber to the resultSetNumber in this ResultSetNode. Expected to be done during generate(). Bind the expressions in this ResultSetNode.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the expressions in this ResultSetNode if it has tables.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the non VTI tables in this ResultSetNode.  This includes getting their descriptors from the data dictionary and numbering them. Bind the result columns for this ResultSetNode to a base table. This is useful for INSERT and UPDATE statements, where the result columns get their types from the table being updated or inserted into. If a result column list is specified, then the verification that the result column list does not contain any duplicates will be done when binding them by name. Bind the result columns of this ResultSetNode when there is no base table to bind them to.  This is useful for SELECT statements, where the result columns get their types from the expressions that live under them. Bind the expressions in the target list.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression.  This is useful for EXISTS subqueries, where we need to validate the target list before blowing it away and replacing it with a SELECT true. Bind untyped nulls to the types in the given ResultColumnList. This is used for binding the nulls in row constructors and table constructors.  In all other cases (as of the time of this writing), we do nothing. Bind the VTI tables in this ResultSetNode.  This includes getting their descriptors from the data dictionary and numbering them. The optimizer's decision on the access path for a result set may require the generation of extra result sets.  For example, if it chooses an index for a FromBaseTable, we need an IndexToBaseRowNode above the FromBaseTable (and the FromBaseTable has to change its column list to match the index. This method in the parent class does not generate any extra result sets. It may be overridden in child classes. * Check whether the column lengths and types of the result columns * match the expressions under those columns.  This is useful for * INSERT and UPDATE statements.  For SELECT statements this method * should always return true.  There is no need to call this for a * DELETE statement. * * @return	true means all the columns match their expressions, *		false means at least one column does not match its *		expression Consider materialization for this ResultSet tree if it is valid and cost effective (It is not valid if incorrect results would be returned.) Create a ResultColumn for a column with a generation clause. Decrement (query block) level (0-based) for all of the tables in this ResultSet tree. This is useful when flattening a subquery. This ResultSet is the source for an Insert.  The target RCL is in a different order and/or a superset of this RCL.  In most cases we will add a ProjectRestrictNode on top of the source with an RCL that matches the target RCL. NOTE - The new or enhanced RCL will be fully bound. Ensure that the top of the RSN tree has a PredicateList. Fill the referencedTableMap with this ResultSetNode. Evaluate whether or not the subquery in a FromSubquery is flattenable. Currently, a FSqry is flattenable if all of the following are true: o  Subquery is a SelectNode. (ie, not a RowResultSetNode or a UnionNode) o  It contains no top level subqueries.  (RESOLVE - we can relax this) o  It does not contain a group by or having clause o  It does not contain aggregates. Generate the RC/expression for an unspecified column in an insert. Use the default if one exists. Put a ProjectRestrictNode on top of this ResultSetNode. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) This is useful for UNIONs, where we want to generate a DistinctNode above the UnionNode to eliminate the duplicates, because DistinctNodes expect their immediate child to be a PRN. Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Get a parent ProjectRestrictNode above us. This is useful when we need to preserve the user specified column order when reordering the columns in the distinct when we combine an order by with a distinct. Generate the code for a NormalizeResultSet. The call must push two items before calling this method <OL> <LI> pushGetResultSetFactoryExpression <LI> the expression to normalize </OL> Generate a ProjectRestrictNode to put on top of this node if it's the source for an insert, and the RCL needs reordering and/or addition of columns in order to match the target RCL. General logic shared by Core compilation and by the Replication Filter compiler. A couple ResultSets (the ones used by PREPARE SELECT FILTER) implement this method. Expand "*" into a ResultColumnList with all of the columns in the table's result list. Get the final cost estimate which we've set so far Get the CostEstimate for this ResultSetNode. return the target table of an updatable cursor result set. since this is not updatable, just return null. Get the final CostEstimate for this ResultSetNode. Get the FromList.  Create and return an empty FromList.  (Subclasses which actuall have FromLists will override this.)  This is useful because there is a FromList parameter to bindExpressions() which is used as the common FromList to bind against, allowing us to support correlation columns under unions in subqueries. Determine whether or not the specified name is an exposed name in the current query block. Try to find a ResultColumn in the table represented by this FromTable that matches the name in the given ColumnReference. Get a cost estimate to use for this ResultSetNode. Get the optimizer being used on this result set Get the optimizer for this result set. Generate an RCL that can replace the original RCL of this node to match the RCL of the target for the insert. Get the referencedTableMap for this ResultSetNode. Must be public in order to satisfy the Optimizable contract. Get the resultColumns for this ResultSetNode Get the resultSetNumber in this ResultSetNode. Expected to be set during generate(). Must be public in order to satisfy the Optimizable contract. Get the scratch estimate Return true if this is a cursor target table Return true if this is the source result set for an INSERT Return whether or not the underlying ResultSet tree is for a NOT EXISTS join. Return whether or not the underlying ResultSet tree will return a single row, at most. This is important for join nodes where we can save the extra next on the right side if we know that it will return at most 1 row. Return whether or not the underlying ResultSet tree is ordered on the specified columns. RESOLVE - This method currently only considers the outermost table of the query block. Is it possible to do a distinct scan on this ResultSet tree. (See SelectNode for the criteria.) Return true if this is a statement result set Determine if this result set is updatable or not, for a cursor (i.e., is it a cursor-updatable select).  This returns false and we expect selectnode to refine it for further checking. Make a ResultDescription for use in a ResultSet. This is useful when generating/executing a NormalizeResultSet, since it can appear anywhere in the tree. Mark this ResultSetNode as the target table of an updatable cursor.  Most types of ResultSetNode can't be target tables. Mark the underlying scan as a distinct scan. This method is overridden to allow a resultset node to know if it is the one controlling the statement -- i.e., it is the outermost result set node for the statement. Modify the access paths according to the decisions the optimizer made.  This can include adding project/restrict nodes, index-to-base-row nodes, etc. Modify the access paths according to the decisions the optimizer made.  This can include adding project/restrict nodes, index-to-base-row nodes, etc. Mark this ResultSetNode as *not* the target table of an updatable cursor. Mark this node and its children as not being a flattenable join. Count the number of distinct aggregates in the list. By 'distinct' we mean aggregates of the form: <I>SELECT MAX(DISTINCT x) FROM T</I> Optimize a ResultSetNode. This means choosing the best access path for each table under the ResultSetNode, among other things. The only RSNs that need to implement their own optimize() are a SelectNode and those RSNs that can appear above a SelectNode in the query tree.  Currently, a ProjectRestrictNode is the only RSN that can appear above a SelectNode. Parse a default and turn it into a query tree. Return whether or not to materialize this ResultSet tree. Preprocess a ResultSetNode - this currently means: o  Generating a referenced table map for each ResultSetNode. o  Putting the WHERE and HAVING clauses in conjunctive normal form (CNF). o  Converting the WHERE and HAVING clauses into PredicateLists and classifying them. o  Ensuring that a ProjectRestrictNode is generated on top of every FromBaseTable and generated in place of every FromSubquery. o  Pushing single table predicates down to the new ProjectRestrictNodes. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Find the unreferenced result columns and project them out. Push down the offset and fetch first parameters, if any. This method should be overridden by the result sets that need this. Presumes a new level has been initialized by {@link #pushQueryExpressionSuffix()}. Push the order by list down from the cursor node into its child result set so that the optimizer has all of the information that it needs to consider sort avoidance. Presumes a new level has been initialized by {@link #pushQueryExpressionSuffix()}. Set up a new level for order by and fetch/offset clauses. See Javadoc for {@link ResultSetNode.QueryExpressionClauses}. Overridden by implementors of pushOrderByNode, pushOffsetFetchFirst. Search to see if a query references the specifed table name. Check for (and reject) ? parameters directly under the ResultColumns. This is done for SELECT statements. Check for (and reject) XML values directly under the ResultColumns. This is done for SELECT/VALUES statements.  We reject values in this case because JDBC does not define an XML type/binding and thus there's no standard way to pass such a type back to a JDBC application. Rename generated result column names as '1', '2' etc... These will be the result column names seen by JDBC clients. Replace any DEFAULTs with the associated tree for the default if allowed, or flag (when inside top level set operator nodes). Subqueries are checked for illegal DEFAULTs elsewhere. Return whether or not this ResultSet tree is guaranteed to return at most 1 row based on heuristics.  (A RowResultSetNode and a SELECT with a non-grouped aggregate will return at most 1 row.) Set the final cost estimate Set the CostEstimate for this ResultSetNode Set whether this is a cursor target table Remember that this node is the source result set for an INSERT. Set the optimizer for use on this result set Set the referencedTableMap in this ResultSetNode Set the resultColumns in this ResultSetNode Set the result column for the subquery to a boolean true, Useful for transformations such as changing: where exists (select ... from ...) to: where (select true from ...) NOTE: No transformation is performed if the ResultColumn.expression is already the correct boolean constant. Set the scratch estimate Set the type of each parameter in the result column list for this table constructor. Return whether or not this ResultSetNode contains a subquery with a reference to the specified target. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Get the lock mode for the target of an update statement (a delete or update).  The update mode will always be row for CurrentOfNodes.  It will be table if there is no where clause. Verify that a SELECT * is valid for this type of subquery.
think about splitting out the position cursor stuff from the fetch stuff

In addition, ResultSet objects are passed for convenient access to any material result set caches. Implementations of this interface should not dereference common layer ResultSet state, as it is passed in, but may dereference material layer ResultSet state if necessary for performance.
Get the estimated row count for the number of rows returned by the associated query or statement. Return information on the scan nodes from the statement execution plan as a String. Return the statement execution plan as a String.
ResultSetStatistics Objects  Get the matching ResultSetStatistics for the specified ResultSet.  RunTimeStatistics Object  RunTimeStatistics creation.
Bind this RevokeNode. Resolve all table, column, and routine references. end of bind Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. end of toString
INTERFACE METHODS This is the guts of the Execution-time logic for REVOKE role. OBJECT SHADOWS
Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. end of toString
Query property system to get the System lock level. <p> This routine will be called during boot after access has booted far enough, to allow access to the property conglomerate.  This routine will call the property system and set the value to be returned by getSystemLockLevel(). <p> ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************* * Concrete methods of RAMAccessManager, interfaces that control locking * level of the system. *************************************************************************** Return the locking level of the system. <p> This routine controls the lowest level of locking enabled for all locks for all tables accessed through this accessmanager.  The concrete implementation may set this value always to table level locking for a client configuration, or it may set it to row level locking for a server configuration. <p> If TransactionController.MODE_RECORD is returned table may either be locked at table or row locking depending on the type of access expected (ie. level 3 will require table locking for heap scans.)
Returns the next (as yet unreturned) role in the transitive closure of the grant or grant<sup>-1</sup> relation. The grant relation forms a DAG (directed acyclic graph). <pre> Example: Assume a set of created roles forming nodes: {a1, a2, a3, b, c, d, e, f, h, j} Assume a set of GRANT statements forming arcs: GRANT a1 TO b;   GRANT b TO e;  GRANT e TO h; GRANT a1 TO c;                  GRANT e TO f; GRANT a2 TO c;   GRANT c TO f;  GRANT f TO h; GRANT a3 TO d;   GRANT d TO f;  GRANT a1 to j; a1            a2         a3 / | \           |          | /  b  +--------&gt; c          d j   |              \        / e---+           \      / \   \           \    / \   \---------+ \  / \             \_ f \             / \           / \         / \       / \     / \   / h </pre> An iterator on the inverse relation starting at h for the above grant graph will return: <pre> closure(h, grant-inv) = {h, e, b, a1, f, c, a2, d, a3} </pre> <p> An iterator on normal (not inverse) relation starting at a1 for the above grant graph will return: <pre> closure(a1, grant)    = {a1, b, j, e, h, f, c} </pre>

Drop this role.descriptor Get the provider's type.  ////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////// Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Is this provider persistent?  A stored dependency will be required if both the dependent and provider are persistent.
Invoked by the metered OutputStream Close all the files. Delete a file in a privilege block Check to see if a file exists in a privilege block Get the length of a file in a privilege block Rename a file in a privilege block Generates and returns File from a pattern Gets a system property in a privileged block Opens a new file that and delegates it to a MeteredStream Opens a file in the privileged block Opens the output files files based on the configured pattern, limit, count, and append mode. Rotates the log files.  The metered OutputStream is closed,the log files are rotated and then a new metered OutputStream is created. Implements the write method of the OutputStream.  This writes the value to the metered stream.
Creates and returns the OutputStream for a RollingFileStream. The <tt>derbylog.properties</tt> file contains the configuration. If the file is not found, then hard coded default values are used to configure the RollingFileStream. <p>The following properties can be specified <dl> <dt>pattern</dt> <dd>The pattern to use, the default is <tt>%d/derby-%g.log</tt></dd> <dt>limit</dt> <dd>The file size limit, the default is <tt>1024000</tt></dd> <dt>count</dt> <dd>The file count, the default is <tt>10</tt></dd> <dt>append</dt> <dd>If true the last logfile is appended to, the default is <tt>true</tt></dd>
Returns an array containing the names of the parameters. As of DERBY 10.3, parameter names are optional (see DERBY-183 for more information). If the i-th parameter was unnamed, parameterNames[i] will contain a string of length 0. Types of the parameters. If there are no parameters then this may return null (or a zero length array). Old releases (10.3 and before) wrote out the runtime DataTypeDescriptor for routine parameter and return types. 10.4 onwards (DERBY-2775) always writes out the catalog type TypeDescriptor. Here we see what object was read from disk and if it was the old type, now mapped to OldRoutineType, we extract the catalog type and use that. Get the formatID which corresponds to this class. Formatable methods Read this object from a stream of stored objects. Set the collation type of all string types declared for use in this routine to the given collation type. Set the paramter types. Useful if they need to be bound. Get this alias info as a string.  NOTE: The "ALIASINFO" column in the SYSALIASES table will return the result of this method on a ResultSet.getString() call.  That said, since the dblook utility uses ResultSet.getString() to retrieve ALIASINFO and to generate the DDL, THIS METHOD MUST RETURN A STRING THAT IS SYNTACTICALLY VALID, or else the DDL generated by dblook will be incorrect. Write this object to a stream of stored objects.

Get the provider's type.  ////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////// Return the name of this Provider.  (Useful for errors.) ----- getter functions for rowfactory ------
This is the guts of the Execution-time logic for GRANT/REVOKE of a routine execute privilege end of executeConstantAction
Get a DataValueDescriptor in a Row by ordinal position (1-based). Set a DataValueDescriptor in a Row by ordinal position (1-based).
Close this RowChanger. Delete a row from the table and perform associated index maintenance. Return what column no in the input ExecRow (cf nextBaseRow argument to #updateRow) would correspond to selected column, if any. Finish processing the changes.  This means applying the deferred inserts for updates to unique indexes. Return the ConglomerateController from this RowChanger. This is useful when copying properties from heap to temp conglomerate on insert/update/delete. Insert a row into the table and perform associated index maintenance. Open this RowChanger. <P>Note to avoid the cost of fixing indexes that do not change during update operations use openForUpdate(). Open this RowChanger. <P>Note to avoid the cost of fixing indexes that do not change during update operations use openForUpdate(). Open this RowChanger to avoid fixing indexes that do not change during update operations. Sets the index names of the tables indices. Used for error reporting. Set the row holder for this changer to use. If the row holder is set, it wont bother saving copies of rows needed for deferred processing.  Also, it will never close the passed in rowHolder. Update a row in the table and perform associated index maintenance.
Close this RowChanger. Delete a row from the table and perform associated index maintenance. Finish processing the changes.  This means applying the deferred inserts for updates to unique indexes.  Insert a row into the table and perform associated index maintenance. Open this RowChanger. <P>Note to avoid the cost of fixing indexes that do not change during update operations use openForUpdate().  Open this RowChanger to avoid fixing indexes that do not change during update operations.  Set the row holder for this changer to use. If the row holder is set, it wont bother saving copies of rows needed for deferred processing.  Also, it will never close the passed in rowHolder. Update a row in the table and perform associated index maintenance.
Generate code. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
ResultSet interface   Gets information from source RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the requested values computed from the next row (if any) <p>  CursorResultSet interface  Gets information from its source. Return the total amount of time spent in this ResultSet Return underlying result set (the source og this result set) if it is a ProjectRestrictResultSet, else null. Override of NoPutResultSetImpl method. Ask the source.  NoPutResultSet interface  Open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... Reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
Get the total estimated number of rows in the container. <p> The number is a rough estimate and may be grossly off.  In general the server will cache the row count and then occasionally write the count unlogged to a backing store.  If the system happens to shutdown before the store gets a chance to update the row count it may wander from reality. <p> For btree conglomerates this call will return the count of both user rows and internal implementaation rows.  The "BTREE" implementation generates 1 internal implementation row for each page in the btree, and it generates 1 internal implementation row for each branch row.  For this reason it is recommended that clients if possible use the count of rows in the heap table to estimate the number of rows in the index rather than use the index estimated row count. Set the total estimated number of rows in the container. <p> Often, after a scan, the client of RawStore has a much better estimate of the number of rows in the container than what store has. Currently, a scan, followed by an update of the estimate, will be performed when: <ul> <li>running SYSCS_UTIL.SYSCS_UPDATE_STATISTICS</li> <li>the automatic update of index statistics kicks in (see {@code IndexStatisticsDaemon})</li> <li>performing table scans</li> <li>creating an index on a populated table</li> </ul> This interface allows the client to set the estimated row count for the container, and store will use that number for all future references. <p> This routine can also be used to set the estimated row count in the index to the number of rows in the base table, another workaround for the problem that index estimated row count includes non-user rows.

needsRowLocation returns true iff this the row source expects the drainer of the row source to call rowLocation after getting a row from getNextRowFromRowSource. rowLocation is a callback for the drainer of the row source to return the rowLocation of the current row, i.e, the row that is being returned by getNextRowFromRowSource.  This interface is for the purpose of loading a base table with index.  In that case, the indices can be built at the same time the base table is laid down once the row location of the base row is known.  This is an example pseudo code on how this call is expected to be used: <BR><pre> boolean needsRL = rowSource.needsRowLocation(); DataValueDescriptor[] row; while((row = rowSource.getNextRowFromRowSource()) != null) { RowLocation rl = heapConglomerate.insertRow(row); if (needsRL) rowSource.rowLocation(rl); } </pre><BR> NeedsRowLocation and rowLocation will ONLY be called by a drainer of the row source which CAN return a row location.  Drainer of row source which cannot return rowLocation will guarantee to not call either callbacks. Conversely, if NeedsRowLocation is called and it returns true, then for every row return by getNextRowFromRowSource, a rowLocation callback must also be issued with the row location of the row.  Implementor of both the source and the drain of the row source must be aware of this protocol. <BR> The RowLocation object is own by the caller of rowLocation, in other words, the drainer of the RowSource.  This is so that we don't need to new a row location for every row.  If the Row Source wants to keep the row location, it needs to clone it (RowLocation is a ClonableObject).
Get an integer representation of the type of the lock. This method is guaranteed to return an integer &gt;= 0 and &lt; R_NUMBER. No correlation between the value and one of the static variables (CIS etc.) is guaranteed, except that the values returned do not change.
Obtain lock on record being read. <p> Assumes that a table level IS has been acquired.  Will acquire a Shared or Update lock on the row, depending on the "forUpdate" parameter. <p> Read lock will be placed on separate group from transaction.
Obtain container level intent lock. <p> This implementation of row locking is 2 level, ie. table and row locking. It will interact correctly with tables opened with ContainerLocking3 locking mode. <p> Updater's will get table level IX locks, and X row locks. <p> Reader's will get table level IS locks, and S row locks. <p> Read locks are put in a separate "group" from the transaction, so that when the container is closed it can release these read locks. Obtain lock on record being read. <p> Assumes that a table level IS has been acquired.  Will acquire a Shared or Update lock on the row, depending on the "forUpdate" parameter. <p> Read lock will be placed on separate group from transaction. Unlock read locks. <p> In Cursor stability release all read locks obtained.  unlockContainer() will be called when the container is closed. <p>
Obtain lock on record being read. <p> Assumes that a table level IS has been acquired.  Will acquire a Shared or Update lock on the row, depending on the "forUpdate" parameter. <p> Read lock will be acquired using zeroDuration lock.
Get type of lock to get while reading data. <p> This routine is provided so that class's like RowLockingRR can override just this routine to get RS2 locks vs RS3 locks, and still reuse all the other code in this class. <p> Get type of lock to get while requesting "update" lock. <p> This routine is provided so that class's like RowLockingRR can override just this routine to get RU2 locks vs RU3 locks, and still reuse all the other code in this class. <p> Get type of lock to get while writing data. <p> This routine is provided so that class's like RowLockingRR can override just this routine to get RX2 locks vs RX3 locks, and still reuse all the other code in this class. <p> Obtain container level intent lock. <p> This implementation of row locking is 2 level, ie. table and row locking. It will interact correctly with tables opened with ContainerLocking3 locking mode. <p> Updater's will get table level IX locks, and X row locks. <p> Reader's will get table level IS locks, and S row locks. Obtain lock on record being read. <p> Assumes that a table level IS has been acquired.  Will acquire a Shared or Update lock on the row, depending on the "forUpdate" parameter. <p> Obtain lock on record being written. <p> Assumes that a table level IX has been acquired.  Will acquire an Exclusive (X) lock on the row. <p> Obtain lock on record being written. <p> Assumes that a table level IX has been acquired.  Will acquire an Exclusive (X) lock on the row. <p>
Escalates Row Locking 3 to Container Locking 3. <p> This call is made by code which tracks the number of locks on a container. When the number of locks exceeds the escalate threshold the caller creates this new locking policy, calls lockContainer(), and substitues it for the old locking policy.  The lockContainer call determines which table lock to get (S or X), gets that table lock, and then releases the row locks on the table. It is assumed that this is called on a open container for lock only. <p>
Unlock a record after it has been locked for read. <p> In repeatable read only unlock records which "did not qualify".  For example in a query like "select * from foo where a = 1" on a table with no index it is only necessary to hold locks on rows where a=1, but in the process of finding those rows the system will get locks on other rows to verify they are committed before applying the qualifier.  Those locks can be released under repeatable read isolation. <p>
/////////////////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////////////////// Get the names of the columns in this row set <p> Get the corresponding JDBC type. </p> /////////////////////////////////////////////////////////////////////////////////  OVERRIDE BEHAVIOR IN BaseTypeIdImpl  ///////////////////////////////////////////////////////////////////////////////// <p> Get the SQL name of this multi set. This is the name suitable for replaying the DDL to create a Table Function. </p> /////////////////////////////////////////////////////////////////////////////////  Formatable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Get the id which indicates which class deserializes us. </p> Get the types of the columns in this row set <p> Read ourself from a formatable stream. </p> Set the types of the columns in this row set <p> Write ourself to a formatable stream. </p>
ValueNode override.
Add a column to this RowOrdering in the current order position. This is a no-op if there are any unordered optimizables in the join order (see below). Add an unordered optimizable to this RowOrdering.  This is to solve the following problem: Suppose we have the query: select * from r, s, t order by r.a, t.b Also suppose there are indexes on r.a and t.b.  When the optimizer considers the join order (r, s, t) using the index on r.a, the heap on s, and the index on t.b, the rows from the join order will *NOT* be ordered on t.b, because there is an unordered result set between r and t.  So, when s is added to the partial join order, and we then add table t to the join order, we want to ensure that we don't add column t.b to the RowOrdering. Ask whether the given table is always ordered. Tell this RowOrdering that it is always ordered on the given column of the given optimizable.  This is useful when a column in the optimizable has an equals comparison with a constant expression or it is involved in a equijoin with an optimizable which is always ordered on the column on which the equijoin is happening. This is reset when the optimizable is removed from this RowOrdering. Copy the contents of this RowOrdering to the given RowOrdering. Return true if the column is always ordered. That will be true if the column has a constant comparison predicate on it or it is involved in a equijoin with an optimizable which is always ordered on the column on which the equijoin is happening. Move to the next order position for adding ordered columns. This is a no-op if there are any unordered optimizables in the join order (see below). Tell this RowOrdering that it is always ordered on the given optimizable This is useful when considering a unique index where there is an equality match on the entire key - in this case, all the columns are ordered, regardless of the direction or position, or even whether the columns are in the index. Tell whether this ordering is ordered on the given column. This is similar to the method above, but it checks whether the column is ordered in any position, rather than a specified position. This is useful for operations like DISTINCT and GROUP BY. Tell whether this ordering is ordered on the given column in the given position Tell this row ordering that it is no longer ordered on the given table.  Also, adjust the current order position, if necessary. This only works to remove ordered columns from the end of the ordering.
Remove all optimizables with the given table number from the given list of optimizables. Returns true if there are unordered optimizables in the join order other than the given one.
************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: *************************************************************************
This is not used in positioned update and delete, so just return a null. If open and not returned yet, returns the row after plugging the parameters into the expressions.  CursorResultSet interface  This is not operating against a stored table, so it has no row location to report. Return the total amount of time spent in this ResultSet  ResultSet interface (leftover from NoPutResultSet)  Sets state to 'open'.
Add a new predicate to the list.  This is useful when doing subquery transformations, when we build a new predicate with the left side of the subquery operator and the subquery's result column.  Bind the expressions in this RowResultSetNode.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the expressions in this ResultSetNode if it has tables.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the non VTI tables in this ResultSetNode.  This includes getting their descriptors from the data dictionary and numbering them. Bind the expressions in the target list.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression.  This is useful for EXISTS subqueries, where we need to validate the target list before blowing it away and replacing it with a SELECT true. Bind any untyped null nodes to the types in the given ResultColumnList. * Check and see if everything below us is a constant or not. * If so, we'll let execution know that it can do some caching. * Before we do the check, we are going to temporarily set * ParameterNodes to CONSTANT.  We do this because we know * that we can cache a row with a parameter value and get * the param column reset by the user setting a param, so * we can skip over parameter nodes.  We are doing this * extra work to optimize inserts of the form: * * prepare: insert into mytab values (?,?); * setParam * execute() * setParam * execute() Modify the RCL of this node to match the target of the insert. Ensure that the top of the RSN tree has a PredicateList. Optimizable interface  Evaluate whether or not the subquery in a FromSubquery is flattenable. Currently, a FSqry is flattenable if all of the following are true: o  Subquery is a SelectNode or a RowResultSetNode (not a UnionNode) o  It contains no top level subqueries.  (RESOLVE - we can relax this) o  It does not contain a group by or having clause o  It does not contain aggregates. o  There is at least one result set in the from list that is not a RowResultSetNode (the reason is to avoid having an outer SelectNode with an empty FromList. The generated ResultSet will be: RowResultSet -- for the VALUES clause Get the exposed name for this table, which is the name that can be used to refer to it in the rest of the query. Try to find a ResultColumn in the table represented by this FromTable that matches the name in the given ColumnReference.   Optimize this SelectNode.  This means choosing the best access path for each table, among other things. Optimize any subqueries that haven't been optimized any where else.  This is useful for a RowResultSetNode as a derived table because it doesn't get optimized otherwise. Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Push down the offset and fetch first parameters, if any, to this node. Push the order by list down from the cursor node into its child result set so that the optimizer has all of the information that it needs to consider sort avoidance. {@inheritDoc } Return whether or not this ResultSet tree is guaranteed to return at most 1 row based on heuristics.  (A RowResultSetNode and a SELECT with a non-grouped aggregate will return at most 1 row.) Set the type of each parameter in the result column list for this table constructor. Verify that a SELECT * is valid for this type of subquery.
closeRowSource tells the RowSource that it will no longer need to return any rows and it can release any resource it may have. Subsequent call to any method on the RowSource will result in undefined behavior.  A closed rowSource can be closed again. Get the next row as an array of column objects. The column objects can be a JBMS Storable or any Serializable/Externalizable/Formattable/Streaming type. <BR> A return of null indicates that the complete set of rows has been read. <p> A null column can be specified by leaving the object null, or indicated by returning a non-null getValidColumns.  On streaming columns, it can be indicated by returning a non-null get FieldStates. <p> If RowSource.needToClone() is true then the returned row (the DataValueDescriptor[]) is guaranteed not to be modified by drainer of the RowSource (except that the input stream will be read, of course) and drainer will keep no reference to it before making the subsequent nextRow call.  So it is safe to return the same DataValueDescriptor[] in subsequent nextRow calls if that is desirable for performance reasons. <p> If RowSource.needToClone() is false then the returned row (the DataValueDescriptor[]) may be be modified by drainer of the RowSource, and the drainer may keep a reference to it after making the subsequent nextRow call.  In this case the client should severe all references to the row after returning it from getNextRowFromRowSource(). getValidColumns describes the DataValueDescriptor[] returned by all calls to the getNextRowFromRowSource() call. If getValidColumns returns null, the number of columns is given by the DataValueDescriptor.length where DataValueDescriptor[] is returned by the preceeding getNextRowFromRowSource() call.  Column N maps to DataValueDescriptor[N], where column numbers start at zero. If getValidColumns return a non null validColumns FormatableBitSet the number of columns is given by the number of bits set in validColumns.  Column N is not in the partial row if validColumns.get(N) returns false.  Column N is in the partial row if validColumns.get(N) returns true.  If column N is in the partial row then it maps to DataValueDescriptor[M] where M is the count of calls to validColumns.get(i) that return true where i &lt; N.  If DataValueDescriptor.length is greater than the number of columns indicated by validColumns the extra entries are ignored. Does the caller of getNextRowFromRowSource() need to clone the row in order to keep a reference to the row past the getNextRowFromRowSource() call which returned the row.  This call must always return the same for all rows in a RowSource (ie. the caller will call this once per scan from a RowSource and assume the behavior is true for all rows in the RowSource).
Fire the trigger based on the event.
Return the column number of the first column out of range, or a number less than zero if all columns are in range. Get the object for a column identifer (0 based) from a complete or partial row. Return a FetchDescriptor which describes a single column set. <p> This routine returns one of a set of constant FetchDescriptor's, and should not be altered by the caller. Get the number of columns represented by a FormatableBitSet. <p> This is simply a count of the number of bits set in the FormatableBitSet. <p> Get a FormatableBitSet representing all the columns represented in a qualifier list. See if a row actually contains no columns. Returns true if row is null or row.length is zero. Generate an "empty" row from an array of DataValueDescriptor objects. <p> Generate an array of new'd objects by using the getNewNull() method on each of the DataValueDescriptor objects. <p> ************************************************************************ Public Methods dealing with cloning and row copying util functions ************************************************************************* Generate a template row of DataValueDescriptor's <p> Generate an array of DataValueDescriptor objects which will be used to make calls to newRowFromClassInfoTemplate(), to repeatedly and efficiently generate new rows.  This is important for certain applications like the sorter and fetchSet which generate large numbers of "new" empty rows. <p> Get the next valid column after or including start column. Returns -1 if no valid columns exist after startColumn Process the qualifier list on the row, return true if it qualifies. <p> A two dimensional array is to be used to pass around a AND's and OR's in conjunctive normal form.  The top slot of the 2 dimensional array is optimized for the more frequent where no OR's are present.  The first array slot is always a list of AND's to be treated as described above for single dimensional AND qualifier arrays.  The subsequent slots are to be treated as AND'd arrays or OR's.  Thus the 2 dimensional array qual[][] argument is to be treated as the following, note if qual.length = 1 then only the first array is valid and it is and an array of and clauses: (qual[0][0] and qual[0][0] ... and qual[0][qual[0].length - 1]) and (qual[1][0] or  qual[1][1] ... or  qual[1][qual[1].length - 1]) and (qual[2][0] or  qual[2][1] ... or  qual[2][qual[2].length - 1]) ... and (qual[qual.length - 1][0] or  qual[1][1] ... or  qual[1][2]) return string version of row. <p> For debugging only. return string version of a HashTable returned from a FetchSet. <p> For debugging only.


**** Look at the received exclusion property and use it to figure out if this test/suite should be skipped based on the actual client and JVM versions in question. @param exclusion The harness property indicating the rules for skipping this test.  For example: "at-or-before:2.0,when-at-or-after:jdk1.5.1". @param clientName Name of the client being used. @param clientMajor The 'major' part of the client version that is actually being used for the test. @param clientMinor The 'minor' part of the client version that is actually being used for the test. @param javaVersion JVM being used, as a string. @return Exception is thrown if this test/suite should be skipped; else we simply return. **** Parses a versionString property and returns the specified number of integers as found in that string.  If the number of integers requested is larger than the number of integers found in the version string, -1 will be used as filler. An example versionString might be any of the following: "2.4" or "at-or-after:2.4" or "when:jdk1.3.1" or "when-at-or-after:jdk1.3.1", etc.  In these examples, the resultant int arrays would be: "2.4"                        ==> [2,4]         // if resultSize == 2. "at-or-after:2.4"            ==> [2.4]         // if resultSize == 2. "when:jdk1.3.1"              ==> [1,3,1]       // if resultSize == 3. "when-at-or-after:jdk1.3.1"  ==> [1,3,1,-1]    // if resultSize == 4. @param versionString The version string to parse. @param resultSize The number of integers to parse out of the received version string. @return An integer array holding resultSize integers as parsed from the version string (with -1 as a filler if needed). **** Looks at a version string and searches for an indication of what kind of versions (lower or higher) need to be excluded.  This method just looks for the keywords "at-or-before" and "at-or-after", and then returns a corresponding value.  If neither of those keywords is found, the default is to exclude versions that are lower (i.e. "at-or-before"). @param versionString The version string in question, for example "2.4" or "jdk1.3.1" or "at-or-before:jdk1.3.1". @return -1 if we want to exclude versions that come before the target, 1 if we want to exclude versions that come after the target.  Default is -1. **** Checks to see if the received string is a recognized keyword for an exclusion property. @param text The string in question. @return True if the received text is a valid keyword for exclusion properties; false otherwise. Locate the suite's properties file Properties for nested suites Properties which may be defined for all suites at the top level suite (such as "nightly") Determine if a test should be skipped or not. These are ad-hoc rules, see comments within for details. Examples of what is checked: JVM version, framework, encryption, jdk12test, Sets some global variables so that skip reporting is clearer. Unloads the embedded JDBC driver and Derby engine in case is has already been loaded. The purpose for doing this is that using an embedded engine that already is loaded makes it impossible to set new system properties for each individual suite or test. **** Takes two versions, each of which is an array of integers, and determines whether or not the first (actual) version should be excluded from running based on the second (target) version and on the received comparisonType. For example, let vActual be [2,1] and vTarget be [2,4]. Then if comparisonType indicates that versions "at or before" the the target version (2.4) should be excluded, this method would return true (because 2.1 is before 2.4); if comparisonType indicates that versions "at or after" the target type should be excluded, this method would return false (because 2.1 is NOT at or after 2.4). @param vActual The actual version, as an int array. @param vTarget The target version, as an int array. @param offset1 Offset into vActual at which to start the comparison. @param offset2 Offset into vTarget at which to start the comparison. @param numParts The maximum number of integer parts to compare. @param comparisonType -1 if we want to exclude versions at or before the target; 1 if we want to exclude versions at or after the target. @return True if the actual version should be excluded from running, false otherwise.

Initiate a visit of the ResultSetStatistics tree from the top. Get the timestamp for the beginning of query compilation. <P> A null is returned if STATISTICS TIMING is not SET ON. Get the timestamp for the beginning of query execution. <P> A null is returned if STATISTICS TIMING is not SET ON. Get the bind time for the associated query in milliseconds. Get the total compile time for the associated query in milliseconds. Compile time can be divided into parse, bind, optimize and generate times. <P> 0 is returned if STATISTICS TIMING is not SET ON. Get the timestamp for the end of query compilation. <P> A null is returned if STATISTICS TIMING is not SET ON. Get the timestamp for the end of query execution. <P> A null is returned if STATISTICS TIMING is not SET ON. Get the estimated row count for the number of rows returned by the associated query or statement. Get the execute time for the associated query in milliseconds. <P> 0 is returned if STATISTICS TIMING is not SET ON. Get the generate time for the associated query in milliseconds. <P> 0 is returned if STATISTICS TIMING is not SET ON. Get the optimize time for the associated query in milliseconds. <P> 0 is returned if STATISTICS TIMING is not SET ON. Get the parse time for the associated query in milliseconds. <P> 0 is returned if STATISTICS TIMING is not SET ON. Get the name of the Stored Prepared Statement used for the statement.  This method returns a value only for <i>EXECUTE STATEMENT</i> statements; otherwise, returns null. <p> Note that the name is returned in the schema.name format (e.g. APP.MYSTMT). Get a String representation of the information on the nodes relating to table and index scans from the execution plan for the associated query or statement. Get a String representation of the information on the nodes relating to table and index scans from the execution plan for the associated query or statement for a particular table. <P> Get a String representation of the execution plan for the associated query or statement. Get the name of the associated query or statement. (This will be an internally generated name if the user did not assign a name.) Get the text for the associated query or statement.
initiate a visit of an XPLAINVisitor from the top of the RS tree Get the timestamp for the beginning of query compilation. Get the timestamp for the beginning of query execution. Get the bind time for the associated query in milliseconds. Get the objects to be displayed when this tree object is expanded. <P> The objects returned can be of any type, including addtional Inspectables. RunTimeStatistics methods Get the total compile time for the associated query in milliseconds. Compile time can be divided into parse, bind, optimize and generate times. Get the timestamp for the end of query compilation. Get the timestamp for the end of query execution. Get the estimated row count for the number of rows returned by the associated query or statement. Get the execute time for the associated query in milliseconds. Get the generate time for the associated query in milliseconds. Get the optimize time for the associated query in milliseconds. Get the parse time for the associated query in milliseconds. Get the name of the Stored Prepared Statement for the statement. Get the information on the nodes relating to table and index scans from the execution plan for the associated query or statement as a String. Get the information on the nodes relating to table and index scans for table tableName from the execution plan for the associated query or statement as a String. Get the execution plan for the associated query or statement as a String. Get the name of the associated query or statement. (This will be an internally generated name if the user did not assign a name.) Get the text for the associated query or statement. Class implementation
Find the {@code DBFiller} instance for the load specified on the command line. Create a load generator for the load specified on the command line. Get the {@code int} value of the specified option. Get the data type to be used for sr_select and sr_update types of load. Checks whether the specified option is set. Main method which starts the Runner application. Create a new client for the load specified on the command line. Parse the command line arguments and set the state variables to reflect the arguments. Parse the load-specific options. It's a comma-separated list of options, where each option is either a keyword or a (keyword, value) pair separated by an equals sign (=). The parsed options will be put into the map {@link #loadOpts}. Print the usage string. Shut down the database if it is a Derby embedded database.
Assert that a sequence of string exists in the statistics. <p>/ The strings in the argument are each assumed to start a line. Leading underscores are converted to tab characters before comparing. Return whether or not the query involved a sort that eliminated duplicates Find all qualifiers in a query plan. Search the RuntimeStatistics for a string.  It must occur at least instances times.  Find the start position ; sometimes using a scan start / stop is a way of doing qualifiers using an index Find the stop position ; sometimes using a scan start / stop is a way of doing qualifiers using an index Return whether or not the query used an equals scan qualifier. Return whether or not the query used a &gt;= scan qualifier. Return whether or not the query used a &lt; scan qualifier. Return whether there are no qualifiers (i.e. qualifiers: None) Return whether or not the query plan includes a line of the form "Number of rows qualified=n" where "n" is the received qualRows argument.  Note that this method will return true if the above string is found anywhere in the query plan.  For queries which specifying more than one table, more advanced parsing will be required to associate the number of rows qualified with the correct table.  Return whether or not a Distinct Scan result set was used in the query.  Check if an exists join (or a not exists join) was used.  Return whether or not an index row to base row result set was used in the query.  Return whether or not an index scan result set was used in the query.  Return whether or not a last key index scan result set was used in the query. A last key index scan is a special optimization for MIN and MAX queries against an indexed column (SELECT MAX(ID) FROM T).   Return whether or not a Table Scan result set was used in the query.  Check if sorting node was added for the query.
/////////////////////////////////////////////////////////////////////////////////  LOGIC TO ADVANCE THE SEQUENCE  ///////////////////////////////////////////////////////////////////////////////// increment the original value. if this overflows and cycling is not allowed return the original value. /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Get the next value in the sequence. Returns null if the sequence is exhausted. </p> Get the upper bound
Compiles this SPS. <p> <em>Note:</em> This SPS may still be marked as invalid after this method has completed, because an invalidation request may have been received while compiling. @GuardedBy("this") Get the provider's type. Get the compilation type schema id when this view was first bound. The time this prepared statement was compiled Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. ////////////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////////////// Return the stored form of this provider  RESOLVE: some descriptors have getName.  some descriptors have getTableName, getColumnName whatever! try and unify all of this to one getDescriptorName!  Gets the name of the sps. Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Get the default parameter values for this statement.  Default parameter values are supplied by a USING clause on either a CREATE or ALTER STATEMENT statement. Get the array of date type descriptors for this statement.  Currently, we do a lookup if we don't already have the parameters saved. When SPSes are cached, the parameters should be set up when the sps is constructed. Get the preparedStatement for this statement. If stmt is invalid or hasn't been compiled yet, it will be recompiled. Get the preparedStatement for this statement. Expects the prepared statement to have already been added to SYS.SYSSTATEMENTS. <p> Side Effects: will update SYS.SYSSTATEMENTS with the new plan if it needs to be recompiled. Gets the full, qualified name of the statement. Gets the SchemaDescriptor for this SPS Descriptor. Get the text used to create this statement. Returns original text in a cleartext string. Gets an identifier telling what type of table this is. Types match final ints in this interface.  Currently returns SPS_TYPE_REGULAR or SPS_TYPE_TRIGGER. Simple little helper function to convert your type to a string, which is easier to use. Gets the UUID of the SPS. Get the text of the USING clause used on CREATE or ALTER statement. Is the statement initially compilable? ////////////////////////////////////////////////////  DEPENDENT INTERFACE  //////////////////////////////////////////////////// Check that all of the dependent's dependencies are valid. Load the underlying generatd class.  This is not expected to be used outside of the datadictionary package.  It is used for optimizing class loading for sps cacheing. Mark the dependent as invalid (due to at least one of its dependencies being invalid). Generate the class for this SPS and immediately release it.  This is useful for cases where we don't want to immediately execute the statement corresponding to this sps (e.g. CREATE STATEMENT). <p> <I>SIDE EFFECTS</I>: will update and SYSDEPENDS with the prepared statement dependency info. FOR TRIGGERS ONLY <p> Generate the class for this SPS and immediately release it.  This is useful for cases where we don't want to immediately execute the statement corresponding to this sps (e.g. CREATE STATEMENT). <p> <I>SIDE EFFECTS</I>: will update and SYSDEPENDS with the prepared statement dependency info. FOR TRIGGERS ONLY <p> Generate the class for this SPS and immediately release it.  This is useful for cases where we don't want to immediately execute the statement corresponding to this sps (e.g. CREATE STATEMENT). <p> <I>SIDE EFFECTS</I>: will update and SYSDEPENDS with the prepared statement dependency info. Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). Get the UUID for the given string Invalidate and revalidate.  The functional equivalent of calling makeInvalid() and makeValid(), except it is optimized. Set the compile time to now Set the parameter defaults for this statement. Set the list of parameters for this statement It is possible that when a trigger is invalidated, the generated trigger action sql associated with it needs to be regenerated. One example of such a case would be when ALTER TABLE on the trigger table changes the length of a column. The need for this code was found as part of DERBY-4874 where the Alter table had changed the length of a varchar column from varchar(30) to varchar(64) but the generated trigger action plan continued to use varchar(30). To fix varchar(30) in in trigger action sql to varchar(64), we need to regenerate the trigger action sql which is saved as stored prepared statement. This new trigger action sql will then get updated into SYSSTATEMENTS table. DERBY-4874 Prints the contents of the TableDescriptor * Update SYSSTATEMENTS with the changed the descriptor. * Always done in the user XACT. * <p> * Ideally, the changes to SYSSTATEMENTS would be made * in a separate xact as the current user xact, but this * is painful (you'ld need to get a new ContextManager * and then push all of the usual langauge contexts * onto it and THEN call AccessManager.getTransaction()), * and it wont work, because the xact is in a different * compatibility space and will self deadlock (e.g. * in the process of call DependencyManager.makeInvalid() * we first did a DDdependableFinder.getDependable() * which called DataDictionaryImpl.getSPSDescriptor() * so we hold a lock on SYS.SYSSTATEMENTS by the * time we get a 2nd xact and try to drop the statement). Validate the type. <B>NOTE</B>: Only SPS_TYPE_REGULAR and SPS_TYPE_TRIGGER are currently valid.
Cacheable interface  Cacheable interface    Get the sps descriptor that is associated with this Cacheable
This method implements the char_length function for bit. Host variables are rejected if their length is bigger than the declared length, regardless of if the trailing bytes are the pad character. Shallow clone a StreamStorable without objectifying. This is used to avoid unnecessary objectifying of a stream object. Beetle 4896 DataValueDescriptor interface     * SQL Operators The = operator as called from the language module, as opposed to the storage module. end of estimateMemoryUsage  length in bytes Return max memory usage for a SQL Binary  Used by JDBC -- string should not contain SQL92 formatting. Gets a trace representation for debugging. The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. Hash code Storable interface, implies Externalizable, TypedFormat see if the Bit value is null. The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. The &lt;&gt; operator as called from the language module, as opposed to the storage module. class interface Read the encoded length of the value from the on-disk format. delegated to bit Read the value from an input stream. The length encoded in the input stream has already been read and determined to be unknown.  DataValueDescriptor interface StreamStorable interface : Adding this method to ensure that super class' setInto method doesn't get called that leads to the violation of JDBC spec( untyped nulls ) when batching is turned on. Set me to the value represented by this stream. The format of the stream is the on-disk format described in this class's javadoc. That is the length is encoded in the first few bytes of the stream. Set the value from the stream which is in the on-disk format. The SQL substr() function. String display of value Truncate this value to the desired width by removing bytes at the end of the byte sequence. Serialize a blob using the 8.1 encoding. Not called if null. Write the value out from the byte array (not called if null) using the 8.1 encoding. Write the length if using the 8.1 encoding.
Return max memory usage for a SQL Bit   Storable interface, implies Externalizable, TypedFormat Return my format identifier. Normalization method - this method may be called when putting a value into a SQLBit, for example, when inserting into a SQLBit column.  See NormalizeResultSet in execution. Set the value from an non-null object. Obtain the value using getBytes. This works for all FOR BIT DATA types. Getting a stream is problematic as any other getXXX() call on the ResultSet will close the stream we fetched. Therefore we have to create the value in-memory as a byte array. Set the width of the to the desired value.  Used when CASTing.  Ideally we'd recycle normalize(), but the behavior is different (we issue a warning instead of an error, and we aren't interested in nullability). DataValueDescriptor interface
Returns a clone of this BLOB value. <p> Unlike the other binary types, BLOBs can be very large. We try to clone the underlying stream when possible to avoid having to materialize the value into memory. Return max memory usage for a SQL Blob  Return a JDBC Blob. Originally implemented to support DERBY-2201. Return my format identifier. Tells if this BLOB value is, or will be, represented by a stream. Tell if this blob is length less. Normalization method - this method may be called when putting a value into a SQLBit, for example, when inserting into a SQLBit column.  See NormalizeResultSet in execution. Set the value from an non-null object.  The method setWidth is only(?) used to adopt the value to the casted domain/size. BLOBs behave different from the BIT types in that a (CAST (X'01' TO BLOB(1024))) does NOT pad the value to the maximal allowed datasize. That it is done for BIT is understandable, however, for BIT VARYING it is a bit confusing. Could be inheritence bug. Anyhow, here we just ignore the call, since there is no padding to be done. We do detect truncation, if the errorOnTrunc flag is set. DB2 does return a WARNING on CAST and ERROR on INSERT. DataValueDescriptor interface
The AND operator.  This implements SQL semantics for AND with unknown truth values - consult any standard SQL reference for an explanation. DataValueDescriptor interface   Orderable interface  Determine whether this SQLBoolean contains the given boolean value. This method is used by generated code to determine when to do short-circuiting for an AND or OR. * SQL Operators The = operator as called from the language module, as opposed to the storage module.    Return an immutable BooleanDataValue with the same value as this.     Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. Hash code The SQL IS operator - consult any standard SQL reference for an explanation. Implements the following truth table: otherValue | TRUE    | FALSE   | UNKNOWN this    |---------------------------- | TRUE    | TRUE    | FALSE   | FALSE FALSE   | FALSE   | TRUE    | FALSE UNKNOWN | FALSE   | FALSE   | TRUE Implements NOT IS. This reverses the sense of the is() call. DataValueDescriptor interface (mostly implemented in DataType) see if the integer value is null. The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. The &lt;&gt; operator as called from the language module, as opposed to the storage module. The OR operator.  This implements SQL semantics for OR with unknown truth values - consult any standard SQL reference for an explanation.  Recycle this SQLBoolean object if possible. If the object is immutable, create and return a new object.  Set the value into a PreparedStatement.   REMIND: do we need this, or is long enough? REMIND: do we need this, or is double enough? REMIND: do we need this, or is long enough? Set the value of this BooleanDataValue to the given String. String is trimmed and upcased.  If resultant string is not TRUE or FALSE, then an error is thrown. REMIND: do we need this, or is long enough?  Throw an exception with the given SQLState if this BooleanDataValue is false. This method is useful for evaluating constraints. String display of value Get a truth value. * Support functions Return the SQL truth value for a comparison. This method first looks at the operands - if either is null, it returns the unknown truth value.  This implements "normal" SQL null semantics, where if any operand is null, the result is null. Note that there are cases where these semantics are incorrect - for example, NULL AND FALSE is supposed to be FALSE, not NULL (the NULL truth value is the same as the UNKNOWN truth value). If neither operand is null, it returns a static final variable containing the SQLBoolean truth value.  It returns different values depending on whether the truth value is supposed to be nullable. This method always returns a pre-allocated static final SQLBoolean. This is practical because there are so few possible return values. Using pre-allocated values allows us to avoid constructing new SQLBoolean values during execution. same as above, but takes a Boolean, if it is null, unknownTruthValue is returned DataValueDescriptor interface  Implementation for BOOLEAN type. Convert to a BigDecimal using long Return an unknown truth value.  Check to be sure the return value is nullable.
************************************************************************ Private/Protected methods of This class: ************************************************************************* * Concatable interface This method implements the char_length function for char. CloneableObject interface Shallow clone a StreamStorable without objectifying.  This is used to avoid unnecessary objectifying of a stream object.  The only difference of this method from cloneValue is this method does not objectify a stream. DataValueDescriptor interface     * SQL Operators The = operator as called from the language module, as opposed to the storage module. end of estimateMemoryUsage ************************************************************************ Public Methods of DataValueDescriptor interface: Mostly implemented in Datatype. ************************************************************************* Get Boolean from a SQLChar. <p> Return false for only "0" or "false" for false. No case insensitivity. Everything else is true. <p> The above matches JCC and the client driver. Get Byte from a SQLChar. <p> Uses java standard Byte.parseByte() to perform coercion. Get a char array.  Typically, this is a simple getter that is cheaper than getString() because we always need to create a char array when doing I/O.  Use this instead of getString() where reasonable. <p> <b>WARNING</b>: may return a character array that has spare characters at the end.  MUST be used in conjunction with getLength() to be safe. This method gets called for the collation sensitive char classes ie CollatorSQLChar, CollatorSQLVarchar, CollatorSQLLongvarchar, CollatorSQLClob. These collation sensitive chars need to have the collation key in order to do string comparison. And the collation key is obtained using the Collator object that these classes already have. Get date from a SQLChar. Static function to Get date from a string. Get double from a SQLChar. <p> Uses java standard Double.doubleValue() to perform coercion. Get float from a SQLChar. <p> Uses java standard Float.floatValue() to perform coercion. Get int from a SQLChar. <p> Uses java standard Short.parseInt() to perform coercion.   Get long from a SQLChar. <p> Uses java standard Short.parseLong() to perform coercion.  Get a SQLVarchar for a built-in string function.  ************************************************************************ Public Methods of This class: ************************************************************************* <p> This is a special accessor used when we wrap passwords in VARCHARs. This accessor copies the wrapped char[] and then fills it with 0s so that the password can't be memory-sniffed. For more information, see the comment on the SQLChar( char[] ) constructor. </p> Get Short from a SQLChar. <p> Uses java standard Short.parseShort() to perform coercion.  Returns the default stream header generator for the string data types. Returns a descriptor for the input stream for this character data value. If possible, use getCharArray() if you don't really need a string.  getString() will cause an extra char array to be allocated when it calls the the String() constructor (the first time through), so may be cheaper to use getCharArray(). Get time from a SQLChar. Static function to Get Time from a string. Get Timestamp from a SQLChar. Static function to Get Timestamp from a string. Gets a trace representation for debugging. Storable interface, implies Externalizable, TypedFormat Return my format identifier. Get the number of bytes needed to represent a string in modified UTF-8, which is the encoding used by {@code writeExternal()} and {@code writeUTF()}.  The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. returns the reasonable minimum amount by which the array can grow . See readExternal. when we know that the array needs to grow by at least one byte, it is not performant to grow by just one byte instead this amount is used to provide a resonable growby size. * Method to check for truncation of non blank chars. Hash code Hash code implementation for collator sensitive subclasses. see if the String value is null. The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. This method implements the like function for char (with no escape value). This method implements the like function for char with an escape value. This method implements the locate function for char.  Normalization method - this method may be called when putting a value into a SQLChar, for example, when inserting into a SQLChar column.  See NormalizeResultSet in execution. The &lt;&gt; operator as called from the language module, as opposed to the storage module. Restores the data value from the source stream, materializing the value in memory. Reads a CLOB from the source stream and materializes the value in a character array. Reads in a string from the specified data input stream. The string has been encoded using a modified UTF-8 format. <p> The first two bytes are read as if by <code>readUnsignedShort</code>. This value gives the number of following bytes that are in the encoded string, not the length of the resulting string. The following bytes are then interpreted as bytes encoding characters in the UTF-8 format and are converted into characters. <p> This method blocks until all the bytes are read, the end of the stream is detected, or an exception is thrown. Resets state after materializing value from an array.  ************************************************************************ Public Methods of StreamStorable interface: ************************************************************************* Only to be called when an application through JDBC is setting a SQLChar to a java.math.BigDecimal. Set the value into a PreparedStatement. Allow any Java type to be cast to a character type using Object.toString. Set this value to the on-disk format stream. Sets the mode for the database being accessed. Set the value from the stream which is in the on-disk format.     /////////////////////////////////////////////////////////////  VariableSizeDataValue INTERFACE  ///////////////////////////////////////////////////////////// Set the width of the to the desired value.  Used when CASTing.  Ideally we'd recycle normalize(), but the behavior is different (we issue a warning instead of an error, and we aren't interested in nullability). Compare two Strings using standard SQL semantics. Compare two Strings using standard SQL semantics. Compare two SQLChars. The SQL substr() function. Wraps an {@code IOException} in a {@code StandardException} then throws the wrapping exception. String display of value This function public for testing purposes. DataValueDescriptor interface  CHAR/VARCHAR/LONG VARCHAR implementation. Convert to a BigDecimal using getString.  Writes the header and the user data for a CLOB to the destination stream. Writes a non-Clob data value to the modified UTF-8 format used by Derby. The maximum stored size is based upon the UTF format used to stored the String. The format consists of a two byte length field and a maximum number of three bytes for each character. <BR> This puts an upper limit on the length of a stored String. The maximum stored length is 65535, these leads to the worse case of a maximum string length of 21844 ((65535 - 2) / 3). <BR> Strings with stored length longer than 64K is handled with the following format: (1) 2 byte length: will be assigned 0. (2) UTF formated string data. (3) terminate the string with the following 3 bytes: first byte is: +---+---+---+---+---+---+---+---+ | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | +---+---+---+---+---+---+---+---+ second byte is: +---+---+---+---+---+---+---+---+ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | +---+---+---+---+---+---+---+---+ third byte is: +---+---+---+---+---+---+---+---+ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | +---+---+---+---+---+---+---+---+ The UTF format: Writes a string to the underlying output stream using UTF-8 encoding in a machine-independent manner. <p> First, two bytes are written to the output stream as if by the <code>writeShort</code> method giving the number of bytes to follow. This value is the number of bytes actually written out, not the length of the string. Following the length, each character of the string is output, in sequence, using the UTF-8 encoding for the character. Write a single character to a stream in the modified UTF-8 format. Writes the user data value to a stream in the modified UTF-8 format. <p> Zero out the wrapped char[] so that it can't be memory-sniffed. This helps us protect passwords. See the comment on the SQLChar( char[] ) constructor. </p>
DataValueDescriptor interface Returns a clone of this CLOB value. <p> Unlike the other binary types, CLOBs can be very large. We try to clone the underlying stream when possible to avoid having to materialize the value into memory. * disable conversions to/from most types for CLOB. * TEMP - real fix is to re-work class hierachy so * that CLOB is towards the root, not at the leaf. Returns the character length of this Clob. <p> If the value is stored as a stream, the stream header will be read. If the stream header doesn't contain the stream length, the whole stream will be decoded to determine the length.   Returns a stream header generator for a Clob. <p> <em>NOTE</em>: To guarantee a successful generation, one of the following two conditions must be met at header or EOF generation time: <ul> <li>{@code setStreamHeaderFormat} has been invoked before the header generator was obtained.</li> <li>There is context at generation time, such that the mode can be determined by obtaining the database context and by consulting the data dictionary.</li> </ul> Returns a descriptor for the input stream for this CLOB value. <p> The descriptor contains information about header data, current positions, length, whether the stream should be buffered or not, and if the stream is capable of repositioning itself. <p> When this method returns, the stream is positioned on the first character position, such that the next read will return the first character in the stream. Gets a trace representation of the CLOB for debugging. Storable interface, implies Externalizable, TypedFormat Return my format identifier. DataValueDescriptor interface. These are actually all implemented in the super-class, but we need to duplicate some of them here so they can be called by byte-code generation, which needs to know the class the method appears in.  Tells if this CLOB value is, or will be, represented by a stream. Investigates the header and returns length information. Normalization method - this method may be called when putting a value into a SQLClob, for example, when inserting into a SQLClob column.  See NormalizeResultSet in execution. Per the SQL standard ,if the clob column is not big enough to hold the value being inserted,truncation error will result if there are trailing non-blanks. Truncation of trailing blanks is allowed. Reads and materializes the CLOB value from the stream. Reads and materializes the CLOB value from the stream. Rewinds the stream to the beginning and then skips the specified number of bytes. Set the value from an non-null Java.sql.Clob object. Sets a new stream for this CLOB. Tells whether the database is being accessed in soft upgrade mode or not. DataValueDescriptor interface @see DataValueDescriptor#typePrecedence Writes the CLOB data value to the given destination stream using the modified UTF-8 format.
DataValueDescriptor interface   Orderable interface Implement the date SQL function: construct a SQL date from a string, number, or timestamp. end of computeDateFunction computeEncodedDate extracts the year, month and date from a Calendar value and encodes them as year &lt;&lt; 16 + month &lt;&lt; 8 + date Use this function will help to remember to add 1 to month which is 0 based in the Calendar class Compute the encoded date given a date Convert a date to the JDBC representation and append it to a string buffer. end of dateToString Get the String version from the encodedDate. end of estimateMemoryUsage Get the value field.  We instantiate the field on demand.  Get the day from the encodedDate.   Get the month from the encodedDate, January is one.   getObject returns the date value  * DataValueDescriptor interface * (mostly implemented in DataType) Convert the date into a milli-seconds since the epoch with the time set to 00:00 based upon the passed in Calendar. getTimestamp returns a timestamp with the date value time is set to 00:00:00.0 Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator Get the year from the encodedDate.  Hash code Check if the value is null. encodedDate is 0 if the value is null * SQL Operators end of parseDate   Set the date portion of a date-time value into the passed in Calendar object from its encodedDate value. Only the YEAR, MONTH and DAY_OF_MONTH fields are modified. The remaining state of the Calendar is not modified. Adding this method to ensure that super class' setInto method doesn't get called that leads to the violation of JDBC spec( untyped nulls ) when batching is turned on. Set the value from a correctly typed Date object. This helper routine tests the nullability of various parameters and sets up the result appropriately. If source is null, a new NumberDataValue is built.   DataValueDescriptor interface  Add a number of intervals to a datetime value. Implements the JDBC escape TIMESTAMPADD function. Finds the difference between two datetime values as a number of intervals. Implements the JDBC TIMESTAMPDIFF escape function. * String display of value
DataValueDescriptor interface  This method implements the / operator for BigDecimal/BigDecimal This method implements the / operator for BigDecimal/BigDecimal Get a BigDecimal representing the value of a DataValueDescriptor 0 or null is false, all else is true  Return the SQL scale of this value, number of digits after the decimal point, or zero for a whole number. This does not match the return from BigDecimal.scale() since in J2SE 5.0 onwards that can return negative scales. Return the SQL scale of this value, number of digits after the decimal point, or zero for a whole number. This does not match the return from BigDecimal.scale() since in J2SE 5.0 onwards that can return negative scales. If we have a value that is greater than the maximum double, exception is thrown.  Otherwise, ok.  If the value is less than can be represented by a double, ti will get set to the smallest double value.  DataValueDescriptor interface (mostly implemented in DataType)     Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator Calculate the number of digits to the left of the decimal point of the passed in value. Hash code This method implements the isNegative method. see if the decimal value is null.  This method implements the unary minus operator for double. This method implements the - operator for "decimal - decimal". Normalization method - this method may be called when putting a value into a SQLDecimal, for example, when inserting into a SQLDecimal column.  See NormalizeResultSet in execution. <p> Note that truncation is allowed on the decimal portion of a numeric only. * SQL Operators This method implements the + operator for DECIMAL. Note the use of rawData: we reuse the array if the incoming array is the same length or smaller than the array length.  Only to be called when the application sets a value using BigDecimal through setBigDecimal calls. END DataValueDescriptor interface Set the value into a PreparedStatement. Set the value from a correctly typed BigDecimal object.     Called when setting a DECIMAL value internally or from through a procedure or function. Handles long in addition to BigDecimal to handle identity being stored as a long but returned as a DECIMAL. <B> WARNING </B> there is no checking to make sure that theValue doesn't exceed the precision/scale of the current SQLDecimal.  It is just assumed that the SQLDecimal is supposed to take the precision/scale of the BigDecimalized String.   /////////////////////////////////////////////////////////////////////////////  VariableSizeDataValue interface  ///////////////////////////////////////////////////////////////////////////// Set the precision/scale of the to the desired values. Used when CASTing.  Ideally we'd recycle normalize(), but the use is different. This method implements the * operator for "double * double". String display of value  DataValueDescriptor interface  DECIMAL implementation. Convert to a BigDecimal using getObject which will return a BigDecimal Distill the BigDecimal to a byte array and write out: <UL> <LI> scale (zero or positive) as a byte </LI> <LI> length of byte array as a byte</LI> <LI> the byte array </LI> </UL>
DataValueDescriptor interface  This method implements the / operator for "double / double". * SQL Operators The = operator as called from the language module, as opposed to the storage module. for lack of a specification: getDouble()==0 gives true independent of the NULL flag   DataValueDescriptor interface (mostly implemented in DataType) JDBC is lax in what it permits and what it returns, so we are similarly lax @see DataValueDescriptor     Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. Hash code This method implements the isNegative method. see if the double value is null.  The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. This method implements the unary minus operator for double. This method implements the - operator for "double - double". The &lt;&gt; operator as called from the language module, as opposed to the storage module. This method implements the + operator for "double + double".   Called for an application setting this value using a BigDecimal Set the value into a PreparedStatement. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values. Set the value from a correctly typed Double object.      This method implements the * operator for "double * double". String display of value  DataValueDescriptor interface  DOUBLE implementation. Convert to a BigDecimal using getString.
creates jdbc4.0 SQLException and its subclass based on sql state

DataValueDescriptor interface  * SQL Operators The = operator as called from the language module, as opposed to the storage module. for lack of a specification: 0 or null is false, all else is true  DataValueDescriptor interface (mostly implemented in DataType) JDBC is lax in what it permits and what it returns, so we are similarly lax @see DataValueDescriptor   Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. Hash code This method implements the isNegative method. see if the integer value is null.  The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. This method implements the unary minus operator for int. mod(int, int) The &lt;&gt; operator as called from the language module, as opposed to the storage module.   Set the value into a PreparedStatement. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values.      This method implements the * operator for "int * int". String display of value  DataValueDescriptor interface
Return max memory usage for a SQL LongVarbit  Return my format identifier. Normalization method - this method may be called when putting a value into a SQLVarbit, for example, when inserting into a SQLVarbit column.  See NormalizeResultSet in execution. This overrides SQLBit -- the difference is that we don't expand SQLVarbits to fit the target. DataValueDescriptor interface
DataValueDescriptor interface  This method implements the / operator for "bigint / bigint". * SQL Operators The = operator as called from the language module, as opposed to the storage module. for lack of a specification: 0 or null is false, all else is true  DataValueDescriptor interface (mostly implemented in DataType) JDBC is lax in what it permits and what it returns, so we are similarly lax @see DataValueDescriptor    Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. Hash code This method implements the isNegative method. see if the integer value is null.  The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. This method implements the unary minus operator for bigint. This method implements the - operator for "bigint - bigint". mod(bigint, bigint) The &lt;&gt; operator as called from the language module, as opposed to the storage module. This method implements the + operator for "bigint + bigint".   Set the value into a PreparedStatement. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values. Set the value from a correctly typed Long object.       This method implements the * operator for "bigint * bigint". String display of value  DataValueDescriptor interface
DataValueDescriptor interface    Storable interface, implies Externalizable, TypedFormat Return my format identifier. DataValueDescriptor interface. These are actually all implemented in the super-class, but we need to duplicate some of them here so they can be called by byte-code generation, which needs to know the class the method appears in.  DataValueDescriptor interface @see DataValueDescriptor#typePrecedence
DataValueDescriptor interface  This method implements the / operator for "real / real". The operator uses DOUBLE aritmetic as DB2 does. * SQL Operators The = operator as called from the language module, as opposed to the storage module. for lack of a specification: 0 or null is false, all else is true     DataValueDescriptor interface (mostly implemented in DataType) JDBC is lax in what it permits and what it returns, so we are similarly lax        Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. Hash code This method implements the isNegative method. Note: This method will return true for -0.0f. see if the real value is null.  The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. This method implements the unary minus operator for real. This method implements the - operator for "real - real". The operator uses DOUBLE aritmetic as DB2 does. The &lt;&gt; operator as called from the language module, as opposed to the storage module. This method implements the + operator for "real + real". The operator uses DOUBLE aritmetic as DB2 does.   Called for an application setting this value using a BigDecimal Set the value into a PreparedStatement. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values. Set the value from a correctly typed Float object.     This method implements the * operator for "real * real". The operator uses DOUBLE aritmetic as DB2 does. String display of value  DataValueDescriptor interface  DOUBLE implementation. Convert to a BigDecimal using getString.
DataValueDescriptor interface  * Orderable interface   end of estimateMemoryUsage  * DataValueDescriptor interface * (mostly implemented in DataType) Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator Adding this overload makes it possible to use SQLRefs as keys in HashMaps.    * String display of value
Get a handle to the session's constraint modes. The caller is responsible for any cloning needed. Get the SQL current user of this SQL connection context Get the schema of this SQL connection context Get state of DEFERRED ALL setting. Get the SQL role of this SQL connection context Return {@code Boolean.TRUE} if the constraint mode for this constraint/index has been set to deferred, {@code Boolean.FALSE} if it has been set to immediate.  Any ALL setting is considered also. If the constraint mode hasn't been set for this constraint, return {@code null}. The constraint mode is the effectively the initial constraint mode in this case. Clear deferred information for this transaction. Initialize a inferior session context with the constraint mode map of the parent session context. Set the schema of this SQL connection context Set the constraint mode for this constraint to {@code deferred}. If {@code deferred} is {@code false}, to immediate checking, if {@code true} to deferred checking. Set the constraint mode for all deferrable constraints to {@code deferred}. If {@code deferred} is {@code false}, set to immediate checking, if {@code true} to deferred checking. {@code null} is allowed: it means no ALL setting exists. Set the SQL role of this SQL connection context Set the SQL current user of this SQL connection context
{@inheritDoc } {@inheritDoc } {@inheritDoc } {@inheritDoc } {@inheritDoc } {@inheritDoc } {@inheritDoc }
DataValueDescriptor interface  * SQL Operators The = operator as called from the language module, as opposed to the storage module. for lack of a specification: 0 or null is false, all else is true     DataValueDescriptor interface (mostly implemented in DataType) JDBC is lax in what it permits and what it returns, so we are similarly lax        Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator The &gt;= operator as called from the language module, as opposed to the storage module. The &gt; operator as called from the language module, as opposed to the storage module. Hash code This method implements the isNegative method. always false for non-nullable columns The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. This method implements the unary minus operator for smallint. mod(smallint, smallint) The &lt;&gt; operator as called from the language module, as opposed to the storage module.   Set the value into a PreparedStatement. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values.        This method implements the * operator for "smallint * smallint". String display of value  DataValueDescriptor interface


DataValueDescriptor interface   Orderable interface Calculate the encoded time from a Calendar object encoded time is hour &lt;&lt; 16 + min &lt;&lt; 8 + sec this function is also used by SQLTimestamp Compute encoded time value Time is represented by hour &lt;&lt; 16 + minute &lt;&lt; 8 + seconds Get the String version from the encodedTime. end of estimateMemoryUsage  Get the encoded hour value (may be different than hour value for current timezone if value encoded in a different timezone)  Get the encoded minute value (may be different than the minute value for current timezone if value encoded in a different timezone)    Get the encoded second value (may be different than the second value for current timezone if value encoded in a different timezone)  Get the time value Since this is a JDBC object we use the JDBC definition we use the JDBC definition, see JDBC API Tutorial and Reference section 47.3.12 Date is set to Jan. 1, 1970 Get a java.sql.Time object from an encoded time and nano-second value. As required by JDBC the date component of the Time object will be set to Jan. 1, 1970 Convert a SQL TIME to a JDBC java.sql.Timestamp. Behaviour is to set the date portion of the Timestamp to the actual current date, which may not match the SQL CURRENT DATE, which remains fixed for the lifetime of a SQL statement. JDBC drivers (especially network client drivers) could not be expected to fetch the CURRENT_DATE SQL value on every query that involved a TIME value, so the current date as seen by the JDBC client was picked as the logical behaviour. See DERBY-1811. Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator  Hash code Check if the value is null. * SQL Operators end of parseTime   Adding this method to ensure that super class' setInto method doesn't get called that leads to the violation of JDBC spec( untyped nulls ) when batching is turned on. Set the value from a correctly typed Time object. Set the time portion of a date-time value into the passed in Calendar object from its encodedTime value. Note that this is only the time down to a resolution of one second. Only the HOUR_OF_DAY, MINUTE and SECOND fields are modified. The remaining state of the Calendar is not modified.   DataValueDescriptor interface  Convert a time to a JDBC escape format string end of timeToString Add a number of intervals to a datetime value. Implements the JDBC escape TIMESTAMPADD function. Finds the difference between two datetime values as a number of intervals. Implements the JDBC TIMESTAMPDIFF escape function. * String display of value
end of addInternal DataValueDescriptor interface   computeEncodedDate sets the date in a Calendar object and then uses the SQLDate function to compute an encoded date The encoded date is year &lt;&lt; 16 + month &lt;&lt; 8 + date computeEncodedTime extracts the hour, minute and seconds from a java.util.Date value and encodes them as hour &lt;&lt; 16 + minute &lt;&lt; 8 + second using the SQLTime function for encoding the data Compute the SQL timestamp function. end of computeTimestampFunction end of estimateMemoryUsage getDate returns the date portion of the timestamp Time is set to 00:00:00.0 Since Date is a JDBC object we use the JDBC definition for the time portion.  See JDBC API Tutorial, 47.3.12.   get storage length     getTime returns the time portion of the timestamp Date is set to 1970-01-01 Since Time is a JDBC object we use the JDBC definition for the date portion.  See JDBC API Tutorial, 47.3.12. Get the value field.  We instantiate the field on demand. Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator  Hash code Check if the value is null.  encodedDate value of 0 is null * SQL Operators Parse a timestamp or a date. DB2 allows timestamps to be used as dates or times. So date('2004-04-15-16.15.32') is valid, as is date('2004-04-15'). This method does not handle localized timestamps. end of parseDateOrTimestamp end of parseDateTimeInteger Parse a localized timestamp. end of parseLocalTimestamp end of parseTimestamp Promotes a DateTimeDataValue to a timestamp. end of promote   end of setCalendar Set the encoded values for the timestamp Set the value from a correctly typed Timestamp object.   DataValueDescriptor interface  Add a number of intervals to a datetime value. Implements the JDBC escape TIMESTAMPADD function. end of timestampAdd Finds the difference between two datetime values as a number of intervals. Implements the JDBC TIMESTAMPDIFF escape function. end of timestampDiff * String display of value
DataValueDescriptor interface  * SQL Operators The = operator as called from the language module, as opposed to the storage module.     ////////////////////////////////////////////////////////////  DataValueDescriptor interface (mostly implemented in DataType)  ////////////////////////////////////////////////////////////        Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator The &gt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. Hash code This method implements the isNegative method.  The &lt;= operator as called from the language module, as opposed to the storage module. The &lt; operator as called from the language module, as opposed to the storage module. This method implements the unary minus operator for tinyint. mod(tinyint, tinyint) The &lt;&gt; operator as called from the language module, as opposed to the storage module.   Set the value into a PreparedStatement. Set this value into a ResultSet for a subsequent ResultSet.insertRow or ResultSet.updateRow. This method will only be called for non-null values.         This method implements the * operator for "tinyint * tinyint". String display of value  DataValueDescriptor interface
Convert .sql test output to JUnit JDBC code. This keeps two lines/blocks of output at a time in currLineType and nextLineType so that the result of statement executions can be handled along with the executing statement, for example to generate the necessary asserts for errors or resultsets. Extract out the SQLSTATE.  Messages are of the form "ERROR <SQLSTATE>: ..." or "WARNING <SQLSTATE>: ...". or "ERROR ... SQLSTATE <SQLSTATE>" Return one ij command, or if it's a result set, return the entire result set in the StringBuffer. Note: the next four methods could probably be condensed into one method: e.g. void isMember(String[], String) Write JDBC code for an ij statement of the form: get cursor c1 as 'select * from t1' TODO: It would be nice to automatically write the prologue to contain the proper sqlAuthorizationDecorator if we have multiple connections in the test, but this would require rethinking how the output is written, since the prologue could not be constructed until the rest of the output has been processed.
Accept the visitor for all visitable children of this node. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. /////////////////////////////////////////////////////////////////////  CODE GENERATION METHODS  ///////////////////////////////////////////////////////////////////// Generate code to get the Java value out of a SQL value. Every SQL type has a corresponding Java type.  The getObject() method on the SQL type gets the right Java type. The generated code will be: (<Java type name>) ((DataValueDescriptor) <generated value>.getObject()) where <Java type name> comes from the getCorrespondingJavaTypeName() method of the value's TypeId. Generate code to cast the SQLValue to a Java value. Generate the code for the returns Null on Null input check.. Stack must contain the DataDescriptorValue. Generate the SQLvalue that this node wraps.  Get the JSQLType that corresponds to this node. Could be a SQLTYPE, a Java primitive, or a Java class. Overrides method in JavaValueNode. Returns the name of the java class type that this node coerces to. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Returns the name of the java primitive type that this node coerces to. Get the type name of the SQLValue we generate. /////////////////////////////////////////////////////////////////////  OTHER VALUE NODE METHODS  ///////////////////////////////////////////////////////////////////// Get the SQL ValueNode that is being converted to a JavaValueNode Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression.
Translate DB2 SQL Type to the non-nullable type. Map DB2 SQL Type to JDBC Type Map jdbc type to the DB2 DRDA SQL Types expected by jcc. Map JDBC Type to DB2 SqlType
Create a table AllDataTypesTable and populate with data Assumes user previously executed "call SYSCS_UTIL.SYSCS_SET_RUNTIMESTATISTICS(1)";
Return max memory usage for a SQL Varbit  Return my format identifier. Normalization method - this method may be called when putting a value into a SQLBit, for example, when inserting into a SQLBit column.  See NormalizeResultSet in execution. Set the width of the to the desired value.  Used when CASTing.  Ideally we'd recycle normalize(), but the behavior is different (we issue a warning instead of an error, and we aren't interested in nullability). DataValueDescriptor interface
DataValueDescriptor interface   Storable interface, implies Externalizable, TypedFormat Return my format identifier. DataValueDescriptor interface.  returns the reasonable minimum amount by which the array can grow . See readExternal. when we know that the array needs to grow by at least one byte, it is not performant to grow by just one byte instead this amount is used to provide a resonable growby size. Normalization method - this method may be called when putting a value into a SQLVarchar, for example, when inserting into a SQLVarchar column.  See NormalizeResultSet in execution. DataValueDescriptor interface @see DataValueDescriptor#typePrecedence
Generates a SQLWarning instance based on the supplied messageId and arguments. It looks up the messageId to generate a localised warning message. Also, SQLState is set correctly based on the messageId.
Creates a datamodel for testing Scrollable Updatable ResultSets and populates the database model with data. The model will be set up with the number of records as defined by the recordCount attribute. Creates a datamodel for testing Scrollable Updatable ResultSets and populates the database model with data. Prints the stack trace. If run in the harness, the harness will mark the test as failed if this method has been called. Creates a datamodel for testing Scrollable Updatable ResultSets and populates the database model with data. Delete the datamodel
Builds a list of columns suitable for creating this Catalog. DERBY-1734 fixed an issue where older code created the BOOLEAN column SYSTEMALIAS with maximum length 0 instead of 1. DERBY-1742 was opened to track if upgrade changes are needed. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a AliasDescriptor out of a SYSALIASES row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSALIASES row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a ViewDescriptor out of a SYSCHECKS row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSCHECKS row
builds a column list for the catalog builds a tuple descriptor from a row end of buildDescriptor builds an index key row for a given index number. end of buildIndexKeyRow end of makeRow Or a set of permissions in with a row from this catalog table end of orPermissions Remove a set of permissions from a row from this catalog table end of removePermissions
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a ColumnDescriptor out of a SYSCOLUMNS row Get the Properties associated with creating the heap. Get the Properties associated with creating the specified index. Get the index number for the primary key index on this catalog. ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSCOLUMNS row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  /////////////////////////////////////////////////////////////////////////  Get the conglomerate's name of the row. Get the conglomerate's UUID of the row. Get the Properties associated with creating the heap. Get the Properties associated with creating the specified index. Get the schema's UUID from the row. Get the table's UUID from the row. Make a SYSCONGLOMERATES row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a ConstraintDescriptor out of a SYSCONSTRAINTS row Encode the characteristics of the constraints into a single character. {deferrable, initiallyDeferred, enforced}     -> 'e' {deferrable, initiallyDeferred, not enforced} -> 'd' {deferrable, immediate, enforced}             -> 'i' {deferrable, immediate, not enforced}         -> 'j' {not deferrable, immediate, enforced}         -> 'E' {not deferrable, immediate, not enforced      -> 'D' Other combinations are prohibited and not used. Note that the value 'E' is only value used prior to version 10.11, and as such upward compatibily since by default, constraints are {not deferrable, immediate, enforced}. Get the constraint ID of the row. Get the constraint name of the row. Get the constraint type out of the row. Get the schema ID of the row. Get the table ID of the row. ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSCONTRAINTS row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a ConstraintDescriptor out of a SYSDEPENDS row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSDEPENDS row
Builds a list of columns suitable for creating this Catalog. Make a SYSDUMMY1 row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a descriptor out of a SYSFILES row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSFILES row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a ViewDescriptor out of a SYSFOREIGNKEYS row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSFOREIGNKEYS row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a SubConstraintDescriptor out of a SYSKEYS row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSKEYS row
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make an  Tuple Descriptor out of a SYSPERMS row builds an index key row given for a given index number. end of buildIndexKeyRow Make a SYSPERMS row Or a set of permissions in with a row from this catalog table Remove a set of permissions from a row from this catalog table end of removePermissions
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make an  Tuple Descriptor out of a SYSROLES row Make a SYSROLES row
builds a column list for the catalog builds a tuple descriptor from a row end of buildDescriptor builds an index key row given for a given index number. end of buildIndexKeyRow end of makeRow Or a set of permissions in with a row from this catalog table Remove a set of permissions from a row from this catalog table end of removePermissions
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make an  Tuple Descriptor out of a SYSSCHEMAS row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSSCHEMAS row
Builds a list of columns suitable for creating this Catalog. Make an  Tuple Descriptor out of a SYSSEQUENCES row Make a SYSSEQUENCES row
Builds a list of columns suitable for creating this Catalog. The last column, the serialized statement, is not added to the column list.  This is done deliberately to make it a 'hidden' column -- one that is not visible to customers, but is visible to the system. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make an  Tuple Descriptor out of a SYSSTATEMENTS row Get the Properties associated with creating the heap. ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSSTATEMENTS row. <p> <B>WARNING</B>: When empty row is true, this method takes a snapshot of the SPSD and creates a row.  It is imperative that that row remain consistent with the descriptor (the valid and StorablePreparedStatement fields must be in sync). If this row is to be written out and valid is true, then this call and the insert should be synchronized on the SPSD. This method has <B>NO</B> synchronization.
Builds a list of columns suitable for creating this Catalog. Make a SYSSTATISTICS row
builds a column list for the catalog builds a tuple descriptor from a row end of buildDescriptor builds a key row given for a given index number. end of buildIndexRow end of makeRow end of orOnePermission Or a set of permissions in with a row from this catalog table end of orPermissions end of removeOnePermission Remove a set of permissions from a row from this catalog table end of removePermissions
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a TableDescriptor out of a SYSTABLES row Make a TableDescriptor out of a SYSTABLES row Builds an empty index row. Get the table name out of this SYSTABLES row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSTABLES row
Builds a list of columns suitable for creating this Catalog. The last column, the serialized statement, is not added to the column list.  This is done deliberately to make it a 'hidden' column -- one that is not visible to customers, but is visible to the system. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make an  Tuple Descriptor out of a SYSTRIGGERS row Get a calendar instance with the correct time zone for storing and retrieving creation timestamps. Creation timestamps are stored in UTC to avoid ambiguities around the change from daylight saving time to standard time, or other time zone changes. If the data dictionary version is less than 10.11, however, the timestamps are stored in the local time zone. a little helper Helper method that contains common logic for {@code makeRow()} and {@code makeEmptyRowForCurrentVersion()}. Creates a row for the SYSTRIGGERS conglomerate. ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSTRIGGERS row.
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a descriptor out of a SYSUSERS row. The password column in the row will be zeroed out. ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSUSERS row. The password in the UserDescriptor will be zeroed by this method.
Builds a list of columns suitable for creating this Catalog. /////////////////////////////////////////////////////////////////////////  ABSTRACT METHODS TO BE IMPLEMENTED BY CHILDREN OF CatalogRowFactory  ///////////////////////////////////////////////////////////////////////// Make a ViewDescriptor out of a SYSVIEWS row ///////////////////////////////////////////////////////////////////////////  METHODS  /////////////////////////////////////////////////////////////////////////// Make a SYSVIEWS row
Generate samples from the specified mixture-of-Gaussian distribution. Get the maximum number of Gaussian components. Get the number of Gaussian components. Specify the mixture-of-Gaussian configuration. Specify the mean parameters for the Gaussian components. Set the number of Gaussian components. Specify the variance parameters for the Gaussian components. Specify the weights for the Gaussian components.
/////////////////////////////////////////////////////////////////////////////////  SQLData BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  FUNCTIONS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  OTHER Object OVERRIDES  /////////////////////////////////////////////////////////////////////////////////
@Override Returns a sample VTI that is empty (has zero rows). @Override @Override @Override Returns a sample VTI with the some test data. @Override
class interface  ASSERT checks the condition, and if it is false, throws AssertFailure. A message about the assertion failing is printed. <p> ASSERT checks the condition, and if it is false, throws AssertFailure. The message will be printed and included in the assertion. <p> The DEBUG calls provide the ability to print information or perform actions based on whether a debug flag is set or not. debug flags are set in configurations and picked up by the sanity manager when the monitor finds them (see CONFIG below). <p> The message is output to the trace stream, so it ends up in db2j.LOG. It will include a header line of DEBUG <flagname> OUTPUT: before the message. <p> If the debugStream stream cannot be found, the message is printed to System.out. This can be used to have the SanityManager return FALSE for any DEBUG_ON check. DEBUG_SET of an individual flag will appear to have no effect. This can be used to have the SanityManager return TRUE for any DEBUG_ON check. DEBUG_CLEAR of an individual flag will appear to have no effect. Set the named debug flag to false. <p> Calls to this method should be surrounded with if (SanityManager.DEBUG) { } so that they can be compiled out completely. This can be called directly if you want to control what is done once the debug flag has been verified -- for example, if you are calling a routine that prints to the trace stream directly rather than returning a string to be printed, or if you want to perform more (or fewer!) <p> Calls to this method should be surrounded with if (SanityManager.DEBUG) { } so that they can be compiled out completely. The DEBUG_PRINT calls provides a convenient way to print debug information to the db2j.LOG file,  The message includes a header <p> DEBUG <flag> OUTPUT: before the message <p> If the debugStream stream cannot be found, the message is printed to System.out. Set the named debug flag to true. <p> Calls to this method should be surrounded with if (SanityManager.DEBUG) { } so that they can be compiled out completely.  class implementation  THROWASSERT throws AssertFailure. This is used in cases where the caller has already detected the assertion failure (such as in the default case of a switch). This method should be used, rather than throwing AssertFailure directly, to allow us to centralize all sanity checking.  The message argument will be printed and included in the assertion. <p> THROWASSERT throws AssertFailure. This flavor will print the stack associated with the exception. The message argument will be printed and included in the assertion. <p> THROWASSERT throws AssertFailure. This flavor will print the stack associated with the exception. <p>

INTERFACE METHODS This is the guts of the Execution-time logic for CREATE TABLE. OBJECT METHODS
We inherit the generate() method from DDLStatementNode. Create the Constant information that will drive the guts of Execution. Returns whether or not this Statement requires a set/clear savepoint around its execution.  The following statement "types" do not require them: Cursor	- unnecessary and won't work in a read only environment Xact	- savepoint will get blown away underneath us during commit/rollback Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
/////////////////////////////////////////////////////////////////////////////  AGGREGATION UTILITIES  ///////////////////////////////////////////////////////////////////////////// Run accumulation on every aggregate in this row.  This method is useful when draining the source or sorter, depending on whether or not there were any distinct aggregates.  Remember, if there are distinct aggregates, then the non-distinct aggregates were calculated on the way into the sorter and only the distinct aggregates will be accumulated here. Otherwise, all aggregates will be accumulated here. If the result set has been opened, close the open scan. This result set has its row from the last fetch done. If the cursor is closed, a null is returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) RESOLVE - THIS NEXT METHOD IS OVERRIDEN IN DistinctScalarResultSet BEACAUSE OF A JIT ERROR. THERE IS NO OTHER REASON TO OVERRIDE IT IN DistinctScalarAggregateResultSet.  THE BUG WAS FOUND IN 1.1.6 WITH THE JIT. Return the next row.  If it is a scalar aggregate scan /////////////////////////////////////////////////////////////////////////////  SCAN ABSTRACTION UTILITIES  ///////////////////////////////////////////////////////////////////////////// Get a row from the input result set. /////////////////////////////////////////////////////////////////////////////  CursorResultSet interface  ///////////////////////////////////////////////////////////////////////////// This result set has its row location from the last fetch done. Always returns null. Return the total amount of time spent in this ResultSet /////////////////////////////////////////////////////////////////////////////  CLASS SPECIFIC  ///////////////////////////////////////////////////////////////////////////// * Run the aggregator initialization method for * each aggregator in the row. * * @param	row	the row to initialize * * @return Nothing. * * @exception	standard Derby exception /////////////////////////////////////////////////////////////////////////////  ResultSet interface (leftover from NoPutResultSet)  ///////////////////////////////////////////////////////////////////////////// Open the scan.  Load the sorter and prepare to get rows from it. reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values...
Close the scan. Return the log instant (as an integer) the scan is currently on - this is the log instant of the log record that was returned by getNextRecord. Return the log instant the scan is currently on - this is the log instant of the log record that was returned by getNextRecord. Return the log instant at the end of the log record on the current LogFile in the form of a log instant. After the scan has been closed, the end of the last log record will be returned except when the scan ended in an empty log file.  In that case, the start of this empty log file will be returned.  (This is done to make sure new log records are inserted into the newest log file.) * Methods of StreamLogScan Read the next log record. Switching log to a previous log file if necessary, Resize the input stream byte array if necessary. Read the previous log record. Switching log to a previous log file if necessary, Resize the input stream byte array if necessary. Read the next log record. Switching log to a previous log file if necessary, Resize the input stream byte array if necessary. returns true if there is partially writen log records before the crash in the last log file. Partiall wrires are identified during forward redo scans for log recovery. Reset the scan to the given LogInstant.
Delete the row at the current position of the scan. A call to allow client to indicate that current row does not qualify. <p> Indicates to the ScanController that the current row does not qualify for the scan.  If the isolation level of the scan allows, this may result in the scan releasing the lock on this row. <p> Note that some scan implimentations may not support releasing locks on non-qualifying rows, or may delay releasing the lock until sometime later in the scan (ie. it may be necessary to keep the lock until either the scan is repositioned on the next row or page). <p> This call should only be made while the scan is positioned on a current valid row. Returns true if the current position of the scan still qualifies under the set of qualifiers passed to the openScan().  When called this routine will reapply all qualifiers against the row currently positioned and return true if the row still qualifies.  If the row has been deleted or no longer passes the qualifiers then this routine will return false. This case can come about if the current scan or another scan on the same table in the same transaction deleted the row or changed columns referenced by the qualifier after the next() call which positioned the scan at this row. Note that for comglomerates which don't support update, like btree's, there is no need to recheck the qualifiers. The results of a fetch() performed on a scan positioned on a deleted row are undefined, note that this can happen even if next() has returned true (for instance the client can delete the row, or if using read uncommitted another thread can delete the row after the next() call but before the fetch). Fetch the (partial) row at the current position of the Scan. The value in the destRow storable row is replaced with the value of the row at the current scan position.  The columns of the destRow row must be of the same type as the actual columns in the underlying conglomerate. The number of elements in fetch must be compatible with the number of scan columns requested at the openScan call time. <BR> A fetch can return a sub-set of the scan columns requested at scan open time by supplying a destRow will less elements than the number of requested columns. In this case the N leftmost of the requested columns are fetched, where N = destRow.length. In the case where all columns are rested and N = 2 then columns 0 and 1 are returned. In the case where the openScan FormatableBitSet requested columns 1, 4 and 7, then columns 1 and 4 would be fetched when N = 2. <BR> The results of a fetch() performed on a scan after next() has returned false are undefined. A fetch() performed on a scan positioned on a deleted row will throw a StandardException with state = SQLState.AM_RECORD_NOT_FOUND.  Note that this can happen even if next() has returned true (for instance the client can delete the row, or if using read uncommitted another thread can delete the row after the next() call but before the fetch). Fetch the location of the current position in the scan. The destination location is replaced with the location corresponding to the current position in the scan. The destination location must be of the correct actual type to accept a location from the underlying conglomerate location. The results of a fetchLocation() performed on a scan after next() has returned false are undefined. The results of a fetchLocation() performed on a scan positioned on a deleted row are undefined, note that this can happen even if next() has returned true (for instance the client can delete the row, or if using read uncommitted another thread can delete the row after the next() call but before the fetchLocation). Fetch the (partial) row at the next position of the Scan. If there is a valid next position in the scan then the value in the destRow storable row is replaced with the value of the row at the current scan position.  The columns of the destRow row must be of the same type as the actual columns in the underlying conglomerate. The resulting contents of destRow after a fetchNext() which returns false is undefined. The result of calling fetchNext(row) is exactly logically equivalent to making a next() call followed by a fetch(row) call.  This interface allows implementations to optimize the 2 calls if possible. The same as fetch, except that the qualifiers passed to the openScan() will not be applied. destRow will contain the current row even if it has been changed and no longer qualifies. Returns true if the current position of the scan is at a deleted row.  This case can come about if the current scan or another scan on the same table in the same transaction deleted the row after the next() call which positioned the scan at this row. The results of a fetch() performed on a scan positioned on a deleted row are undefined. Return true is the scan has been closed after a commit, but was opened with holdability and can be reopened using positionAtRowLocation. Move to the next position in the scan.  If this is the first call to next(), the position is set to the first row. Returns false if there is not a next row to move to. It is possible, but not guaranteed, that this method could return true again, after returning false, if some other operation in the same transaction appended a row to the underlying conglomerate. Positions the scan at row location and locks the row. If the scan is not opened, it will be reopened if this is a holdable scan and there has not been any operations which causes RowLocations to be invalidated. Replace the (partial) row at the current position of the scan.

Close this scan. Get the group for the current log record. Get the DatabaseInstant for the current log record. Get the Loggable associated with the currentLogRecord Get an InputStream for reading the optional data associated with the current log record. This may only be called once per log record. Get the TransactionId for the current log record. Position to the next log record.
Return all information gathered about the scan. <p> This routine returns a list of properties which contains all information gathered about the scan.  If a Property is passed in, then that property list is appeneded to, otherwise a new property object is created and returned. <p> Not all scans may support all properties, if the property is not supported then it will not be returned.  The following is a list of properties that may be returned. These names have been internationalized, the names shown here are the old, non-internationalized names: scanType - type of the scan being performed: btree heap sort numPagesVisited - the number of pages visited during the scan.  For btree scans this number only includes the leaf pages visited. numDeletedRowsVisited - the number of deleted rows visited during the scan.  This number includes only those rows marked deleted. numRowsVisited - the number of rows visited during the scan.  This number includes all rows, including: those marked deleted, those that don't meet qualification, ... numRowsQualified - the number of rows which met the qualification. treeHeight (btree's only) - for btree's the height of the tree.  A tree with one page has a height of 1.  Total number of pages visited in a btree scan is (treeHeight - 1 + numPagesVisited). numColumnsFetched - the number of columns Fetched - partial scans will result in fetching less columns than the total number in the scan. columnsFetchedBitSet - The BitSet.toString() method called on the validColumns arg. to the scan, unless validColumns was set to null, and in that case we will return "all". NOTE - this list will be expanded as more information about the scan is gathered and returned.
Close scan as part of terminating a transaction. <p> Use this call to close the scan resources as part of committing or aborting a transaction.  The normal close() routine may do some cleanup that is either unnecessary, or not correct due to the unknown condition of the scan following a transaction ending error.  Use this call when closing all scans as part of an abort of a transaction. Insert all rows that qualify for the current scan into the input Hash table. <p> This routine scans executes the entire scan as described in the openScan call.  For every qualifying unique row value an entry is placed into the HashTable. For unique row values the entry in the Hashtable has a key value of the object stored in row[key_column_number], and the value of the data is row.  For row values with duplicates, the key value is also row[key_column_number], but the value of the data is a Vector of rows.  The caller will have to call "instanceof" on the data value object if duplicates are expected, to determine if the data value of the Hashtable entry is a row or is a Vector of rows. <p> Note, that for this routine to work efficiently the caller must ensure that the object in row[key_column_number] implements the hashCode and equals method as appropriate for it's datatype. <p> It is expected that this call will be the first and only call made in an openscan.  Qualifiers and stop position of the openscan are applied just as in a normal scan.  This call is logically equivalent to the caller performing the following: import java.util.Hashtable; hash_table = new Hashtable(); while (next()) { row = create_new_row(); fetch(row); if ((duplicate_value = hash_table.put(row[key_column_number], row)) != null) { Vector row_vec; // inserted a duplicate if ((duplicate_value instanceof vector)) { row_vec = (Vector) duplicate_value; } else { // allocate vector to hold duplicates row_vec = new Vector(2); // insert original row into vector row_vec.addElement(duplicate_value); // put the vector as the data rather than the row hash_table.put(row[key_column_number], row_vec); } // insert new row into vector row_vec.addElement(row); } } <p> The columns of the row will be the standard columns returned as part of a scan, as described by the validColumns - see openScan for description. RESOLVE - is this ok?  or should I hard code somehow the row to be the first column and the row location? <p> No overflow to external storage is provided, so calling this routine on a 1 gigabyte conglomerate will incur at least 1 gigabyte of memory (probably failing with a java out of memory condition).  If this routine gets an out of memory condition, or if "max_rowcnt" is exceeded then then the routine will give up, empty the Hashtable, and return "false." <p> On exit from this routine, whether the fetchSet() succeeded or not the scan is complete, it is positioned just the same as if the scan had been drained by calling "next()" until it returns false (ie. fetchNext() and next() calls will return false). reopenScan() can be called to restart the scan. <p> RESOLVE - until we get row counts what should we do for sizing the the size, capasity, and load factor of the hash table. For now it is up to the caller to create the Hashtable, Access does not reset any parameters. <p> RESOLVE - I am not sure if access should be in charge of allocating the new row objects.  I know that I can do this in the case of btree's, but I don't think I can do this in heaps. Maybe this is solved by work to be done on the sort interface.
Set the info in a ScanQualifier
Can we get instantaneous locks when getting share row locks at READ COMMITTED. Close the result set. Get the lock mode based on the language isolation level. Always do row locking unless the isolation level is serializable or the table is marked as table locked. Return the isolation level of the scan in the result set. Initialize the isolation level and the lock mode. If the result set was constructed with an explicit isolation level, or if the isolation level has already been initialized, this is a no-op. All sub-classes should invoke this method from their <code>openCore()</code> methods. Determine whether this scan should return row locations Translate isolation level from language to store.

Add constraint tests to suite. junit tests to create schema Run a Order Entry script.
Drop this schema. Drops the schema if it is empty. If the schema was the current default then the current default will be reset through the language connection context. Methods so that we can put SchemaDescriptors on hashed lists Determine if two SchemaDescriptors are the same. Gets the authorization id of the schema Get the provider's type. Returns the collation type associated with this schema  Provider interface     Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Gets the name of the schema Gets the oid of the schema Get a hashcode for this SchemaDescriptor Indicate whether this is a system schema with grantable routines Indicate whether this is a system schema or not Examples of system schema's include: SYS, SYSIBM, SYSCAT, SYSFUN, SYSPROC, SYSSTAT, and SYSCS_DIAG Sets the authorization id of the schema. This is only used by the DataDictionary during boot in order to patch up the authorization ids on system schemas. Sets the oid of the schema  class interface  Prints the contents of the SchemaDescriptor
/////////////////////////////////////////////////////////////////////////  VisitableFilter BEHAVIOR  /////////////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////  APPLICATION BEHAVIOR  ////////////////////////////////////////////////////// <p> Run the Scores application, showcasing Derby features. </p> //////////////////////////////////////////////////////  ENTRY POINT  ////////////////////////////////////////////////////// <p> Run the Scores application, showcasing Derby features. </p> <ul> <li>args[ 0 ] = name of server jar</li> <li>args[ 1 ] = name of math library</li> </ul>
class implementation  Add a row to the backing hash table, keyed on position. When a row gets updated when using scrollable insensitive updatable result sets, the old entry for the row will be deleted from the hash table and this method will be called to add the new values for the row to the hash table, with the parameter rowUpdated = true so as to mark the row as updated. The latter is done in order to implement detectability of own changes for result sets of this type. Determine if the cursor is before the first row in the result set. If the result set has been opened, close the open scan. Returns the row at the absolute position from the query, and returns NULL when there is no such position. (Negative position means from the end of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: An exception will be thrown on 0. Gets information from last getNextRow call. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Get the column array from the current position in the hash table Returns the first row from the query, and returns NULL when there are no rows. Returns the last row from the query, and returns NULL when there are no rows.  Get the next row from the source ResultSet tree and insert into the hash table Returns the previous row from the query, and returns NULL when there are no more previous rows. Returns the row at the relative position from the current cursor position, and returns NULL when there is no such position. (Negative position means toward the beginning of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: 0 is valid. NOTE: An exception is thrown if the cursor is not currently positioned on a row. Get the row data at the specified position from the hash table. Get the row at the specified position from the hash table.  CursorResultSet interface  Gets information from its source. We might want to have this take a CursorResultSet in its constructor some day, instead of doing a cast here? Returns the row number of the current row.  Row numbers start from 1 and go to 'n'.  Corresponds to row numbering used to position current row in the result set (as per JDBC). Return the total amount of time spent in this ResultSet Returns TRUE if the row was been deleted within the transaction, otherwise returns FALSE Returns TRUE if the row was been updated within the transaction, otherwise returns FALSE   ResultSet interface (leftover from NoPutResultSet)  open a scan on the source. scan parameters are evaluated at each open, so there is probably some way of altering their values... Positions the cursor in the last fetched row. This is done before navigating to a row that has not previously been fetched, so that getNextRowCore() will re-start from where it stopped. reopen a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... Sets the current position to after the last row and returns NULL because there is no current row. Sets the current position to before the first row and returns NULL because there is no current row.


retrieve the package name and consistency token information Add a finalizer to free() the section, useful for Statement.executes that result in exceptions Store the Packagename and consistency token information for reuse. <ul> <li>Case 1: if it is generated section, just store the byte array in PKGNAMCBytes.</li> <li>Case 2: for not a generated section, information is stored in the correct byte array depending on the holdability in SectionManager.</li> </ul>
------------------------entry points---------------------------------------- Get a section for either a jdbc update or query statement. Get a section for a jdbc 1 positioned update/delete for the corresponding query. A positioned update section must come from the same package as its query section. Get a section for a jdbc 2 positioned update/delete for the corresponding query. A positioned update section must come from the same package as its query section. Store the Packagename and consistency token information This is called from Section.setPKGNAMCBytes

Inspect the class of the passed in Object for security risks. This inspects, at this level only, the actual type of the object, not the declared type. E.g. for DriverManager.getConnection the declared type is java.sql.Connection which has no security risks, but the implementation type returned may have many. <code> Connection conn = DriverManager.getConnection(url); // will inspect the implementation call, eg. EmbedConnection30 SecurityManager.inspect(conn); </code> No output is generated by this call, the caller must call report() to obtain the risks. Perform security analysis of the public api for the embedded engine. Prints a report to System.out on completion. Inspect a Derby class for security risks. This includes following potential references exposed through the class. <P> Risks looked at: <UL> <LI> public constructors in non-public class - No justification for the constructor to be public. <LI> public constructors in non-final class and non-sealed package - Allows the class to be sub-classes through a injected class in the same package. <LI> public non-final field - Allows any one with a handle to the object to change the field. </UL> <P> The type of any public field or return type of any public method is also inspected. The assumption is that if such a field or method exists they have the potential to be called and return a valid object. <P> Note that this inspection is through the declared type of exposed references, not the actual runtime type. The actual runtime type might expose other classes that should be inspected. Inspect a class for security risks. No output is generated by this call, the caller must call report() to obtain the risks. Is the passed in class part of the declared public api. Currently only the emebdded public api Produce a report on System.out of all inspected classes that have risks associated with them. Produce a report on System.out of all inspected classes that have risks associated with them. If reportClear is true then additionally all classes that have been inspected will be returned.
Determine the settings of the classpath in order to configure the variables used in the testing policy files. Looks for three items: Location of derbyTesting.jar via this class Location of derby.jar via org.apache.derby.jdbc.EmbeddedDataSource Location of derbyclient.jar via org.apache.derby.jdbc.ClientDataSource Two options are supported, either all are in jar files or all are on the classpath. Properties are set as follows: <P> Classpath: <BR> derbyTesting.codeclasses set to URL of classes folder <BR> derbyTesting.ppcodeclasses set to URL of the 'classes.pptesting' folder if it exists on the classpath. The existence of the package private tests is determined via org.apache.derby.PackagePrivateTestSuite <P> Jar files: <BR> derbyTesting.codejar - URL of derby.jar, derbynet.jar and derbytools.jar, all assumed to be in the same location. <BR> derbyTesting.clientjar - URL of derbyclient.jar <BR> derbyTesting.testjar - URL of derbyTesting.jar <BR> derbyTesting.testjarpath - File system path to derbyTesting.jar if the jar has a URL with a file protocol. Return the name of the default policy. Returns the location of the effective policy resource. <p> If two valid policy resources from different locations are specified, they will be merged into one policy file. Return the policy file system properties for use by the old test harness. This ensures a consistent approach to setting the properties. There are the properties used to define the jar file location in any policy files. Returns a URL for the given policy resource. Get the URL of the code base from a class. Get the URL of the code base from a class name. If the class cannot be loaded, null is returned. Install a SecurityManager with the default test policy file: org/apache/derbyTesting/functionTests/util/derby_tests.policy Merges the two specified policy resources (typically files), and writes the combined policy to a new file. Creates the specified directory if it doesn't exist. "Install" no security manager. Get a decorator that will ensure no security manger is installed to run a test. Not supported for suites. <BR> An empty suite is returned if a security manager was installed externally, i.e. not under the control of the BaseTestCase and this code. In this case the code can not support the mode of no security manager as it may not have enough information to re-install the security manager. So the passed in test will be skipped. Opens the resource stream in a privileged block. Prints a debug message if debugging is enabled. Install specific policy file with the security manager including the special case of no security manager. Strip off the last token which will be the jar name. The returned string includes the trailing slash. Remove the security manager.
Raise an exception if the current user does not have permission to perform the indicated operation. Verify that we have been granted permission to use Derby internals Checks that a Subject has a Permission under the SecurityManager. To perform this check the following policy grant is required <ul> <li> to run the encapsulated test: permission javax.security.auth.AuthPermission "doAsPrivileged"; </ul> or an AccessControlException will be raised detailing the cause. <p> Checks that a User has a Permission under the SecurityManager. To perform this check the following policy grant is required <ul> <li> to run the encapsulated test: permission javax.security.auth.AuthPermission "doAsPrivileged"; </ul> or an AccessControlException will be raised detailing the cause. <p> Creates a (read-only) Subject representing a given user as a System user within Derby. Returns the Authorization Identifier for a principal name. Privileged lookup of a Context. Must be private so that user code can't call this entry point.
end doWork The arguments should be the names of the input and output files end exec This just does JCC changes on the output master file
Accept the visitor for all visitable children of this node. Add a new predicate to the list.  This is useful when doing subquery transformations, when we build a new predicate with the left side of the subquery operator and the subquery's result column. Bind the expressions in this SelectNode.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the expressions in this ResultSetNode if it has tables.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the tables in this SelectNode.  This includes getting their TableDescriptors from the DataDictionary and numbering the FromTables. NOTE: Because this node represents the top of a new query block, we bind both the non VTI and VTI tables under this node in this method call. Bind the result columns for this ResultSetNode to a base table. This is useful for INSERT and UPDATE statements, where the result columns get their types from the table being updated or inserted into. If a result column list is specified, then the verification that the result column list does not contain any duplicates will be done when binding them by name. Bind the result columns of this ResultSetNode when there is no base table to bind them to.  This is useful for SELECT statements, where the result columns get their types from the expressions that live under them. Bind the expressions in the target list.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression.  This is useful for EXISTS subqueries, where we need to validate the target list before blowing it away and replacing it with a SELECT true. Bind any untyped null nodes to the types in the given ResultColumnList. Decrement (query block) level (0-based) for all of the tables in this ResultSet tree. This is useful when flattening a subquery. Ensure that the top of the RSN tree has a PredicateList. Find colName in the result columns and return underlying columnReference. Note that this function returns null if there are more than one FromTable for this SelectNode and the columnReference needs to be directly under the resultColumn. So having an expression under the resultSet would cause returning null. Evaluate whether or not the subquery in a FromSubquery is flattenable. Currently, a FSqry is flattenable if all of the following are true: o  Subquery is a SelectNode. (ie, not a RowResultSetNode or a UnionNode) o  It contains a single table in its FROM list. o  It contains no subqueries in the SELECT list. o  It does not contain a group by or having clause o  It does not contain aggregates. o  It is not a DISTINCT. o  It does not have an ORDER BY clause (pushed from FromSubquery). Replace this SelectNode with a ProjectRestrictNode, since it has served its purpose. Assumes that isCursorUpdatable has been called, and that it is only called for updatable cursors. Get the final CostEstimate for this SelectNode. Return the fromList for this SelectNode. Determine whether or not the specified name is an exposed name in the current query block. Get an optimizer to use for this SelectNode.  Only get it once - subsequent calls return the same optimizer. Return the selectSubquerys for this SelectNode. Return the whereClause for this SelectNode. Return the wherePredicates for this SelectNode. Return the whereSubquerys for this SelectNode.  Used by SubqueryNode to avoid flattening of a subquery if a window is defined on it. Note that any inline window definitions should have been collected from both the selectList and orderByList at the time this method is called, so the windows list is complete. This is true after preprocess is completed. Is the result of this node an ordered result set.  An ordered result set means that the results from this node will come in a known sorted order. This means that the data is ordered according to the order of the elements in the RCL. Today, the data is considered ordered if: o The RCL is composed entirely of CRs or ConstantNodes o The underlying tree is ordered on the CRs in the order in which they appear in the RCL, taking equality predicates into account. Future Enhancements: o The prefix will not be required to be in order.  (We will need to reorder the RCL and generate a PRN with an RCL in the expected order.) Determine if this select is updatable or not, for a cursor. Modify the access paths according to the choices the optimizer made. Modify the access paths according to the decisions the optimizer made.  This can include adding project/restrict nodes, index-to-base-row nodes, etc. Put the expression trees in conjunctive normal form Optimize this SelectNode.  This means choosing the best access path for each table, among other things. Peform the various types of transitive closure on the where clause. The 2 types are transitive closure on join clauses and on search clauses. Join clauses will be processed first to maximize benefit for search clauses. Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Push an expression into this SELECT (and possibly down into one of the tables in the FROM list).  This is useful when trying to push predicates into unflattened views or derived tables. Push down the offset and fetch first parameters to this node. Push the order by list down from the cursor node into its child result set so that the optimizer has all of the information that it needs to consider sort avoidance. Return true if the node references SESSION schema tables (temporary or permanent) Search to see if a query references the specifed table name. Check for (and reject) ? parameters directly under the ResultColumns. This is done for SELECT statements. {@inheritDoc } A no-op for SelectNode. Return whether or not this ResultSet tree is guaranteed to return at most 1 row based on heuristics.  (A RowResultSetNode and a SELECT with a non-grouped aggregate will return at most 1 row.) Return whether or not this ResultSetNode contains a subquery with a reference to the specified target table. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Determine whether or not this subquery, the SelectNode is in a subquery, can be flattened into the outer query block based on a uniqueness condition. A uniqueness condition exists when we can guarantee that at most 1 row will qualify in each table in the subquery.  This is true if every table in the from list is (a base table and the set of columns from the table that are in equality comparisons with expressions that do not include a column from the same table is a superset of any unique index on the table) or an ExistsBaseTable. Get the lock mode for the target of an update statement (a delete or update).  The update mode will always be row for CurrentOfNodes.  It will be table if there is no where clause. Verify that a SELECT * is valid for this type of subquery.

Drop this sequence descriptor. Only restricted drops allowed right now. Get the provider's type.    ////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////// Get the provider's UUID Return the name of this Provider.  (Useful for errors.)  Accessor methods  Is this provider persistent?  A stored dependency will be required if both the dependent and provider are persistent. Check that all of the dependent's dependencies are valid. Mark the dependent as invalid (due to at least one of its dependencies being invalid). Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid).
/////////////////////////////////////////////////////////////////////////////////  PRIVATE BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Advance the sequence generator. Pre-allocate a range of new values if necessary. </p> <p> Allocate a new range. Is a NOP if the current value is not what we expected. See the class header comment for more information on how this method is used. </p> <p> Clone this sequence generator. This method supports the special bulk-insert optimization in InsertResultSet. </p> <p> Clone this sequence generator. This method supports the special bulk-insert optimization in InsertResultSet. </p> <p> Compute the number of values to allocate. The range may wrap around. </p> /////////////////////////////////////////////////////////////////////////////////  SETUP/TEARDOWN MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> This method returns the number of values to pre-allocate when we grab a new chunk of values. This is a bit of defensive coding to cover the case when the sequence's parameters are absurdly large. </p> <p> Get the number of values remaining until we bump against an endpoint of the legal range of values. This is a positive number and so may understate the number of remaining values if the datatype is BIGINT. </p> <p> Get the next sequence number managed by this generator and advance the number. Could raise an exception if the legal range is exhausted and wrap-around is not allowed--that is, if NO CYCLE was specified when the sequence was defined. See the class header comment for a description of how this method operates. </p> <p> Get the name of this sequence generator. Technically, this doesn't need to be synchronized. But it is simpler to just maintain a rule that all public methods should be synchronized. </p> /////////////////////////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Get the name of the schema of this sequence generator. Technically, this doesn't need to be synchronized. But it is simpler to just maintain a rule that all public methods should be synchronized. </p> <p> Mark the generator as exhausted. </p> <p> Return true if an overflow/underflow occurred. This happens if the originalValue and incrementedValue have opposite sign. Overflow also occurs if the incrementedValue falls outside the range of the sequence. </p> <p> Peek at the current value of the sequence generator without advancing the generator. Returns null if the generator is exhausted. </p>
/////////////////////////////////////////////////////////////////////////////////  UTILITY METHODS  ///////////////////////////////////////////////////////////////////////////////// make the name of a sequence make the name of a table
<p> This method returns the size of the next pre-allocated range for the specified sequence. Names are case-sensitive, as specified in CREATE SEQUENCE and CREATE TABLE statements. </p>
/////////////////////////////////////////////////////////////////////////////////  SequencePreallocator BEHAVIOR  /////////////////////////////////////////////////////////////////////////////////
Print the current memory status Implementation of run() method to check the sequence counter.
/////////////////////////////////////////////////////////////////////////////////  Cacheable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  ABSTRACT OR OVERRIDABLE BEHAVIOR TO BE IMPLEMENTED BY CHILDREN  ///////////////////////////////////////////////////////////////////////////////// <p> Initialize the sequence generator. Work is done inside a read-only subtransaction of the session's execution transaction. </p> <p> Get the SequenceUpdater used for the bulk-insert optimization in InsertResultSet. </p> Privileged lookup of a Context. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. <p> Get the next sequence number managed by this generator and advance the number. Could raise an exception if the legal range is exhausted and wrap-around is not allowed. Only one thread at a time is allowed through here. We do not want a race between the two calls to the sequence generator: getCurrentValueAndAdvance() and allocateNewRange(). </p> /////////////////////////////////////////////////////////////////////////////////  UTILITY MINIONS  ///////////////////////////////////////////////////////////////////////////////// Make a new range allocator (called when the generator is instantiated) <p> Get the current value of the sequence generator without advancing it. May return null if the generator is exhausted. </p> /////////////////////////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Reset the sequence generator to a new start value. This is used by the special bulk-insert optimization in InsertResultSet. </p>  <p> Create an exception to state that there is too much contention on the generator. For backward compatibility reasons, different messages are needed by sequences and identities. See DERBY-5426. </p> Report an unimplemented feature /////////////////////////////////////////////////////////////////////////////////  DISK WRITING MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Update the value on disk. Does its work in a subtransaction of the user's execution transaction. If that fails, raises a TOO MUCH CONTENTION exception. </p> <p> Update the sequence value on disk. This method does its work in a subtransaction of the user's execution transaction. </p>
Serializes and writes a number of Derby data sources to disk, or attempts to read information from existing files. Attempts to read information from a file assumed to contain a serialized data source. <p> All information is printed to the console. Serialize and write data sources to file.
Specify a dbPath to use in place of the default. Specify a JDBCClient to use in place of the default DERBYNETCLIENT.

Returns a reference to the service that had a change occur in its lifecycle. <p> This reference is the source of the event. Returns the type of event. The event type values are: <ul> <li>{@link #REGISTERED} <li>{@link #MODIFIED} <li>{@link #UNREGISTERING} </ul>
Creates a new service object. <p> The Framework invokes this method the first time the specified <code>bundle</code> requests a service object using the <code>BundleContext.getService(ServiceReference)</code> method. The service factory can then return a specific service object for each bundle. <p> The Framework caches the value returned (unless it is <code>null</code>), and will return the same service object on any future call to <code>BundleContext.getService</code> from the same bundle. <p> The Framework will check if the returned service object is an instance of all the classes named when the service was registered. If not, then <code>null</code> is returned to the bundle. Releases a service object. <p> The Framework invokes this method when a service has been released by a bundle. The service object may then be destroyed.
Receives notification that a service has had a lifecycle change.
Determines the equalty of two ServicePermission objects. Checks that specified object has the same class name and action as this <code>ServicePermission</code>. Returns the canonical string representation of the actions. Always returns present actions in the following order: <code>get</code>, <code>register</code>. Returns the current action mask. Used by the ServicePermissionCollection object. Parse action string into action mask. Returns the hash code value for this object. Determines if a <code>ServicePermission</code> object "implies" the specified permission. Called by constructors and when deserialized. Returns a new <code>PermissionCollection</code> object for storing <code>ServicePermission<code> objects. readObject is called to restore the state of this permission from a stream. WriteObject is called to save the state of this permission to a stream. The actions are serialized, and the superclass takes care of the name. Adds a permission to the <code>ServicePermission</code> objects using the key for the hash as the name. Returns an enumeration of all the <code>ServicePermission</code> objects in the container. Determines if a set of permissions implies the permissions expressed in <code>permission</code>.

Compares this <code>ServiceReference</code> with the specified <code>ServiceReference</code> for order. <p> If this <code>ServiceReference</code> and the specified <code>ServiceReference</code> have the same {@link Constants#SERVICE_ID service id} they are equal. This <code>ServiceReference</code> is less than the specified <code>ServiceReference</code> if it has a lower {@link Constants#SERVICE_RANKING service ranking} and greater if it has a higher service ranking. Otherwise, if this <code>ServiceReference</code> and the specified <code>ServiceReference</code> have the same {@link Constants#SERVICE_RANKING service ranking}, this <code>ServiceReference</code> is less than the specified <code>ServiceReference</code> if it has a higher {@link Constants#SERVICE_ID service id} and greater if it has a lower service id. Returns the bundle that registered the service referenced by this <code>ServiceReference</code> object. <p> This method must return <code>null</code> when the service has been unregistered. This can be used to determine if the service has been unregistered. Returns the property value to which the specified property key is mapped in the properties <code>Dictionary</code> object of the service referenced by this <code>ServiceReference</code> object. <p> Property keys are case-insensitive. <p> This method must continue to return property values after the service has been unregistered. This is so references to unregistered services (for example, <code>ServiceReference</code> objects stored in the log) can still be interrogated. Returns an array of the keys in the properties <code>Dictionary</code> object of the service referenced by this <code>ServiceReference</code> object. <p> This method will continue to return the keys after the service has been unregistered. This is so references to unregistered services (for example, <code>ServiceReference</code> objects stored in the log) can still be interrogated. <p> This method is <i>case-preserving </i>; this means that every key in the returned array must have the same case as the corresponding key in the properties <code>Dictionary</code> that was passed to the {@link BundleContext#registerService(String[],Object,java.util.Dictionary)} or {@link ServiceRegistration#setProperties} methods. Returns the bundles that are using the service referenced by this <code>ServiceReference</code> object. Specifically, this method returns the bundles whose usage count for that service is greater than zero. Tests if the bundle that registered the service referenced by this <code>ServiceReference</code> and the specified bundle use the same source for the package of the specified class name. <p> This method performs the following checks: <ol> <li>Get the package name from the specified class name.</li> <li>For the bundle that registered the service referenced by this <code>ServiceReference</code> (registrant bundle); find the source for the package. If no source is found then return <code>true</code> if the registrant bundle is equal to the specified bundle; otherwise return <code>false</code>.</li> <li>If the package source of the registrant bundle is equal to the package source of the specified bundle then return <code>true</code>; otherwise return <code>false</code>.</li> </ol>
Returns a <code>ServiceReference</code> object for a service being registered. <p> The <code>ServiceReference</code> object may be shared with other bundles. Updates the properties associated with a service. <p> The {@link Constants#OBJECTCLASS} and {@link Constants#SERVICE_ID} keys cannot be modified by this method. These values are set by the Framework when the service is registered in the OSGi environment. <p> The following steps are required to modify service properties: <ol> <li>The service's properties are replaced with the provided properties. <li>A service event of type {@link ServiceEvent#MODIFIED} is fired. </ol> Unregisters a service. Remove a <code>ServiceRegistration</code> object from the Framework service registry. All <code>ServiceReference</code> objects associated with this <code>ServiceRegistration</code> object can no longer be used to interact with the service. <p> The following steps are required to unregister a service: <ol> <li>The service is removed from the Framework service registry so that it can no longer be used. <code>ServiceReference</code> objects for the service may no longer be used to get a service object for the service. <li>A service event of type {@link ServiceEvent#UNREGISTERING} is fired so that bundles using this service can release their use of it. <li>For each bundle whose use count for this service is greater than zero: <br> The bundle's use count for this service is set to zero. <br> If the service was registered with a {@link ServiceFactory} object, the <code>ServiceFactory.ungetService</code> method is called to release the service object for the bundle. </ol>
Do whatever it is that you want the daemon to do for you. There may be multiple daemon objects on different thread calling performWork at the same time. The DaemonService will always call performWork with a context manager set up.  the DaemonService will clean up the context if an exception is thrown.  However, it is up to performWork to manage its own transaction.  If you start a transaction in performWork, you <B>must</B> commit or abort it at the end.  You may leave the transaction open so that other serviceable may use the transaction and context without starting a new one.  On the same token, there may already be an opened transaction on the context.  Serviceable performWork should always check the state of the context before use. A Serviceable object should be well behaved while it is performing the daemon work, i.e., it should not take too many resources or hog the CPU for too long or deadlock with anyone else. the request to be serviced again later. If this work should be done as soon as possible, then return true. If it doesn't make any difference if it is done sooner rather than later, then return false. The difference is whether or not the daemon service will be notified to work on this when this work is enqueued or subscribed, in case the serviceable work is put together but not sent to the daemon service directly, like in post commit processing <P>MT - MT safe If this work should be done immediately on the user thread then return true. If it doesn't make any difference if this work is done on a the user thread immediately or if it is performed by another thread asynchronously later, then return false.
Add database to session table Close session - close connection sockets and set state to closed Get connection number Get database Get requried security checkpoint. Used to verify EXCSAT/ACCSEC/SECCHK order. initialize a server trace for the DRDA protocol Get session into initial state Get whether tracing is on Check if a security codepoint is required Set Session state Set tracing off Set tracing on
This is the guts of the execution time logic for SET CONSTRAINT.
Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Checks that all required jars are found in the jar directory. Verifies that the correct number of tags were replaced in the POM. Quits the JVM if a failure is detected. Displays a prompt and obtains a yes / no answer from standard in. Checks the current Derby jars in the source tree directory, obtains the Derby version from them, and replaces the value of the placeholder version tags in the POM files. <p> After this method has been successfully run you should be ready to generate the Maven 2 artifacts for Derby. Reads the contents of a text file. Replaces the relevant version tags in the various POM files. Replaces all qualifying version tags in the specified POM. Prints a warning message and sets the internal warning flag.
end of advanceRightPastDuplicates If the result set has been opened, close the currently open source. end of close end of compare Return the left source input of this <code>SetOpResultSet</code>  end of getNextRowCore Return the set operation of this <code>SetOpResultSet</code> Return the result set number Return the right source input of this <code>SetOpResultSet</code>  Return the number of rows returned from the result set Return the number of rows seen on the left source input Return the number of rows seen on the right source input Return the total amount of time spent in this ResultSet end of getTimeSpent open the first source. end of openCore
Bind the result columns for this ResultSetNode to a base table. This is useful for INSERT and UPDATE statements, where the result columns get their types from the table being updated or inserted into. If a result column list is specified, then the verification that the result column list does not contain any duplicates will be done when binding them by name. Bind the result columns of this ResultSetNode when there is no base table to bind them to.  This is useful for SELECT statements, where the result columns get their types from the expressions that live under them. Bind the expressions in the target list.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression.  This is useful for EXISTS subqueries, where we need to validate the target list before blowing it away and replacing it with a SELECT true. Bind the result columns of a table constructor to the types in the given ResultColumnList.  Use when inserting from a table constructor, and there are nulls in the values clauses. Build the RCL for this node.  We propagate the RCL up from the left child to form this node's RCL. Ensure that the top of the RSN tree has a PredicateList. Evaluate whether or not the subquery in a FromSubquery is flattenable. Currently, a FSqry is flattenable if all of the following are true: o  Subquery is a SelectNode. (ie, not a RowResultSetNode or a UnionNode) o  It contains no top level subqueries.  (RESOLVE - we can relax this) o  It does not contain a group by or having clause o  It does not contain aggregates. Determine whether or not the specified name is an exposed name in the current query block. Retrieve the list of optimizable predicates that are targeted for the left child.  Create a new (empty) list if the list is null.  Get the parameter types from the given RowResultSetNode into the given array of types.  If an array position is already filled in, don't clobber it. Retrieve the list of optimizable predicates that are targeted for the right child.  Create a new (empty) list if the list is null. It's possible that we tried to push predicates to this node's children but failed to do so. This can happen if this node's children both satisfy the criteria for pushing a predicate (namely, they reference base tables) but the children's children do not (see modifyAccessPaths() above for an example of how that can happen).  So this method determines whether or not this particular SetOperatorNode has predicates which were *not* successfully pushed to both of its children (note: this currently only applies to UnionNodes).  Return whether or not to materialize this ResultSet tree. Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.  Push down the offset and fetch first parameters, if any, to this node.  Push the order by list down from the cursor node into its child result set so that the optimizer has all of the information that it needs to consider sort avoidance. {@inheritDoc } Set the type of each ? parameter in the given RowResultSetNode according to its ordinal position in the given array of types. Set the result column for the subquery to a boolean true, Useful for transformations such as changing: where exists (select ... from ...) to: where (select true from ...) NOTE: No transformation is performed if the ResultColumn.expression is already the correct boolean constant. This method is used during binding of EXISTS predicates to map a subquery's result column list into a single TRUE node.  For SELECT and VALUES subqueries this transformation is pretty straightforward.  But for set operators (ex. INTERSECT) we have to do some extra work.  To see why, assume we have the following query: select * from ( values 'BAD' ) as T where exists ((values 1) intersect (values 2)) If we treated the INTERSECT in this query the same way that we treat SELECT/VALUES subqueries then the above query would get transformed into: select * from ( values 'BAD' ) as T where ((values TRUE) intersect (values TRUE)) Since both children of the INTERSECT would then have the same value, the result of set operation would be a single value (TRUE), which means the WHERE clause would evaluate to TRUE and thus the query would return one row with value 'BAD'.  That would be wrong. To avoid this problem, we internally wrap this SetOperatorNode inside a "SELECT *" subquery and then we change the new SelectNode's result column list (as opposed to *this* nodes' result column list) to a singe boolean true node: select * from ( values 'BAD' ) as T where SELECT TRUE FROM ((values 1) intersect (values 2)) In this case the left and right children of the INTERSECT retain their values, which ensures that the result of the intersect operation will be correct.  Since (1 intersect 2) is an empty result set, the internally generated SELECT node will return zero rows, which in turn means the WHERE predicate will return NULL (an empty result set from a SubqueryNode is treated as NULL at execution time; see impl/sql/execute/AnyResultSet). Since NULL is not the same as TRUE the query will correctly return zero rows.  DERBY-2370. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Verify that a SELECT * is valid for this type of subquery.
Loggable methods  Return my format identifier. Read this in method to support BeforeImageLogging - This log operation is not undoable in the logical sense , but all log operations that touch a page must support physical undo during RRR transaction. restore the before image of the page DEBUG: Print self.
INTERFACE METHODS This is the guts of the Execution-time logic for SET ROLE. /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
Override: Returns the type of activation this class generates. Override: Generate code, need to push parameters Generate the code to create the ParameterValueSet, if necessary, when constructing the activation.  Also generate the code to call a method that will throw an exception if we try to execute without all the parameters being set. Override to allow committing of reading SYSROLES, cf. SetRoleConstantAction's call to userCommit to retain idle state. If atomic, that commit will fail. Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
INTERFACE METHODS This is the guts of the Execution-time logic for SET SCHEMA. /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
Returns the type of activation this class generates. Generate code, need to push parameters Generate the code to create the ParameterValueSet, if necessary, when constructing the activation.  Also generate the code to call a method that will throw an exception if we try to execute without all the parameters being set. Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
INTERFACE METHODS This is the guts of the Execution-time logic for SET TRANSACTION ISOLATION. /////////////////////////////////////////////  OBJECT SHADOWS  /////////////////////////////////////////////
generates a the code. Create the Constant information that will drive the guts of Execution. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Does this ResultSet cause a commit or rollback.

This lockable want to participate in the Virtual LockTable when we want to print LATCH information. Any lockable object which DOES NOT want to participate should override this function.

Get the short value out of this object

Test shut down master server during replication. FIXME?! Factor out this as common with ShutdownSlave!
Adds this class to the *existing server* suite. Test shut down master database during replication.
Adds this class to the *existing server* suite. Test shut down master server during replication.
Adds this class to the *existing server* suite. Test shut down master server during replication.
Adds this class to the *existing server* suite. Test shut down master server during replication.
public static Test suite() throws Exception { System.out.println("**** ShutdownSlave.suite()"); System.out.println("'ShutdownSlave' can not be run outside the 'ReplicationRun' framework."); setEnv(); BaseTestSuite suite = new BaseTestSuite("ShutdownSlave"); suite.addTest(ShutdownSlave.suite(masterServerHost, masterServerPort)); return (Test)suite; } Adds this class to the *existing server* suite. public static Test suite(String serverHost, int serverPort) throws IOException { System.out.println("*** ShutdownSlave.suite("+serverHost+","+serverPort+")"); Test t = TestConfiguration.existingServerSuite(ShutdownSlave.class,false,serverHost,serverPort); System.out.println("*** Done TestConfiguration.existingServerSuite(ShutdownSlave.class,false," +serverHost+":"+serverPort+")"); return t; } Test shut down slave server during replication. public void testShutdownSlave() throws SQLException, IOException, InterruptedException { String slaveServerURL = "jdbc:derby:" +"//"+slaveServerHost+":"+slaveServerPort+"/"; String slaveDbURL = slaveServerURL +ReplicationRun.slaveDatabasePath +"/"+ReplicationRun.slaveDbSubPath +"/"+ReplicationRun.replicatedDb; // shutdown(null, false, true); // -,-,true: Use OS kill on server! // shutdown(null, false, false); // null,-,-: use networkservercontrol! shutdown(slaveServerURL, false, false); // serverURL,false,-: shutdown server! // shutdown(slaveDbURL, true, false); // dbURL,true,-: shutdown database! } FIXME! Factor out this as common with ShutdownMaster!
Adds this class to the *existing server* suite. Test shut down slave server during replication.
Adds this class to the *existing server* suite. Test shut down slave server during replication.
Adds this class to the *existing server* suite. Test shut down slave server during replication.
Adds this class to the *existing server* suite. Test shut down slave server during replication.
<p> Prepared a routine invocation in order to check whether it matches a Java method. </p> <p> Count up the arguments to the user-coded procedures. We use reflection to look up the getFunctionColumns() method because that method does not appear in the JSR169 api for DatabaseMetaData. Update {@link #_functions}. </p> <p> Count up the arguments to the user-coded procedures in {@link #_procedures} and update that data structure accordingly </p> /////////////////////////////////////////////////////////////////////////////////  MACHINERY  ///////////////////////////////////////////////////////////////////////////////// <p> Get a connection to a database and then match the signatures of routines in that database. </p> <p> Find all of the user-declared functions. We use reflection to get our hands on getFunctions() because that method does not appear in the JSR169 api for DatabaseMetaData. Update {@link #_functions}. </p> <p> Find all of the user-declared procedures. </p> Format a localizable message. Get a function descriptor from {@link #_functions} . /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// We use reflection to get the J2SE connection so that references to DriverManager will not generate linkage errors on old J2ME platforms which may resolve references eagerly. Get the message resource. Get a procedure descriptor from {@link #_procedures}. Return true if the schema is a system schema. /////////////////////////////////////////////////////////////////////////////////  ENTRY POINT  ///////////////////////////////////////////////////////////////////////////////// <p> Make a human readable signature for a routine. This can be used in error messages. </p> <p> Match the signatures of functions in this database. </p> <p> Match the signatures of procedures in this database. </p> <p> Match the signatures of routines in the database attached to this connection. </p> Store a function descriptor. Updates {@link #_functions}. Store a procedure descriptor. Updates {@link #_procedures}.
Build a Java int from a 4-byte big endian signed binary representation. Build a Java long from an 8-byte big endian signed binary representation. Build a Java short from a 2-byte big endian signed binary representation. Build a Java int from a 4-byte signed binary representation. <p> Depending on machine type, byte orders are {@link #BIG_ENDIAN BIG_ENDIAN} for signed binary integers, and {@link #LITTLE_ENDIAN LITTLE_ENDIAN} for pc8087 signed binary integers. Build a Java long from an 8-byte signed binary representation. <p> Depending on machine type, byte orders are {@link #BIG_ENDIAN BIG_ENDIAN} for signed binary integers, and {@link #LITTLE_ENDIAN LITTLE_ENDIAN} for pc8087 signed binary integers. <p> Build a Java short from a 2-byte signed binary representation. <p> Depending on machine type, byte orders are {@link #BIG_ENDIAN BIG_ENDIAN} for signed binary integers, and {@link #LITTLE_ENDIAN LITTLE_ENDIAN} for pc8087 signed binary integers. Build a Java int from a 4-byte little endian signed binary representation. Build a Java long from an 8-byte little endian signed binary representation. Build a Java short from a 2-byte little endian signed binary representation.
<p> Starts the actual demo activities. This includes creating a database by making a connection to Derby (automatically loading the driver), creating a table in the database, and inserting, updating and retrieving some data. Some of the retrieved data is then verified (compared) against the expected results. Finally, the table is deleted and, if the embedded framework is used, the database is shut down.</p> <p> Generally, when using a client/server framework, other clients may be (or want to be) connected to the database, so you should be careful about doing shutdown unless you know that no one else needs to access the database until it is rebooted. That is why this demo will not shut down the database unless it is running Derby embedded.</p> <p> Starts the demo by creating a new instance of this class and running the <code>go()</code> method.</p> <p> When you run this application, you may give one of the following arguments: <ul> <li><code>embedded</code> - default, if none specified. Will use Derby's embedded driver. This driver is included in the derby.jar file.</li> <li><code>derbyclient</code> - will use the Derby client driver to access the Derby Network Server. This driver is included in the derbyclient.jar file.</li> </ul> <p> When you are using a client/server framework, the network server must already be running when trying to obtain client connections to Derby. This demo program will will try to connect to a network server on this host (the localhost), see the <code>protocol</code> instance variable. </p> <p> When running this demo, you must include the correct driver in the classpath of the JVM. See <a href="example.html">example.html</a> for details. </p> Parses the arguments given and sets the values of this class's instance variables accordingly - that is, which framework to use, the name of the JDBC driver class, and which connection protocol to use. The protocol should be used as part of the JDBC URL when connecting to Derby. <p> If the argument is "embedded" or invalid, this method will not change anything, meaning that the default values will be used.</p> <p> Prints details of an SQLException chain to <code>System.err</code>. Details included are SQL State, Error code, Exception message. Reports a data verification failure to System.err with the given message.

Populate the CUSTOMER table for a given district for a specific warehouse. See population requirements in section 4.3.3.1 <BR> Populate the DISTRICT table for a given warehouse. See population requirements in section 4.3.3.1 <BR> Populate the ITEM table. See population requirements in section 4.3.3.1 <BR> Populate the ORDER table See population requirements in section 4.3.3.1 Follow the initial database population requirements in Section 4.3.3 and populate all the required tables. Populate all the tables needed for a specific warehouse. for each row in warehouse table, load the stock, district table. For each row in district table, load the customer table. for each row in customer table, load the customer table. for each row in customer table, load Setup the random number generator to be used for the load. Ignore, this is a single threaded load. Set the connection up to the intended state. Intended for use by sub-classes. Perform the necessary setup before database population. Populate the STOCK table for a given warehouse. See population requirements in section 4.3.3.1 <BR> Populate the WAREHOUSE table for a given warehouse. See population requirements in section 4.3.3.1
/////////////////////////////////////////////////////////////////////////////////  (UN)REGISTRATION MINIONS  ///////////////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  SQL MINIONS  ////////////////////////////////////////////////////////////////////// Grant permissions to use the newly loaded UDT and FUNCTIONs. /////////////////////////////////////////////////////////////////////////////////  OptionalTool BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Binds a UDT to JSONArray and creates a function to turn a query result into a JSONArray. There are no parameters to this method. </p> /////////////////////////////////////////////////////////////////////////////////  SQL FUNCTIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Compile a query (with optional ? parameters) and pack the results into a JSONArray. This is the entry point which the simpleJson tool binds to the function name "toJSON". </p> <p> Removes the function and UDT created by loadTool(). </p> </ul>
SQL FUNCTION to convert a JSONArray into a CLOB. ///////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////// <p> Turns an object into something which is a legal JSON value. </p> Construct a JSONArray from a Reader. SQL FUNCTION to read a JSONArray from a File. This function is registered by the simpleJson optional tool. Read a JSONArray from an InputStream. Close the stream after reading the JSONArray. <p> SQL FUNCTION to convert a JSON document string into a JSONArray. This function is registered by the simpleJson optional tool. </p> SQL FUNCTION to read a JSONArray from an URL address. This function is registered by the simpleJson optional tool. ///////////////////////////////////////////////////////////////  CONSTANTS  /////////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////////  STATE  /////////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  /////////////////////////////////////////////////////////////// <p> Pack a ResultSet into a JSONArray. This method could be called client-side on any query result from any DBMS. Each row is converted into a JSONObject whose keys are the corresponding column names from the ResultSet. Closes the ResultSet once it has been drained. Datatypes map to JSON values as follows: </p> <ul> <li><i>NULL</i> - The JSON null literal.</li> <li><i>SMALLINT, INT, BIGINT</i> - JSON integer values.</li> <li><i>DOUBLE, FLOAT, REAL, DECIMAL, NUMERIC</i> - JSON floating point values.</li> <li><i>CHAR, VARCHAR, LONG VARCHAR, CLOB</i> - JSON string values.</li> <li><i>BLOB, VARCHAR FOR BIT DATA, LONG VARCHAR FOR BIT DATA</i> - The byte array is turned into a hex string (2 hex digits per byte) and the result is returned as a JSON string.</li> <li><i>All other types</i> - Converted to JSON string values via their toString() methods.</li> </ul>
<p> Add a warning to the connection. </p> //////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ////////////////////////////////////////////////////////////////////// <p> Get the i-th (1-based) column as an object. </p> //////////////////////////////////////////////////////////////////////  MINIONS  ////////////////////////////////////////////////////////////////////// <p> Get the i-th (1-based) column as a Number. </p> //////////////////////////////////////////////////////////////////////  TYPE-SPECIFIC ACCESSORS  ////////////////////////////////////////////////////////////////////// <p> Add a "wrong type" warning and return true if the object has the wrong type. Return true if the object is null. Otherwise, return false. </p> //////////////////////////////////////////////////////////////////////  TABLE FUNCTIONS (to be bound by CREATE FUNCTION ddl)  ////////////////////////////////////////////////////////////////////// <p> Create a SimpleJsonVTI from a JSONArray object. </p>
Creates a client data source and sets all the necessary properties in order to connect to Derby Network Server The server is assumed to be running on 1527 and on the localhost Get a database connection from DataSource Get a client connection using the DriverManager Test a connection by executing a sample query
Used to return an embedded Derby connection. The protocol used is "jdbc:derby:dbName" where dbName is the database name Returns a string with information as to how to connect to Derby Network Server Setting the derby.drda.startNetworkServer property to true, either in the System properties as we do here or in the derby.properties file, will cause the Network Server to start as soon as Derby is loaded. To load Derby, we just need to load the embedded driver with: Class.forName("org.apache.derby.jdbc.EmbeddedDriver").newInstance(); Then we will test for a while and make sure it is up, before we give up. Alternatively, the Network Server might be started from the command line or from some other program. Note: only the JVM that starts Network Server can make an embedded connection. Start Derby Network Server using the property derby.drda.startNetworkServer. This property can be set as a system property or or in the derby.properties file. Setting this property to true starts the Network Server when Derby boots up. The port at which the Derby Network Server listens can be changed by setting the derby.drda.portNumber property. By default, the server starts at port 1527. Server output goes to derby.log. Test a connection by executing a sample query This method waits until the user presses Enter to stop the server and eventually exit this program Allows clients to continue to connect using client connections from other JVMs to the Derby Network Server that was started in this program Tries to check if the Network Server is up and running by calling ping If successful, it returns; otherwise, it tries for 50 seconds before giving up and throwing an exception.
Execute customerAddressChange() with random parameters. Update a customers address with a new random value. Update of a single row through a primary key. <BR> Primary key update against the CUSTOMER table (which of course can be arbitrarily large depending on the scale of the database. The cardinality of the CUSTOMER is 30,000 rows per warehouse, for example with a 20 warehouse system this test would perform a primary key lookup against 600,000 rows. Execute customerInquiry() with random parameters. Lookup a customer's information (name, address, balance) fetching it by the identifier. <BR> Primary key lookup against the CUSTOMER table (which of course can be arbitrarily large depending on the scale of the database. The cardinality of the CUSTOMER is 30,000 rows per warehouse, for example with a 20 warehouse system this test would perform a primary key lookup against 600,000 rows. Return an SimpleNonStandardOperations implementation based upon SimpleNonStandardOperations with a single difference. In this implementation the reset() executed after each PreparedStatement execute does nothing. Sees if there is any performance impact of explicitly closing each ResultSet and clearing the parameters. <P> Each ResultSet will be closed implicitly either at commit time or at the next execution of the same PreparedStatement object.
Bind this operator Bind a ? parameter operand of the upper/lower function. This is a length operator node.  Overrides this method in UnaryOperatorNode for code generation purposes.
Accept the visitor for all visitable children of this node. Add a new predicate to the list.  This is useful when doing subquery transformations, when we build a new predicate with the left side of the subquery operator and the subquery's result column.    Decrement (query block) level (0-based) for this FromTable. This is useful when flattening a subquery. Ensure that the top of the RSN tree has a PredicateList. Evaluate whether or not the subquery in a FromSubquery is flattenable. Currently, a FSqry is flattenable if all of the following are true: o  Subquery is a SelectNode. o  It contains no top level subqueries.  (RESOLVE - we can relax this) o  It does not contain a group by or having clause o  It does not contain aggregates.  Return the childResult from this node. Get the final CostEstimate for this node. Determine whether or not the specified name is an exposed name in the current query block.   Return whether or not the underlying ResultSet tree is for a NOT EXISTS join. Return whether or not the underlying ResultSet tree will return a single row, at most. This is important for join nodes where we can save the extra next on the right side if we know that it will return at most 1 row. Return whether or not the underlying ResultSet tree is ordered on the specified columns. RESOLVE - This method currently only considers the outermost table of the query block.  Optimize this SingleChildResultSetNode. Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work.  Push expressions down to the first ResultSetNode which can do expression evaluation and has the same referenced table map. RESOLVE - This means only pushing down single table expressions to DistinctNodes today.  Once we have a better understanding of how the optimizer will work, we can push down join clauses. Return true if the node references SESSION schema tables (temporary or permanent) Search to see if a query references the specifed table name. Determine whether we need to do reflection in order to do the projection. Reflection is only needed if there is at least 1 column which is not simply selecting the source column. Set the childResult for this node. Set the (query block) level (0-based) for this FromTable. Return whether or not this ResultSetNode contains a subquery with a reference to the specified target.  Get the lock mode for the target of an update statement (a delete or update).  The update mode will always be row for CurrentOfNodes.  It will be table if there is no where clause.
Get the name of a table generated by this class. Pick a random byte string. Pick a random string.
Make sure the text column is retrieved and read. Different methods are used for the retrieval based on whether the column is a VARCHAR, a BLOB or a CLOB.

Make a header print writer out of a file name. If it is a relative path name then it is taken as relative to derby.system.home if that is set, otherwise relative to the current directory. If the path name is absolute then it is taken as absolute.  Used when no configuration information exists for a stream. Privileged Monitor lookup. Must be private so that user code can't call this entry point. create a HeaderPrintWriter based on the header. Will still need to determine the target type. Return a new header object.  class interface  Make the stream; note that service properties override application and system properties.  InfoStreams interface  Used when creating a stream creates an error.
Daemon factory method make a daemon service with the default timer Privileged lookup of the ContextService. Must be private so that user code can't call this entry point.
ModuleControl interface methods Currently does nothing, singleton Timer instance is initialized in the constructor. Implements the ModuleControl interface. Helper methods Check if the current context class loader could cause a memory leak (DERBY-3745) if it is inherited by the timer thread, and return it if that is the case. Return any warnings generated during the initialization of this class, or null if none TimerFactory interface methods Cancels the singleton Timer instance. Implements the ModuleControl interface.
Get the estimated size of the cacheable object.
Used to get the IP Address corresponding to the host name of the slave. Used to get the port number of the slave.
////////////////////////////////////////////////////////// Implementation of methods from interface ModuleControl // ////////////////////////////////////////////////////////// Used by Monitor.bootServiceModule to start the service. It will set up basic variables ////////////////////////////////////////////////////////////// Implementation of methods from interface ModuleSupportable // ////////////////////////////////////////////////////////////// Used by Monitor.bootServiceModule to check if this class is usable for replication. To be usable, we require that slave replication mode is specified in startParams by checking that a property with key SlaveFactory.REPLICATION_MODE has the value SlaveFactory.SLAVE_MODE. Performs failover on this database. May be called because a failover command has been received from the master, or because a client has requested a failover after the network connection with the master has been lost. Used to return the host name of the slave. Used to return the port number of the slave. Write the reason for the lost connection to the log (derby.log) and reconnect with the master. Once the network is up and running, a new LogReceiverThread is started. The method returns without doing anything if inReplicationSlaveMode=false, which means that stopSlave() has been called by another thread. Handles fatal errors for slave replication functionality. These are errors that requires us to stop replication. Calling this method has the following effects: 1) Debug messages are written to the log file (usually derby.log) if ReplicationLogger#LOG_REPLICATION_MESSAGES is true. 2) If the network connection is up, the master is notified of the problem. 3) All slave replication functionality is stopped, and the database is then shut down without being booted. The method will return without doing anything if inReplicationSlaveMode=false, meaning that stopSlave has been called. Check if the repliation network connection to the master is working  ////////////////////////////////////////////////////////// Private Methods                                        // ////////////////////////////////////////////////////////// Establish a connection with the replication master. Listens for a connection on the slavehost/port for DEFAULT_SOCKET_TIMEOUT milliseconds. Starts the LogReceiverThread that will listen for chunks of log records from the master and apply the log records to the local log file. ///////////////////////////////////////////////////////// Implementation of methods from interface SlaveFactory // ///////////////////////////////////////////////////////// Start slave replication. This method establishes a network connection with the associated replication master and starts a thread that applies operations received from the master (in the form of log records) to the local slave database. Will tear down the replication slave service. Will perform all work that is needed to stop replication
/////////////////////////// ModuleControl interface // /////////////////////////// Determines whether this Database implementation should be used to boot the database. Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Used to shutdown this database. If an error occurs as part of the database boot process, we hand the exception that caused boot to fail to the client thread. The client thread will in turn shut down this database. If an error occurs at a later stage than during boot, we shut down the database by setting up a connection with the shutdown attribute. The internal connection is required because database shutdown requires EmbedConnection to do cleanup. ////////////////////// Database interface // ////////////////////// If slaveFac (the reference to the SlaveFactory) has not already been set, this method will try to set it by calling Monitor.findServiceModule. If slavFac was already set, the method does not do anything. Called by Monitor when this module is stopped, i.e. when the database is shut down. When the database is shut down using the stopSlave command, the stopReplicationSlave method has already been called when this method is called. In this case, the replication functionality has already been stopped. If the database is shutdown as part of a system shutdown, however, we need to cleanup slave replication as part of database shutdown. Stop replication slave mode if replication slave mode is active and the network connection with the master is down Verify that a connection to stop the slave has been made from here. If verified, the database context is given to the method caller. This will ensure this database is shutdown when an exception with database severity is thrown. If not verified, an exception is thrown. ////////////////// Private Methods// ////////////////// Verify that the slave functionality has been properly started. This method will block until a successful slave startup has been confirmed, or it will throw the exception that caused it to fail.
<p> Used to turn this slave instance of the database into a normal instance that clients can connect to, assuming that the connection with the master is down. This is typically done in cases where a fatal error has happened on the master instance of the database, or when the master database is unreachable due to network problems. </p> <p> By calling failover, this slave instance of the database will be recovered so that all committed operations that have been received from the master are reflected here. On the other hand, operations from transactions where the commit log record has not been received from the master will not be reflected. </p> <p> Note that even though an operation has been executed (and even committed) on the master, it is not neccessarily reflected in the slave instance of the database. This depends on the replication strategy used by the MasterFactory. </p> Check whether or not slave replication mode has been successfully started. Required methods Start slave replication. This method establishes a network connection with the associated replication master and starts a daemon that applies operations received from the master (in the form of log records) to the local slave database. Stop replication slave mode. Causes the database to abort the boot process, and should only be used when shutting down this database. If forcedStop is false, the method will fail with an exception if connected with the master. If forcedStop is true, the slave will be shut down even if connected to the master. A forcedStop value of true should only be used by system shutdown.
Used to read the object messages that are sent. waits on the input stream until a data is present that can be read and returns this data. Closes the <code>Socket</code> and the object streams obtained from it. Used to send the object messages across the socket conection.
Drop the sort - this means release all its resources. <p> Note: drop is like close, it has to be tolerant of being called more than once, it must succeed or at least not throw any exceptions. Open a sort controller. <p> The sort may have been dropped already, in which case this method should thrown an exception. Open a row Source to get rows out of the sorter. <p> The sort may have been dropped already, in which case this method should thrown an exception. Open a scan controller. <p> The sort may have been dropped already, in which case this method should thrown an exception.
Return the number of elements this sorter can sort. It's the capacity of the node allocator minus one because the sorter uses one node for the head. Delete the node with the lowest key from the subtree defined by 'thisNode', maintaining balance in the subtree.  Returns the node that should be the new root of the subtree.  This method sets this.subtreeShrunk if the subtree of thisNode decreased in height. Saves the key that was in the deleted node in 'deletedKey'. Retrieve the aux value from the last node deallocated from the tree. Grow by a certain percent if we can Initialize.  Returns false if the allocator couldn't be initialized. Insert a key k into the tree. Returns true if the key was inserted, false if the tree is full.  Silently ignores duplicate keys. <P> See Knuth Vol. 3, Sec. 6.2.3, pp. 455-457 for the algorithm. Return the lowest key and delete it from the tree, preserving the balance of the tree. Perform either a single or double rotation, as appropriate, to get the subtree 'thisNode' back in balance, assuming that the right subtree of 'thisNode' is higher than the left subtree.  Returns the node that should be the new root of the subtree. <P> These are the cases depicted in diagrams (1) and (2) of Knuth (p. 454), and the node names reflect these diagrams. However, in deletion, the single rotation may encounter a case where the "beta" and "gamma" subtrees are the same height (b.balance == 0), so the resulting does not always shrink. <P> Note: This code will not do the "mirror image" cases. It only works when the right subtree is the higher one (this is the only case encountered when deleting leftmost nodes from the tree). Arrange that the next node allocated in the tree have it's aux field set to the argument.
Close the scan Close the scan Close the rowSource Fetch the row at the current position of the Scan. Fetch the row at the current position of the Scan and does not apply the qualifiers. This method will always throw an exception. (SQLState.SORT_IMPROPER_SCAN_METHOD) Private/Protected methods of This class: Public Methods of This class: Public Methods of RowSource class: All columns are always set from a sorter  Disable illegal and dangerous scan controller interface call
Close the scan. Close the scan. Methods of MergeSortScan Move to the next position in the scan.
Inform SortController that all the rows have been inserted into it. Return SortInfo object which contains information about the current state of the sort. <p> Insert a row into the sort.
Close the controller. <p> Close the open controller.  This method always succeeds, and never throws any exceptions. Callers must not use the StoreCostController after closing it; they are strongly advised to clear out the StoreCostController reference after closing. <p> Calculate the cost of a sort. <p> The cost of a sort includes the time spent in the sorter inserting the rows into the sort, and the time spent in the sorter returning the rows.  Note that it does not include the cost of scanning the rows from the source table, for insert into the sort. <p> Arguments to getSortCost(), should be the same as those to be passed to TransactionController.createSort().
Create the sort and return a sort object for it. Return an open SortCostController. <p> Return an open SortCostController which can be used to ask about the estimated costs of SortController() operations. <p>
Return all information gathered about the sort. <p> This routine returns a list of properties which contains all information gathered about the sort.  If a Property is passed in, then that property list is appended to, otherwise a new property object is created and returned. <p> Not all sorts may support all properties, if the property is not supported then it will not be returned.  The following is a list of properties that may be returned: sortType - type of the sort being performed: internal external numRowsInput - the number of rows input to the sort.  This number includes duplicates. numRowsOutput - the number of rows to be output by the sort.  This number may be different from numRowsInput since duplicates may not be output. numMergeRuns - the number of merge runs for the sort. Applicable to external sorts only. Note: when a SortController is closed, numMergeRuns may increase by 1, to reflect the additional merge run that may be created for any data still in the sort buffer. mergeRunsSize - the size (number of rows) of each merge run for the sort. Applicable to external sorts only. e.g. [3,3,2] indicates 3 merge runs, where the first two runs have 3 rows each, and the last run has 2 rows. Note: when a SortController is closed, this vector may get an additional element, to reflect the additional merge run that may be created for any data still in the sort buffer. NOTE - this list will be expanded as more information about the sort is gathered and returned.
Overridden by subclasses that observe sorters with uniqueness checking. Overridden by subclasses that observe sorters with uniqueness checking. Called prior to inserting a duplicate sort key.   This method will typically be used to perform some aggregation on a row that is going to be discarded by the sorter. Called prior to inserting a distinct sort key; in other words, the first time that a key is inserted into the sorter, this method is called.  Subsequent inserts with the same key generate a call to insertDuplicateKey() instead. <p> This method will most commonly be used to clone the row that is retained by the sorter, or possibly to do some initialization of that row. Overridden by subclasses that observe sorters with uniqueness checking. Will be called by sorters iff deferrable() and deferred() and uniqueness violation, so implementations that sometimes return true to these must implement this method to save duplicate information till commit time.
If the result set has been opened, close the open scan. Close the source of whatever we have been scanning. Filter out the new row if it has the same contents as the current row.  (This allows us to process in-order distincts without a sorter.) This result set has its row from the last fetch done. If the cursor is closed, a null is returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the next row. /////////////////////////////////////////////////////////////////////////////  SCAN ABSTRACTION UTILITIES  ///////////////////////////////////////////////////////////////////////////// Get the next output row for processing Get a row from the input result set. Get a row from the sorter.  Side effects: sets currentRow. /////////////////////////////////////////////////////////////////////////////  CursorResultSet interface  ///////////////////////////////////////////////////////////////////////////// This result set has its row location from the last fetch done. If the cursor is closed, a null is returned. Return the total amount of time spent in this ResultSet Load up the sorter.  Feed it every row from the source scan.  When done, close the source scan and open the sort.  Return the sort scan controller. /////////////////////////////////////////////////////////////////////////////  ResultSet interface (leftover from NoPutResultSet)  ///////////////////////////////////////////////////////////////////////////// Open the scan.  Load the sorter and prepare to get rows from it.
Close the scan.	@see ScanController#close Fetch the row at the current position of the Scan. Abstract methods of Scan Fetch the row at the next position of the Scan. If there is a valid next position in the scan then the value in the template storable row is replaced with the value of the row at the current scan position.  The columns of the template row must be of the same type as the actual columns in the underlying conglomerate. The resulting contents of templateRow after a fetchNext() which returns false is undefined. The result of calling fetchNext(row) is exactly logically equivalent to making a next() call followed by a fetch(row) call.  This interface allows implementations to optimize the 2 calls if possible. RESOLVE (mikem - 2/24/98) - come back to this and see if coding this differently saves in sort scans, as did the heap recoding. Fetch the row at the current position of the Scan and does not apply the qualifiers. This method will always throw an exception. (SQLState.SORT_IMPROPER_SCAN_METHOD)
Get the estimated number of allocated pages Get the estimated number of free pages Get the estimated number of unfilled pages Get the page size
Get the estimated number of allocated pages Get the estimated number of free pages Get the estimated number of unfilled pages Get the page size for the conglomerate. record the page size for the conglomerate.
VTI costing interface
Cleans up the process, explicitly closing the streams associated with it. Closes the specified stream, ignoring any exceptions. Waits for the process to terminate. <p> This call will block until one of the following conditions are met: <ul> <li>the process terminates on its own</li> <li>the hung-process watchdog mechanism forcibly terminates the process (see {@linkplain #scheduleKill})</li> Waits for the process to terminate, forcibly terminating it if it takes longer than the specified timeout. <p> This call will block until one of the following conditions are met: <ul> <li>the process terminates on its own</li> <li>the timeout is exceeded, at which point the process is forcibly destroyed</li> <li>the hung-process watchdog mechanism forcibly terminates the process (see {@linkplain #scheduleKill})</li> Get a fail message that is the passed in reason plus the stderr and stdout for any output written. Allows easier debugging if the reason the process failed is there! Get the full server error output (stderr) as a string using the default encoding which is assumed is how it was originally written. <p> This method should only be called after the process has completed. That is, {@link #complete()} or {@link #complete(long)} should be called first. <p> Get the full server output (stdout) as a string using the default encoding which is assumed is how it was originally written. </p> <p> This method should only be called after the process has completed. That is, {@link #complete()} or {@link #complete(long)} should be called first. </p> Get the next set of server output (stdout) as a string using the default encoding which is assumed is how it was originally written. Assumes a single caller is executing the calls to this method. Return the pid if on Unixen, or -1 on Windows (can't be obtained). Get the Java Process object Joins up with the specified thread. Return the jstack(1) dump of the process if possible. It will only work if we are running with a full JDK, not a simple JRE. It will not work on Windows, and just return an empty string. Prints diagnostics to stdout/stderr if the process failed. Schedules a task to kill/terminate the task after a predefined timeout. Creates and starts a stream saver that reads the specified input stream in a separate stream. Causes output obtained from the process to be suppressed when executing the {@code complete}-methods. Return {@code true} if the subprocess {@code p} has exited within {@code patience} milliseconds. Sleep {@code sleepInterval} between each check}. Note: you still need to call one of the {@link #complete} overloads even if using this method (which is optional). It can be used before trying a {@link #jstack} call.

Binding this special function means setting the result DataTypeServices. In this case, the result type is based on the operation requested. Generate an expression that returns a DataValueDescriptor and calls a method off the language connection or the activation. Return the variant type for the underlying expression. All supported special functions are QUERY_INVARIANT print the non-node subfields
ModuleControl implementation (overriden)  Check if we should activate this authentication service.
Return the SQL code represented by this instance.
Return a single SQLException without the "next" pointing to another SQLException. Because the "next" is a private field in java.sql.SQLException, we have to create a new SqlException in order to break the chain with "next" as null. This routine provides singleton access to an instance of MessageUtil that is constructed for client messages.  It is recommended to use this singleton rather than create your own instance. The only time you need this instance is if you need to directly format an internationalized message string.  In most instances this is done for you when you invoke a SqlException constructor Convert this SqlException into a java.sql.SQLException Helper method to construct an exception which basically says that we encountered an underlying Java exception Label an exception element in a batched update exception chain. This text will be prepended onto the exceptions message text dynamically when getMessage() is called. Called by the Agent. Set the cause of this exception based on its type. {@code SQLException}s and {@code SqlException}s are linked with {@code setNextException()} and {@code initCause()}. All other exception types are linked with {@code initCause()}.
Get the java.sql.SQLWarning for this SqlWarning
Take the received string, which is an XML query expression, compile it, and store the compiled query locally.  Note that for now, we only support XPath because that's what Xalan supports. Evaluate this object's compiled XML query expression against the received xmlContext.  Then if returnResults is false, return an empty sequence (ArrayList) if evaluation yields at least one item and return null if evaluation yields zero items (the caller can then just check for null to see if the query returned any items).  If returnResults is true, then return return a sequence (ArrayList) containing all items returned from evaluation of the expression.  This array list can contain any combination of atomic values and XML nodes; it may also be empty. Assumption here is that the query expression has already been compiled and is stored in this.query. **** Helper classes and methods. Evaluate the XPath query on the specified document. Create an instance of Xalan serializer for the sake of serializing an XML value according the SQL/XML specification for serialization. Take a string representing an XML value and serialize it according SQL/XML serialization rules.  Right now, we perform this serialization by first parsing the string into a JAXP Document object, and then applying the serialization semantics to that Document.  That seems a bit inefficient, but neither Xalan nor JAXP provides a more direct way to do this. Take an array list (sequence) of XML nodes and/or string values and serialize that entire list according to SQL/XML serialization rules, which ultimately point to XML serialization rules as defined by w3c.  As part of that serialization process we have to first "normalize" the sequence.  We do that by iterating through the list and performing the steps for "sequence normalization" as defined here: http://www.w3.org/TR/xslt-xquery-serialization/#serdm This method primarily focuses on taking the steps for normalization; for the rest of the serialization work, we just make calls on the DOMSerializer class provided by Xalan.
Get a {@code java.sql.DataTruncation} warning based on the information in this SQLCA. <p> Get the error code based on the SQL code received from the server. </p> <p> The conversion from SQL code to error code happens like this: </p> <ul> <li>If the SQL code is 0, there is no error code because the Sqlca doesn't represent an error. Return 0.</li> <li>If the SQL code is positive, the Sqlca represents a warning, and the SQL code represents the actual error code. Return the SQL code.</li> <li>If the SQL code is negative, the Sqlca represents an error, and the error code is {@code -(sqlCode+1)}.</li> </ul> May or may not get the formatted message depending upon datasource directives.  cannot throw exeption. Gets the formatted message, can throw an exception. Get the SQL state for a given error. Get the unformatted message text (in case we cannot ask the server). Initialize and build the arrays <code>sqlErrmcMessages_</code> and <code>sqlStates_</code>. Returns the number of messages this SQLCA contains. ------------------- helper methods ----------------------------------------

Fetch the order details having obtained the customer information and display it. Return an Operations implementation based upon Standard with a single difference. In this implementation the reset() executed after each PreparedStatement execute does nothing. Sees if there is any performance impact of explicitly closing each ResultSet and clearing the parameters. <P> Each ResultSet will be closed implicitly either at commit time or at the next execution of the same PreparedStatement object. Order status by customer identifier. Based up the example SQL queries in appendix A.3 Order status by customer last name. Based up the example SQL queries in appendix A.3 Payment by customer identifier. Section 2.5.2. The CUSTOMER row is update and then fetched. Payment by customer last name. Section 2.5.2 The CUSTOMER row will be fetched and then updated. This is due to the need to select the specific customer first based upon last name (which will actually fetch and hence lock a number of customers). Schedule a delivery using the database as the queuing mechanism and the results file. See delivery.sql. Stock Level transaction. Described in section 2.8.2. SQL based upon sample prgram in appendix A.5.
Returns the number of bytes that can be read (or skipped over) from this input stream without blocking by the next caller of a method for this input stream. <p> This subclass implements this method by calling available on the current buffer, which is a ByteInputStreamReader. Returns the number of bytes returned by this stream. <p> The length includes data which has been already read at the invocation time, but doesn't include any meta data (like the Derby-specific EXTDTA status byte). Fetches the next buffer. Cleans up and closes the stream. Reads the next byte of data from the input stream. <p> This subclass of InputStream implements this method by reading the next byte from the current buffer. If there is more data, it will be requested a new buffer from the DDMReader. Reads up to <code>len</code> bytes of data from the input stream into an array of bytes.  An attempt is made to read as many as <code>len</code> bytes, but a smaller number may be read, possibly zero. The number of bytes actually read is returned as an integer. This subclass implements this method by calling this method on the current buffer, which is an instance of ByteArrayInputStream. If the current buffer does not have any data, it will be requested a new buffer from the DDMReader.
* A special exception to close a session. Unpack the exception, looking for a StandardException, which carries the Derby messageID and arguments. * End of constructors /** Returns the arguments for this exception, if there are any. Get the error code for an error given a type. The value of the property messageId.type will be returned, e.g. deadlock.sqlstate. * Static methods * Message handling The message stored in the super class Throwable must be set up object creation. At this time we cannot get any information about the object itself (ie. this) in order to determine the natural language message. Ie. we need to class of the objec in order to look up its message, but we can't get the class of the exception before calling the super class message. <P> Thus the message stored by Throwable and obtained by the getMessage() of Throwable (ie. super.getMessage() in this class) is the message identifier. The actual text message is stored in this class at the first request. Return the message identifier that is used to look up the error message text in the messages.properties file. Get the next {@code SQLException} that should be put into the parent exception when this instance is converted to an {@code SQLException}. Return the 5 character SQL State. If you need teh identifier that was used to create the message, then use getMessageId(). getMessageId() will return the string that corresponds to the field in org.apache.derby.iapi.reference.SQLState. Convert a message identifer from org.apache.derby.iapi.reference.SQLState to a SQLState five character string. Get the severity given a message identifier from org.apache.derby.iapi.reference.SQLState. Is this a lock timeout exception. <p> Is this a lock timeout or lock deadlock exception. <p> Is this a self-deadlock exception caused by a nested transaction being blocked by its parent's locks. <p> Check if the top-level throwable is just a vacuous wrapper that does not carry any useful information except what's returned by the {@link Throwable#getCause()} method. Mark this exception as one that is thrown by a public API method. The purpose is to signal that this should be a top-level exception, so that it doesn't get wrapped inside multiple layers of other SQLExceptions or StandardExceptions as it travels up through the code layers. 3 arguments Dummy overload which should never be called. Only used to detect incorrect usage, at compile time. Dummy overload which should never be called. Only used to detect incorrect usage, at compile time. Creates a new StandardException using message text that has already been localized. * SQL warnings * Set of static methods to obtain exceptions. * * Possible parameters: * String sqlState - SQL State * int severity - Severity of message * Throwable t - exception to wrap * Object aN - argument to error message * * Calls that can be made after the exception has been created. * * setExceptionCategory() * setReport() specific exceptions Similar to unexpectedUserException but makes no assumtion about when the execption is being called. The error is wrapped as simply as possible. Yes, report me. Errors that need this method to return false are in the minority. Set my report type. Don't print the class name in the toString() method.



Return the SQL string that this statement is for. Generates an execution plan without executing it. Generates an execution plan without executing it. Generates an execution plan given a set of named parameters. For generating a storable prepared statement (which has some extensions over a standard prepared statement).
Privileged lookup of a Context. Must be private so that user code can't call this entry point.
Closes all open logical statements created by this cache interactor. <p> A cache interactor is bound to a single (caching) logical connection. Creates a logical callable statement. Creates a logical prepared statement. Returns the associated statement cache. Designates the specified logical statement as closed.
Chains a warning onto the statement. A query has been opened on the server.
Add one user's set of permitted columns to a list of permitted columns. end of addPermittedColumns Returns false if the current role is necessary to cover the necessary permission(s).  end of check Method to check if another instance of column access descriptor matches this. Used to ensure only one access descriptor for a table/columns of given privilege is created. Return list of columns that need access This method gets called in execution phase after it is established that all the required privileges exist for the given sql. This method gets called by create view/trigger/constraint to record their dependency on various privileges. Special code is required to track column level privileges. It is possible that some column level privileges are available to the passed authorizer id but the rest required column level privileges are available at PUBLIC level. In this method, we check if all the required column level privileges are found for the passed authorizer. If yes, then simply return null, indicating that no dependency is required at PUBLIC level, because all the required privileges were found at the user level. But if some column level privileges are not available at user level, then they have to exist at the PUBLIC level when this method gets called.  Try to use the supplied role r to see what column privileges are we entitled to.
Track a Dependency within this StatementContext. (We need to clear any dependencies added within this context on an error. Indicate that the statement which has allocated this statement context should cancel its execution. Usually called as a consequence of Statement.cancel() or a query timeout set with Statement.setQueryTimeout(). Mark this context as not in use.  This is important because we always leave the top statement context on the stack, and we don't want to clean it up if a statement level exception happens while the context is not in use. Clear the save point for the current statement. Get activation associated with this statement context, if any. Used to link up stack of activations of calls in nested connections, see GenericPreparedStatement#getActivation. Get the setting of the SQL allowed state. Get the current SQL session context. Return the text of the current statement. Note that this may be null.  It is currently not set up correctly for ResultSets that aren't single row result sets (e.g SELECT) and setXXXX/getXXXX jdbc methods. Tells if this statement has been invalidated. Get the subquery tracking array for this query. (Useful for runtime statistics.) Return true if this statement is system code. Returns whether we started from within the context of a trigger or not. Is this statement context in use or not. Indicates whether the statement needs to be executed atomically or not, i.e., whether a commit/rollback is permitted by a connection nested in this statement. Checks if the statement which has allocated this statement context should cancel its execution. Is this statement for a read only, non-updatable ResultSet Reports whether this StatementContext is on the context stack. If this statement context has a savepoint, then it is reset to the current point.  Otherwise, it is a noop. Mark this statement context as associated with this activation. Mark this context as being in use. Indicate that, in the event of a statement-level exception, this context is NOT the last one that needs to be rolled back--rather, it is nested within some other statement context, and that other context needs to be rolled back, too. Set the level of SQL allowed in this and subsequent nested statements due to a routine call. Value must be one of RoutineAliasInfo.{MODIFIES_SQL_DATA, READS_SQL_DATA, CONTAINS_SQL, NO_SQL} Set the current SQL session context Set a save point for the current statement. NOTE: This needs to be off of the StatementContext so that it gets cleared on a statement error. Set the appropriate entry in the subquery tracking array for the specified subquery. Useful for closing down open subqueries on an exception. Set to indicate statement is system code. For example a system procedure, view, function etc. Set the top ResultSet in the ResultSet tree for close down on an error.
All columns in StatementDuration VTI have String data types.  Turn a string into a Timestamp
Reinit is used to redirect the finder to another stream. The previous stream should not have been in a PEEK state. If an output stream was given when constructing this StatementFinder and the input is standard input, continuation prompting will be enabled. get the next statement in the input stream. Returns it, dropping its closing semicolon if it has one. If there is no next statement, return a null. return the next character in the source stream, without advancing. Advance the source stream to the end of a comment if it is on one, assuming the first character of a potential bracketed comment has been found. If it is not a comment, do not advance the stream. return the next character in the source stream and append it to the statement buffer. Advance the source stream to the end of a comment if it is on one, assuming the first character of a potential single line comment has been found. If it is not a comment, do not advance the stream. <p> The form of a single line comment is, in regexp, XX.*$, where XX is two instances of commentChar. Advance the stream to the end of the string. Assumes the opening delimiter of the string has been read. This handles the SQL ability to put the delimiter within the string by doubling it, by reading those as two strings sitting next to one another.  I.e, 'Mary''s lamb' is read by this class as two strings, 'Mary' and 's lamb'. <p> The delimiter of the string is expected to be repeated at its other end. If the other flavor of delimiter occurs within the string, it is just a normal character within it. <p> All characters except the delimiter are permitted within the string. If EOF is hit before the closing delimiter is found, the end of the string is assumed. Parsers using this parser will detect the error in that case and return appropriate messages. Determine if the given character is considered whitespace
accessors
Get the address from a query against an order entry WAREHOUSE, DISTRICT or CUSTOMER table. Prepare a statement, looking in the map first. If the statement does not exist in the map then it is prepared and put into the map for future use. Reset a PreparedStatement. Closes its open ResultSet and clears the parameters. While clearing the parameters is not required since any future execution will override them, it is done here to reduce the chance of errors. E.g. using the wrong prepared statement for a operation or not setting all the parameters. It is assumed the prepared statement was just executed.

Creates a key for a callable statement. <p> Unspecified settings will be according to the JDBC standard; result set type will be <code>ResultSet.TYPE_FORWARD_ONLY</code>, concurrency will be <code>ResultSet.CONCUR_READ_ONLY</code>. Creates a key for a callable statement specifying result set type and concurrency. <p> The returned key is for a statement not returning auto-generated keys. Creates a key for a query with default settings. <p> Defaults are according to the JDBC standard; result set type will be <code>ResultSet.TYPE_FORWARD_ONLY</code>, concurrency will be <code>ResultSet.CONCUR_READ_ONLY</code> and the statement will not return auto-generated keys. Creates a key for a query specifying whether auto-generated keys shall be returned. <p> Unspecified settings will be according to the JDBC standard; result set type will be <code>ResultSet.TYPE_FORWARD_ONLY</code>, concurrency will be <code>ResultSet.CONCUR_READ_ONLY</code>. Creates a key for a query specifying result set type and concurrency. <p> The returned key is for a statement not returning auto-generated keys.
Perform the binding operation statement.  Binding consists of permissions checking, view resolution, datatype resolution, and creation of a dependency list (for determining whether a tree or plan is still up to date). This bindStatement() method does nothing. Each StatementNode type that can appear at the top of a tree can override this method with its own bindStatement() method that does "something". Returns name of schema in EXECUTE STATEMENT command. Returns null for all other commands. Returns the name of statement in EXECUTE STATEMENT command. Returns null for all other commands. Do code generation for this statement. Get an object with information about the cursor if there is one. Get the name of the SPS that is used to execute this statement. Only relevant for an ExecSPSNode -- otherwise, returns null. By default, assume StatementNodes are atomic. The rare statements that aren't atomic (e.g. CALL method()) override this. We need to get some kind of table lock (IX here) at the beginning of compilation of DMLModStatementNode and DDLStatementNode, to prevent the interference of insert/update/delete/DDL compilation and DDL execution, see beetle 3976, 4343, and $WS/language/SolutionsToConcurrencyIssues.txt Only DML statements have result descriptions - for all others return null. This method is overridden in DMLStatementNode. Returns whether or not this Statement requires a set/clear savepoint around its execution.  The following statement "types" do not require them: Cursor	- unnecessary and won't work in a read only environment Xact	- savepoint will get blown away underneath us during commit/rollback <p> ONLY CALLABLE AFTER GENERATION <P> This implementation returns true, sub-classes can override the method to not require a savepoint. Generates an optimized statement from a bound StatementNode.  Actually, it annotates the tree in place rather than generating a new tree. For non-optimizable statements (for example, CREATE TABLE), return the bound tree without doing anything.  For optimizable statements, this method will be over-ridden in the statement's root node (DMLStatementNode in all cases we know about so far). Throws an exception if the tree is not bound, or if the binding is out of date. Convert this object to a String. See comments in QueryTreeNode.java for how this should be done for tree printing. Returns a list of base tables for which the index statistics of the associated indexes should be updated. <p> This default implementation always returns an empty list.
Generic logic called by check() for USAGE and EXECUTE privileges. Throws an exception if the correct permission cannot be found. end of genericCheck Get the type of the privileged object. Get the PermissionsDescriptor for the passed authorization id for this object. This method gets called during the execution phase of create view/constraint/trigger. The return value of this method is saved in dependency system to keep track of views/constraints/triggers dependencies on required permissions. This happens in execution phase after it has been established that passed authorization id has all the permissions it needs to create that view/constraint/trigger. Which means that we can only get to writing into dependency system once all the required privileges are confirmed. Get the privileged object associated with this permission. Return true if the passed in permission matches the one required by this StatementPermission.


In addition, Material Statement objects are passed for convenient access to any material statement caches. Implementations of this interface should not dereference common layer Statement state, as it is passed in, but may dereference material layer Statement state if necessary for performance.
Role level permission is never required as list of privileges required for triggers/constraints/views and hence we don't do any work here, but simply return null
Return routine UUID for this access descriptor
Schema level permission is never required as list of privileges required for triggers/constraints/views and hence we don't do any work here, but simply return null
end of check Routine to check if another instance of access descriptor matches this. Used to ensure only one access descriptor for a table of given privilege is created. Otherwise, every column reference from a table may create a descriptor for that table. end of equals  Return privilege needed for this access as string end of getPrivName Return privilege access requested for this access descriptor end of getTableDescriptor Return table UUID for this access descriptor Check if current session has permission on the table (current user, PUBLIC or role) and, if applicable, register a dependency of ps on the current role. Return hash code for this instance end of hasPermissionOnTable
Fire the trigger based on the event.

Get the descriptor for the named schema. If the schemaName parameter is NULL, it gets the descriptor for the current compilation schema.

Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views.  Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query CONSTANT			- constant Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Remap all ColumnReferences in this tree to be clones of the underlying expression.
routine for internal use of store only.
Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. <p> Coerce an actual method parameter to the declared type of the corresponding routine argument. </p> index of actual method parameter in array of parameters Do code generation for this method call Push extra code to generate the casts within the arrays for the parameters passed as arrays. Add code to set up the SQL session context for a stored procedure or function which needs a nested SQL session context (only needed for those which can contain SQL). The generated code calls pushNestedSessionContext. Set default privilege of EXECUTE for this node. Get the aggregate, if any, which this method call resolves to. Wrap a parameter in a CAST node. If this SQL function has parameters which are SQLToJavaValueNode over JavaToSQLValueNode and the java value node underneath is a SQL function defined with CALLED ON NULL INPUT, then we can get rid of the wrapper nodes over the java value node for such parameters. This is because SQL functions defined with CALLED ON NULL INPUT need access to only java domain values. This can't be done for parameters which are wrappers over SQL function defined with RETURN NULL ON NULL INPUT because such functions need access to both sql domain value and java domain value. - Derby479 This optimization is not available if the outer function is RETURN NULL ON NULL INPUT. That is because the SQLToJavaNode is responsible for compiling the byte code which skips the method call if the parameter is null--if we remove the SQLToJavaNode, then we don't compile that check and we get bug DERBY-1030. Returns true if the routine permits SQL. Resolve a routine. Obtain a list of routines from the data dictionary of the correct type (functions or procedures) and name. Pick the best routine from the list. Currently only a single routine with a given type and name is allowed, thus if changes are made to support overloaded routines, careful code inspection and testing will be required. Flag that this function invocation appears in a GROUP BY clause Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.

Returns the estimated number of rows in the index.
----- getter functions for rowfactory ------
{@inheritDoc } ------------------- Formatable Interface ------------------  ------------------ Externalizable Interface ------------------    Write this object to a stream of stored objects.




Return whether the value is null or not. Restore this object to its (SQL)null value.
Compare this Orderable with a given Orderable for the purpose of index positioning.  This method treats nulls as ordered values - that is, it treats SQL null as equal to null and less than all other values. ************************************************************************ Public Methods implementing DataValueDescriptor interface. ************************************************************************* Gets the length of the data value.  The meaning of this is implementation-dependent.  For string types, it is the number of characters in the string.  For numeric types, it is the number of bytes used to store the number.  This is the actual length of this value, not the length of the type it was defined as. For example, a VARCHAR value may be shorter than the declared VARCHAR (maximum) length. Get a new null value of the same type as this data value. Gets the value in the data value descriptor as a Java Object. The type of the Object will be the Java object type corresponding to the data value's SQL type. JDBC defines a mapping between Java object types and SQL types - we will allow that to be extended through user type definitions. Throws an exception if the data value is not an object (yeah, right). Gets the value in the data value descriptor as a String. Throws an exception if the data value is not a string. Storable interface, implies Externalizable, TypedFormat Return my format identifier. Get the SQL name of the datatype Private methods  Set the value of this DataValueDescriptor from another. Set the value based on the value for the specified DataValueDescriptor from the specified ResultSet.
Load up the class from the saved bytes.
Create and returns a temporary file in temporary file system of database Get the canonical name of the database. This is a name that uniquely identifies it. It is system dependent. The normal, disk based implementation uses method java.io.File.getCanonicalPath on the directory holding the database to construct the canonical name. Get the pathname separator character used by the StorageFile implementation. This is the separator that must be used in directory and file name strings.  Get the abstract name of the directory that holds temporary files. <p> The StorageFactory implementation is not required to make temporary files persistent. That is, files created in the temp directory are not required to survive a shutdown of the database engine. <p> However, files created in the temp directory must be writable, <b>even if the database is otherwise read-only</b>. Classes implementing the StorageFactory interface must have a null constructor.  The init method is called when the database is booted up to initialize the class. It should perform all actions necessary to start the basic storage, such as creating a temporary file directory. This method should not create the database directory. <p> The init method will be called once, before any other method is called, and will not be called again. This method is used to determine whether the storage is fast (RAM based) or slow (disk based). It may be used by the database engine to determine the default size of the page cache. Determine whether the database is read only. The database engine supports read-only databases, even in file systems that are writable. Construct a StorageFile from a path name. Construct a non-temporary StorageFile from a directory and file name. Construct a StorageFile from a directory and file name. The StorageFile may denote a temporary file or a non-temporary database file, depending upon the directoryName parameter. Set the canonicalName. May need adjustment due to DERBY-5096 The shutdown method is called during the normal shutdown of the database. However, the database engine cannot guarantee that shutdown will be called. If the JVM terminates abnormally then it will not be called. Determine whether the storage supports random access. If random access is not supported then it will only be accessed using InputStreams and OutputStreams (if the database is writable).
end of createDataWarningFile Properties cannot be saved end of createServiceRoot Checks if the specified file exists. Return a list of all the directoies in the system directory. end of getCanonicalServiceName end of getDirectoryPath Privileged Monitor lookup. Must be private so that user code can't call this entry point. Helper method returning the "best-effort-most-accurate" path. Returns the protocol lead in for this service. Open the service properties in the directory identified by the service name. end of getServiceProperties Get the StorageFactory implementation for this PersistentService Get an initialized StorageFactoryInstance end of getStorageFactoryInstance The type of the service is 'directory' * Methods of PersistentService  end of isSameService end of privGetStorageFactoryInstance *Recreates service root if required depending on which of the following *attribute is specified on the conection URL: * Attribute.CREATE_FROM (Create database from backup if it does not exist): * When a database not exist, the service(database) root is created * and the PersistentService.PROPERTIES_NAME (service.properties) file * is restored from the backup. * Attribute.RESTORE_FROM (Delete the whole database if it exists and then restore * it from backup) * Existing database root  is deleted and the new the service(database) root is created. * PersistentService.PROPERTIES_NAME (service.properties) file is restored from the backup. * Attribute.ROLL_FORWARD_RECOVERY_FROM:(Perform Rollforward Recovery; * except for the log directory everthing else is replced  by the copy  from * backup. log files in the backup are copied to the existing online log * directory.): * When a database not exist, the service(database) root is created. * PersistentService.PROPERTIES_NAME (service.properties) file is deleted * from the service dir and  recreated with the properties from backup. end of recreateServiceRoot end of removeServiceRoot Resolves situations where a failure condition left the service properties file, and/or the service properties file backup, in an inconsistent state. <p> Note that this method doesn't resolve the situation where both the current service properties file and the backup file are missing. Save service.properties during backup  end of saveServiceProperties Verify that the service directory looks ok before objecting that the database already exists.
Determine whether the named file is writable. If the named file does not already exist then create it as an empty normal file. The implementation must synchronize with other threads accessing the same file (in the same or a different process). If two threads both attempt to create a file with the same name at the same time then at most one should succeed. Deletes the named file or empty directory. This method does not delete non-empty directories. Deletes the named file and, if it is a directory, all the files and directories it contains. Tests whether the named file exists. Converts this StorageFile into a canonical pathname string. The form of the canonical path is system dependent. Get an exclusive lock with this name. This is used to ensure that two or more JVMs do not open the same database at the same time. ny StorageFile that has getExclusiveFileLock() called on it is not intended to be read from or written to. It's sole purpose is to provide a locked entity to avoid multiple instances of Derby accessing the same database. getExclusiveFileLock() may delete or overwrite any existing file. Creates an input stream from a file name.  Creates an output stream from a file name. If a normal file already exists with this name it will first be truncated to zero length. Creates an output stream from a file name. Get the name of the parent directory if this name includes a parent. Converts this StorageFile into a pathname string. The character returned by StorageFactory.getSeparator() is used to separate the directory and file names in the sequence. <p> <b>The returned path may include the database directory. Therefore it cannot be directly used to make an StorageFile equivalent to this one.</b> Get a random access file. This method is not called if the StorageFactory is read only. It is unspecified if the StorageFactory that created it is not a WritableStorageFactory. Tests whether the named file is a directory, or not. This is only called in writable storage factories. Use when creating a new file. By default, a file created in an underlying file system, if applicable, will have read and write access for the file owner unless the property {@code derby.useDefaultFilePermissions} is set to {@code true}. Get the names of all files and sub-directories in the directory named by this path name. This method is only used in a writable database. Creates the named directory. Creates the named directory, and all nonexistent parent directories. Release the resource associated with an earlier acquired exclusive lock releaseExclusiveFileLock() may delete the file Rename the file denoted by this name. Note that StorageFile objects are immutable. This method renames the underlying file, it does not change this StorageFile object. The StorageFile object denotes the same name as before, however the exists() method will return false after the renameTo method executes successfully. <p>It is not specified whether this method will succeed if a file already exists under the new name. Make the named file or directory read-only. This interface does not specify whether this also makes the file undeletable.
Clone this file abstraction Closes this file. Get the current offset in this file. Gets the length of this file. Reads up to <code>len</code> bytes of data from this file into an array of bytes. This method blocks until at least one byte of input is available. <p> Set the file pointer. It may be moved beyond the end of the file, but this does not change the length of the file. The length of the file is not changed until data is actually written.. Sets the length of this file, either extending or truncating it. <p> If the file is extended then the contents of the extension are not defined. <p> If the file is truncated and the file pointer is greater than the new length then the file pointer is set to the new length. Force any changes out to the persistent store. If the database is to be transient, that is, if the database does not survive a restart, then the sync method implementation need not do anything.

Close the controller. <p> Close the open controller.  This method always succeeds, and never throws any exceptions. Callers must not use the StoreCostController Cost controller after closing it; they are strongly advised to clear out the scan controller reference after closing. <p> Return the cost of exact key lookup. <p> Return the estimated cost of calling ScanController.fetch() on the current conglomerate, with start and stop positions set such that an exact match is expected. <p> This call returns the cost of a fetchNext() performed on a scan which has been positioned with a start position which specifies exact match on all keys in the row. <p> Example: <p> In the case of a btree this call can be used to determine the cost of doing an exact probe into btree, giving all key columns.  This cost can be used if the client knows it will be doing an exact key probe but does not have the key's at optimize time to use to make a call to getScanCost() <p> Return the cost of calling ConglomerateController.fetch(). <p> Return the estimated cost of calling ConglomerateController.fetch() on the current conglomerate.  This gives the cost of finding a record in the conglomerate given the exact RowLocation of the record in question. <p> The validColumns parameter describe what kind of row is being fetched, ie. it may be cheaper to fetch a partial row than a complete row. <p> Calculate the cost of a scan. <p> Cause this object to calculate the cost of performing the described scan.  The interface is setup such that first a call is made to calcualteScanCost(), and then subsequent calls to accessor routines are made to get various pieces of information about the cost of the scan. <p> For the purposes of costing this routine is going to assume that a page will remain in cache between the time one next()/fetchNext() call and a subsequent next()/fetchNext() call is made within a scan. <p> The result of costing the scan is placed in the "cost_result". The cost of the scan is stored by calling cost_result.setEstimatedCost(cost). The estimated row count is stored by calling cost_result.setEstimatedRowCount(row_count). <p> The estimated cost of the scan assumes the caller will execute a fetchNext() loop for every row that qualifies between start and stop position.  Note that this cost is different than execution a next(),fetch() loop; or if the scan is going to be terminated by client prior to reaching the stop condition. <p> The estimated number of rows returned from the scan assumes the caller will execute a fetchNext() loop for every row that qualifies between start and stop position. <p> Return an "empty" row location object of the correct type. <p>
Get the estimated cost. Get the estimated row count. Set the estimated cost. Set the estimated row count.
Makes sure the Clob has not been released. <p> All operations are invalid on a released Clob. Returns the number of characters in the Clob. Returns the cached character count for the Clob, if any. Returns an internal reader for the Clob, initialized at the specified character position. Returns a stream serving the raw bytes of this Clob. <p> Note that the stream returned is an internal stream, and it should not be pulished to end users. Returns a reader for the Clob, initialized at the specified character position. Returns the update count of this Clob. <p> Always returns zero, as this Clob cannot be updated. Not supported. Not supported. Tells if this Clob has been released. Tells if this Clob can be modified. Wrap real exception in a {@link SQLException} to avoid changing the state of the connection child by cleaning it up. Releases resources associated with this Clob. Not supported.
************************************************************************ Get accessors for testing bits in the status field. ************************************************************************* Get the status of the field <BR> MT - single thread required read the field data length ************************************************************************ routines used to read a field header from an ObjectInput stream, array ************************************************************************* read the field status read the length of the field and hdr. <p> Optimized routine used to skip a field on a page.  It returns the total length of the field including the header portion.  It operates directly on the array and does no checking of it's own for limits on the array length, so an array out of bounds exception may be thrown - the routine is meant to be used to read a field from a page so this should not happen. <p> ************************************************************************ Set accessors for setting bits in the status field. ************************************************************************* ************************************************************************ routines used to write a field header to a OutputStream ************************************************************************* write out the field status and field data Length

Insert a new slot entry into the current slot array. <p> Shift the existing slots from slot to (slotsInUse - 1) up by one. Up here means from low slot to high slot (e.g from slot 2 to slot 3). Our slot table grows backward so we have to be careful here. Is there enough space on the page to insert a minimum size row? <p> Calculate whether there is enough space on the page to insert a minimum size row.  The calculation includes maintaining the required reserved space on the page for existing rows to grow on the page. <p>  Calculate the slot field size from the page size.  See if reserved space should be reclaimed for the input row. <p> See if the row on this page has reserved space that should be shrunk once the update commits.  Will only indicate space should be reclaimed if at least RawTransaction.MINIMUM_RECORD_SIZE_DEFAULT bytes can be reclaimed. <p> clean the page for first use or reuse Initialize the freeSpace count and set the firstFreeByte on page Zero out a portion of the page.  Compress out the space specified by startByte and endByte. <p> As part of moving rows, updating rows, purging rows compact the space left between rows. <p> Create the output streams. <p> Create the output streams, these are created on demand to avoid creating unrequired objects for pages that are never modified during their lifetime in the cache. <p> Create a new StoredPage. <p> Make this object represent a new page (ie. a page that never existed before, as opposed to reading in an existing page from disk). <p> Create the space to update a portion of a record. This method ensures there is enough room to replace the old data of length oldLength at the given offset, with the new data of length newLength. This method does put any new data on the page, it moves old data around and zeros out any old data when newLength &lt; oldLength. This method does update the information in the slot table. The passed in offset is the correct place to put the data when this method returns, ie. it only moves data that has an offset greater then this. Time stamp support - this page supports time stamp Get a time stamp for this page Perform an update. ************************************************************************ Record based routines. ************************************************************************* Is entire record on the page? <p> compare given PageVersion with pageVersion on page Free up required bytes by shifting rows "down" the page. <p> Expand page, move all the data from start Offset down the page by the amount required to free up the required bytes. Get the number of fields on the row at slot The current free space on the page. * Get the offset of the field header of the given field for the record in the given slot. Field number is the absolute number for the complete record, not just this portion. E.g. if this is a record portion that starts at field 3 and has 6 fields then the second field on this *page* has field number 4. return the max datalength allowed with the space available The maximum free space on this page possible. <p> The the maximum amount of space that can be used on the page for the records and the slot offset table. NOTE: subclass may have overwitten it to report less freeSpace Get an empty overflow page.  Return the next recordHandle in a long column chain. <p> Return a recordHandle pointing to the next piece of the column chain. This page must be an overflow page that is in a column chain.  If this is the last piece of the overflow colum, return null. <p> ************************************************************************ Private/Protected methods of This class: ************************************************************************* get scratch space for over flow record header. <p> *  Overflow related methods Get the overflow page for a record that has already overflowed. Get a overflow page that potentially can handle a new overflowed record.  Get the overflow slot for a record that has already overflowed. ************************************************************************ Public Methods specific to StoredPage: ************************************************************************* Get the full size of the page. Get the page offset of the record associated with the input slot. <p> This is the actual offset on the page of the beginning of the record. Return length of row on this page. <p> Return the total length of data and header stored on this page for this record.  This length is stored as the second "field" of the slot table entry. Return reserved length of row on this page. <p> Return the reserved length of this record. This length is stored as the third "field" of the slot table entry. ************************************************************************ Slot Offset & Length table manipulation ************************************************************************* Get the page offset of a given slot entry. <p> Get the page offset of a slot entry, this is not the offset of the record stored in the slot, but the offset of the actual slot. Return the total number of bytes used, reserved, or wasted by the record at this slot. <p> The amount of space the record on this slot is currently taking on the page. If there is any reserve space or wasted space, count that in also Do NOT count the slot entry size <p> Return my format identifier. Handle an update of a record portion that is incomplete. <p> Handle an update of a record portion that is incomplete. Ie. Columns have expanded that require other columns to move off the page into a new portion. <P> This method works out of the columns that need to be moved which are not being updated and makes a copy of their data. It then throws an exception with this data, much like the long column exception which will then allow the original insert to complete. <P> If no columns need to be saved (ie all the ones that would move are being updated) then no exception is thrown, logRow() will return and the update completes normally. <p> Initialize the page from values in the page buffer. <p> Initialize in memory structure using the buffer in pageData.  This is how a StoredPage object is intialized to represent page read in from disk. <p> Initialize the page. If reuse, then Clean up any in memory or on disk structure to ready the page for reuse. This is not only reusing the page buffer, but reusing a free page which may or may not be cleaned up the the client of raw store when it was deallocated. Initialize the in-memory slot table. <p> Initialize the in-memory slot table, ie. that of our super-class BasePage.  Go through all the records on the page and set the freeSpace and firstFreeByte on page. <p> ************************************************************************ Page space usage ************************************************************************* initialize the in memory variables associated with space maintenance. <p> Get the total available space on an empty page. initSlotTable() must be called after the page has been read in. Initialize the StoredPage. <p> Initialize the object, ie. perform work normally perfomed in constructor. Called by setIdentity() and createIdentity() - the Cacheable interfaces which are used to move a page in/out of cache. * Overidden methods of BasePage Override insertAtSlot to provide long row support. get record count without checking for latch See if there is a orphaned long colum chain or not. <p> See if there is a orphaned long colum chain or not.  This is a helper function for removeOrphanedChain.  This page, which may be a head page or overflow page, contains the column specified in columnId.  It used to point to a long column chain at oldPageId and oldRecordId.  Returns true if it no longer points to that long column chain. <p> return whether the field has exceeded the max threshold for this page it compares the fieldSize with the largest possible field for this page methods that is called underneath a page action update page version and instance due to actions by a log record Log a Storable to a stream. <p> Log a Storable into a stream.  This is used by update field operations <P> Write the column in its field format to the stream. Field format is a field header followed the data of the column as defined by the data itself.  See this class's description for the specifics of the header. Log column from input row to the given output stream. <p> Read data from row[arrayPosition], and write the column data in raw store page format to the given column.  Along the way determine if the column will fit on the current page. <p> Action taken in this routine is determined by the kind of column as specified in the columnFlag: COLUMN_NONE   - the column is insignificant COLUMN_FIRST  - this is the first column in a logRow() call COLUMN_LONG   - this is a known long column, therefore we will store part of the column on the current page and overflow the rest if necessary. <p> Upon entry to this routine logicalDataOut is tied to the DynamicByteArrayOutputStream out. <BR> If a column is a long column and it does not totally fit on the current page, then a LongColumnException is thrown.  We package up info about the current long column in the partially filled in exception so that callers can take correct action.  The column will now be set a as a stream. Log a field to the ObjectOutput stream. <P> Find the field in the record and then write out the complete field, i.e. header and data. Log a long column into a DataOuput. <p> Log a long column into a DataOuput.  This is used by insert operations <P> Write the column in its field format to the stream. Field format is a field header followed the data of the column as defined by the data itself.  See this class's description for the specifics of the header. Create and write a long row header to the log stream. <p> Called to log a new overflow record, will check for space available and throw an exception if the record header will not fit on the page. <p> Log a record to the ObjectOutput stream. <p> Write out the complete on-page record to the store stream.  Data is preceeded by a  compressed int that gives the length of the following data. Log a row into the StoreOuput stream. <p> Write the row in its record format to the stream. Record format is a record header followed by each field with its field header. See this class's description for the specifics of these headers. startColumn is used to specified which column for this logRow to start logging.  When realStartColumn is specified, that means part of the row has already been logged.  startColumn here indicates that the first column was logged in the logBuffer, need to continue log the rest of the row starting at realStartColumn. This is used when a longColumn is encountered during a long row. After done logging the long column, we need to continue logging the rest of the row. A -1 value for realStartColumn, means that it is not significant. logRow will not throw an noSpaceOnPage exception, if it is an overflow page, and the record we are inserting is the only record on the page. We are supporting rows expanding multiple pages through this mechanism. logRow expects row to be a sparse row. <p> Move record to a page toward the beginning of the file. <p> As part of compressing the table records need to be moved from the end of the file toward the beginning of the file.  Only the contiguous set of free pages at the very end of the file can be given back to the OS.  This call is used to purge the row from the current page, insert it into a previous page, and return the new row location Mark the record identified by position as deleted. The record may be undeleted sometime later using undelete() by any transaction that sees the record. <p> The interface is optimized to work on a number of rows at a time, optimally processing all rows on the page at once.  The call will process either all rows on the page, or the number of slots in the input arrays - whichever is smaller. <B>Locking Policy</B> <P> MUST be called with table locked, no locks are requested.  Because it is called with table locks the call will go ahead and purge any row which is marked deleted.  It will also use purge rather than delete to remove the old row after it moves it to a new page.  This is ok since the table lock insures that no other transaction will use space on the table before this transaction commits. <BR> A page latch on the new page will be requested and released. Create a new record handle. <p> Return the next record id for allocation.  Callers of this interface expect the next id to get bumped some where else - probably by storeRecordForInsert(). <p> Create a new record id based on current one passed in. <p> This interface is used for the "copy" insert interface of raw store where multiple rows are inserted into a page in a single logged operation.  We don't want to bump the id until the operation is logged so we just allocated each id in order and then bump the next id at the end of the operation. <p> Create a new record handle, and bump the id. <p> Create a new record handle, and bump the id while holding the latch so that no other user can ever see this record id.  This will lead to unused record id's in the case where an insert fails because there is not enough space on the page. <p> Provide a hex dump of the data in the in memory version of the page. <p> The output looks like: 00000000: 4d5a 9000 0300 0000 0400 0000 ffff 0000  MZ.............. 00000010: b800 0000 0000 0000 4000 0000 0000 0000  ........@....... 00000020: 0000 0000 0000 0000 0000 0000 0000 0000  ................ 00000030: 0000 0000 0000 0000 0000 0000 8000 0000  ................ 00000040: 0e1f ba0e 00b4 09cd 21b8 014c cd21 5468  ........!..L.!Th 00000050: 6973 2070 726f 6772 616d 2063 616e 6e6f  is program canno 00000060: 7420 6265 2072 756e 2069 6e20 444f 5320  t be run in DOS 00000070: 6d6f 6465 2e0d 0a24 0000 0000 0000 0050  mode...$.......P 00000080: 4500 004c 0109 008b abfd 3000 0000 0000  E..L......0..... 00000090: 0000 00e0 000e 210b 0102 3700 3405 0000  ......!...7.4... 000000a0: 8401 0000 6400 0000 6004 0000 1000 0000  ....d...`....... 000000b0: 5005 0000 0008 6000 1000 0000 0200 0001  P.....`......... 000000c0: 0000 0000 0000 0004 0000 0000 0000 0000  ................ 000000d0: 9007 0000 0400 0009 a207 0002 0000 0000  ................ 000000e0: 0010 0000 1000 0000 0010 0000 1000 0000  ................ 000000f0: 0000 0010 0000 0000 6006 00ef 8100 0000  ........`....... 00000100: 5006 00e6 0c00 0000 0007 00d0 0400 0000  P............... 00000110: 0000 0000 0000 0000 0000 0000 0000 0000  ................ 00000120: 1007 00c8 7100 0000 0000 0000 0000 0000  ....q........... 00000130: 0000 0000 0000 0000 0000 0000 0000 0000  ................ <p> purge long columns chains which eminate from this page. <p> Purge all the long column chains emanating from the record on this slot of this page.  The headRowHandle is the record handle of the head row piece of this row - if this page is the head row, then headRowHandle is the record handle at the slot.  Otherwise, headRowHandle points to a row on a different page, i.e., the head page. <p> Purge the column chain that starts at overflowPageId, overflowRecordId <p> Purge just the column chain that starts at the input address. The long column chain is pointed at by a field in a row.  The long column is then chained as a sequence of "rows", the last column then points to the next segment of the chain on each page. Long columns chains currently are only one row per page so the next slot of a row in a long row chain should always be the first slot. <p> Purge one row on an overflow page. <p> HeadRowHandle is the recordHandle pointing to the head row piece. <p> purgeRecord from page.  Move following slots up by one. Purge all the overflow columns and overflow rows of the record at slot. <p> Purge all the overflow columns and overflow rows of the record at slot. This is called by BasePage.purgeAtSlot, the head row piece is purged there. <p> Process the qualifier list on the row, return true if it qualifies. <p> A two dimensional array is to be used to pass around a AND's and OR's in conjunctive normal form.  The top slot of the 2 dimensional array is optimized for the more frequent where no OR's are present.  The first array slot is always a list of AND's to be treated as described above for single dimensional AND qualifier arrays.  The subsequent slots are to be treated as AND'd arrays or OR's.  Thus the 2 dimensional array qual[][] argument is to be treated as the following, note if qual.length = 1 then only the first array is valid and it is and an array of and clauses: (qual[0][0] and qual[0][0] ... and qual[0][qual[0].length - 1]) and (qual[1][0] or  qual[1][1] ... or  qual[1][qual[1].length - 1]) and (qual[2][0] or  qual[2][1] ... or  qual[2][qual[2].length - 1]) ... and (qual[qual.length - 1][0] or  qual[1][1] ... or  qual[1][2]) Process the list of qualifiers on the row in the stream. <p> The rawDataIn stream is expected to be positioned after the record header. <p> Check all qualifiers in the qualifier array against row.  Return true if all compares specified by the qualifier array return true, else return false. <p> This routine assumes client caller has already checked if the row is deleted or not.  The row that it get's is expected to match the partial column list of the scan. <p> On entering this routine the stream should be positioned to the beginning of the row data, just after the row header.  On exit the stream will also be positioned there. A two dimensional array is to be used to pass around a AND's and OR's in conjunctive normal form.  The top slot of the 2 dimensional array is optimized for the more frequent where no OR's are present.  The first array slot is always a list of AND's to be treated as described above for single dimensional AND qualifier arrays.  The subsequent slots are to be treated as AND'd arrays or OR's.  Thus the 2 dimensional array qual[][] argument is to be treated as the following, note if qual.length = 1 then only the first array is valid and it is and an array of and clauses: (qual[0][0] and qual[0][0] ... and qual[0][qual[0].length - 1]) and (qual[1][0] or  qual[1][1] ... or  qual[1][qual[1].length - 1]) and (qual[2][0] or  qual[2][1] ... or  qual[2][qual[2].length - 1]) ... and (qual[qual.length - 1][0] or  qual[1][1] ... or  qual[1][2]) Read just one column from stream into row. <p> The routine reads just one column from the row, it is mostly code taken from readRecordFromStream, but highly optimized to just get one column from a non-overflow row.  It can only be called to read a row from the pageData array as it directly accesses the page array to avoid the Stream overhead while processing non-user data which does not need the limit functionality. <p> It is expected that this code will be called to read in a column associated with a qualifiers which are applied one column at a time, and has been specialized to proved the greatest peformance for processing qualifiers.  This kind of access is done when scanning large datasets while applying qualifiers and thus any performance gain at this low level is multiplied by the large number of rows that may be iterated over. <p> The column is read into the object located in row[qual_colid]. ************************************************************************ Page header routines ************************************************************************* Read the page header from the page array. <p> Read the page header from byte form in the page array into in memory variables. restore a record from a stream. <p> The rawDataIn stream is expected to be positioned after the record header. create the record header for the specific slot. <p> Create a new record header object, initialize it, and add it to the array of cache'd record headers on this page.  Finally return reference to the initialized record header. ************************************************************************ Protected Methods of Cacheable Interface: ************************************************************************* ************************************************************************ Protected OverRidden Methods of BasePage: ************************************************************************* Ensure that the page is released from the cache when it is unlatched. Remove a column chain that may have been orphaned by an update. <p> Remove a column chain that may have been orphaned by an update.  This is executed as a post commit operation.  This page is the head page of the row which used to point to the column chain in question.  The location of the orphaned column chain is in the ReclaimSpace record. <BR> MT - latched.  No lock will be gotten, the head record must already be locked exclusive with no outstanding changes that can be rolled back. <p> package Remove slot entry from slot array. <p> Remove a storage slot at slot. Shift the existing slots from slot+1 to (slotsInUse - 1) down by one.. Down here means from high slot to low slot (e.g from slot 3 to slot 2) reserveSpaceForSlot This method will reserve at least specified "spaceToReserve" bytes for the record in the slot. Reset the logical output stream. <p> Reset the logical output stream (logicalDataOut) to be attached to the page array stream as is the norm, no limits are placed on any writes. Restore a portion of a long column. <p> Restore a portion of a long column - user must supply two streams on top of the same data, one implements ObjectInput interface that knows how to restore the object, the other one implements LimitInputStream. Read the record at the given slot into the given row. <P> This reads and initializes the columns in the row array from the raw bytes stored in the page associated with the given slot.  If validColumns is non-null then it will only read those columns indicated by the bit set, otherwise it will try to read into every column in row[]. <P> If there are more columns than entries in row[] then it just stops after every entry in row[] is full. <P> If there are more entries in row[] than exist on disk, the requested excess columns will be set to null by calling the column's object's restoreToNull() routine (ie.  ((Object) column).restoreToNull() ). <P> If a qualifier list is provided then the row will only be read from disk if all of the qualifiers evaluate true.  Some of the columns may have been read into row[] in the process of evaluating the qualifier. <p> This routine should only be called on the head portion of a row, it will call a utility routine to read the rest of the row if it is a long row.  Restore a storable row from a LimitInputStream. <p> Restore a storable row from an LimitInputStream - user must supply two streams on top of the same data, one implements ObjectInput interface that knows how to restore the object, the other one implements LimitInputStream. <p> Set the deleted status Tie the logical output stream to a passed in OutputStream. <p> Tie the logical output stream to a passed in OutputStream with no limit as to the number of bytes that can be written. Set page status Set the page offset of the record associated with the input slot. <p> This is the actual offset on the page of the beginning of the record. Set the row reserved space. Set up a new slot entry. <p> Set given pageVersion to be the as what is on this page Shift data within a record to account for an update. Shrink page. <p> move all the data from start Offset up the page by the amount shrunk. Skip a field header and its data on the given stream. Does this page have enough space to move the row to it. <p> Calculate if a row of length "spaceNeeded" with current record id "source_id" will fit on this page. Does this page have enough space to insert the input rows? <p> Can the rows with lengths spaceNeeded[0..num_rows-1] be copied onto this page? <p> Is there minimal space for insert? <p> Does quick calculation to see if average size row on this page could be inserted on the page.  This is done because the actual row size being inserted isn't known until we actually copy the columns from their object form into their on disk form which is expensive.  So we use this calculation so that in the normal case we only do one copy of the row directly onto the page. <p> Is row guaranteed to be inserted successfully on this page? <p> Return true if this record is guaranteed to be inserted successfully using insert() or insertAtSlot(). This guarantee is only valid while the row remains unchanged and the page latch is held. <p> Is row guaranteed to be inserted successfully on this page? <p> Return true if this record is guaranteed to be inserted successfully using insert() or insertAtSlot(). This guarantee is only valid while the row remains unchanged and the page latch is held. <p> This is a private call only used when calculating whether an overflow page can be used to insert part of an overflow row/column. storeField Store a record at the given slot. debugging, print this page Is this page unfilled? <p> Returns true if page is relatively unfilled, which means the page is &lt; 1/2 full and has enough space to insert an "average" sized row onto the page. <p> Recalculate checksum and write it to the page array. <p> Recalculate the checksum of the page, and write the result back into the last bytes of the page. Update field at specified slot  Update a record handle to point to an overflowed record portion. Note that the record handle need not be the current page. Update an already overflowed record. Update the page header in the page array. <p> Write the bytes of the page header, taking the values from those in the in memory variables. Update the page version number in the byte array Update the length of data stored on this page for this record Update the length of data stored on this page for this record <p> Update both the record length "field" and the reserved space "field" of the slot table entry associated with "slot".  This length is stored as the second "field" of the slot table entry.  The changes to these 2 fields are represented as the delta to apply to each field as input in "delta" and "reservedDelta." <p> ************************************************************************ Protected Methods of CachedPage class: (create, read and write a page.) ************************************************************************* use this passed in page buffer as this object's page data. <p> The page content may not have been read in from disk yet. For pagesize smaller than 64K: Size of the record offset stored in a slot (unsigned short) Size of the record portion length stored in a slot (unsigned short) Size of the record portion length stored in a slot (unsigned short) For pagesize greater than 64K, but less than 2gig: Size of the record offset stored in a slot (int) Size of the record portion length stored in a slot (int) Size of the record portion length stored in a slot (int) <p> Validate the check sum on the page. <p> Compare the check sum stored in the page on disk with the checksum calculated from the bytes on the page. <p> Write out the format id of this page Write information about page from variables into page byte array. <p> This routine insures that all information about the page is reflected in the page byte buffer.  This involves moving information from local variables into encoded version on the page in page header and checksum. <p>
************************************************************************ Public Accessor "Get" Methods of This class: ************************************************************************* Get a record handle for the record. <p> <BR> MT - single thread required Get the record identifier <BR> MT - thread safe Return length on disk of the record id portion of the record header Record id is part of the record header and is stored in an internal compressed format.  The length of this format depends on the value of the record id. Get the deleted state of the record. <p> <BR> MT - single thread required ************************************************************************ Public Accessor "Set" Methods of This class: ************************************************************************* Set the deleted state of the record. <p> return   1, if delete status from not deleted to deleted return  -1, if delete status from deleted to not deleted return   0, if status unchanged. <BR> MT - single thread required return the size of the record header. <p> Calculates the size of the record header, mostly used to allow a reader to skip over the record header and position on the 1st field of the record. <p> This low level routine is performance critical to processing lots of rows, so calls to CompressNumber have been hand inlined. ************************************************************************ Public Methods implmenting read/write of Storable Interface: *************************************************************************
The key, don't boot a store! Privileged Monitor lookup. Must be private so that user code can't call this entry point.

Close me. After using this method the caller must throw away the reference to the Container object, e.g. <PRE> ref.close(); ref = null; </PRE> <BR> The container will be closed automatically at the commit or abort of the transaction if this method is not called explictly. Fetch the next record. Fills in the Storable columns within the passed in row if row is not null, otherwise the record is not fetched. If the row.length is less than the number of fields in the row, then, will fill the row, and ignore the rest of the row. <BR> When no more row is found, then false is returned. <P> <B>Locking Policy</B> <BR> No locks. Request the system properties associated with a container. <p> Request the value of properties that are associated with a stream table. The following properties can be requested: derby.storage.streamFileBufferSize <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(ConglomerateController cc) { Properties prop = new Properties(); prop.put("derby.storage.streamFileBufferSize", ""); cc.getTableProperties(prop); System.out.println( "table's buffer size = " + prop.getProperty("derby.storage.streamFileBufferSize"); } Return my identifier. remove the stream container
Close the stream file. <p> Close this stream file, and all streams associated with it. <p> Request the system properties associated with a stream container. <p> Request the value of properties associated with a stream container. The following properties can be requested: derby.storage.streamFileBufferSize <p> To get the value of a particular property add it to the property list, and on return the value of the property will be set to it's current value.  For example: get_prop(ConglomerateController cc) { Properties prop = new Properties(); prop.put("derby.storage.streamFileBufferSize", ""); cc.getContainerProperties(prop); System.out.println( "stream table's buffer size = " + prop.getProperty("derby.storage.streamFileBufferSize"); } Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Return a file name for the identity. <p> Return a valid file name for the identity, or null if the data directory for this segment cannot be created Request the container key associated with the stream container. Privileged module lookup. Must be private so that user code can't call this entry point. ************************************************************************ Public Methods of This class: ************************************************************************* Return my format identifier. load data into this container. <p> populate the stream container with data in the rowSource <p> ************************************************************************ Private/Protected methods of This class: ************************************************************************* Open a stream file container. <p> Open a container. Open the file that maps to this container, if the file does not exist then we assume the container was never created and return. If the file exists but we have trouble opening it then we throw some exception. <p> Close the stream file and remove the file. PrivilegedAction method Can I use this container? <p> This method always return true right now. In the future when there are different uses for this container, we may need to add qualifications for this.  Write the buffer to the file. <p> If the database is encrypted, the dataFactory.getEncryptionBlockSize() - 1 reserved bytes will be used to pad the byte array to be dataFactory.getEncryptionBlockSize() aligned.  Before the bytes are encrypted and written to the file stream, the actual length of the byte array is written out as a compressed integer.  This number will be used when decrypting the data. If the database is not encrypted, then, we don't reserve the bytes upfront, and we simple just write the bytes out to the file stream.
fetch a row from the container. * Methods from StreamContainerHandle Request the system properties associated with a container. get the container key for the stream container Return the RawTransaction this object was opened in. remove the stream container * Implementation specific methods for myself and my sub-classes *	Methods of Observer Called when the transaction is about to complete. * Implementation specific methods, these are public so that they can be called * in other packages that are specific implementations of Data, ie. * a directory at the level * * org.apache.derby.impl.store.raw.data.* Attach me to a container. If this method returns false then I cannot be used anymore, and any reference to me must be discarded.
Tells if the header encodes a character or byte count. Generates the header for the specified length and writes it into the provided buffer, starting at the specified offset. Generates the header for the specified length and writes it into the destination stream. Returns the maximum length of the header. Writes a Derby-specific end-of-stream marker to the buffer for a stream of the specified length, if required. Writes a Derby-specific end-of-stream marker to the destination stream for the specified length, if required.
Close this log scan. Get the instant of the record just retrieved with getNextRecord(). Get the LogInstant for the record just retrieved with getNextRecord(). Get the log instant that is right after the record just retrieved with getNextRecord().  Only valid for a forward scan and on a successful retrieval. Get the next record in the scan and place its data in the passed in array.  The scan is advanced to the next log record. If the input array is of insufficient size, getNextRecord must expand the array to accomodate the log record.  User can optionally pass in a transaction Id and a group mask.  If provided, only log record that matches the transaction Id and the group mask is returned.  Reset the scan to the given LogInstant so that getNextRecord get the log record AFTER the given LogInstant.
Set the value by reading the stream and converting it to an object form. Return the on-disk stream state of the object. sets the on-disk stream state for the object.


<p> Complain that we don't like the stack. </p> The stack looks like this:  getXXX() getString() getRawColumn() deduceGetXXXCaller()  Except if the actual getXXX() method is getString()  <p> Look for a  method name on a stack and return its location as an index into the stack. Returns -1 if the expected name is not found. </p> /////////////////////////////////////////////////////////////////////////////////  ABSTRACT StringColumn BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  FUNCTIONS  ///////////////////////////////////////////////////////////////////////////////// <p> This SQL function returns the list of getXXX() calls made to the last StringArrayVTI. </p> /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Find the getXXX() method above us on the stack. The stack looks like this: </p> <ul> <li>getXXX()</li> <li>getString()</li> <li>getRawColumn()</li> <li>deduceGetXXXCaller()</li> </ul> </p> Except if the actual getXXX() method is getString() </p> /////////////////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Extract the names of methods on a stack. </p> <p> Turn an array into a printable String. </p>
/////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Set the wasNull flag based on whether this column value turned out to be null. </p> <p> Get the number of columns. </p> <p> Get name of a column (1-based indexing). </p> <p> Turn a string into an appropriately encoded ByteArrayInputStream. </p> /////////////////////////////////////////////////////////////////////////////////  StringColumnVTI BEHAVIOR TO BE IMPLEMENTED BY SUBCLASSES  ///////////////////////////////////////////////////////////////////////////////// <p> Get the string value of the column in the current row identified by the 1-based columnNumber. </p> <p> Construct a SQLException from a SQLState and args. </p> <p> Translate a date/time expression into the corresponding long number of milliseconds. </p> /////////////////////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////////////////////// <p> Set the column names for this table function. This is useful for AwareVTIs, which need to figure out their column names after analyzing their execution context. Throws an exception if the column names have already been set. </p> /////////////////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Wrap an exception in a SQLException. </p>
The SQL Ansi trim function. The SQL concatenation '||' operator. Get a char array.  Typically, this is a simple getter that is cheaper than getString() because we always need to create a char array when doing I/O.  Use this instead of getString() where reasonable. <p> <b>WARNING</b>: may return a character array that has spare characters at the end.  MUST be used in conjunction with getLength() to be safe. Returns the stream header generator for the string data value. <p> The generator writes the correct header into the destination buffer or stream and also keeps track of whether appending an end-of-stream marker is required or not. <p> Note that the generator may fail to generate a header if there is no context at the time the header is asked for, and the mode hasn't been set explicitly. Returns a descriptor for the input stream for this data value. <p> The descriptor contains information about header data, current positions, length, whether the stream should be buffered or not, and if the stream is capable of repositioning itself. Gets either SQLChar/SQLVarchar/SQLLongvarchar/SQLClob(base classes) or CollatorSQLChar/CollatorSQLVarchar/CollatorSQLLongvarch/CollatorSQLClob (subclasses). Whether this method returns the base class or the subclass depends on the value of the RuleBasedCollator. If RuleBasedCollator is null, then the object returned would be baseclass otherwise it would be subcalss. The SQL like() function with out escape clause. The SQL like() function WITH escape clause. Position in searchFrom of the first occurrence of this.value. The search begins from position start.  0 is returned if searchFrom does not contain this.value.  Position 1 is the first character in searchFrom. Convert the string to lower case. Tells the data value descriptor which CLOB stream header format to use. Stuff a StringDataValue with a Clob. Convert the string to upper case.

Compares two strings Strings will be uppercased in english and compared equivalent to s1.equalsIgnoreCase(s2) throws NPE if s1 is null The functions below are used for uppercasing SQL in a consistent manner. Derby will uppercase Turkish to the English locale to avoid i uppercasing to an uppercase dotted i. In future versions, all casing will be done in English.   The result will be that we will get only the 1:1 mappings  in http://www.unicode.org/Public/3.0-Update1/UnicodeData-3.0.1.txt and avoid the 1:n mappings in http://www.unicode.org/Public/3.0-Update1/SpecialCasing-3.txt  Any SQL casing should use these functions Convert string to uppercase Always use the java.util.ENGLISH locale Compress 2 adjacent (single or double) quotes into a single (s or d) quote when found in the middle of a String. NOTE:  """" or '''' will be compressed into "" or ''. This function assumes that the leading and trailing quote from a string or delimited identifier have already been removed. Reg.exp substitute:<br/> <p/> Pattern pat_a = Pattern.compile("\\A\\t*");<br/> Matcher m_a = pat_a.matcher(src);<br/> src = m_a.replaceFirst(indent.toString());<br/> Reg.exp substitute:<br/> <p/> Pattern pat_b = Pattern.compile("\\n+\\Z");<br/> Matcher m_b = pat_b.matcher(formatted);<br/> formatted = m_b.replaceFirst("");<br/> Reg.exp substitute:<br/> <p/> Pattern pat_c = Pattern.compile("\\n\\t*");<br/> Matcher m_c = pat_c.matcher(formatted);<br/> formatted = m_c.replaceAll("\n" + indent.toString());<br/> Utility for formatting which bends a multi-line string into shape for outputting it in a context where there is <i>depth</i> tabs. Trailing newlines are discarded as well. <p> Replace     "^[\t]*" with "depth" number of tabs.<br> Replace     "\n+$" with "". Replace all "\n[\t]*" with "\n" + "depth" number of tabs.<br> </p> Used to print out a string for error messages, chops is off at 60 chars for historical reasons. Convert a hexidecimal string generated by toHexString() back into a byte array. Get 7-bit ASCII character array from input String. The lower 7 bits of each character in the input string is assumed to be the ASCII character value. Hexadecimal - Character | 00 NUL| 01 SOH| 02 STX| 03 ETX| 04 EOT| 05 ENQ| 06 ACK| 07 BEL| | 08 BS | 09 HT | 0A NL | 0B VT | 0C NP | 0D CR | 0E SO | 0F SI | | 10 DLE| 11 DC1| 12 DC2| 13 DC3| 14 DC4| 15 NAK| 16 SYN| 17 ETB| | 18 CAN| 19 EM | 1A SUB| 1B ESC| 1C FS | 1D GS | 1E RS | 1F US | | 20 SP | 21  ! | 22  " | 23  # | 24  $ | 25  % | 26  &amp; | 27  ' | | 28  ( | 29  ) | 2A  * | 2B  + | 2C  , | 2D  - | 2E  . | 2F  / | | 30  0 | 31  1 | 32  2 | 33  3 | 34  4 | 35  5 | 36  6 | 37  7 | | 38  8 | 39  9 | 3A  : | 3B  ; | 3C  &lt; | 3D  = | 3E  &gt; | 3F  ? | | 40  @ | 41  A | 42  B | 43  C | 44  D | 45  E | 46  F | 47  G | | 48  H | 49  I | 4A  J | 4B  K | 4C  L | 4D  M | 4E  N | 4F  O | | 50  P | 51  Q | 52  R | 53  S | 54  T | 55  U | 56  V | 57  W | | 58  X | 59  Y | 5A  Z | 5B  [ | 5C  \ | 5D  ] | 5E  ^ | 5F  _ | | 60  ` | 61  a | 62  b | 63  c | 64  d | 65  e | 66  f | 67  g | | 68  h | 69  i | 6A  j | 6B  k | 6C  l | 6D  m | 6E  n | 6F  o | | 70  p | 71  q | 72  r | 73  s | 74  t | 75  u | 76  v | 77  w | | 78  x | 79  y | 7A  z | 7B  { | 7C  | | 7D  } | 7E  ~ | 7F DEL| Convert a byte array to a human-readable String for debugging purposes. Normalize a SQL identifer, up-casing if <regular identifer>, and handling of <delimited identifer> (SQL 2003, section 5.2). The normal form is used internally in Derby. Quote a string so that it can be used as an identifier or a string literal in SQL statements. Identifiers are surrounded by double quotes and string literals are surrounded by single quotes. If the string contains quote characters, they are escaped. Quote a string so that it can be used as a string literal in an SQL statement. Get the short database name from the canonical name. Return a slice (substring) of the passed in value, optionally trimmed. WARNING - endOffset is inclusive for historical reasons, unlike String.substring() which has an exclusive ending offset. Convert a byte array to a String with a hexidecimal format. The String may be converted back to a byte array using fromHexString. <BR> For each byte (b) two characaters are generated, the first character represents the high nibble (4 bits) in hexidecimal (<code>b &amp; 0xf0</code>), the second character represents the low nibble (<code>b &ampxs; 0x0f</code>). <BR> The byte at <code>data[offset]</code> is represented by the first two characters in the returned String. A method that receive an array of Objects and return a String array representation of that array. Trim off trailing blanks but not leading blanks end of trimTrailing Truncate a String to the given length with no warnings or error raised if it is bigger.
available to force synchronization of get_countlock(), ...
Get the text of the check constraint definition. Get the ReferencedColumns. Does this constraint have a backing index? Convert the SubCheckConstraintDescriptor to a String.

Returns the cached TableDescriptor, if supplied, that the constraint is on. Gets the UUID of the constraint. Does this constraint have a backing index? Sets the UUID of the constraint. Caches the TableDescriptor of the table that the constraint is on. Convert the SubConstraintDescriptor to a String.
* Methods of Runnable (from ExtendingInterface) * Methods of ExtendingInterface
Gets the UUID of the backing index. Gets the UUID of the referenced key constraint Gets a referential action rule on a  DELETE Gets a referential action rule on a UPDATE Does this constraint have a backing index? Convert the SubKeyConstraintDescriptor to a String.

Reset the transaction counts to zero. Generate a new random number generator that follows the rules according to 2.1.6.1 Get the executed transaction counts. Return one of transaction constants to run that transaction. This mix in not correct for TPC-C specification. With the official spec the final mix of transactions must match a certain profile. With this setup the mix is fixed upon input following the TPC-C final ratios. This will give approximately correct results, but the final mix will not be in line with TPC-C rules. This is because different transactions have different execution times. Return a Submitter than only executes new order transactions with no rollback Return a Submitter than only executes order status by identifier transactions. Return a Submitter than only executes order status by name transactions. Return a Submitter than only executes payment by identifier transactions. Return a Submitter than only executes payment by name transactions. Print a simple report of the activity. Run an order status transaction with random input values. Run a payment transaction with random input values. Run a stock level transaction with random input values. Run an order entry transaction picking the specific transaction at random with a hard-coded mix. Run a fixed number of transactions returning the time in milli-seconds required to execute all of them. Return a Submitter than only executes stock level transactions. Return a random warehouse
Add a subquery to the list. Decrement (query block) level (0-based) for all of the tables in this subquery list. This is useful when flattening a subquery. Mark all of the subqueries in this list as being part of a having clause, so we can avoid flattening later. Mark all of the subqueries in this list as being part of a where clause so we can avoid flattening later if needed. Modify the access paths for all subqueries in this list. Optimize the subqueries in this list. Return true if the node references SESSION schema tables (temporary or permanent) Search to see if a query references the specifed table name. Set the point of attachment in all subqueries in this list.
Accept the visitor for all visitable children of this node. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Can NOT IN, ALL be falttened to NOT EXISTS join?  We can't or the flattening doesn't easily make sense if either side of the comparison is nullable. (beetle 5173) Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. Finish putting an expression into conjunctive normal form.  An expression tree in conjunctive normal form meets the following criteria: o  If the expression tree is not null, the top level will be a chain of AndNodes terminating in a true BooleanConstantNode. o  The left child of an AndNode will never be an AndNode. o  Any right-linked chain that includes an AndNode will be entirely composed of AndNodes terminated by a true BooleanConstantNode. o  The left child of an OrNode will never be an OrNode. o  Any right-linked chain that includes an OrNode will be entirely composed of OrNodes terminated by a false BooleanConstantNode. o  ValueNodes other than AndNodes and OrNodes are considered leaf nodes for purposes of expression normalization. In other words, we won't do any normalization under those nodes. In addition, we track whether or not we are under a top level AndNode. SubqueryNodes need to know this for subquery flattening. Convert this IN/ANY subquery, which is known to return at most 1 row, to an equivalent expression subquery. Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. Flatten this subquery into the outer query block as an exists join. At this point we are only flattening non-aggregate subqueries with a single FBT in the from list. So, we transform all FBTs in the from list into ExistBaseTables, update the dependency lists for each of the tables and then flatten the subquery. RESOLVE - we will need to modify this logic to account for aggregates as we support flattening for them. Flatten this subquery into the outer query block. At this point we are only flattening based on a uniqueness condition and only flattening non-aggregate subqueries. So, we promote the subquery's from list, as is, into the outer from list.  For EXISTS subquerys, we return a TRUE.  Otherwise we return a new comparison between the leftOperand and the expression in the subquery's SELECT list. RESOLVE - we will need to modify this logic to account for exists joins and aggregates as we support flattening for them. Anyway, here's what we do: o We remove ourself from the outer subquery list. o We decrement the nesting level for all tables in the subquery tree. o We append the subquery's from list to the outer from list. o We add the subquery's predicate list to the outer predicate list.  (The subquery has already been preprocessed.) o We add the subquery's subquery list to the outer subquery list. o For EXISTS, we return a true. o Otherwise, we return a new comparison between the leftOperand and the expression in the inner select's RCL. Do code generation for this subquery. * Materialize the subquery in question.  Given the expression * that represents the subquery, this returns fieldX where * fieldX is set up as follows: * * private ... fieldX * * execute() * { *	fieldX = <subqueryExpression> *	... * } * * So we wind up evaluating the subquery when we start * execution.  Obviously, it is absolutely necessary that * the subquery is invariant and has no correlations * for this to work. * * Ideally we wouldn't evaluate the expression subquery * until we know we need to, but because we are marking * this expression subquery as pushable, we must evaluate * it up front because it might wind up as a qualification, * and we cannot execute a subquery in the store as a * qualification because the store executes qualifications * while holding a latch. * * @param acb * @param type * @param subqueryExpression Get FETCH FIRST (used to construct FROM_SUBQUERY only), cf. FromSubquery, for which this node is transient. Build a new join condition between the leftOperand and the rightOperand.  The comparison operator is dependent on the subquery type. Get OFFSET  (used to construct FROM_SUBQUERY only), cf. FromSubquery, for which this node is transient. Get ORDER BY list (used to construct FROM_SUBQUERY only), cf. FromSubquery, for which this node is transient. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Get the ResultSet # for the point of attachment for this SubqueryNode. Get whether or not this SubqueryNode has already been preprocessed. Return the resultSet for this SubqueryNode. Get the node that will be the right operand in the join condition if this ALL/ANY/SOME/(NOT) IN subquery is flattened to a join. Return the type of this subquery. Private methods on private variables Return whether or not this subquery is immediately under a top level AndNode. Check to see if this subquery has correlated column references.  Only useful results if called AFTER binding (after CRs have been bound). Return true if the offset/fetchFirst clauses were added by JDBC LIMIT escape syntax. This method is used to construct a FROM_SUBQUERY only, cf. FromSubquery, for which this node is transient. {@inheritDoc } Is this subquery part of a having clause? Check to see if we have a Variant value below us. If so, return true.  Caches the result so multiple calls are ok. * Subquery is materializable if * it is an expression subquery that * has no correlations and is invariant. Check whether this is a WHERE EXISTS | ANY | IN subquery with a subquery in its own WHERE clause. Used in flattening decision making. DERBY-3301 reported wrong results from a nested WHERE EXISTS, but according to the derby optimizer docs this applies to a broader range of WHERE clauses in a WHERE EXISTS subquery. No WHERE EXISTS subquery with anohter subquery in it own WHERE clause can be flattened. Is this subquery part of a whereclause? Make any changes to the access paths, as decided by the optimizer. Optimize this SubqueryNode. Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Transform: expression QuantifiedOperator (select x from ...) into (select true from .. where expression <BinaryComparisonOperator> x ...) IS [NOT] NULL or, if we have an aggregate: (select true from (select AGG(x) from ...) where expression <BinaryComparisonOperator> x ...) IS [NOT] NULL For ANY and IN subqueries: o  We generate an IS NULL above the SubqueryNode and return the top of the new tree to the caller. o  The operator in the new predicate that is added to the subquery will correspond to the operator that modifies the ANY. (eg, = for = ANY, with = for IN.) For ALL and NOT IN subqueries: o  We generate an IS NOT NULL above the SubqueryNode and return the top of the new tree to the caller. o  The operator in the new predicate that is added to the subquery will be a BinaryAllOperatorNode whose bcoNodeType corresponds to the negation of the operator that modifies the ALL. (eg, &lt;&gt; for = ALL, with &lt;&gt; for NOT IN.) NOTE: This method is called after the underlying subquery has been preprocessed, so we build a new Predicate, not just a new expression. Remap all ColumnReferences in this tree to be clones of the underlying expression. <p> Check if the right operand is on a form that makes it possible to flatten this query to a NOT EXISTS join. We don't allow flattening if the right operand doesn't reference the base table of the subquery. (Requirement added as part of DERBY-4001.) </p> <p> The problem with the right operand not referencing the base table of the subquery, is that the join condition may then be used to filter rows from the right side (outer) table in the NOT EXISTS join. In a NOT EXISTS join, the join condition can only safely be applied to the left side (inner) table of the join. Otherwise, it will filter out all the interesting rows too early. </p> <p>Take the query below as an example:</p> <pre><code> SELECT * FROM T1 WHERE X NOT IN (SELECT 1 FROM T2) </code></pre> <p> Here, the right operand is 1, and the join condition is {@code T1.X=1}. If flattened, the join condition will be used directly on the outer table, and hide all rows with {@code X<>1}, although those are the only rows we're interested in. If the join condition had only been used on the inner table, the NOT EXISTS join logic would do the correct thing. </p> <p> If the join condition references the inner table, the condition cannot be used directly on the outer table, so it is safe to flatten the query. </p> Mark this subquery as being part of a having clause. Set the parent BCON.  Useful when considering flattening expression subqueries. Set the point of attachment of this subquery. Set the type of this subquery. Mark this subquery as being part of a where clause. Does the from list from the subquery contain a single entry which is a FBT or a PRN/FBT. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
used by GroupByNode to process expressions by complexity level.
<p> Subversion formats timestamps thusly: "2007-09-16 11:17:37 -0700 (Sun, 16 Sep 2007)" </p> /////////////////////////////////////////////////////////////////////////////////  ResultSet METHOD  ///////////////////////////////////////////////////////////////////////////////// <p> Subversion formats timestamps thusly: "2007-09-16 11:17:37 -0700 (Sun, 16 Sep 2007)" </p> <p> Returns true if a line is a record header. </p> /////////////////////////////////////////////////////////////////////////////////  FlatFileVTI BEHAVIOR TO BE IMPLEMENTED BY SUBCLASSES  ///////////////////////////////////////////////////////////////////////////////// <p> Parse the next chunk of text, using readLine(), and return the next row. Returns null if the file is exhausted. </p> /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// <p> Read the next field in the main line of the record. Fields are delimited by | or line-end. </p> <p> Read a line, possibly just using the last line that was pushed back. </p> /////////////////////////////////////////////////////////////////////////////////  TABLE FUNCTION METHOD  ///////////////////////////////////////////////////////////////////////////////// <p> This is the method which is registered as a table function. </p>
Accumulate //////////////////////////////////////////////////////////  FORMATABLE INTERFACE  /////////////////////////////////////////////////////////// Get the formatID which corresponds to this class.
Determines the result datatype.  Accept NumberDataValues only. <P> <I>Note</I>: In the future you should be able to do a sum user data types.  One option would be to run sum on anything that implements plus().  In which case avg() would need divide(). Return the aggregator class. Return the arithmetic operator corresponding to this operation. This is set by the parser.
Obtain a File for the local copy of a read-only resource. Get the full name of the file. Obtain the URL to the local copy of a read-only resource. Obtain a File for the local copy of a read-write resource. Get the full name of the file. Obtain the URL to the local copy of a read-write resource. Obtain a File for the local copy of a write-only resource. Obtain the URL to the local copy of a write-only resource.



Get the formatID which corresponds to this class. Formatable methods Read this object from a stream of stored objects. Write this object to a stream of stored objects.


Write out sysinfo for a suite or test
///////////////////////////////////////////////////////////  EXTERNALIZABLE INTERFACE  ///////////////////////////////////////////////////////////
Gets the name of this column. Return the type of this column.
Create a system column for a builtin type. Create a system column for a builtin type. Create a system column for an identifer with consistent type of VARCHAR(128) Create a system column for a character representation of an indicator column with consistent type of CHAR(1) NOT NULL Create a system column for a java column. Gets the name of this column. Return the type of this column. Create a system column for a character representation of a UUID with consistent type of CHAR(36)
Privileged Monitor lookup. Must be private so that user code can't call this entry point.
Build a comma-separated actions string suitable for returning from {@code getActions()}. Does this permission equal another object. True if its and identical class with same name and (canonical) actions. Get a mask of bits that represents the actions and can be used for the implies method. Return the permission's actions in a canonical form. Return a canonical form of the passed in actions. Actions are lower-cased, in the order of LEGAL_ACTIONS and only appear once. Does this permission imply another. Only true if the other permission is a SystemPermission with the same name and all the actions of the permission are present in this. Note that none of the actions imply any other with this SystemPermission. DERBY-6717: Must override newPermissionCollection() since BasicPermission's implementation ignores actions. Get a set of all actions specified in a string. Actions are transformed to lower-case, and leading and trailing blanks are stripped off. Called upon deserialization for restoring the state of this SystemPermission from a stream. Check if name and actions are valid, normalize the actions string, and calculate the actions mask.
Compares this principal to the specified object. Returns true if the object passed in matches the principal represented by the implementation of this interface. Returns the name of this principal. Returns a hashcode for this principal. Called upon deserialization for restoring the state of this SystemPrincipal from a stream. Returns a string representation of this principal. Verify that the specified name of the principal is valid.
Cotangent function. SYSFUN.COT * SQLJ Procedures. Install a jar file in the database. SQLJ.INSTALL_JAR Map SYSIBM.METADATA to appropriate EmbedDatabaseMetaData methods for now, using the sps in org.apache.derby.iapi.db.jdbc.datadictionary.metadata_net.properties Method to return the constant PI. SYSFUN.PI(). Pseudo-random number function. Remove a jar file from the database. Replace a jar file in the database. SQLJ.REPLACE_JAR Method to return the sign of the given value. SYSFUN.SIGN(). Method used by Derby Network Server to get localized message (original call from jcc. Map SQLColPrivileges to EmbedDatabaseMetaData.getColumnPrivileges Map SQLColumns to EmbedDatabaseMetaData.getColumns Map SQLForeignKeys to EmbedDatabaseMetaData.getImportedKeys, getExportedKeys, and getCrossReference Map SQLFunctionParameters to EmbedDatabaseMetaData.getFunctionColumns() Map SQLFunctions to EmbedDatabaseMetaData.getFunctions Map SQLGetTypeInfo to EmbedDatabaseMetaData.getTypeInfo Map SQLPrimaryKeys to EmbedDatabaseMetaData.getPrimaryKeys Map SQLProcedureCols to EmbedDatabaseMetaData.getProcedureColumns Map SQLProcedures to EmbedDatabaseMetaData.getProcedures Map SQLSpecialColumns to EmbedDatabaseMetaData.getBestRowIdentifier and getVersionColumns Map SQLStatistics to EmbedDatabaseMetaData.getIndexInfo Map SQLTablePrivileges to EmbedDatabaseMetaData.getTablePrivileges Map SQLTables to EmbedDatabaseMetaData.getSchemas, getCatalogs, getTableTypes and getTables, and return the result of the DatabaseMetaData calls. <p>JCC and DNC overload this method: <ul> <li>If options contains the string 'GETSCHEMAS=1', call getSchemas()</li> <li>If options contains the string 'GETSCHEMAS=2', call getSchemas(String, String)</li> <li>If options contains the string 'GETCATALOGS=1', call getCatalogs()</li> <li>If options contains the string 'GETTABLETYPES=1', call getTableTypes()</li> <li>otherwise, call getTables()</li> </ul> Map SQLUDTS to EmbedDatabaseMetaData.getUDTs Backup the database to a backup directory. This procedure will throw error, if there are any unlogged operation executed in the same transaction backup is started. If there any unlogged operations in progess in other transaction, it will wait until those transactions are completed before starting the backup. Examples of unlogged operations include: create index and bulk insert. Note that once the backup begins these operations will not block, instead they are automatically converted into logged operations. Backup the database to a backup directory and enable the log archive mode that will keep the archived log files required for roll-forward from this version of the backup. This procedure will throw error if there are any unlogged operation executed in the same transaction backup is started. If there any unlogged operations in progess in other transaction, it will wait until those transactions are completed before starting the backup. Examples of unlogged operations include: create index and bulk insert. Note that once the backup begins these operations will not block, instead they are automatically converted into logged operations. Backup the database to a backup directory and enable the log archive mode that will keep the archived log files required for roll-forward from this version backup. This procedure will throw error, if there are any uncommitted unlogged operation before stating the backup. It will not wait for the unlogged operations to complete. Examples of unlogged operations include: create index and bulk insert. Note that once the backup begins these operations will not block, instead they are automatically converted into logged operations. Backup the database to a backup directory. This procedure will throw error, if there are any uncommitted unlogged operation before stating the backup. It will not wait for the unlogged operations to complete. Examples of unlogged operations include: create index and bulk insert. Note that once the backup begins these operations will not block, instead they are automatically converted into logged operations. Perform bulk insert using the specificed vti . <p> Will be called as SYSCS_BULK_INSERT (IN SCHEMANAME VARCHAR(128), IN TABLENAME VARCHAR(128), IN VTINAME VARCHAR(32672), IN VTIARG VARCHAR(32672)) Compress the table. <p> Calls the "alter table compress {sequential}" sql.  This syntax is not db2 compatible so it mapped by a system routine.  This routine will be called when an application calls: SYSCS_UTIL.SYSCS_COMPRESS_TABLE <p> Create a new user. Disables the log archival process, i.e No old log files will be kept around for a roll-forward recovery. Drop the statistics for 1)all the indexes or 2)a specific index on a table. Drop a user. Empty as much of the cache as possible. It is not guaranteed that the cache is empty after this call, as statements may be kept by currently executing queries, activations that are about to be garbage collected. Export data from a  select statement to given file. <p> Will be called as SYSCS_EXPORT_QUERY(IN SELECTSTATEMENT  VARCHAR(32672), IN FILENAME VARCHAR(32672) , IN COLUMNDELIMITER CHAR(1),  IN CHARACTERDELIMITER CHAR(1) , IN CODESET VARCHAR(128)) Export data from a  select statement to given file. Large objects are exported to an external file and the reference to it is written in the main export file. <p> Will be called as SYSCS_EXPORT_QUERY_LOBS_TO_EXTFILE(IN SELECTSTATEMENT  VARCHAR(32672), IN FILENAME VARCHAR(32672) , IN COLUMNDELIMITER CHAR(1),  IN CHARACTERDELIMITER CHAR(1) , IN CODESET VARCHAR(128), IN LOBSFILENAME VARCHAR(32672)) Export data from a table to given file. <p> Will be called by system procedure: SYSCS_EXPORT_TABLE(IN SCHEMANAME  VARCHAR(128), IN TABLENAME    VARCHAR(128),  IN FILENAME VARCHAR(32672) , IN COLUMNDELIMITER CHAR(1),  IN CHARACTERDELIMITER CHAR(1) , IN CODESET VARCHAR(128)) Export data from a table to given files. Large objects are exported to an external file and the reference to it is written in the main export file. <p> Will be called by system procedure: SYSCS_EXPORT_TABLE_LOBS_TO_EXTFILE(IN SCHEMANAME  VARCHAR(128), IN TABLENAME    VARCHAR(128),  IN FILENAME VARCHAR(32672) , IN COLUMNDELIMITER CHAR(1),  IN CHARACTERDELIMITER CHAR(1) , IN CODESET VARCHAR(128), IN LOBSFILENAME VARCHAR(32672)) Freeze the database. <p> Call internal routine to freeze the database so that a backup can be made. Return the database name Get the value of a property of the database in current connection. <p> Will be called as SYSCS_UTIL.SYSCS_GET_DATABASE_PROPERTY. Get the connection level authorization for a specific user - SYSCS_UTIL.SYSCS_GET_USER_ACCESS. This procedure returns the current status of the xplain mode. If the XPLAIN mode is non-zero, meaning that it is ON, then statements are being XPLAIN'd only, not executed. This procedure returns the current set XPLAIN_SCHEMA Import data from a given file into the specified table columns from the specified columns in the file. <p> Will be called as SYSCS_IMPORT_DATA (IN SCHEMANAME VARCHAR(128), IN TABLENAME VARCHAR(128), IN INSERTCOLUMNLIST VARCHAR(32672), IN COLUMNINDEXES VARCHAR(32672), IN FILENAME VARCHAR(32672), IN COLUMNDELIMITER CHAR(1), IN CHARACTERDELIMITER CHAR(1), IN CODESET VARCHAR(128), IN REPLACE SMALLINT) Import data from a given file into the specified table columns skipping header lines from the specified columns in the file. <p> Will be called as SYSCS_IMPORT_DATA_BULK (IN SCHEMANAME VARCHAR(128), IN TABLENAME VARCHAR(128), IN INSERTCOLUMNLIST VARCHAR(32672), IN COLUMNINDEXES VARCHAR(32672), IN FILENAME VARCHAR(32672), IN COLUMNDELIMITER CHAR(1), IN CHARACTERDELIMITER CHAR(1), IN CODESET VARCHAR(128), IN REPLACE SMALLINT, IN SKIP SMALLINT) Import data from a given file into the specified table columns from the  specified columns in the file. Data for large object columns is in an  external file, the reference to it is in the main input file. Read the lob data from the external file using the lob location info in the main import file. <p> Will be called as SYSCS_IMPORT_DATA_LOBS_FROM_EXTFILE(IN SCHEMANAME VARCHAR(128), IN TABLENAME VARCHAR(128), IN INSERTCOLUMNLIST VARCHAR(32672), IN COLUMNINDEXES VARCHAR(32672), IN FILENAME VARCHAR(32672), IN COLUMNDELIMITER CHAR(1), IN CHARACTERDELIMITER CHAR(1), IN CODESET VARCHAR(128), IN REPLACE SMALLINT) Import  data from a given file to a table. <p> Will be called by system procedure as SYSCS_IMPORT_TABLE(IN SCHEMANAME  VARCHAR(128), IN TABLENAME    VARCHAR(128),  IN FILENAME VARCHAR(32672) , IN COLUMNDELIMITER CHAR(1),  IN CHARACTERDELIMITER CHAR(1) , IN CODESET VARCHAR(128), IN  REPLACE SMALLINT) import  data from a given file to a table skipping header lines. <p> Will be called by system procedure as SYSCS_IMPORT_TABLE_BULK(IN SCHEMANAME  VARCHAR(128), IN TABLENAME    VARCHAR(128),  IN FILENAME VARCHAR(32672) , IN COLUMNDELIMITER CHAR(1),  IN CHARACTERDELIMITER CHAR(1) , IN CODESET VARCHAR(128), IN  REPLACE SMALLINT IN SKIP SMALLINT) @exception SQLException if a database error occurs Import  data from a given file to a table. Data for large object columns is in an external file, the reference to it is in the main input file. Read the lob data from the external file using the lob location info in the main import file. <p> Will be called by system procedure as SYSCS_IMPORT_TABLE_LOBS_FROM_EXTFILE(IN SCHEMANAME  VARCHAR(128), IN TABLENAME    VARCHAR(128),  IN FILENAME VARCHAR(32672) , IN COLUMNDELIMITER CHAR(1),  IN CHARACTERDELIMITER CHAR(1) , IN CODESET VARCHAR(128), IN  REPLACE SMALLINT) Implementation of SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE(). <p> Code which implements the following system procedure: void SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE( IN SCHEMANAME        VARCHAR(128), IN TABLENAME         VARCHAR(128), IN PURGE_ROWS        SMALLINT, IN DEFRAGMENT_ROWS   SMALLINT, IN TRUNCATE_END      SMALLINT) <p> Use the SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE system procedure to reclaim unused, allocated space in a table and its indexes. Typically, unused allocated space exists when a large amount of data is deleted from a table, and there have not been subsequent inserts to use the space freed by the deletes. By default, Derby does not return unused space to the operating system. For example, once a page has been allocated to a table or index, it is not automatically returned to the operating system until the table or index is destroyed. SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE allows you to return unused space to the operating system. <p> This system procedure can be used to force 3 levels of in place compression of a SQL table: PURGE_ROWS, DEFRAGMENT_ROWS, TRUNCATE_END.  Unlike SYSCS_UTIL.SYSCS_COMPRESS_TABLE() all work is done in place in the existing table/index. <p> Syntax: SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE( IN SCHEMANAME        VARCHAR(128), IN TABLENAME         VARCHAR(128), IN PURGE_ROWS        SMALLINT, IN DEFRAGMENT_ROWS   SMALLINT, IN TRUNCATE_END      SMALLINT) <p> SCHEMANAME: An input argument of type VARCHAR(128) that specifies the schema of the table. Passing a null will result in an error. <p> TABLENAME: An input argument of type VARCHAR(128) that specifies the table name of the table. The string must exactly match the case of the table name, and the argument of "Fred" will be passed to SQL as the delimited identifier 'Fred'. Passing a null will result in an error. <p> PURGE_ROWS: If PURGE_ROWS is set to non-zero then a single pass is made through the table which will purge committed deleted rows from the table.  This space is then available for future inserted rows, but remains allocated to the table. As this option scans every page of the table, it's performance is linearly related to the size of the table. <p> DEFRAGMENT_ROWS: If DEFRAGMENT_ROWS is set to non-zero then a single defragment pass is made which will move existing rows from the end of the table towards the front of the table.  The goal of the defragment run is to empty a set of pages at the end of the table which can then be returned to the OS by the TRUNCATE_END option.  It is recommended to only run DEFRAGMENT_ROWS, if also specifying the TRUNCATE_END option.  This option scans the whole table and needs to update index entries for every base table row move, and thus execution time is linearly related to the size of the table. <p> TRUNCATE_END: If TRUNCATE_END is set to non-zero then all contiguous pages at the end of the table will be returned to the OS.  Running the PURGE_ROWS and/or DEFRAGMENT_ROWS passes options may increase the number of pages affected. This option itself does no scans of the table, so performs on the order of a few system calls. <p> SQL example: To compress a table called CUSTOMER in a schema called US, using all available compress options: call SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE('US', 'CUSTOMER', 1, 1, 1); To quickly just return the empty free space at the end of the same table, this option will run much quicker than running all phases but will likely return much less space: call SYSCS_UTIL.SYSCS_INPLACE_COMPRESS_TABLE('US', 'CUSTOMER', 0, 0, 1); Java example: To compress a table called CUSTOMER in a schema called US, using all available compress options: CallableStatement cs = conn.prepareCall ("CALL SYSCS_UTIL.SYSCS_COMPRESS_TABLE(?, ?, ?, ?, ?)"); cs.setString(1, "US"); cs.setString(2, "CUSTOMER"); cs.setShort(3, (short) 1); cs.setShort(4, (short) 1); cs.setShort(5, (short) 1); cs.execute(); To quickly just return the empty free space at the end of the same table, this option will run much quicker than running all phases but will likely return much less space: CallableStatement cs = conn.prepareCall ("CALL SYSCS_UTIL.SYSCS_COMPRESS_TABLE(?, ?, ?, ?, ?)"); cs.setString(1, "US"); cs.setString(2, "CUSTOMER"); cs.setShort(3, (short) 0); cs.setShort(4, (short) 0); cs.setShort(5, (short) 1); cs.execute(); <p> It is recommended that the SYSCS_UTIL.SYSCS_COMPRESS_TABLE procedure is issued in auto-commit mode. Note: This procedure acquires an exclusive table lock on the table being compressed. All statement plans dependent on the table or its indexes are invalidated. For information on identifying unused space, see the Derby Server and Administration Guide. TODO LIST: o defragment requires table level lock in nested user transaction, which will conflict with user lock on same table in user transaction. Invalidate all the stored statements so they will get recompiled when executed next time around. Change a user's password. Peek at the current value of an identity generator without advancing it. Peek at the current value of a sequence generator without advancing it. Reload the policy file. <p> System procedure called thusly: SYSCS_UTIL.SYSCS_RELOAD_SECURITY_POLICY() Reset a user's password. Set/delete the value of a property of the database in current connection. <p> Will be called as SYSCS_UTIL.SYSCS_SET_DATABASE_PROPERTY. Set the connection level authorization for a specific user - SYSCS_UTIL.SYSCS_SET_USER_ACCESS. this procedure switches between the different xplain modes This procedure sets the current xplain schema. If the schema is not set, runtime statistics are captured as a textual stream printout. If it is set, statisitcs information is stored in that schema in user tables. Unfreeze the database. <p> Call internal routine to unfreeze the database, which was "freezed" by calling SYSCS_FREEZE_DATABASE(). can be made. Update the statistics for 1)all the indexes or 2)a specific index on a table. <p> Calls either "alter table tablename all update statistics " sql or "alter table tablename update statistics indexname" sql This routine will be called when an application calls: SYSCS_UTIL.SYSCS_UPDATE_STATISTICS <p> Create a new user (this entry is called when bootstrapping the credentials of the DBO at database creation time. Do following checks a)Schema name can't be empty string b)If schema name is null, then we use current schema c)Table name can't be null or empty string Raise an exception if the user doesn't exist. See commentary on DERBY-5648. Create the XPLAIN table if it doesn't already exist. Also, make a first order check that we'll be able to insert rows into the table, by preparing the INSERT statement for the table. The actual INSERT statment is saved, as simple string text, in the LCC, to be executed later when the runtime statistics are being collected.  Get the DatabaseMetaData for the current connection for use in mapping the jcc SYSIBM.* calls to the Derby DatabaseMetaData methods Get the default or nested connection corresponding to the URL jdbc:default:connection. We do not use DriverManager here as it is not supported in JSR 169. IN addition we need to perform more checks for null drivers or the driver returing null from connect as that logic is in DriverManager. Helper routine which looks up the monitor. Helper for SQLForeignKeys and SQLTables This method exists so that we can get a property value without performing authorization checks. Helper for ODBC metadata calls. Normalize the user name so that there is only one set of credentials for a given authorization id. Utility method for SYSCS_SET_USER_ACCESS removes a user from one of the access lists, driven by the property name. Reset the password for an already normalized authorization id. issue a rollback when SQLException se occurs. If SQLException ouccurs when rollback, the new SQLException will be added into the chain of se.
Check the consistency of the table descriptor held by this TDCacheable versus an uncached table descriptor. Cacheable interface  Get the table descriptor that is associated with this Cacheable
Followng call waits until post commit thread queue is empty. This call is useful for tests which checks for the following type of cases: 1) Checking for space usage after delete statements 2) Checking for locks when delete statements are involved, because post commit thread might be holding locks when checking for snap shot of locks, so best thing to do to get consistent results is to call the following function before checking for locks (eg: store/updatelocks.sql) 3) Depending on whethere the  space is not released yet by the post commit thread for commited deletes or not can change the order of rows in the heap. In such cases , it is good idea to call this method before doing inserts(Even adding/dropping constraints can have effect because they do inderectly deletes/inserts on system tables.) eg: lang/fk_nonsps.sql
Test the access level alter table interface for adding columns. <p> Open a scan on the conglomerate for the given conglom id, and verify that it has rows with the given test value.  This is a way of verifying that we got the right conglomerate.  Returns the number of rows that it checked (-1 if the conglomerate doesn't exist). test various flavors of commit Privileged startup. Must be private so that user code can't call this entry point. Insert a single row with a single column containing the first argument integer, delete it, make sure subsequent delete, replace, and replace a single column return false.  Privileged startup. Must be private so that user code can't call this entry point. * Methods of T_AccessFactory. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods of UnitTest. * Methods required by T_Generic Test the access level getTableProperties() call. <p> test various flavors of commit Insert a single row with a single column containing the argument integer, and fetch it back, making sure that we read the correct value.  Insert a single row with a single column containing the first argument integer, update it to the second value, and make sure the update happened.  Simple insert into heap performance test Test partial scans. <p> test position at row location, in terms of holdability Test critical cases for read uncommitted. <p> test 1 - test heap fetch, delete and replace of row on page which does not exist. test 2 - test heap fetch, delete and replace of row on page where row does not exist.  Test the access level ScanInfo interface. <p> Test the access level SortCost interface. <p> Test the access level StoreCost interface. <p> Test temporary conglomerates. Test transactional properties
Set the number of columns in the row to ncols, preserving the existing contents.
Verify that the database enforces the expected access mode appropriatly. This function depends on DDL performed by the authorize.jsql test.
Arm a bomb to go off. If the bomb does not exist make it. Cause a bomb to explode. If the bomb does not exist make it. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. Make an armed bomb set to go off in 1 hour.


* The tests Test the find and findCached calls. Get the name of the protocol for the module to test. This is the 'factory.MODULE' variable. 'moduleName' to the name of the module to test.  Privileged startup. Must be private so that user code can't call this entry point. A call to findCached() that is expected to return nothing. A call to findCached() that is expected to find something. A call to find() that is expected to return nothing. A call to findCached() that is expected to find something. * Multi-user tests
T_CachedInteger range - 0 - 100 pick a key randomly 48%/48%/4% chance of Int/String/invalid key 90%/5%/5% chance of can find / can't find / raise exception 50%/30%/20% find/findCached/create
* Implementation specific methods Returns true of the object is dirty. Will only be called when the object is unkept. <BR> MT - thread safe * Cacheable methods
Put the object into the No Identity state. <BR> MT - single thread required - Method must only be called be cache manager and the cache manager will guarantee only one thread can be calling it.  Get the identity of this object. <BR> MT - thread safe. * Cacheable methods
see if 2 byte arrays are identical private void testBlowfish() { System.out.println("Running testBlowfish"); try { // set up the provider java.security.Provider sunJce = new com.sun.crypto.provider.SunJCE(); java.security.Security.addProvider(sunJce); // String key = "Paula bla la da trish123 sdkfs;ldkg;sa'jlskjgklad"; String key = "Paulabla123456789012345"; byte[] buf = key.getBytes(); System.out.println("key length is " + buf.length); SecretKeySpec sKeySpec = new SecretKeySpec(buf,"Blowfish"); // SecretKeySpec sKeySpec = new SecretKeySpec(buf,"DESede"); Cipher cipher = Cipher.getInstance("Blowfish/CBC/NoPadding"); // Cipher cipher = Cipher.getInstance("DESede/CBC/NoPadding"); // Cipher cipher = Cipher.getInstance("Blowfish/CBC/PKCS5Padding"); cipher.init(Cipher.ENCRYPT_MODE,sKeySpec); // only works with NoPadding if size is a multiple of 8 bytes // with PKCS5Padding, works for all sizes byte[] original = "This is what should get encrypte".getBytes(); System.out.println("original length is " + original.length); byte[] encrypted = cipher.doFinal(original); // works // AlgorithmParameters algParam = cipher.getParameters(); byte[] iv = cipher.getIV(); System.out.println("length of iv is " + iv.length); Cipher cipher2 = Cipher.getInstance("Blowfish/CBC/NoPadding"); // Cipher cipher2 = Cipher.getInstance("DESede/CBC/NoPadding"); // Cipher cipher2 = Cipher.getInstance("Blowfish/CBC/PKCS5Padding"); // works // cipher2.init(Cipher.DECRYPT_MODE,sKeySpec,algParam); IvParameterSpec ivClass = new IvParameterSpec(iv); cipher2.init(Cipher.DECRYPT_MODE,sKeySpec,ivClass); byte[] decrypted = cipher2.doFinal(encrypted); if (byteArrayIdentical(original,decrypted,0,original.length)) System.out.println("PASSED"); else System.out.println("FAILED"); System.out.println("original length is " + original.length); System.out.println("encrypted length is " + encrypted.length); System.out.println("decrypted length is " + decrypted.length); } catch (Throwable t) { System.out.println("got an exception"); t.printStackTrace(); } System.out.println("Finished testBlowfish"); } private void testCryptix() { System.out.println("Running testCryptix"); try { // set up the provider Class jceClass = Class.forName("cryptix.jce.provider.Cryptix"); java.security.Provider cryptixProvider = (java.security.Provider) jceClass.newInstance(); java.security.Security.addProvider(cryptixProvider); byte[] userkey = "a secret".getBytes(); System.out.println("userkey length is " + userkey.length); Key secretKey = (Key) (new SecretKeySpec(userkey, "DES")); byte[] IV = "anivspec".getBytes(); Cipher enCipher = Cipher.getInstance("DES/CBC/NoPadding","Cryptix"); Cipher deCipher = Cipher.getInstance("DES/CBC/NoPadding","Cryptix"); IvParameterSpec ivspec = new IvParameterSpec(IV); enCipher.init(Cipher.ENCRYPT_MODE,secretKey,ivspec); deCipher.init(Cipher.DECRYPT_MODE,secretKey,ivspec); int patternLength = 8; byte[] pattern = new byte[patternLength]; for (int i = 0; i < patternLength; i++) pattern[i] = (byte)(i & 0xFF); byte[] cipherOutput1 = new byte[patternLength]; byte[] cipherOutput2 = new byte[patternLength]; int retval = 0; retval = enCipher.doFinal(pattern, 0, 8, cipherOutput1, 0); retval = deCipher.doFinal(cipherOutput1, 0, 8, cipherOutput2, 0); if (byteArrayIdentical(cipherOutput2,pattern,0,patternLength)) System.out.println("PASSED TEST 1"); else System.out.println("FAILED TEST 1"); retval = deCipher.doFinal(cipherOutput1, 0, 8, cipherOutput2, 0); if (byteArrayIdentical(cipherOutput2,pattern,0,patternLength)) System.out.println("PASSED TEST 2"); else System.out.println("FAILED TEST 2"); } catch (Throwable t) { System.out.println("got an exception"); t.printStackTrace(); } System.out.println("Finished testCryptix"); } private void testMessageDigest() { // No provider needs to be installed for this to work. try { MessageDigest md = MessageDigest.getInstance("MD5"); byte[] data = "Paulas digest".getBytes(); byte[] digest = md.digest(data); byte[] digest2 = md.digest(data); if (byteArrayIdentical(digest,digest2,0,digest.length)) System.out.println("PASSED"); else System.out.println("FAILED"); System.out.println("data length is " + data.length); System.out.println("digest length is " + digest.length); System.out.println("digest2 length is " + digest2.length); } catch (Throwable t) { System.out.println("got an exception"); t.printStackTrace(); } System.out.println("Finished testBlowfish"); } // PT private void testPCBC() { System.out.println("Running testPCBC"); try { // set up the provider Class jceClass = Class.forName("com.sun.crypto.provider.SunJCE"); java.security.Provider myProvider = (java.security.Provider) jceClass.newInstance(); java.security.Security.addProvider(myProvider); // java.security.Provider sunJce = new com.sun.crypto.provider.SunJCE(); // java.security.Security.addProvider(sunJce); // String key = "Paula bla la da trish123 sdkfs;ldkg;sa'jlskjgklad"; String key = "PaulablaPaulablaPaulabla"; byte[] buf = key.getBytes(); System.out.println("key length is " + buf.length); SecretKeySpec sKeySpec = new SecretKeySpec(buf,"DESede"); Cipher cipher = Cipher.getInstance("DESede/PCBC/NoPadding"); // Cipher cipher = Cipher.getInstance("DESede/CBC/NoPadding"); // Cipher cipher = Cipher.getInstance("Blowfish/CBC/PKCS5Padding"); cipher.init(Cipher.ENCRYPT_MODE,sKeySpec); // only works with NoPadding if size is a multiple of 8 bytes // with PKCS5Padding, works for all sizes byte[] original = "This is what should get encrypte".getBytes(); System.out.println("original length is " + original.length); byte[] encrypted = cipher.doFinal(original); // works // AlgorithmParameters algParam = cipher.getParameters(); byte[] iv = cipher.getIV(); System.out.println("length of iv is " + iv.length); Cipher cipher2 = Cipher.getInstance("DESede/PCBC/NoPadding"); // Cipher cipher2 = Cipher.getInstance("DESede/CBC/NoPadding"); // Cipher cipher2 = Cipher.getInstance("Blowfish/CBC/PKCS5Padding"); // works // cipher2.init(Cipher.DECRYPT_MODE,sKeySpec,algParam); IvParameterSpec ivClass = new IvParameterSpec(iv); cipher2.init(Cipher.DECRYPT_MODE,sKeySpec,ivClass); byte[] decrypted = cipher2.doFinal(encrypted); if (byteArrayIdentical(original,decrypted,0,original.length)) System.out.println("PASSED"); else System.out.println("FAILED"); System.out.println("original length is " + original.length); System.out.println("encrypted length is " + encrypted.length); System.out.println("decrypted length is " + decrypted.length); } catch (Throwable t) { System.out.println("got an exception"); t.printStackTrace(); } System.out.println("Finished testPCBC"); } private void testPCBC2() { System.out.println("Running testPCBC2"); try { // set up the provider Class jceClass = Class.forName("com.sun.crypto.provider.SunJCE"); java.security.Provider myProvider = (java.security.Provider) jceClass.newInstance(); java.security.Security.addProvider(myProvider); byte[] userkey = "a secreta secreta secret".getBytes(); System.out.println("userkey length is " + userkey.length); Key secretKey = (Key) (new SecretKeySpec(userkey, "DESede")); byte[] IV = "anivspec".getBytes(); Cipher enCipher = Cipher.getInstance("DESede/PCBC/NoPadding","SunJCE"); Cipher deCipher = Cipher.getInstance("DESede/PCBC/NoPadding","SunJCE"); IvParameterSpec ivspec = new IvParameterSpec(IV); enCipher.init(Cipher.ENCRYPT_MODE,secretKey,ivspec); deCipher.init(Cipher.DECRYPT_MODE,secretKey,ivspec); int patternLength = 24; byte[] pattern = new byte[patternLength]; for (int i = 0; i < patternLength; i++) pattern[i] = (byte)(i & 0xFF); byte[] cipherOutput1 = new byte[patternLength]; byte[] cipherOutput2 = new byte[patternLength]; int retval = 0; retval = enCipher.doFinal(pattern, 0, 24, cipherOutput1, 0); retval = deCipher.doFinal(cipherOutput1, 0, 24, cipherOutput2, 0); if (byteArrayIdentical(cipherOutput2,pattern,0,patternLength)) System.out.println("PASSED TEST 1"); else System.out.println("FAILED TEST 1"); retval = deCipher.doFinal(cipherOutput1, 0, 24, cipherOutput2, 0); if (byteArrayIdentical(cipherOutput2,pattern,0,patternLength)) System.out.println("PASSED TEST 2"); else System.out.println("FAILED TEST 2"); } catch (Throwable t) { System.out.println("got an exception"); t.printStackTrace(); } System.out.println("Finished testPCBC2"); } private void testIAIK() { System.out.println("Running testIAIK"); try { // set up the provider Class jceClass = Class.forName("iaik.security.provider.IAIK"); java.security.Provider myProvider = (java.security.Provider) jceClass.newInstance(); java.security.Security.addProvider(myProvider); // iaik.security.provider.IAIK.addAsProvider(true); // iaik.utils.Util.loadClass("iaik.security.provider.IAIK",true); // IAIK p=new IAIK(); // iaik.security.provider.IAIK.getMd5(); byte[] userkey = "a secret".getBytes(); System.out.println("userkey length is " + userkey.length); Key secretKey = (Key) (new SecretKeySpec(userkey, "DES")); byte[] IV = "anivspec".getBytes(); Cipher enCipher = Cipher.getInstance("DES/CBC/NoPadding","IAIK"); Cipher deCipher = Cipher.getInstance("DES/CBC/NoPadding","IAIK"); IvParameterSpec ivspec = new IvParameterSpec(IV); enCipher.init(Cipher.ENCRYPT_MODE,secretKey,ivspec); deCipher.init(Cipher.DECRYPT_MODE,secretKey,ivspec); int patternLength = 8; byte[] pattern = new byte[patternLength]; for (int i = 0; i < patternLength; i++) pattern[i] = (byte)(i & 0xFF); byte[] cipherOutput1 = new byte[patternLength]; byte[] cipherOutput2 = new byte[patternLength]; int retval = 0; retval = enCipher.doFinal(pattern, 0, 8, cipherOutput1, 0); retval = deCipher.doFinal(cipherOutput1, 0, 8, cipherOutput2, 0); if (byteArrayIdentical(cipherOutput2,pattern,0,patternLength)) System.out.println("PASSED TEST 1"); else System.out.println("FAILED TEST 1"); retval = deCipher.doFinal(cipherOutput1, 0, 8, cipherOutput2, 0); if (byteArrayIdentical(cipherOutput2,pattern,0,patternLength)) System.out.println("PASSED TEST 2"); else System.out.println("FAILED TEST 2"); } catch (Throwable t) { System.out.println("got an exception"); t.printStackTrace(); } System.out.println("Finished testIAIK"); } private void printByteArray(String name, byte[] array) { System.out.println("printing array " + name); for (int i = 0; i < array.length; i++) System.out.println("index " + i + " : " + array[i]); } Delete a file in a Privileged block as these tests are run under the embedded engine code. * Methods required by T_Generic Privileged startup. Must be private so that user code can't call this entry point.





Methods of ColumnOrdering
Loggable methods methods to support prepared log the following two methods should not be called during recover Compensation methods. Formatable methods
Check to make sure that there are no active dependencies (stored or in memory). Check to make sure that there are no open conglomerates, scans or sorts. Delete the first row from the heap, without deleting it from the indexes on the table. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Get the various contexts Get the various descriptors Get a heap row full of nulls Return the ConglomerateDescriptor for the index Get a template row for the specified index Get the first row from the heap and insert it into the specified index, with a bad row location, without inserting it into the heap or the other indexes on the table. Set all of the columns in the first row from the heap to null, without updating the indexes on the table. Open the heap conglomerate for update Open the index conglomerate for update Open an unqualified scan on the heap for update Open an unqualified scan on the index for update Get the first row from the heap and insert it into the heap again, without inserting it from the indexes on the table. Run all of the consistency checkers which do not take parameters. Actually, just run the ones that "make sense" to run.  Today, that is: countOpens() Swap the values in the specified columns of the first row from the heap, without updating the indexes on the table.
tests test 1 - basic subscription test 1 - basic enqueue test 4 - mixture * Methods required by T_Generic * Methods required by T_MultiThreadedIterations * @exception T_Fail unexpected behaviour from the API  * Methods required by T_MultiIterations * @exception T_Fail unexpected behaviour from the API Privileged startup. Must be private so that user code can't call this entry point.
* Methods required by T_Generic Driver routine for the btree secondary index tests. <p> Public Methods of T_MultiIterations class: Routine one once per invocation of the test by the driver. <p> Do work that should only be done once, no matter how many times runTests() may be executed. Private/Protected methods of This class: Simple test of DiagnosticUtil interfaces. <p> Simple test of DiagnosticUtil.toDiagString() and DiagnosticUtil.findDiagnostic() interfaces.
Check a test condition. If it is false, throw a T_Fail exception. Check a test condition. If it is false, throw a T_Fail exception which includes a message. return a T_Fail exception to indicate the test failed due to an exception. <P>Note: Since the Test Service catches all exceptions this seems to be of limited value. return a T_Fail exception to indicate the configuration does not specify the module to test. return a T_Fail exception to indicate the configuration does not contain the module to test. return a T_Fail exception to indicate the test failed. return a T_Fail exception which includes a user message indicating why a test failed.
MT tests on  the same container   create a container that all threads can use Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic T_MultiThreadedIteration method run the test Run the tests
* Public methods of UnitTest UnitTest.Execute Emit a message indicating why the test failed. RESOLVE: Should this be localized? Emit a message saying the test passed. You may use this to emit messages indicating individual test cases within a unit test passed. <P>RESOLVE:Localize this. Emit a message during a unit test run, indent the message to allow the PASS/FAIL messages to stand out. UnitTest.UnitTestDuration UnitTest.UnitTestType * Public methods of ModuleControl ModuleControl.start Get the name of the protocol for the module to test. This is the 'factory.MODULE' variable. 'moduleName' to the name of the module to test. Abstract methods to implement for your test. Run the test. The test should raise an exception if it fails. runTests should return if the tests pass. ModuleControl.stop
Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic  Test Qualifiers.
48%/48%/4% chance of Int/String/invalid key 90%/5%/5% chance of can find / can't find / raise exception
* Lockable methods (Simple, qualifier assumed to be null). Qualififier is assumed to be null. Qualififier is assumed to be null.
* Lockable methods (Simple, qualifier assumed to be null), allows * up to 'allowed' lockers in at the same time.
override to mark the test as failed when dumping the message. testing support: * Methods required by T_Generic  testing mechanism:
* Multi-user tests. Multi-user test 001. Create two lockable objects and pass them off to two threads. Each thread will run lock the first object, set its value then lock the second object and set its value, yield and then release the lock on one and then on two. Various checks are made to ensure the values are as expected. Multi-user test 002 Create a single lockable and have three threads lock it, yield and then release it. The single lockable can only have one locker. Multi-user test 003 Create a single lockable and have three threads lock it, yield and then release it. The single lockable is a semaphore that can have two lockers. Multi-user test 004 As M003 but each thread will lock the object twice, to ensure that lock manager grantes the lock when the compatability space and qualifier match. * Test functions Single user API test 001. Lock an single object in a single group with all lock methods and then unlock the object with all unlock methods. Single user API test 002. Lock an object in different groups and check unlocks apply to a single group. Single user API test 003. Lock multiple objects in different groups and check unlocks apply to a single group. Single user API test 004. Lock multiple objects in different groups and transfer locks between groups. Single user API test 005. Create two compatability spaces and ensure that locks block each other out. Single user API test 007. Tests on groups and compatibility spaces never seen by the lock manager. * Utility functions Check to see if the total number of locks we have is as expected. Check to see if the number of locks in a group we have is as expected. * The tests Run once per-iteration to run the actual test. Run all the tests, each test that starts with 'S' is a single user test, each test that starts with 'M' is a multi-user test. Privileged startup. Must be private so that user code can't call this entry point.
Methods required by T_Generic
Run once per-iteration to run the actual test. * methods required by T_Generic Run the test. The test should raise an exception if it fails. runTests should return if the tests pass. *	  Abstract methods to implement for your test. Run once to set up the test.
class specific method multi threaded test abstract methods joins an existing setup - do whatever remaining setup the test may need to do given that setupTest has already been run by another test object This call will be executed in the main (parent) thread make a new test object instance run each worker test thread Run the test. The test should raise an exception if it fails. runTests should return if the tests pass.
C010 - Create a container within a transaction, commit and the re-open the container twice. C011 - Create a container withina transaction, commit and the re-open the container in update and non-update mode. C012 - Drop a container within a transaction, commit, see that it is deleted. Drop a container within a transaction, rollback and re-open and see that it is not deleted. C014 - Open a container for locking only.  C201 - Create container with different page size, minimum record size, inserting into these containers to check if the variables are set correctly. Test checkpoint test writing out large log records Page tests Create a container, ensure it has one page with no records. Then test all the things we can do with an empty page opened read-only in the container. Then add a new page, ensure it has the correct page number and is empty. Insert rows on the first page until the page is full, then add a page and repeat the test (for a total of three pages with full rows). Fetch the rows back by handle and slot methods. test repeated insert P006 test page time stamp - make sure all operation changes page time stamp P007 this test exercises repeated updates on a 1K page 2 rows (with 1 column) will be inserted into the page. We expand the row data in slot 0 by 1 byte until the page is completely full, and overflows the record to an overflow page. P008 this test exercises repeated inserts with small rows on a 1K page we will insert as many rows as possible into the page.  Then we reduce the row by 1 byte at a time, we will try to insert another smaller row. This test also tests spaceForInsert(). P009 this test exercises repeated shrinking and expanding of fields using updateFieldBySlot we will insert as many rows as possible into the page. Then set some of the columns to null, That should not create more space on the page for inserts, because the extra space become reservedspace for the row.  So, the next insert should fail. P011 this test exercises insertAtSlot, (LogicalUndo)null P012 this test exercises updateAtSlot P013 this test exercises deleteAtSlot and isDeletedAtSlot P014 this test exercises purgeAtSlot P015 this test exercises updateAtSlot P016 this test exercises copyAndPurge @exception T_Fail Unexpected behaviour from the API @exception StandardException Unexpected exception from the implementation P017 this test getInvalidRecordHandle and makeRecordHandle @exception T_Fail Unexpected behaviour from the API @exception StandardException Unexpected exception from the implementation P018 this test exercises track # 590, test that copyRows successfully notices that a copy can't be done. @exception T_Fail Unexpected behaviour from the API @exception StandardException Unexpected exception from the implementation Test bulk load and preallocation Test create container with initial page set to 100 pages Test preAllocate Test minimumRecordSize: this is to make sure that logRow and storeRecord are consistent with each other when it comes to reserve space. Test overflowThreshold: this is to make sure that logRow and storeRecord are consistent with each other when it comes to reserve space. Test that latches are exclusive. Insert small rows and update them so that they overflow a page. Insert 4-column long rows into 1K pages, each column is less than a page. Insert 60-column long rows into 1K pages, each column is less than a page. Insert 100-column long rows into 1K pages, each column is less than a page. Insert 401 column long row with many small columns in the beginning, and one large column at the end into 4K pages. Insert a single long column long row into a 1K page. Test space reclaimation - purging of a long row gets back all the row pieces. Test space reclaimation - purging of a row with serveral long columns get back all the column chains. Test space reclaimation - rollback of an insert (with purge) of a row that overflows and with long column get back all the space in the row and column chain. Test space reclaimation - shrink a head row piece. Test space reclaimation - shrink a non head row piece. Test space reclaimation - update a long column to another long column. Test space reclaimation - rollback of an update that create a long column. Test space reclaimation - rollback of an update that create a new row piece. Test that post commit processor does not stubbify a drop table that is rolled back in a savepoint Test rollback of Page.insert Test insertAtSlot that rolls back with a purge Test internal transaction Test rollback of partial row update. Create a long row with 10 columns on 2 pages (5 columns on each page). Update the 1st column on the 2nd page (the 6th column) which causes the last column (10th column) to move off the page. Then abort and make sure that all the original columns are there and correct. NOTE: stored length is twice string length + 2 Test rollback of partial row update. Create a long row with 15 columns on 3 pages (5 columns on each page). Update the 1st column on the 2nd page (the 6th column) which causes the last column of that page (10th column) to move off the page. Then abort and make sure that all the original columns are there and correct. NOTE: stored length is twice string length + 2 Sparse row test. Test sparse representation of rows using the FormatableBitSet class. Insert, fetch and update a row having gaps. Serializable column test. Want to make sure we hit some otherwise dead code in StoredPage, used for storing/reading Serializable/Externalizable data to/from a page. * Update and update partial tests aimed at long rows Insert a single row and keep updating it, adding columns not using partial rows. * Update and update partial tests aimed at long rows Insert a single row and keep updating it, adding columns using partial rows. Simple set of partial row updates on a singel page with shrinking and expanding columns. Insert a single row with multiple portions. Update fields in the various portions that grow. Same as 704 but update fields in the reverse order. Insert a single row with single or multiple portions. Update every other field with a long col The update each column back to a null Insert a single record that has several chunks and every other column is a long column Insert a single row with single or multiple portions. Update every other field with a long col rollback. The update each column back to a null and rollback P709: this test exercises purgeAtSlot , rollsback and purges the slot again, to make sure not logging the data does not have any impact on repurging the rollbacked purges. Test space reclaimation - purging of a long rows with a rollback and purging againg with no data logging for purges Test space reclaimation - purging of a row with serveral long columns rollback and repurge them again. * The tests *		Tnnn indicates a test that is mainly testing the Transaction interface *		Cnnn indicates a test that is mainly testing the ContainerHandle interface *		Pnnn indicates a test that is mainly testing the Page interface * *	nnn < 200 tends to indicate purely API tests, ie checking methods *		are callable and return the right value. This includes negative tests. * *  nnn >= 200 tends to indicate more involved tests, ie ones that test the *			methods actually did something. T000 - ensure a transaction starts out idle. T001 - start and commit an empty transaction. T002 - start and abort an empty transaction. T003 - start and commit an empty transaction and then ensure that the transaction remains open for another commit. T004 - start and abort an empty transaction and then ensure that the transaction remains open for a commit and another abort. T005 check transaction identifiers on idle transactions. T006 - savepoint basic API testing T007 - savepoint nesting testing T008 - savepoint  testing, ensure save points disappear at commit or abort. T009 - add a container and remove it within the same transaction. T010 - add a container with a default size and remove it within the same transaction. T011 - see that a container remains open over the commit of an open transaction.. Test Xact.makeRecordHandle() TC001 - Test the drop on commit mode for temp containers. A clone of P002 for temporary containers. Insert rows on the first page until the page is full, then add a page and repeat the test (for a total of three pages with full rows). Fetch the rows back by handle methods. Commit or abort the transaction, and see if table is empty. Can be used as follows: <PRE> mode                   doCommit TRUNCATE_ON_COMMIT     true       Ensure the table has only one empty page after commit TRUNCATE_ON_COMMIT     false      Ensure the table has only one empty page after abort 0                      false      Ensure the table has only one empty page after abort </PRE> Add a number of rows to a temp table opened in various modes, and drop it before commit/abort. Open a temp table several time swith different modes and ensure the correct behaviour (most severe open wins). Open a temp table several times with different modes and ensure the correct behaviour (most severe open wins).  Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic Privileged Monitor lookup. Must be private so that user code can't call this entry point. T_MultiThreadedIteration method code to populate a temp table for some temp tests run the test Set up test

test recovery of test 1 test recovery of test 2 test recovery of test3 test recovery of test 4 test recovery of test 5 test recovery of test 6 test recovery of test 7 test1 manufactures a log with the following recoverable 'defects': - a log file that only have a single large 1/2 written log record test2 manufactures a log with the following recoverable 'defects': - a log file that ends with a large 1/2 written log record test3 manufactures a log with the following recoverable 'defects': - a log with multiple files but no checkpoint log record - a last log file with a paritally written log record at the end test4 manufactures a log with the following recoverable 'defects': - a log file that only has the partial log instance(7 bytes instead of 8 bytes writtne) of a log record written test5 manufactures a log with the following recoverable 'defects': - a log file that only has the partial log record length (3 bytes instead of 4 bytes writtne) of a log record written in the beginning test6 manufactures a log with the following recoverable 'defects': - a log file that only has the log record with partial data portion written (approximately (1997/2 (data)+ 16(log records ov))) test7 manufactures a log with the following recoverable 'defects': - a log file that has the last log record with partial end length written( 3 of 4 bytes). instead of (1997(data) + 16 (log records overhead)) write (1997 + 15) Privileged startup. Must be private so that user code can't call this entry point. Privileged startup. Must be private so that user code can't call this entry point.  Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic See T_Recovery for the general testing frame work simulate log corruption to test the checksuming of log records. Privileged startup. Must be private so that user code can't call this entry point.
Privileged startup. Must be private so that user code can't call this entry point. fill up the log immediately Privileged service lookup. Must be private so that user code can't call this entry point.  Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic See T_Recovery for the general testing frame work Privileged startup. Must be private so that user code can't call this entry point. A basic routine to write a bunch of stuff to the log There will be some committed transactions, some uncommitted transactions, serveral checkpoints.
recover test 1 recover test 2 recover test 3 recover test 4 recover test 5 recover test 6 recover test 7 recover test 8 recover test 9 recover test 20 recover test 022 - drop container recover S100 recover test 101 recover test 200 recover test 201 recover test 202 recover test 203 test recovery of 204 test recovery of 300 test recovery of 301 recover test 302 recover test 303: repurge the same rows whose purging we rolled back in s303. recover test 304: repurge the same rows whose purging we rolled back in s304 at recovery. test 1 - insert 1 row test 2 - insert a bunch of rows into one container test 3 -  update row test 4 - update field test 5 - purge and delete test 6 - page allocation test 7 - page deallocation test 8 - page deallocation with rollback test 9 - deallocation and reuse pag test 10 - allocation/deallocation with overflow page test 11 - allocate a lot of pages so that we need > 1 allocation pages test 12 - test estimated page count test 20 - create multiple containers test 022 - drop containers test 100 - multiple intervening committed transactions test 101 - transaction with rollback to savepoint the following tests has recovery undo work, cannot test Rnnn during setup because it hasn't been rolled back during setup yet.  Test the state in Snnn. test 200 - incomplete transaction test 201 - multiple intervening incomplete transaction test 202 - incomplete transaction with rollback to savepoints test 203 -  incomplete and committed and aborted transaction with intervening rollback to savepoints test 204 - incomplete and committed and aborted internal transactions test 300 - incomplete transaction with drop containers incomplete transactions with create container test 302 - purge and delete with no data logging for purges Test space reclaimation - purging of a row with serveral long columns rollback and repurge them again. Test space reclaimation - purging of a long rows with a rollback and purging again after recovery in R304 S999 - last test of the recovery unit test, this leave the end of the log in a fuzzy state, do NOT write any more log record after this or you will corrupt the database DO NOT run this test if recovery test is ever run iteratively Privileged startup. Must be private so that user code can't call this entry point. Privileged service lookup. Must be private so that user code can't call this entry point.  Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic Privileged Monitor lookup. Must be private so that user code can't call this entry point. Tests in here come in pairs (Snnn Rnnn), one to set it up, one to test it after recovery.  Information that needs to be passed from the setup to the recovery should, ideally, be written out to a database.  For now, it is written out as a pair of (key,value) long in the file T_Recovery.info. To make sure you don't accidently tramples on someone else's key, encode your test number (nnn) by shifting over 32 bits and then add your key.  Multiple invocations which needs paramaters saved should be encoded futher. 001 &lt; nnn &lt; 200 -  no recovery undo 200 &lt; nnn &lt; 400 -  recovery undo Privileged startup. Must be private so that user code can't call this entry point.
closeRowSource tells the RowSource that it will no longer need to return any rows and it can release any resource it may have. Subsequent call to any method on the RowSource will result in undefined behavior.  A closed rowSource can be closed again. Get the next row as an array of column objects. The column objects can be a JBMS Storable or any Serializable/Externalizable/Formattable/Streaming type. Get a copy of the template row.  Cast each column to a CloneableObject and clone it. getValidColumns describes the DataValueDescriptor[] returned by all calls to the getNextRowFromRowSource() call. methods for RowSource  needsRowLocation returns true iff this the row source expects the drainer of the row source to call rowLocation after getting a row from getNextRowFromRowSource.  rowLocation  is not implemented here set all column of the row to integer object
Return the secondary index row. <p> Return the DataValueDescriptor array that represents the branch row, for use in raw store calls to fetch, insert, and update. <p> Private/Protected methods of This class: Public Methods of T_SecondaryIndexRow class: get the rows location field. private RowLocation getRowLocationField() throws StandardException { return(init_rowlocation); } Initialize the class. <p> Save away pointers to the base table template row, and the rowlocation class.  Build default map of base columns to key columns, this map can be changed with setMap(). <p>
Serviceable interface @return true, if this work needs to be done on a user thread immediately test utilities
Methods of SortObserver Methods of SortObserver Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods of T_SortController  Test a sort where all the rows are duplicates Test a sorts with one or zero rows. This test covers specific code paths in the external sort's sort buffer.  It really should live closer to the sort buffer since the effectiveness of this test is very very implementation dependent. Test a sort where we have some ascending and descending keys. This test is more of an example, with lots of comments to explain what's going on. Insert the given rows into the given sort, and check that the rows retrieved from the sort match the output rows. Methods of SortObserver

Get the estimated cost. Get the estimated row count. Set the estimated cost. Set the estimated row count.
create a stream container load with rowCount number of rows. fetch it all back, and check to make sure all rows are correct. this test test the rowSource over head. when param set to 1, also gets the overhead for writeExternal for Storables  create a container that all threads can use Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic T_MultiThreadedIteration method run the test Run the tests

Privileged Monitor lookup. Must be private so that user code can't call this entry point. Run all the tests, each test that starts with 'S' is a single user test, each test that starts with 'M' is a multi-user test. * Tests
Loggable methods Undoable methods. methods to support prepared log the following two methods should not be called during recover Object methods. Formatable methods

Return a string of stringLen characters that starts with data and is padded with nulls. Add in encryption parameters to the startParam if "testDataEncryption" is set to a non-null string. abort a transaction with context abort a transaction Add a new container in the transaction Add a new container in the transaction with a specified page size Add a new container in the transaction with specified pageSize, spareSpace, minimumRecordSize, and reusableRecordId Add a page to a container. Take an empty page and check it does actually seem to be empty. Fetch a record from a container that is expected to exist using a record handle. Calls recordExists() before fetch to ensure that the record is there. Fetch a record that is expected to exist using a record handle. The record contains the values in the passed in row, which is a T_RawStoreRow.  A T_RawStoreRow of the same number of columns will be made and fetched from the page and compared with the passed in row. Fetch a record that is expected to exist using a record handle. The record has a T_RawStoreRow of 1 column and this column as value as specified by data, which could be null. Calls recordExists() before fetch to ensure that the record is there. Fetch and check the slot on the page. The slot number is NOT a stable reference once the page is unlatched, this check is only valid if you know the page has not been unlatched since you put the row in, or you know nobody has touched the page since you determined the slot number The slot refers to a row in the page which has a T_RawStoreRow of 1 column, the column has the value of data input. @param page the page in question @param slot the slot number (see above) @param data the column value @param deleted if the row is deleted, set to true @param forUpdate If you want to lock the row for update, set forUpdate to true. Using sparse row representation: Fetch a column of a record that is expected to exist, using a record handle and a FormatableBitSet object. Check that column colNum has value data. check a column value from a slot on the page The slot number is NOT a stable reference once the page is unlatched, this check is only valid if you know the page has not been unlatched since you put the row in, or you know nobody has touched the page since you determined the slot number The storable in the specified column put into the input column and it is check for the same value as the input data @param page the page in question @param slot the slot number (see above) @param fieldId the field Id on the row @param column the storable to put the column in @param forUpdate true if you want to lock the row for update @param data the expected value in the column Fetch a deleted record from a container using a record handle. Check to make sure record is NOT there the following is a sequence of fetches, fetching the first row, fetching the next and previous rows, and fetching the last row in the page. The row is assumed to be a T_RawStoreRow with 1 column, which value is the string specified in data. fetch and check the first row in the page. Return the first row's recordHandle. Fetch and check the last row in the page. Return the last row's recordHandle Fetch and check the next (next to rh) row in the page. Return the next row's recordHandle Fetch and check the previous (previous to rh) row in the page. Return the previous row's recordHandle check the number of fields in the slot Check that it's not possible to get a page which is already latched by the same transaction. same as above, check an invalid savepoint in the given transaction context Save point checks Negative test - check that an invalid savepoint is detected. function that checks for a condition, throws T_Fail exception if the condition is not met. check that transaction does not hold any lock check that page number on the page matches the input page number check that the number of record on the page matches input. @param page the page in question @param count the total number of record - this include deleted as well as non-deleted @param nonDeleted the number of non-deleted record Check to make sure a row (possibly with overflow) is of the correct length Using sparse representation: Update a column of a record and check resulting value. close a transaction with context commit a transaction with context commit a transaction Drop a container Lazy people's random file generator: Generate a random file with specified name and file size Get the last page in a container. Always returns a valid page or null if there is no page in the container. Get a specific page in a container. Always returns a valid page. Insert a record on the last page, if the row doesn't fit on the last page create a new page and insert there. Call page.insert() and ensure that the return record handle is not null. This assumes the caller has called spaceForInsert. Call page.insert() and ensure that the return record handle is not null. This assumes the caller has called spaceForInsert. Call page.insert() and ensure that the return record handle is not null. This assumes the caller has called spaceForInsert. Call page.insert() and ensure that the return record handle is not null. This assumes the caller has called spaceForInsert. Open a container. Check to see the correct behaviour for read only operations that take a slot when the slot is out of range. Remove a page from a container. Start a user transaction, ensures that the startTransaction method does not return null (which it shouldn't). start an internal transaction function that actually do something, start, commit, abort a trasaction, get a page, insert a row, etc. Start a user transaction, ensures that the startTransaction method does not return null (which it shouldn't). start a user transaction with its own context (T_TWC) Update a record. Check to see the correct behaviour for update operations that take a slot when the slot is out of range. Make this thread wait a bit, probably for post commit to finish
************************************************************************ Utility methods. ************************************************************************* ************************************************************************ Test Cases. ************************************************************************* one phase commit xa transaction. <p> simple two phase commit xa transaction. <p> Test aborts of unprepared xa transaction. <p> Test aborts of prepared two phase commit xa transaction. <p> Very simple testing of the recover() call. <p> Very simple testing of changing a local transaction to a global. <p> Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Package protected so that user code can't call this entry point. * Methods of UnitTest. * Methods required by T_Generic
Utility routine to create base table for tests. <p> A little utility routine to create base tables for tests.  Just here to make tests a little more readable.  It currently just creates a heap table with "num_cols" SQLLongint columns. Privileged startup. Must be private so that user code can't call this entry point. Privileged startup. Must be private so that user code can't call this entry point. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Privileged lookup of the ContextService. Must be private so that user code can't call this entry point. * Methods required by T_Generic Check wheather the database is active or not Driver routine for the btree secondary index tests. <p> Methods required by T_MultiIterations Routine one once per invocation of the test by the driver. <p> Do work that should only be done once, no matter how many times runTests() may be executed. Test BTreeController.insert() <p> Just verify that insert code works for a secondary index.  Just call the interface and make sure the row got there. Test backout during critical times of splits. <p> Use trace points to force errors in split at critical points: leaf_split_abort{1,2,3,4} Test BTree.openScan(), BtreeScan.init(), BtreeScan.next(), BtreeScan.fetch(). Test qualifiers. Test Branch splits - number of rows necessary to cause splits is raw store implementation dependant (currently 5 rows per page in in-memory implementation). Test unimplemented interfaces. The following ScanController interfaces are not supported by the btree implementation, because row locations are not returned outside the interface.  At some point we may package a key as a row location but that does not really give any more functionality than using scan to find your key: ScanController.fetchLocation() ScanController.newRowLocationTemplate() ScanController.replace() ConglomerateController.delete() ConglomerateController.fetch() ConglomerateController.insertAndFetchLocation() ConglomerateController.newRowLocationTemplate() ConglomerateController.replace() Test multiple scans in a single page/no split Test multiple scans in a single table/with splits Test unique/nonunique indexes - both positive and negative cases. <p> Test restoreToNull Test Special cases of split. <p> Testing: restartSplitFor() call in LeafControlRow(). The first case is where we split down the tree and reach the leaf, pick a split point, and then find that there is not enough room to insert row into parent branch page. The second case is the same as the first except the calling code is trying to split a branch page and the parent branch page doesn't have room for the row. Test Special cases of split. <p> Testing: restartSplitFor() call in BranchControlRow(). The second case is the same as the first except the calling code is trying to split a branch page and the parent branch page doesn't have room for the row. Test backout during critical times of splits. <p> Force logical undo of an operation which generated an internal update of a btree record: case 1: o insert into unique btree key1, rowlocation_1 o delete from btree        key1, rowlocation_1 - this will mark the record logically deleted. o insert enough records to move the logically deleted row to another page to exercise logical undo of the delete. o insert into btree        key1, rowlocation_2 - this internally will generate a logical update field on the record. o insert enough records to move the logically deleted row to another page to exercise logical undo of the delete. o abort. case 2: o same as case 1 but don't change the rowlocation_1 value.  This simulates what the language will generate on an update of a key field. Test getTableProperties() of BTreeController. <p> Test latch release during critical time during row level locking. <p> Use trace points to force errors in split at critical points: leaf_split_abort{1,2,3,4} Test deadlocks during critical times of row level locking. <p> Use trace points to force errors in split at critical points: leaf_split_abort{1,2,3,4} Test BTree.openScan(), BtreeScan.init(), BtreeScan.next(), BtreeScan.fetch() with descending indexes. Test BTree.openScan(), BtreeScan.init(), BtreeScan.next(), BtreeScan.fetch() with alternating ascending and descending coulmn sort order indexes. Test BTree.openScan(), BtreeScan.init(), BtreeScan.next(), BtreeScan.fetch() with alternating ascending and descending coulmn sort order indexes. Test read uncommitted cases on scan. <p> Test latch release at critical time during delete on an index scan that uses update locks. test cases for ASC DESC DESC ASC column sort order index SORTED DATA col1, col2, col3 DS AS DS  -- sort order 9, 1, 21 7, 1, 20 6, 1, 19 5, 2, 16 5, 4, 17 5, 6, 18 4, 2, 13 4, 4, 14 4, 6, 15 3, 1, 12 1, 1, 11 test cases for ASC DESC ASC DESC column sort order index SORTED DATA col1, col2, col3 AS DS AS  -- sort order 1, 1, 11 3, 2, 12 4, 6, 15 4, 4, 14 4, 2, 13 5, 6, 18 5, 4, 17 5, 2, 16 6, 1, 19 7, 1, 20 9, 1, 21 delete a single key, given key value.  assumes 3 column table, first column = 1, second column is a unique key, and 3rd column is a RecordHandle into the base table. Test simple btree insert performance Test a single scan.
Given a key row, delete all matching heap rows and their index rows. <p> LOCKING: row locking if there is a key; otherwise, table locking. Delete the set of rows defined by a scan on an index from the table. Most of the parameters are simply passed to TransactionController.openScan. Please refer to the TransactionController documentation for details. <p> LOCKING: row locking if there is a start and a stop key; otherwise, table locking  Get the base column position for a column within a catalog given the (0-based) index number for this catalog and the (0-based) column number for the column within the index. Get the CatalogRowFactory for this. Get the Properties associated with creating the heap. Get the Properties associated with creating the specified index. Get the conglomerate for the heap. Get the column count for the specified index number. Get the conglomerate for the specified index. Get the index name. Get an index row based on a row from the heap. Get the IndexRowGenerator for the specified index number. Get the number of indexes on this catalog. Given a key row, return the first matching heap row. <p> LOCKING: shared row locking. Given a key row, return the first matching heap row. <p> LOCKING: shared row locking. Gets a row changer for this catalog.  Given an index row and index number return the RowLocation in the heap of the first matching row. Used by the autoincrement code to get the RowLocation in syscolumns given a &lt;tablename, columname&gt; pair. Get the table name. Inserts a base row into a catalog and inserts all the corresponding index rows. Inserts a list of base rows into a catalog and inserts all the corresponding index rows. Insert logic to insert a list of rows into a table. This logic has two odd features. <OL> <LI>Returns an indication if any returned row was a duplicate. <LI>Returns the RowLocation of the last row inserted. </OL> Is this fully initialized. (i.e., is all conglomerate info initialized) Return whether or not this index is declared unique Set the heap conglomerate for this. Set the index conglomerate for the table. Set the index conglomerate for the table. Set the IndexRowGenerator for the specified index number. Updates a base row in a catalog and updates all the corresponding index rows. Updates a set of base rows in a catalog with the same key on an index and updates all the corresponding index rows.
Empty the constraint descriptor list Empty the trigger descriptor list Gets all of the relevant constraints for a statement, given its statement type and its list of updated columns. Builds a list of all triggers which are relevant to a given statement type, given a list of updated columns. Tells if the index statistics for the indexes associated with this table are consideres up-to-date, and clears the state. gets an array of increment values for autoincrement columns in the target table. If column is not an autoincrement column, then increment value is 0. If table has no autoincrement columns, returns NULL. Get the provider's type. Return an array of collation ids for this table. <p> Return an array of collation ids, one for each column in the columnDescriptorList.  This is useful for passing collation id info down to store, for instance in createConglomerate(). This is only expected to get called during ddl, so object allocation is ok.  Get the descriptor for a column in the table, either by the column name or by its ordinal position (column number). Returns NULL for columns that do not exist. Gets the column descriptor list Turn an array of column names into the corresponding 1-based column positions. Gets an array of column names. Gets a conglomerate descriptor for the given table and conglomerate number. Gets a conglomerate descriptor for the given table and conglomerate UUID String. Gets the conglomerate descriptor list Gets a ConglomerateDescriptor[] to loop through all the conglomerate descriptors for the table. Gets array of conglomerate descriptors for the given table and conglomerate number.  More than one descriptors if duplicate indexes share one conglomerate. Gets array of conglomerate descriptors for the given table and conglomerate UUID.  More than one descriptors if duplicate indexes share one conglomerate. Gets the constraint descriptor list Privileged lookup of a Context. Must be private so that user code can't call this entry point.  Provider interface     Gets an ExecRow for rows stored in the table this describes. Gets the list of columns defined by generation clauses. Gets the id for the heap conglomerate of the table. There may also be keyed conglomerates, these are stored separately in the conglomerates table. Gets an object which lists out all the index row generators on a table together with their conglomerate ids. Returns the update criteria telling why the statistics are considered stale. <p> This method is used for debugging. Gets the lock granularity for the table. Gets the highest column id in the table. For now this is the same as the number of columns. However, in the future, after we implement ALTER TABLE DROP COLUMN, this correspondence won't hold any longer. Gets the name of the table. Gets the number of columns in the table. Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Gets the primary key, may return null if no primary key Gets the full, qualified name of the table. Returns the number of indexes matching the criteria. Get the referenced column map of the table. Gets the SchemaDescriptor for this TableDescriptor.  TableDescriptor interface  Gets the name of the schema the table lives in. Returns a list of statistics for this table. Gets an identifier telling what type of table this is (base table, declared global temporary table, view, etc.) Gets the number of indexes on the table, including the backing indexes. Gets the trigger descriptor list Gets the UUID of the table. Gets the view descriptor for this TableDescriptor. Gets the on commit behavior for the declared global temporary table. Gets the on rollback behavior for the declared global temporary table. Is this provider persistent?  A stored dependency will be required if both the dependent and provider are persistent. Is this descriptor represents a synonym? ////////////////////////////////////////////////////  DEPENDENT INTERFACE  //////////////////////////////////////////////////// Check that all of the dependent's dependencies are valid. Given a list of columns in the table, construct a bit  map of those columns' ids. Mark the dependent as invalid (due to at least one of its dependencies being invalid).  Always an error for a table -- should never have gotten here. Make the name of an identity sequence generator from a table ID Marks the cardinality statistics for the indexes associated with this table for update if they are considered stale, or for creation if they don't exist, and if it is considered useful to update/create them. Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). Remove this descriptor Remove this descriptor.  Warning, removes by using object reference, not uuid. Sets the heapConglomNumber to -1 for temporary table since the table was dropped and recreated at the commit time and hence its conglomerate id has changed. This is used for temporary table descriptors only For this conglomerate (index), return the selectivity of the first numKeys. This basically returns the reciprocal of the number of unique values in the leading numKey columns of the index. It is assumed that statistics exist for the conglomerate if this function is called. However, no locks are held to prevent the statistics from being dropped, so the method also handles the case of missing statistics by using a heuristic to estimate the selectivity. Sets the constraint descriptor list Sets the lock granularity for the table to the specified value. Set the referenced column map of the table. Sets the the table name in case of rename table. This is used only by rename table Sets the trigger descriptor list Sets the UUID of the table Set (cache) the view descriptor for this TableDescriptor Are there statistics for this particular conglomerate. Does the table have an auto-increment column or not? Compare the tables descriptors based on the names. Null schema names match.  class interface  Prints the contents of the TableDescriptor
Add a TableElementNode to this TableElementList Append goobered up ResultColumns to the table's RCL. This is useful for binding check constraints for CREATE and ALTER TABLE. Checks if any of the columns in the constraint can be null. Bind and validate all of the check constraints in this list against the specified FromList. Bind and validate all of the generation clauses in this list against the specified FromList. Check to make sure that there are no duplicate column names in the list.  (The comparison here is case sensitive. The work of converting column names that are not quoted identifiers to upper case is handled by the parser.) RESOLVE: This check will also be performed by alter table. Check to make sure that there are no duplicate constraint names in the list.  (The comparison here is case sensitive. The work of converting column names that are not quoted identifiers to upper case is handled by the parser.) RESOLVE: This check will also be performed by alter table. Checks if the index should use a larger page size. If the columns in the index are large, and if the user hasn't already specified a page size to use, then we may need to default to the large page size in order to get an index with sufficiently large pages. For example, this DDL should use a larger page size for the index that backs the PRIMARY KEY constraint: create table t (x varchar(1000) primary key) check if one array is same as another Determine whether or not the parameter matches a column name in this list. Count the number of constraints of the specified type. Count the number of generation clauses. Count the number of columns. Find the column definition node in this list that matches the passed in column name. Complain if a generation clause references other generated columns. This is required by the SQL Standard, part 2, section 4.14.8. Fill in the ColumnInfo[] for this table element list. Fill in the ConstraintConstantAction[] for this create/alter table. utility to generated the call to create the index. <p> Use the passed schema descriptor's collation type to set the collation of a character string column. Use the passed schema descriptor's collation type to set the collation of the character string types in create table node Set all columns in that appear in a PRIMARY KEY constraint in a CREATE TABLE statement to NOT NULL. Validate this TableElementList.  This includes checking for duplicate columns names, and checking that user types really exist. Prevent foreign keys on generated columns from violating the SQL spec, part 2, section 11.8 (<column definition>), syntax rule 12: the referential action may not specify SET NULL or SET DEFAULT and the update rule may not specify ON UPDATE CASCADE. Validate nullability of primary keys. This logic was moved out of the main validate method so that it can be called after binding generation clauses. We need to perform the nullability checks later on because the datatype may be omitted on the generation clause--we can't set/vet the nullability of the datatype until we determine what the datatype is. Verify that a primary/unique table constraint has a valid column list. (All columns in table and no duplicates.)
Get the type of this table element. Get the name from this node. Does this element have a check constraint. Does this element have a constraint on it. Does this element have a foreign key constraint. Does this element have a primary key constraint. Does this element have a unique key constraint. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
2 TableKeys are equal if their both their schemaIds and tableNames are equal. Get the schema id. Get the table name (without the schema name).
/////////////////////////////////////////////////////////////////////  BIND METHODS  ///////////////////////////////////////////////////////////////////// Bind this TableName. This means filling in the schema name if it wasn't specified. Clone this TableName Compares two TableNames. Needed for hashing logic to work. 2 TableNames are equal if their both their schemaNames and tableNames are equal, or if this node's full table name is null (which happens when a SELECT * is expanded).  Also, only check table names if the schema name(s) are null. 2 TableNames are equal if their both their schemaNames and tableNames are equal, or if this node's full table name is null (which happens when a SELECT * is expanded).  Also, only check table names if the schema name(s) are null. Get the full SQL name of this object, properly quoted and escaped. Get the full table name (with the schema name, if explicitly specified). Get the schema name. Get the table name (without the schema name). Return true if this instance was initialized with not null schemaName. /////////////////////////////////////////////////////////////////////  OBJECT INTERFACE  ///////////////////////////////////////////////////////////////////// Returns a hash code for this tableName. This allows us to use TableNames as keys in hash lists. Set the schema name. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.

Accept the visitor for all visitable children of this node.   Bind the expressions under this TableOperatorNode.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the expressions in this ResultSetNode if it has tables.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Bind the non VTI tables in this TableOperatorNode. This means getting their TableDescriptors from the DataDictionary. We will build an unbound RCL for this node.  This RCL must be "bound by hand" after the underlying left and right RCLs are bound. Bind the result columns for this ResultSetNode to a base table. This is useful for INSERT and UPDATE statements, where the result columns get their types from the table being updated or inserted into. If a result column list is specified, then the verification that the result column list does not contain any duplicates will be done when binding them by name. Bind the result columns of this ResultSetNode when there is no base table to bind them to.  This is useful for SELECT statements, where the result columns get their types from the expressions that live under them. DERBY-4365 Bind untyped nulls to the types in the given ResultColumnList. This is used for binding the nulls in row constructors and table constructors. Bind the VTI tables in this TableOperatorNode. This means getting their TableDescriptors from the DataDictionary. We will build an unbound RCL for this node.  This RCL must be "bound by hand" after the underlying left and right RCLs are bound. Decrement (query block) level (0-based) for all of the tables in this ResultSet tree. This is useful when flattening a subquery. Return the exposed name for this table, which is the name that can be used to refer to this table in the rest of the query. Determine whether or not the specified name is an exposed name in the current query block. Get the leftResultSet from this node. Get the rightResultSet from this node.   apparently something special needs to be done for me.... Optimize a TableOperatorNode. Optimize a source result set to this table operator. Put a ProjectRestrictNode on top of each FromTable in the FromList. ColumnReferences must continue to point to the same ResultColumn, so that ResultColumn must percolate up to the new PRN.  However, that ResultColumn will point to a new expression, a VirtualColumnNode, which points to the FromTable and the ResultColumn that is the source for the ColumnReference. (The new PRN will have the original of the ResultColumnList and the ResultColumns from that list.  The FromTable will get shallow copies of the ResultColumnList and its ResultColumns.  ResultColumn.expression will remain at the FromTable, with the PRN getting a new VirtualColumnNode for each ResultColumn.expression.) We then project out the non-referenced columns.  If there are no referenced columns, then the PRN's ResultColumnList will consist of a single ResultColumn whose expression is 1. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Find the unreferenced result columns and project them out. This is used in pre-processing joins that are not flattened into the where clause. Return true if the node references SESSION schema tables (temporary or permanent) Search to see if a query references the specifed table name. Check for (and reject) ? parameters directly under the ResultColumns. This is done for SELECT statements.  For TableOperatorNodes, we simply pass the check through to the left and right children. Set the (query block) level (0-based) for this FromTable. Mark whether or not this node is nested in parens.  (Useful to parser since some trees get created left deep and others right deep.) The resulting state of this cal was never used so its field was removed to save runtimespace for this node. Further cleanup can be done including parser changes if this call is really nor required. Set the referenced columns in the column list if it may not be correct. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Get the provider's type.  ////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////// Return the name of this Provider.  (Useful for errors.) ----- getter functions for rowfactory ------
Determines whether a user is the owner of an object (table, function, or procedure). Note that the database creator can access database objects without needing to be their owner. Determines if the privilege is grantable by this grantor for the given view. Note that the database owner can access database objects without needing to be their owner.  This method should only be called if it is a GRANT. This is the guts of the Execution-time logic for GRANT/REVOKE of a table privilege end of executeConstantAction end of getPermString
Add one action to the privileges for this table end of addAction Add all actions end of addAll Bind. Retrieve all the underlying stored dependencies such as table(s), view(s) and routine(s) descriptors which the view depends on. This information is then passed to the runtime to determine if the privilege is grantable to the grantees by this grantor at execution time. Go through the providers regardless who the grantor is since the statement cache may be in effect.
Can we get instantaneous locks when getting share row locks at READ COMMITTED. Shallow clone this result set.  Used in trigger reference. beetle 4373. If the result set has been opened, close the open scan. This result set has its row from the last fetch done. If the cursor is closed, the row has been deleted, or no longer qualifies (for forward only result sets) a null is returned. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) Return the next row (if any) from the scan (if open).  CursorResultSet interface  This result set has its row location from the last fetch done. If the cursor is closed, or the row has been deleted a null is returned. Return the total amount of time spent in this ResultSet Initialize the {@code startPosition} and {@code stopPosition} fields which are used to limit the rows returned by the scan. Is this ResultSet or it's source result set for update  ResultSet interface (leftover from NoPutResultSet)  open a scan on the table. scan parameters are evaluated at each open, so there is probably some way of altering their values... * Open the scan controller * * @param transaction controller will open one if null  Return a start or stop positioner as a String. If we already generated the information, then use that.  Otherwise, invoke the activation to get it. Print the parameters that constructed this result set to the trace stream. private final void traceScanParameters() { if (SanityManager.DEBUG) { HeaderPrintWriter traceStream = SanityManager.GET_DEBUG_STREAM(); traceStream.println(""); traceStream.println("TableScanResultSet number " + resultSetNumber + " parameters:"); traceStream.println(""); traceStream.println("\tTable name: " + tableName); if (indexName != null) { traceStream.println("\tIndex name: " + indexName); } traceStream.println(""); traceStream.println("\tStart position is: "); tracePrintPosition(traceStream, startSearchOperator, startKeyGetter); traceStream.println(""); traceStream.println("\tStop position is: " ); tracePrintPosition(traceStream, stopSearchOperator, stopKeyGetter); traceStream.println(""); traceStream.println("\tQualifiers are: "); tracePrintQualifiers(traceStream, qualifiers, 2); traceStream.println(""); } } Print I/O statistics about a scan when it closes. private final void traceClose() { if (SanityManager.DEBUG) { InfoStreams			infoStreams; HeaderPrintWriter	traceStream; traceStream = SanityManager.GET_DEBUG_STREAM(); traceStream.println("TableScanResultSet number " + resultSetNumber + " closed."); if (isKeyed) { traceStream.println("\t" + rowCount() + " row(s) qualified from " + "keyed" + " table " + tableName + " using index " + indexName); } else { traceStream.println("\t" + rowCount() + " row(s) qualified from " + "non-keyed" + " table " + tableName); } traceStream.println(""); } } Print a start or stop positioner to the trace stream. private final void tracePrintPosition(HeaderPrintWriter traceStream, int searchOperator, GeneratedMethod positionGetter) { if (SanityManager.DEBUG) { if (positionGetter == null) { traceStream.println("\t\tNone"); return; } ExecIndexRow	positioner = null; try { positioner = (ExecIndexRow) positionGetter.invoke(activation); } catch (StandardException e) { traceStream.println("\t\tUnexpected exception " + e + " getting positioner."); e.printStackTrace(traceStream.getPrintWriter()); return; } if (positioner == null) { traceStream.println("\t\tNone"); return; } String searchOp = null; switch (searchOperator) { case ScanController.GE: searchOp = "GE"; break; case ScanController.GT: searchOp = "GT"; break; default: searchOp = "unknown value (" + searchOperator + ")"; break; } traceStream.println("\t\t" + searchOp + " on first " + positioner.nColumns() + " column(s)."); traceStream.print( "\t\tOrdered null semantics on the following columns: "); for (int position = 0; position < positioner.nColumns(); position++) { if (positioner.areNullsOrdered(position)) { traceStream.print(position + " "); } } traceStream.println(""); } } Print an array of Qualifiers to the trace stream. private final void tracePrintQualifiers(HeaderPrintWriter traceStream, Qualifier[][] qualifiers, int depth) { if (SanityManager.DEBUG) { char[] indentchars = new char[depth]; /* * Form an array of tab characters for indentation. while (depth > 0) { indentchars[depth - 1] = '\t'; depth--; } String indent = new String(indentchars); if (qualifiers == null) { traceStream.println(indent + MessageService.getTextMessage( SQLState.LANG_NONE) ); return; } // RESOLVE (mikem) We don't support 2-d qualifiers yet. if (SanityManager.DEBUG) { SanityManager.ASSERT(qualifiers.length == 1); } for (int i = 0; i < qualifiers[0].length; i++) { Qualifier qual = qualifiers[0][i]; traceStream.println(""); traceStream.println(indent + "Column Id: " + qual.getColumnId()); int operator = qual.getOperator(); String opString = null; switch (operator) { case Orderable.ORDER_OP_EQUALS: opString = "="; break; case Orderable.ORDER_OP_LESSOREQUALS: opString = "<="; break; case Orderable.ORDER_OP_LESSTHAN: opString = "<"; break; default: opString = "unknown value (" + operator + ")"; break; } traceStream.println(indent + "Operator: " + opString); traceStream.println(indent + "Ordered nulls: " + qual.getOrderedNulls()); traceStream.println(indent + "Unknown return value: " + qual.getUnknownRV()); traceStream.println(indent + "Negate comparison result: " + qual.negateCompareResult()); traceStream.println(""); } } } Reopen a table scan.  Here we take advantage of the reopenScan() interface on scanController for optimimal performance on joins where we are an inner table. * reopen the scan controller  Update the number of rows in the scan controller. NOTE: It would be more efficient to only update the scan controller if the optimizer's estimated number of rows were wrong by more than some threshold (like 10%). This would require a little more work than I have the time for now, however, as the row estimate that is given to this result set is the total number of rows for all scans, not the number of rows per scan.
* JDBC 2.0 methods                                          * JDBC 3.0 methods                                                          JDBC 3.0 methods - not implemented             java.sql.ResultSet calls, passed through to our result set.
/////////////////////////////////////////////////////////////////////////  VisitableFilter BEHAVIOR  /////////////////////////////////////////////////////////////////////////
<p> Starting at the current position, search for a substring in the content. If the substring is found, return everything from the cursor up to the start of the substring and position the reader AFTER the substring. If the substring is not found, return null and do not alter the cursor. </p> Initialization common to all constructors <p> Starting at the current position, search for a substring in the content. If the substring is found, positions the reader AFTER the substring and returns that new cursor position. If the substring is not found, does not advance the cursor, but returns -1. </p> ///////////////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  /////////////////////////////////////////////////////////////////////// <p> Resets the reader to the beginning of the content. </p>
Pass a changed row and the row location for that row to the target result set. Preprocess the source row prior to getting it back from the source. This is useful for bulk insert where the store stands between the target and the source.
Add a page without locking the container, only one user will be accessing this table at a time.  Returns true if only a single handle is connected to this container. Discontinue use of this container. Note that the unlockContainer call made from this method may not release any locks. The container lock may be held until the end of the transaction. Preallocate page.  Since we don't sync when we write page anyway, no need to preallocate page.    Lock the container and mark the container as in-use by this container handle. Write the page, if it's within range of the current page range of the container. If we do write it then don't request that it be synced.
Return the savepoint level when the table was declared Return the savepoint level when the table was dropped Return the savepoint level when the table was last modified Return the table descriptor Matches by name and only temp tables that have not been dropped (that's when droppededInSavepointLevel will be -1) Set the savepoint level when the table was declared Return the savepoint level when the table was dropped Set the savepoint level when the table was last modified Set the table descriptor. Will be called while temporary is being restored
Private/Protected methods of This class: Allocate new objects to array based on format id's and column_list. <p> Check that columns in the row conform to a set of format id's, both in number and type. Generate an "empty" row to match the format id + coluumn specification. <p> Generate an array of new'd objects matching the format id specification passed in, and the column passed in.  The new row is first made up of columns matching the format ids, and then followed by one other column matching the column passed in.  This routine is mostly used by the btree code to generate temporary branch rows needed during operations like split. <p> Generate an "empty" row to match the format id specification. <p> Generate an array of new'd objects matching the format id specification passed in.  This routine is mostly used by the btree code to generate temporary rows needed during operations like split. <p> Generate an "empty" row to match the format id specification. <p> Generate an array of new'd objects matching the format id specification passed in.  This routine is mostly used by the btree code to generate temporary rows needed during operations like split.  It is more efficient to allocate new objects based on the old object vs. calling the Monitor. <p> Public Methods of This class: Constuctor for creating a template row which stores n SQLLongint's
Makes sure the Clob has not been released. <p> All operations are invalid on a released Clob. Clones the content of another internal Clob. Clones the content of another internal Clob. Copies the content of another Clob into this one. Copies the content of another Clob into this one. Converts a string into the modified UTF-8 byte encoding. Returns the size of the Clob in bytes. Finds the corresponding byte position for the given UTF-8 character position, starting from the byte position <code>startPos</code>. See comments in SQLChar.readExternal for more notes on processing the UTF8 format. @GuardedBy(this) Returns a character stream descriptor for the stream. <p> All streams from the underlying source ({@code LOBStreamControl}) are position aware and can be moved to a specific byte position cheaply. The maximum length is not really needed, nor known, at the moment, so the maximum allowed Clob length in Derby is used. Returns number of characters in the Clob. Returns the cached character count for the Clob, if any.  Returns a stream serving the raw bytes of this Clob. <p> The stream is managed by the underlying byte store, and can serve bytes both from memory and from a file on disk. Constructs and returns a <code>Reader</code>. Returns the update count of this Clob. Constructs and returns a <code>Writer</code> for the CLOB value. Inserts a string at the given position. Tells if this Clob has been released. Tells if this Clob is intended to be writable. Releases this Clob by freeing assoicated resources. Truncate the Clob to the specifiec size. Updates the internal state after a modification has been performed on the Clob content. <p> Currently the state update consists of dicarding the internal reader to stop it from delivering stale data, to reset the byte/char position cache if necessary, and to reset the cached length.
Clean up return the conglom id of the position index it maintains Get a result set for scanning what has been inserted so far. returns the conglomerate number it created Insert a row sets the type of the temporary row holder to unique stream
Avoid materializing a stream just because it goes through a temp table. It is OK to have a stream in the temp table (in memory or spilled to disk). The assumption is that one stream does not appear in two rows. For "update", one stream can be in two rows and the materialization is done in UpdateResultSet. Note to future users of this class who may insert a stream into this temp holder: (1) As mentioned above, one un-materialized stream can't appear in two rows; you need to objectify it first otherwise. (2) If you need to retrieve an un-materialized stream more than once from the temp holder, you need to either materialize the stream the first time, or, if there's a memory constraint, in the first time create a RememberBytesInputStream with the byte holder being BackingStoreByteHolder, finish it, and reset it after usage. A third option is to create a stream clone, but this requires that the container handles are kept open until the streams have been drained. Beetle 4896. Clean up Get a result set for scanning what has been inserted so far. Accessor to get the id of the temporary conglomerate. Temporary conglomerates have negative ids. An id equal to zero means that no temporary conglomerate has been created. Insert a row Maintain an index that will allow us to read  from the temporary heap in the order we inserted. Maintain an unique index based on the input row's row location in the base table, this index make sures that we don't insert duplicate rows into the temporary heap. Purge the row holder of all its rows. Resets the row holder so that it can accept new inserts.  A cheap way to recycle a row holder.
Determine if the cursor is before the first row in the result set. Tells the system to clean up on an error. Clear the current row Shallow clone this result set.  Used in trigger reference. beetle 4373. Clean up closeRowSource tells the RowSource that it will no longer need to return any rows and it can release any resource it may have. Subsequent call to any method on the RowSource will result in undefined behavior.  A closed rowSource can be closed again. Tells the system that there will be no more access to any database information via this result set; in particular, no more calls to open(). Will close the result set if it is not already closed. Returns the row at the absolute position from the query, and returns NULL when there is no such position. (Negative position means from the end of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: An exception will be thrown on 0. Return the <code>Activation</code> for this result set.  Get the Timestamp for the beginning of execution. This result set has its row from the last fetch done. If the cursor is closed, a null is returned. Returns the name of the cursor, if this is cursor statement of some type (declare, open, fetch, positioned update, positioned delete, close). Get the Timestamp for the end of execution. Get the estimated row count from this result set. Get the execution time in milliseconds. Returns the first row from the query, and returns NULL when there are no rows. Returns the last row from the query, and returns NULL when there are no rows. Whip up a new Temp ResultSet that has a single row. This row will either have all the columns from the current row of the passed resultset or a subset of the columns from the passed resulset. It all depends on what columns are needed by the passed trigger and what columns exist in the resulset. The Temp resulset should only have the columns required by the trigger. get the next row inserted into the temporary holder Returns the next row from the query, and returns NULL when there are no more rows. Get the next row. ///////////////////////////////////////////////////////  Access/RowSource -- not implemented  /////////////////////////////////////////////////////// Get the next row as an array of column objects. The column objects can be a JBMS Storable or any Serializable/Externalizable/Formattable/Streaming type. <BR> A return of null indicates that the complete set of rows has been read. <p> A null column can be specified by leaving the object null, or indicated by returning a non-null getValidColumns.  On streaming columns, it can be indicated by returning a non-null get FieldStates. <p> If RowSource.needToClone() is true then the returned row (the DataValueDescriptor[]) is guaranteed not to be modified by drainer of the RowSource (except that the input stream will be read, of course) and drainer will keep no reference to it before making the subsequent nextRow call.  So it is safe to return the same DataValueDescriptor[] in subsequent nextRow calls if that is desirable for performance reasons. <p> If RowSource.needToClone() is false then the returned row (the DataValueDescriptor[]) may be be modified by drainer of the RowSource, and the drainer may keep a reference to it after making the subsequent nextRow call.  In this case the client should severe all references to the row after returning it from getNextRowFromRowSource(). Return the point of attachment for this subquery. (Only meaningful for Any and Once ResultSets, which can and will only be at the top of a ResultSet for a subquery.) Returns the previous row from the query, and returns NULL when there are no more previous rows. Returns the row at the relative position from the current cursor position, and returns NULL when there is no such position. (Negative position means toward the beginning of the result set.) Moving the cursor to an invalid position leaves the cursor positioned either before the first row (negative position) or after the last row (positive position). NOTE: 0 is valid. NOTE: An exception is thrown if the cursor is not currently positioned on a row. Returns a ResultDescription object, which describes the results of the statement this ResultSet is in. This will *not* be a description of this particular ResultSet, if this is not the outermost ResultSet. Returns the row location of the current base table row of the cursor. If this cursor's row is composed of multiple base tables' rows, i.e. due to a join, then a null is returned.  For a temporary row holder, we always return null. Returns the row number of the current row.  Row numbers start from 1 and go to 'n'.  Corresponds to row numbering used to position current row in the result set (as per JDBC). Return the isolation level of the scan in the result set. Only expected to be called for those ResultSets that contain a scan. Get the subquery ResultSet tracking array from the top ResultSet. (Used for tracking open subqueries when closing down on an error.) Return the total amount of time spent in this ResultSet getValidColumns describes the DataValueDescriptor[] returned by all calls to the getNextRowFromRowSource() call. If getValidColumns returns null, the number of columns is given by the DataValueDescriptor.length where DataValueDescriptor[] is returned by the preceeding getNextRowFromRowSource() call.  Column N maps to DataValueDescriptor[N], where column numbers start at zero. If getValidColumns return a non null validColumns FormatableBitSet the number of columns is given by the number of bits set in validColumns.  Column N is not in the partial row if validColumns.get(N) returns false.  Column N is in the partial row if validColumns.get(N) returns true.  If column N is in the partial row then it maps to DataValueDescriptor[M] where M is the count of calls to validColumns.get(i) that return true where i &lt; N.  If DataValueDescriptor.length is greater than the number of columns indicated by validColumns the extra entries are ignored. Find out if the ResultSet is closed or not. Will report true for result sets that do not return rows. Class implementation Is this ResultSet or it's source result set for update This method will be overriden in the inherited Classes if it is true Return an array which contains the column positions of all the +ve columns in the passed array ///////////////////////////////////////////////////////  NoPutResultSet  /////////////////////////////////////////////////////// Mark the ResultSet as the topmost one in the ResultSet tree. Useful for closing down the ResultSet on an error.  ///////////////////////////////////////////////////////  Access/RowLocationRetRowSource -- not implemented  /////////////////////////////////////////////////////// needsRowLocation returns true iff this the row source expects the drainer of the row source to call rowLocation after getting a row from getNextRowFromRowSource. Does the caller of getNextRowFromRowSource() need to clone the row in order to keep a reference to the row past the getNextRowFromRowSource() call which returned the row.  This call must always return the same for all rows in a RowSource (ie. the caller will call this once per scan from a RowSource and assume the behavior is true for all rows in the RowSource). Tells the system that there will be calls to getNextRow(). Open the scan and evaluate qualifiers and the like. For us, there are no qualifiers, this is really a noop.  postion scan to start from after where we stopped earlier Reopen the scan.  Typically faster than open()/close()  Reset the exec row array and reinitialize Get the number of this ResultSet, which is guaranteed to be unique within a statement. ////////////////////////////////////////////////////////////////////////  MISC FROM RESULT SET  /////////////////////////////////////////////////////////////////////// Returns TRUE if the statement returns rows (i.e. is a SELECT or FETCH statement), FALSE if it returns no rows. rowLocation is a callback for the drainer of the row source to return the rowLocation of the current row, i.e, the row that is being returned by getNextRowFromRowSource.  This interface is for the purpose of loading a base table with index.  In that case, the indices can be built at the same time the base table is laid down once the row location of the base row is known.  This is an example pseudo code on how this call is expected to be used: <BR><pre> boolean needsRL = rowSource.needsRowLocation(); DataValueDescriptor[] row; while((row = rowSource.getNextRowFromRowSource()) != null) { RowLocation rl = heapConglomerate.insertRow(row); if (needsRL) rowSource.rowLocation(rl); } </pre><BR> NeedsRowLocation and rowLocation will ONLY be called by a drainer of the row source which CAN return a row location.  Drainer of row source which cannot return rowLocation will guarentee to not call either callbacks. Conversely, if NeedsRowLocation is called and it returns true, then for every row return by getNextRowFromRowSource, a rowLocation callback must also be issued with the row location of the row.  Implementor of both the source and the drain of the row source must be aware of this protocol. <BR> The RowLocation object is own by the caller of rowLocation, in other words, the drainer of the RowSource.  This is so that we don't need to new a row location for every row.  If the Row Source wants to keep the row location, it needs to clone it (RowLocation is a ClonableObject). Sets the current position to after the last row and returns NULL because there is no current row. Sets the current position to before the first row and returns NULL because there is no current row. Set the current row to the row passed in. Set whether or not the NPRS need the row location when acting as a row source.  (The target result set determines this.) Notify a NPRS that it is the source for the specified TargetResultSet.  This is useful when doing bulk insert. open the scan of the temporary heap and the position index The passed array can have some -1 elements and some +ve elements Return an array containing just the +ve elements Make an array which is a superset of the 2 passed column arrays. The superset will not have any duplicates
Accept the visitor for all visitable children of this node. end of bindDateTimeArg Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. This method gets called for non-character string types and hence no need to set any collation info. Collation applies only to character string types. end of bindParameter bind arguments to built in types cast arg to a varchar Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views.  Do code generation for this ternary operator. Get the leftOperand Get the rightOperand Return whether or not this expression tree represents a constant expression. Bind locate operator The variable receiver is the string which will searched The variable leftOperand is the search character that will looked in the receiver variable. Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set the leftOperand to the specified ValueNode * set result type for operator Set the rightOperand to the specified ValueNode Bind substr expression. throw bad type message Bind TIMESTAMPADD expression. end of timestampAddBind Bind TIMESTAMPDIFF expression. End of timestampDiffBind Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing. Bind trim expression. The variable receiver is the string that needs to be trimmed. The variable leftOperand is the character that needs to be trimmed from receiver.

Save all the exceptions seen during load. Get the next warehouse to populate. If all have been populated then -1 is returned. Warehouse is one based. Populate the database. Run the load for a thread. Loop insert the data for a single warehouse while there are warehouses left to do. Set the thread count. If the scale is less than the number of threads then the number of threads inserting data will be equal to the scale. Initialize the load by calling the super-class's method and default the number of threads to the smaller of the number of cpus and the scale. Populate the database using multiple threads. The main thread (ie the one calling this method). is one of the threads that will insert the data It will handle the ITEM table by itself and its fair share of the remaining tables.
constructor which will start the threads
TypeCompiler methods User types are convertible to other user types only if (for now) they are the same type and are being used to implement some JDBC type.  This is sufficient for date/time types; it may be generalized later for e.g. comparison of any user type with one of its subtypes.    User types are storable into other user types that they are assignable to. The other type must be a subclass of this type, or implement this type as one of its interfaces. Built-in types are also storable into user types when the built-in type's corresponding Java type is assignable to the user type.

The static entry way to get the LockTable in the system. buildLockTableString creates a LockTable info String A static entry way to get the LockTable in the system. For track 3311 cpArray helps built the output string (outputRow). createException creates a StandardException based on: currentLock a snapshot of the lockTable dumpLock puts information about currentLock into currentRow for output later. Copies the needed information from currentRow into the StringBuffer for output
Cancel a task. Schedule a task.
/////////////////////////////////////////////////////////////////////////////////  Thread BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Implementation of run() method to sleep, wake up, and kill the program. /////////////////////////////////////////////////////////////////////////////////  OTHER BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// This method marks the thread for rundown
Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. end of bindExpression Do code generation for this binary operator. end of generateExpression
Tell whether this type (timestamp) is compatible with the given type. TypeCompiler methods User types are convertible to other user types only if (for now) they are the same type and are being used to implement some JDBC type.  This is sufficient for date/time types; it may be generalized later for e.g. comparison of any user type with one of its subtypes.    User types are storable into other user types that they are assignable to. The other type must be a subclass of this type, or implement this type as one of its interfaces. Built-in types are also storable into user types when the built-in type's corresponding Java type is assignable to the user type.
Returns a new Token object, by default. However, if you want, you can create and return subclass objects based on the value of ofKind. Simply add the cases to the switch for all those special cases. For example, if you have a subclass of Token called IDToken that you want to create if ofKind is ID, simlpy add something like : case MyParserConstants.ID : return new IDToken(); to the following switch statement. Then you can cast matchedToken variable to the appropriate type and use it in your lexical actions. Returns the image.
Return a suite of tool tests from the list of script names. Each test is surrounded in a decorator that cleans the database, and adds authentication and authorization for each script. Return a suite of tool tests from the list of script names. Each test is surrounded in a decorator that cleans the database. Run a set of tool scripts (.sql files) passed in on the command line. Note the .sql suffix must not be provided as part of the script name. <code> example java org.apache.derbyTesting.functionTests.tests.tool.ToolScripts case union </code> Return the suite that runs all the tool scripts.
Convert a raw string into a properly cased and escaped Derby identifier Get the current user <p> Get the owner of the indicated schema. Returns null if the schema doesn't exist. </p> <p> Raise an exception if SQL authorization is enabled and the current user isn't the DBO. </p> <p> Raise an exception if SQL authorization is enabled and the current user isn't the DBO or the owner of the indicated schema or if the indicated schema doesn't exist. </p> Make a SQLException from a SQLState and optional args //////////////////////////////////////////////////////////////////////  CONSTANTS  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  STATE  ////////////////////////////////////////////////////////////////////// //////////////////////////////////////////////////////////////////////  ENTRY POINTS  ////////////////////////////////////////////////////////////////////// Returns true if SQL authorization is enabled in the connected database. ///////////////////////////////////////////////////////////////////  ERROR HANDLING  /////////////////////////////////////////////////////////////////// Turn a StandardException into a SQLException Wrap an external exception
Copy a (possibly null) array of booleans Copy a (possibly null) array of bytes Copy a (possibly null) array of ints /////////////////////////////////////////////////////////////////  Methods to copy arrays. We'd like to use java.util.copyOf(), but we have to run on Java 5. The same methods also appear in org.apache.derby.iapi.services.io.ArrayUtil. They are repeated here in order to avoid sealing issues.  ///////////////////////////////////////////////////////////////// Copy an array of objects; the original array could be null Copy a (possibly null) array of strings Copy a (possibly null) 2-dimensional array of ints
Add a running module into the protocol hash table. Return true if the module was added successfully, false if it couldn't be added. In the latter case the module should be shutdown if its reference count is 0. Boot a module, performs three steps. <OL> <LI> Look for an existing module in the protocol table <LI> Look for a module in the implementation table that handles this protocol <LI> Create an instance that handles this protocol. </OL> Find an module in the protocol table that supports the required protocol name combination and can handle the properties. Returns the instance of the module or null if one does not exist in the protocol table. Find a {@code ModuleInstance} object whose {@code getInstance()} method returns the object specified by the {@code instance} parameter. If the service is already beign shutdown we return false.
Abort all changes made by this transaction since the last commit, abort or the point the transaction was started, whichever is the most recent. All savepoints within this transaction are released. Add a new stream container to the segment and load the stream container. This stream container doesn't not have locks, and do not log. It does not have the concept of a page. It is used by the external sort only. <P><B>Synchronisation</B> <P> This call will mark the container as dropped and then obtain an CX lock on the container. Once a container has been marked as dropped it cannot be retrieved by any openContainer() call. <P> Once the exclusive lock has been obtained the container is removed and all its pages deallocated. The container will be fully removed at the commit time of the transaction. Add a new container to the segment. The new container initially has one page, page Container.FIRST_PAGE_NUMBER. <BR> If pageSize is equal to ContainerHandle.DEFAULT_PAGESIZE or invalid then a default page size will be picked. <BR> SpareSpace indicates that percent (0% - 100%) of page space that will be attempted to be reserved for updates. E.g. with a value of 20 a page that would normally hold 40 rows will be limited to 32 rows, actual calculation for the threshold where no more inserts are all accepted is up to the implementation.  Whatever the value of spaceSpace an empty page will always accept at least one insert. If spare space is equal to ContainerHandle.DEFAULT_PAGESIZE or invalid then a default value will be used. <P><B>Synchronisation</B> <P> The new container is exclusivly locked by this transaction until it commits. Add to the list of post abort work that may be processed after this transaction aborts. Add to the list of post commit work that may be processed after this transaction commits.  If this transaction aborts, then the post commit work list will be thrown away.  No post commit work will be taken out on a rollback to save point. Add to the list of post termination work that may be processed after this transaction commits or aborts. Return true if any transaction is blocked, even if not by this one. Close this transaction, the transaction must be idle. This close will pop the transaction context off the stack that was pushed when the transaction was started. Commit this transaction. All savepoints within this transaction are released. "Commit" this transaction without sync'ing the log. Everything else is identical to commit(), use this at your own risk. <BR>bits in the commitflag can turn on to fine tuned the "commit": KEEP_LOCKS - no locks will be released by the commit and no post commit processing will be initiated.  If, for some reasons, the locks cannot be kept even if this flag is set, then the commit will sync the log, i.e., it will revert to the normal commit. Convert a local transaction to a global transaction. <p> Get a transaction controller with which to manipulate data within the access manager.  Tbis controller allows one to manipulate a global XA conforming transaction. <p> Must only be called a previous local transaction was created and exists in the context.  Can only be called if the current transaction is in the idle state. <p> The (format_id, global_id, branch_id) triplet is meant to come exactly from a javax.transaction.xa.Xid.  We don't use Xid so that the system can be delivered on a non-1.2 vm system and not require the javax classes in the path. If this transaction is not idle, abort it.  After this call close(). Drop a container. <P><B>Synchronisation</B> <P> This call will mark the container as dropped and then obtain an CX lock on the container. Once a container has been marked as dropped it cannot be retrieved by any openContainer() call. <P> Once the exclusive lock has been obtained the container is removed and all its pages deallocated. The container will be fully removed at the commit time of the transaction. Drop a stream container. <P><B>Synchronisation</B> <P> This call will remove the container. get string ID of the actual transaction ID that will be used when transaction is in  active state. Get the compatibility space of the transaction. <p> Returns an object that can be used with the lock manager to provide the compatibility space of a transaction.  2 transactions with the same compatibility space will not conflict in locks.  The usual case is that each transaction has it's own unique compatibility space. <p> Return the context manager this transaction is associated with. Get DataValueFactory. <p> Return a DataValueFactory that can be used to allocate objects.  Used to make calls to: DataValueFactory.getInstanceUsingFormatIdAndCollationType() Get the current default locking policy for all operations within this transaction. The transaction is initially started with a default locking policy equivalent to <PRE> newLockingPolicy( LockingPolicy.MODE_RECORD, LockingPolicy.ISOLATION_SERIALIZABLE, true); </PRE> This default can be changed by subsequent calls to setDefaultLockingPolicy(LockingPolicy policy). Get an object to handle non-transactional files. Return my transaction identifier. Transaction identifiers may be re-used for transactions that do not modify the raw store. May return null if this transaction has no globalId. Reveals whether the transaction has ever read or written data. Reveal whether the transaction is in a pristine state, which means it hasn't done any updates since the last commit. Log an operation and then action it in the context of this transaction. The Loggable Operation is logged in the transaction log file and then its doMe method is called to perform the required change. If this transaction aborts or a rollback is performed of the current savepoint (if any) then a compensation Operation needs to be generated that will compensate for the change of this Operation. Obtain a locking policy for use in openContainer(). The mode and isolation must be constants from LockingPolicy. If higherOK is true then the object returned may implement a stricter form of locking than the one requested. <BR> A null LockingPolicy reference is identical to a LockingPolicy obtained by using MODE_NONE which is guaranteed to exist. Open a container, with the transaction's default locking policy. <p> Note that if NOWAIT has been specified lock will be requested with no wait time, and if lock is not granted a SQLState.LOCK_TIMEOUT exception will be thrown. <P> The release() method of ContainerHandle will be called when this transaction is aborted or commited, it may be called explicitly to release the ContainerHandle before the end of the transaction. Open a container, with the defined locking policy, otherwise as openContainer(int containerId,  boolean forUpdate). <P> Calls locking.lockContainer(this, returnValue, forUpdate) to lock the container.  Note that if NOWAIT has been specified lock will be requested with no wait time, and if lock is not granted a SQLState.LOCK_TIMEOUT exception will be thrown. Open a stream container. Release the save point of the given name. Relasing a savepoint removes all knowledge from this transaction of the named savepoint and any savepoints set since the named savepoint was set. Rollback all changes made since the named savepoint was set. The named savepoint is not released, it remains valid within this transaction, and thus can be named it future rollbackToSavePoint() calls. Any savepoints set since this named savepoint are released (and their changes rolled back). Set the default locking policy for all operations within this transaction. The transaction is intially started with a default locking policy equivalent to <PRE> newLockingPolicy( LockingPolicy.MODE_RECORD, LockingPolicy.ISOLATION_SERIALIZABLE, true); </PRE> Tell this transaction whether it should time out immediately if a lock cannot be granted without waiting. This could be used in a nested transaction to prevent long waits if there is a lock conflict between the nested transaction and its parent. If it is used this way, the calling code should catch timeout exceptions from the nested transaction and retry the operation (without disabling waiting) in the parent transaction. Set a save point in the current transaction. A save point defines a point in time in the transaction that changes can be rolled back to. Savepoints can be nested and they behave like a stack. Setting save points "one" and "two" and the rolling back "one" will rollback all the changes made since "one" (including those made since "two") and release savepoint "two". Called after the transaction has been attached to an Access Manger TransactionController. Thus may not be called for all transactions. Purpose is to allow a transaction access to database (service) properties. Will not be called for transactions early in the boot process, ie. before the property conglomerate is set up. This method is called to commit the current XA global transaction. <p> RESOLVE - how do we map to the "right" XAExceptions. <p> This method is called to ask the resource manager to prepare for a transaction commit of the transaction specified in xid. <p> rollback the current global transaction. <p> The given transaction is roll'ed back and it's history is not maintained in the transaction table or long term log. <p>
Add a listener to the curent transaction. A listener may be added multiple times and it will receive multiple callbacks. Get number of isolation string mappings Map Derby isolation level to SQL text values Map a Derby isolation level to the corresponding JDBC level Notify all listeners that a commit is about to occur. If a listener throws an exception then no further listeners will be notified and a StandardException with rollback severity will be thrown. Notify all listeners that a rollback is about to occur. If a listener throws an exception then no further listeners will be notified and a StandardException with shutdown database(?) severity will be thrown. Remove a listener from the current transaction.
Abort all changes made by this transaction since the last commit, abort or the point the transaction was started, whichever is the most recent. All savepoints within this transaction are released, and all resources are released (held or non-held). Add a column to a conglomerate. The Storage system will block this action until it can get an exclusive container level lock on the conglomerate.  The conglomerate must not be open in the current transaction, this means that within the current transaction there must be no open ConglomerateController's or ScanControllers.  It may not be possible in some implementations of the system to catch this error in the store, so it is up to the caller to insure this. The column can only be added at the spot just after the current set of columns. The template_column must be nullable. After this call has been made, all fetches of this column from rows that existed in the table prior to this call will return "null". ************************************************************************ Interfaces previously defined in TcTransactionIface: ************************************************************************* Return true if any transaction is blocked (even if not by this one). Commit this transaction.  All savepoints within this transaction are released.  All non-held conglomerates and scans are closed. "Commit" this transaction without sync'ing the log.  Everything else is identical to commit(), use this at your own risk. <BR>bits in the commitflag can turn on to fine tuned the "commit": KEEP_LOCKS                          - no locks will be released by the commit and no post commit processing will be initiated.  If, for some reasons, the locks cannot be kept even if this flag is set, then the commit will sync the log, i.e., it will revert to the normal commit. READONLY_TRANSACTION_INITIALIZATION - Special case used for processing while creating the transaction. Should only be used by the system while creating the transaction to commit readonly work that may have been done using the transaction while getting it setup to be used by the user.  In the future we should instead use a separate tranaction to do this initialization.  Will fail if called on a transaction which has done any updates. Return free space from the conglomerate back to the OS. <p> Returns free space from the conglomerate back to the OS.  Currently only the sequential free pages at the "end" of the conglomerate can be returned to the OS. <p> Check whether a conglomerate exists. Report on the number of open conglomerates in the transaction. <p> There are 4 types of open "conglomerates" that can be tracked, those opened by each of the following: openConglomerate(), openScan(), createSort(),  and openSort().  Scans opened by openSortScan() are tracked the same as those opened by openScan().  This routine can be used to either report on the number of all opens, or may be used to track one particular type of open. <p> This routine is expected to be used for debugging only.  An implementation may only track this info under SanityManager.DEBUG mode. If the implementation does not track the info it will return -1 (so code using this call to verify that no congloms are open should check for return &lt;= 0 rather than == 0). <p> The return value depends on the "which_to_count" parameter as follows: <UL> <LI> OPEN_CONGLOMERATE  - return # of openConglomerate() calls not close()'d. <LI> OPEN_SCAN          - return # of openScan() + openSortScan() calls not close()'d. <LI> OPEN_CREATED_SORTS - return # of sorts created (createSort()) in current xact.  There is currently no way to get rid of these sorts before end of transaction. <LI> OPEN_SORT          - return # of openSort() calls not close()'d. <LI> OPEN_TOTAL         - return total # of all above calls not close()'d. </UL> - note an implementation may return -1 if it does not track the above information. <p> Create a conglomerate and load (filled) it with rows that comes from the row source without loggging. <p>Individual rows that are loaded into the conglomerate are not logged. After this operation, the underlying database must be backed up with a database backup rather than an transaction log backup (when we have them). This warning is put here for the benefit of future generation. <p> This function behaves the same as @see createConglomerate except it also populates the conglomerate with rows from the row source and the rows that are inserted are not logged. Create a HashSet which contains all rows that qualify for the described scan. <p> All parameters shared between openScan() and this routine are interpreted exactly the same.  Logically this routine calls openScan() with the passed in set of parameters, and then places all returned rows into a newly created HashSet and returns, actual implementations will likely perform better than actually calling openScan() and doing this.  For documentation of the openScan parameters see openScan(). <p> Create a conglomerate. <p> Currently, only "heap"'s and ""btree secondary index"'s are supported, and all the features are not completely implemented. For now, create conglomerates like this: <p> <blockquote><pre> TransactionController tc; long conglomId = tc.createConglomerate( "heap", // we're requesting a heap conglomerate template, // a populated template is required for heap and btree. null, // no column order null, // default collation order for all columns null, // default properties 0); // not temporary </blockquote></pre> Each implementation of a conglomerate takes a possibly different set of properties.  The "heap" implementation currently takes no properties. The "btree secondary index" requires the following set of properties: <UL> <LI> "baseConglomerateId" (integer).  The conglomerate id of the base conglomerate is never actually accessed by the b-tree secondary index implementation, it only serves as a namespace for row locks. This property is required. <LI> "rowLocationColumn" (integer).  The zero-based index into the row which the b-tree secondary index will assume holds a @see RowLocation of the base row in the base conglomerate.  This value will be used for acquiring locks.  In this implementation RowLocationColumn must be the last key column. This property is required. <LI>"allowDuplicates" (boolean).  If set to true the table will allow rows which are duplicate in key column's 0 through (nUniqueColumns - 1). Currently only supports "false". This property is optional, defaults to false. <LI>"nKeyFields"  (integer) Columns 0 through (nKeyFields - 1) will be included in key of the conglomerate. This implementation requires that "nKeyFields" must be the same as the number of fields in the conglomerate, including the rowLocationColumn. Other implementations may relax this restriction to allow non-key fields in the index. This property is required. <LI>"nUniqueColumns" (integer) Columns 0 through "nUniqueColumns" will be used to check for uniqueness.  So for a standard SQL non-unique index implementation set "nUniqueColumns" to the same value as "nKeyFields"; and for a unique index set "nUniqueColumns" to "nKeyFields - 1 (ie. don't include the rowLocationColumn in the uniqueness check). This property is required. <LI>"maintainParentLinks" (boolean) Whether the b-tree pages maintain the page number of their parent.  Only used for consistency checking.  It takes a certain amount more effort to maintain these links, but they're really handy for ensuring that the index is consistent. This property is optional, defaults to true. </UL> A secondary index i (a, b) on table t (a, b, c) would have rows which looked like (a, b, row_location).  baseConglomerateId is set to the conglomerate id of t.  rowLocationColumns is set to 2.  allowsDuplicates would be set to false.  To create a unique secondary index set uniquenessColumns to 2, this means that the btree code will compare the key values but not the row id when determing uniqueness.  To create a nonunique secondary index set uniquenessColumns to 3, this would mean that the uniqueness test would include the row location and since all row locations will be unique  all rows inserted into the index will be differentiated (at least) by row location. ************************************************************************ Interfaces previously defined in TcSortIface: ************************************************************************* Create a sort.  Rows are inserted into the sort with a sort controller, and subsequently retrieved with a sort scan controller. The rows come out in the order specified by the parameters. <p> Sorts also do aggregation. The input (unaggregated) rows have the same format as	the aggregated rows, and the aggregate results are part of the both rows.  The sorter, when it notices that a row is a duplicate of another, calls a user-supplied aggregation method (see interface Aggregator), passing it both rows.  One row is known as the 'addend' and the other the 'accumulator'. The aggregation method is assumed to merge the addend into the accumulator. The sort then discards the addend row. <p> So, for the query: <pre><blockquote> select a, sum(b) from t group by a </blockquote></pre> The input row to the sorter would have one column for a and another column for sum(b).  It is up to the caller to get the format of the row correct, and to initialize the	aggregate values correctly (null for most aggregates, 0 for count). <p> Nulls are always considered to be ordered in a sort, that is, null compares equal to null, and less than anything else. Convert a local transaction to a global transaction. <p> Get a transaction controller with which to manipulate data within the access manager.  Tbis controller allows one to manipulate a global XA conforming transaction. <p> Must only be called a previous local transaction was created and exists in the context.  Can only be called if the current transaction is in the idle state.  Upon return from this call the old tc will be unusable, and all references to it should be dropped (it will have been implicitly destroy()'d by this call. <p> The (format_id, global_id, branch_id) triplet is meant to come exactly from a javax.transaction.xa.Xid.  We don't use Xid so that the system can be delivered on a non-1.2 vm system and not require the javax classes in the path. XATransactionController Return a string with debug information about opened congloms/scans/sorts. <p> Return a string with debugging information about current opened congloms/scans/sorts which have not been close()'d. Calls to this routine are only valid under code which is conditional on SanityManager.DEBUG. <p> Compress table in place. <p> Returns a GroupFetchScanController which can be used to move rows around in a table, creating a block of free pages at the end of the table.  The process will move rows from the end of the table toward the beginning.  The GroupFetchScanController will return the old row location, the new row location, and the actual data of any row moved.  Note that this scan only returns moved rows, not an entire set of rows, the scan is designed specifically to be used by either explicit user call of the SYSCS_ONLINE_COMPRESS_TABLE() procedure, or internal background calls to compress the table. The old and new row locations are returned so that the caller can update any indexes necessary. This scan always returns all collumns of the row. All inputs work exactly as in openScan().  The return is a GroupFetchScanController, which only allows fetches of groups of rows from the conglomerate. <p> Abort the current transaction and pop the context. Drop a conglomerate.  The conglomerate must not be open in the current transaction.  This also means that there must not be any active scans on it. Drop a sort. <p> Drop a sort created by a call to createSort() within the current transaction (sorts are automatically "dropped" at the end of a transaction.  This call should only be made after all openSortScan()'s and openSort()'s have been closed. Retrieve the maximum value row in an ordered conglomerate. <p> Returns true and fetches the rightmost non-null row of an ordered conglomerate into "fetchRow" if there is at least one non-null row in the conglomerate.  If there are no non-null rows in the conglomerate it returns false.  Any row with a first column with a Null is considered a "null" row. <p> Non-ordered conglomerates will not implement this interface, calls will generate a StandardException. <p> RESOLVE - this interface is temporary, long term equivalent (and more) functionality will be provided by the openBackwardScan() interface. <p> ISOLATION_SERIALIZABLE and MODE_RECORD locking for btree max: The "BTREE" implementation will at the very least get a shared row lock on the max key row and the key previous to the max. This will be the case where the max row exists in the rightmost page of the btree.  These locks won't be released.  If the row does not exist in the last page of the btree then a scan of the entire btree will be performed, locks acquired in this scan will not be released. <p> Note that under ISOLATION_READ_COMMITTED, all locks on the table are released before returning from this call. For debugging, find the conglomid given the containerid. <p> For debugging, find the containerid given the conglomid. <p> Will have to change if we ever have more than one container in a conglomerate. ************************************************************************ Interfaces previously defined in TcAccessIface: ************************************************************************* Get reference to access factory which started this transaction. <p> Get string id of the transaction that would be when the Transaction is IN active state. This method increments the Tx id of  current Tx object if it is in idle state. Note: Use this method only  getTransactionIdString() is not suitable. Get the context manager that the transaction was created with. <p> Return dynamic information about the conglomerate to be dynamically reused in repeated execution of a statement. <p> The dynamic info is a set of variables to be used in a given ScanController or ConglomerateController.  It can only be used in one controller at a time.  It is up to the caller to insure the correct thread access to this info.  The type of info in this is a scratch template for btree traversal, other scratch variables for qualifier evaluation, ... <p> Get an object to handle non-transactional files. Return an object that when used as the compatibility space for a lock request, <strong>and</strong> the group object is the one returned by a call to <code>getOwner()</code> on that object, guarantees that the lock will be removed on a commit or an abort. Return static information about the conglomerate to be included in a a compiled plan. <p> The static info would be valid until any ddl was executed on the conglomid, and would be up to the caller to throw away when that happened.  This ties in with what language already does for other invalidation of static info.  The type of info in this would be containerid and array of format id's from which templates can be created. The info in this object is read only and can be shared among as many threads as necessary. <p> Get string id of the transaction. <p> This transaction "name" will be the same id which is returned in the TransactionInfo information, used by the lock and transaction vti's to identify transactions. <p> Although implementation specific, the transaction id is usually a number which is bumped every time a commit or abort is issued. <p> A superset of properties that "users" can specify. <p> A superset of properties that "users" (ie. from sql) can specify.  Store may implement other properties which should not be specified by users. Layers above access may implement properties which are not known at all to Access. <p> This list is a superset, as some properties may not be implemented by certain types of conglomerates.  For instant an in-memory store may not implement a pageSize property.  Or some conglomerates may not support pre-allocation. <p> This interface is meant to be used by the SQL parser to do validation of properties passsed to the create table statement, and also by the various user interfaces which present table information back to the user. <p> Currently this routine returns the following list: derby.storage.initialPages derby.storage.minimumRecordSize derby.storage.pageReservedSpace derby.storage.pageSize Reveals whether the transaction is a global or local transaction. Reveals whether the transaction has ever read or written data. Reveals whether the transaction is read only. ************************************************************************ Interfaces previously defined in TcLogIface: ************************************************************************* Log an operation and then action it in the context of this transaction. <P>This simply passes the operation to the RawStore which logs and does it. Open a conglomerate for use, optionally include "compiled" info. <p> Same as openConglomerate(), except that one can optionally provide "compiled" static_info and/or dynamic_info.  This compiled information must have be gotten from getDynamicCompiledConglomInfo() and/or getStaticCompiledConglomInfo() calls on the same conglomid being opened. It is up to caller that "compiled" information is still valid and is appropriately multi-threaded protected. <p> Open a scan on a conglomerate, optionally providing compiled info. <p> Same as openScan(), except that one can optionally provide "compiled" static_info and/or dynamic_info.  This compiled information must have be gotten from getDynamicCompiledConglomInfo() and/or getStaticCompiledConglomInfo() calls on the same conglomid being opened. It is up to caller that "compiled" information is still valid and is appropriately multi-threaded protected. <p> Open a conglomerate for use. <p> The lock level indicates the minimum lock level to get locks at, the underlying conglomerate implementation may actually lock at a higher level (ie. caller may request MODE_RECORD, but the table may be locked at MODE_TABLE instead). <p> The close method is on the ConglomerateController interface. Open a scan which gets copies of multiple rows at a time. <p> All inputs work exactly as in openScan().  The return is a GroupFetchScanController, which only allows fetches of groups of rows from the conglomerate. <p> Open a scan on a conglomerate.  The scan will return all rows in the conglomerate which are between the positions defined by {startKeyValue, startSearchOperator} and {stopKeyValue, stopSearchOperator}, which also match the qualifier. <P> The way that starting and stopping keys and operators are used may best be described by example. Say there's an ordered conglomerate with two columns, where the 0-th column is named 'x', and the 1st column is named 'y'.  The values of the columns are as follows: <blockquote><pre> x: 1 3 4 4 4 5 5 5 6 7 9 y: 1 1 2 4 6 2 4 6 1 1 1 </blockquote></pre> <P> A {start key, search op} pair of {{5.2}, GE} would position on {x=5, y=2}, whereas the pair {{5}, GT} would position on {x=6, y=1}. <P> Partial keys are used to implement partial key scans in SQL. For example, the SQL "select * from t where x = 5" would open a scan on the conglomerate (or a useful index) of t using a starting position partial key of {{5}, GE} and a stopping position partial key of {{5}, GT}. <P> Some more examples: <p> <blockquote><pre> +-------------------+------------+-----------+--------------+--------------+ | predicate         | start key  | stop key  | rows         | rows locked  | |                   | value | op | value |op | returned     |serialization | +-------------------+-------+----+-------+---+--------------+--------------+ | x = 5             | {5}   | GE | {5}   |GT |{5,2} .. {5,6}|{4,6} .. {5,6}| | x &gt; 5             | {5}   | GT | null  |   |{6,1} .. {9,1}|{5,6} .. {9,1}| | x &gt;= 5            | {5}   | GE | null  |   |{5,2} .. {9,1}|{4,6} .. {9,1}| | x &lt;= 5            | null  |    | {5}   |GT |{1,1} .. {5,6}|first .. {5,6}| | x &lt; 5             | null  |    | {5}   |GE |{1,1} .. {4,6}|first .. {4,6}| | x &gt;= 5 and x &lt;= 7 | {5},  | GE | {7}   |GT |{5,2} .. {7,1}|{4,6} .. {7,1}| | x = 5  and y &gt; 2  | {5,2} | GT | {5}   |GT |{5,4} .. {5,6}|{5,2} .. {5,6}| | x = 5  and y &gt;= 2 | {5,2} | GE | {5}   |GT |{5,2} .. {5,6}|{4,6} .. {5,6}| | x = 5  and y &lt; 5  | {5}   | GE | {5,5} |GE |{5,2} .. {5,4}|{4,6} .. {5,4}| | x = 2             | {2}   | GE | {2}   |GT | none         |{1,1} .. {1,1}| +-------------------+-------+----+-------+---+--------------+--------------+ </blockquote></pre> <P> As the above table implies, the underlying scan may lock more rows than it returns in order to guarantee serialization. <P> For each row which meets the start and stop position, as described above the row is "qualified" to see whether it should be returned.  The qualification is a 2 dimensional array of @see Qualifiers, which represents the qualification in conjunctive normal form (CNF).  Conjunctive normal form is an "and'd" set of "or'd" Qualifiers. <P> For example x = 5 would be represented is pseudo code as: qualifier_cnf[][] = new Qualifier[1]; qualifier_cnf[0]  = new Qualifier[1]; qualifier_cnr[0][0] = new Qualifer(x = 5) <P> For example (x = 5) or (y = 6) would be represented is pseudo code as: qualifier_cnf[][] = new Qualifier[1]; qualifier_cnf[0]  = new Qualifier[2]; qualifier_cnr[0][0] = new Qualifer(x = 5) qualifier_cnr[0][1] = new Qualifer(y = 6) <P> For example ((x = 5) or (x = 6)) and ((y = 1) or (y = 2)) would be represented is pseudo code as: qualifier_cnf[][] = new Qualifier[2]; qualifier_cnf[0]  = new Qualifier[2]; qualifier_cnr[0][0] = new Qualifer(x = 5) qualifier_cnr[0][1] = new Qualifer(x = 6) qualifier_cnr[0][0] = new Qualifer(y = 5) qualifier_cnr[0][1] = new Qualifer(y = 6) <P> For each row the CNF qualfier is processed and it is determined whether or not the row should be returned to the caller. The following pseudo-code describes how this is done: <blockquote><pre> if (qualifier != null) { <blockquote><pre> for (int and_clause; and_clause &lt; qualifier.length; and_clause++) { boolean or_qualifies = false; for (int or_clause; or_clause &lt; qualifier[and_clause].length; or_clause++) { <blockquote><pre> DataValueDescriptor key     = qualifier[and_clause][or_clause].getOrderable(); DataValueDescriptor row_col = get row column[qualifier[and_clause][or_clause].getColumnId()]; boolean or_qualifies = row_col.compare(qualifier[i].getOperator, <blockquote><pre> key, qualifier[i].getOrderedNulls, qualifier[i].getUnknownRV); </blockquote></pre> if (or_qualifies) { break; } } if (!or_qualifies) { <blockquote><pre> don't return this row to the client - proceed to next row; </blockquote></pre> } </blockquote></pre> } </blockquote></pre> } </blockquote></pre> Open a sort controller for a sort previously created in this transaction.  Sort controllers are used to insert rows into the sort. <p> There may (in the future) be multiple sort inserters for a given sort, the idea being that the various threads of a parallel query plan can all insert into the sort.  For now, however, only a single sort controller per sort is supported. Return an open SortCostController. <p> Return an open SortCostController which can be used to ask about the estimated costs of SortController() operations. <p> Open a scan for retrieving rows from a sort.  Returns a RowSource for retrieving rows from the sort. Open a scan for retrieving rows from a sort.  Returns a scan controller for retrieving rows from the sort (NOTE: the only legal methods to use on the returned sort controller are next() and fetch() - probably there should be scan controllers and updatable scan controllers). <p> In the future, multiple sort scans on the same sort will be supported (for parallel execution across a uniqueness sort in which the order of the resulting rows is not important).  Currently, only a single sort scan is allowed per sort. <p> In the future, it will be possible to open a sort scan and start retrieving rows before the last row is inserted. The sort controller would block till rows were available to return.  Currently, an attempt to retrieve a row before the sort controller is closed will cause an exception. Return an open StoreCostController for the given conglomid. <p> Return an open StoreCostController which can be used to ask about the estimated row counts and costs of ScanController and ConglomerateController operations, on the given conglomerate. <p> Purge all committed deleted rows from the conglomerate. <p> This call will purge committed deleted rows from the conglomerate, that space will be available for future inserts into the conglomerate. <p> Recreate a conglomerate and possibly load it with new rows that come from the new row source. <p> This function behaves the same as @see createConglomerate except it also populates the conglomerate with rows from the row source and the rows that are inserted are not logged. <p>Individual rows that are loaded into the conglomerate are not logged. After this operation, the underlying database must be backed up with a database backup rather than an transaction log backup (when we have them). This warning is put here for the benefit of future generation. Release the save point of the given name. Releasing a savepoint removes all knowledge from this transaction of the named savepoint and any savepoints set since the named savepoint was set. Rollback all changes made since the named savepoint was set. The named savepoint is not released, it remains valid within this transaction, and thus can be named it future rollbackToSavePoint() calls. Any savepoints set since this named savepoint are released (and their changes rolled back). <p> if "close_controllers" is true then all conglomerates and scans are closed (held or non-held). <p> If "close_controllers" is false then no cleanup is done by the TransactionController.  It is then the responsibility of the caller to close all resources that may have been affected by the statements backed out by the call.  This option is meant to be used by the Language implementation of statement level backout, where the system "knows" what could be affected by the scope of the statements executed within the statement. <p> Tell this transaction whether it should time out immediately if a lock cannot be granted without waiting. This mechanism can for instance be used if an operation is first attempted in a nested transaction to reduce the lifetime of locks held in the system tables (like when a stored prepared statement is compiled and stored). In such a case, the caller must catch timeout exceptions and retry the operation in the main transaction if a lock timeout occurs. Set a save point in the current transaction. A save point defines a point in time in the transaction that changes can be rolled back to. Savepoints can be nested and they behave like a stack. Setting save points "one" and "two" and the rolling back "one" will rollback all the changes made since "one" (including those made since "two") and release savepoint "two". Get an nested user transaction. <p> A nested user transaction can be used exactly as any other TransactionController, except as follows.  For this discussion let the parent transaction be the transaction used to make the startNestedUserTransaction() call, and let the child transaction be the transaction returned by the startNestedUserTransaction() call. <p> A parent transaction can nest a single readonly transaction and a single separate read/write transaction. If a subsequent nested transaction creation is attempted against the parent prior to destroying an existing nested user transaction of the same type, an exception will be thrown. <p> The nesting is limited to one level deep.  An exception will be thrown if a subsequent getNestedUserTransaction() is called on the child transaction. <p> The locks in the child transaction of a readOnly nested user transaction will be compatible with the locks of the parent transaction.  The locks in the child transaction of a non-readOnly nested user transaction will NOT be compatible with those of the parent transaction - this is necessary for correct recovery behavior. <p> A commit in the child transaction will release locks associated with the child transaction only, work can continue in the parent transaction at this point. <p> Any abort of the child transaction will result in an abort of both the child transaction and parent transaction, either initiated by an explict abort() call or by an exception that results in an abort. <p> A TransactionController.destroy() call should be made on the child transaction once all child work is done, and the caller wishes to continue work in the parent transaction. <p> AccessFactory.getTransaction() will always return the "parent" transaction, never the child transaction.  Thus clients using nested user transactions must keep track of the transaction, as there is no interface to query the storage system to get the current child transaction.  The idea is that a nested user transaction should be used to for a limited amount of work, committed, and then work continues in the parent transaction. <p> Nested User transactions are meant to be used to implement system work necessary to commit as part of implementing a user's request, but where holding the lock for the duration of the user transaction is not acceptable.  2 examples of this are system catalog read locks accumulated while compiling a plan, and auto-increment. <p> Once the first write of a non-readOnly nested transaction is done, then the nested user transaction must be committed or aborted before any write operation is attempted in the parent transaction. <p> fix for DERBY-5493 introduced a behavior change for commits executed against an updatable nested user transaction.  Prior to this change commits would execute a "lazy" commit where commit log record would only be written to the stream, not guaranteed to disk.  After this change commits on these transactions will always be forced to disk.  To get the previous behavior one must call commitNoSync() instead. <p> examples of current usage of nested updatable transactions in Derby include: o recompile and saving of stored prepared statements, changed with DERBY-5493 to do synchronous commit.  Code in SPSDescriptor.java. o sequence updater reserves new "range" of values in sequence catalog, changed with DERBY-5493 to do synchronous commit.  Without this change crash of system might lose the updat of the range and then return same value on reboot.  Code in SequenceUpdater.java o in place compress defragment phase committing units of work in moving tuples around in heap and indexes.  changed with DERBY-5493 to do synchronous commit. code in AlterTableConstantAction.java. o used for creation of users initial default schema in SYSSCHEMAS. moving tuples around in heap and indexes.  changed with DERBY-5493 to do synchronous commit. code in DDLConstantAction.java. o autoincrement/generated key case.  Kept behavior previous to DERBY-5493 by changing to use commitNoSync, and defaulting flush_log_on_xact_end to false.  Changing every key allocation to be a synchronous commit would be a huge performance problem for existing applications depending on current performance. code in InsertResultSet.java
Checks if there any backup blocking operations are in progress and prevents new ones from starting until the backup is finished. Database creation finished Find a transaction using a transactionId and make the passed in transaction assume the identity and properties of that transaction. Used in recovery only. Find a user transaction within the given raw store and the given contextMgr.  If no user transaction exist, then start one with name transName. This method will push a transaction context as described in RawStoreFactory.startTransaction The first log instant that belongs to a transaction that is still active in the raw store. This is the first log record of the longest running transaction at this moment. Get the LockFactory to use with this store.  Return the transaction table so it can get logged with the checkpoint log record. Return the module providing XAresource interface to the transaction table. XAResourceManager Run through all prepared transactions known to this factory and restore their state such that they remain after recovery, and can be found and handled by a XA transaction manager.  This includes creating a context manager for each, pushing a xact context, and reclaiming update locks on all data changed by the transaction. Used only in recovery. Check if there are any prepared transanctions. <P>MT - unsafe, called during boot, which is single threaded. Returns true if the transaction factory has no active updating transaction Reset any resettable transaction Id Rollback and close all transactions known to this factory using a passed in transaction.  Used only in recovery. make Transaction factory aware of which raw store factory it belongs to Start a new transaction within the given raw store. This method will push a transaction context as described in RawStoreFactory.startTransaction Start a new internal transaction within the given raw store. This method will push a transaction context as described in RawStoreFactory.startInternalTransaction Start a new read only transaction within the given raw store. This method will push a transaction context as described in RawStoreFactory.startNestedTransaction Start a new nested top transaction within the given raw store. This method will push a transaction context as described in RawStoreFactory.startNestedTopTransaction Start a new update transaction within the given raw store. This method will push a transaction context as described in RawStoreFactory.startNestedTransaction Start a new transaction within the given raw store. This method will push a transaction context as described in RawStoreFactory.startTransaction Submit a post commit work to the post commit daemon. The work is always added to the deamon, regardless of the state it returns. Backup completed. Allow backup blocking operations. Use this transaction table, which is gotten from a checkpoint operation.  Use ONLY during recovery.
Return the maximum number of bytes the transactionId will take to store using writeExternal.

Notifies registered listener that the transaction is about to commit. Called before the commit is recorded and flushed to the transaction log device. Notifies registered listener that the transaction is about to rollback. Called before any physical rollback. The listener will be removed from the current transaction once the method returns.
Add to the list of post commit work. <p> Add to the list of post commit work that may be processed after this transaction commits.  If this transaction aborts, then the post commit work list will be thrown away.  No post commit work will be taken out on a rollback to save point. <p> This routine simply delegates the work to the Rawstore transaction. Check to see if a database has been upgraded to the required level in order to use a store feature. The ConglomerateController.close() method has been called on "conglom_control". <p> Take whatever cleanup action is appropriate to a closed conglomerateController.  It is likely this routine will remove references to the ConglomerateController object that it was maintaining for cleanup purposes. The SortController.close() method has been called on "sort_control". <p> Take whatever cleanup action is appropriate to a closed sortController.  It is likely this routine will remove references to the SortController object that it was maintaining for cleanup purposes. The ScanManager.close() method has been called on "scan". <p> Take whatever cleanup action is appropriate to a closed scan.  It is likely this routine will remove references to the scan object that it was maintaining for cleanup purposes. Return existing Conglomerate after doing lookup by ContainerKey <p> Throws exception if it can't find a matching conglomerate for the ContainerKey. Get an Internal transaction. <p> Start an internal transaction.  An internal transaction is a completely separate transaction from the current user transaction.  All work done in the internal transaction must be physical (ie. it can be undone physically by the rawstore at the page level, rather than logically undone like btree insert/delete operations).  The rawstore guarantee's that in the case of a system failure all open Internal transactions are first undone in reverse order, and then other transactions are undone in reverse order. <p> Internal transactions are meant to implement operations which, if interupted before completion will cause logical operations like tree searches to fail.  This special undo order insures that the state of the tree is restored to a consistent state before any logical undo operation which may need to search the tree is performed. <p> Get the Transaction from the Transaction manager. <p> Access methods often need direct access to the "Transaction" - ie. the raw store transaction, so give access to it.
clean up error and print it to derby.log if diagActive is true context management An error happens in the constructor, pop the context. Resolve: probably superfluous local transaction demarcation - note that global or xa transaction cannot commit thru the connection, they can only commit thru the XAResource, which uses the xa_commit or xa_rollback interface as a safeguard. need to be public because it is in the XATransactionResource interface Return instance variables to EmbedConnection.  RESOLVE: given time, we should perhaps stop giving out reference to these things but instead use the transaction resource itself. TransactionResource methods exception handling clean up the error and wrap the real exception in some SQLException. class specific methods is the underlaying database still active? Determine if the exception thrown is a login exception. Needed for DERBY-5427 fix to prevent inappropriate thread dumps and javacores. This exception is special because it is SESSION_SEVERITY and database.isActive() is true, but the session hasn't started yet,so it is not an actual crash and should not report extended diagnostics. Called only in EmbedConnection construtor. The Local Connection sets up the database in its constructor and sets it here. Called only in EmbedConnection constructor.  Create a new transaction by creating a lcc. The arguments are not used by this object, it is used by XATransactionResoruceImpl.  Put them here so that there is only one routine to start a local connection. Wrap a <code>Throwable</code> in an <code>SQLException</code>.
COMMIT and ROLLBACK are allowed to commit and rollback, duh. Returns whether or not this Statement requires a set/clear savepoint around its execution.  The following statement "types" do not require them: Cursor	- unnecessary and won't work in a read only environment Xact	- savepoint will get blown away underneath us during commit/rollback
VTI costing interface   All columns in TransactionTable VTI is of String type.
Cloneable Methods of TransactionInfo Return my format identifier. ************************************************************************ get instance variables ************************************************************************* set my transaction instance variable for a recovery transaction

Get the trigger action sps UUID Get the trigger action sps from SYSSTATEMENTS. If we find that the sps is invalid and the trigger is defined at row level and it has OLD/NEW transient variables through REFERENCES clause, then the sps from SYSSTATEMENTS may not be valid anymore. In such a case, we regenerate the trigger action sql and use that for the sps and update SYSSTATEMENTS using this new sps. This update of SYSSTATEMENTS was introduced with DERBY-4874 Get the provider's type. Privileged lookup of a Context. Must be private so that user code can't call this entry point. Get the time that this trigger was created. //////////////////////////////////////////////////////////////////  PROVIDER INTERFACE  //////////////////////////////////////////////////////////////////    Get the trigger name Get the new Referencing name, if any, from the REFERENCING clause. Get the provider's UUID Return the name of this Provider.  (Useful for errors.) Get the old Referencing name, if any, from the REFERENCING clause. Get the referenced table descriptor for this trigger. caller converts referencedCols to referencedColsDescriptor... public ReferencedColumns getReferencedColumnsDescriptor() throws StandardException { return (referencedCols == null) ? (ReferencedColumns)null : new ReferencedColumnsDescriptorImpl(referencedCols); } Get the referenced column array for this trigger, used in "alter table drop column", we get the handle and change it Get the referenced column array for the trigger action columns. Get whether or not NEW was replaced in the REFERENCING clause. Get whether or not OLD was replaced in the REFERENCING clause. Get the SPS for the triggered SQL statement or the WHEN clause. Get the triggers schema descriptor Get the trigger table descriptor Get the original trigger definition. Get the trigger event mask.  Currently, a trigger may only listen for a single event, though it may OR multiple events in the future. Get the formatID which corresponds to this class. Get the trigger UUID Get the trigger when clause sps UUID Get the trigger when clause sps Get the SQL text of the WHEN clause. Is this a before trigger Is this trigger enforced Is this a row trigger ////////////////////////////////////////////////////  DEPENDENT INTERFACE  Triggers are dependent on the underlying table, and their spses (for the trigger action and the WHEN clause).  //////////////////////////////////////////////////// Check that all of the dependent's dependencies are valid. Indicate whether this trigger listens for this type of event. Mark the dependent as invalid (due to at least one of its dependencies being invalid).  Always an error for a trigger -- should never have gotten here. Does this trigger need to fire on this type of DML? Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). ////////////////////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////////////////////// Read this object from a stream of stored objects. Mark this trigger as disabled Mark this trigger as enforced Update the array of referenced columns Set the referenced column array for trigger actions Write this object to a stream of stored objects.

Get the type number of this trigger Get the type number of this trigger Is this an after trigger Is this a before trigger
Clean up and release resources. Handle the given event. Reopen the trigger activator.  Just creates a new trigger execution context.  Note that close() still must be called when you are done -- you cannot just do a reopen() w/o a first doing a close.

Get the last auto-increment value for the specified column. Get the text of the statement that caused the trigger to fire. Get the type for the event that caused the trigger to fire. Like getNewRowSet(), but returns a result set positioned on the first row of the after (new) result set.  Used as a convenience to get a column for a row trigger.  Equivalent to getNewRowSet() followed by next(). <p> Will return null if the call is inapplicable for the trigger that is currently executing.  For example, will return null if called during the firing of a DELETE trigger. Returns a result set of the new (after) images of the changed rows. For a row trigger, this result set will have a single row.  For a statement trigger, this result set has every row that has changed or will change.  If a statement trigger does not affect a row, then the result set will be empty (i.e. ResultSet.next() will return false). <p> Will return null if the call is inapplicable for the trigger that is currently executing.  For example, will return null if called during the firing of a DELETE trigger. Like getOldRowSet(), but returns a result set positioned on the first row of the before (old) result set.  Used as a convenience to get a column for a row trigger.  Equivalent to getOldRowSet() followed by next(). <p> Will return null if the call is inapplicable for the trigger that is currently executing.  For example, will return null if called during a the firing of an INSERT trigger. Returns a result set of the old (before) images of the changed rows. For a row trigger, this result set will have a single row.  For a statement trigger, this result set has every row that has changed or will change.  If a statement trigger does not affect a row, then the result set will be empty (i.e. ResultSet.next() will return false). <p> Will return null if the call is inapplicable for the trigger that is currently executing.  For example, will return null if called during a the firing of an INSERT trigger. Get the target table UUID upon which the trigger event is declared. Get the target table name upon which the trigger event is declared.
Get the formatID which corresponds to this class. Do we have a trigger or triggers that meet the criteria Read this object from a stream of stored objects. ////////////////////////////////////////////////////////////  Misc  //////////////////////////////////////////////////////////// ////////////////////////////////////////////  FORMATABLE  //////////////////////////////////////////// Write this object out



used for performance numbers

////////////////////////////////////////////////////////////////  BEHAVIOR. These are only used by Replication!!  //////////////////////////////////////////////////////////////// each descriptor has a name Each descriptor must identify itself with its type; i.e index, check constraint whatever. Is this provider persistent?  A stored dependency will be required if both the dependent and provider are persistent.
Pump a row through the Filter. Initialize a Filter with a vector of parameters.

Called before the code parses a descriptor analyze exception handling some more here analyze exception handling some more here analyze exception handling some more here -------------------------private and package friendly methods--------------- Called after the code parses a descriptor Populates netCursor descriptors, rename this populateCursorDescriptors()
* Class specific methods. Get the VM Type name (java/lang/Object) Get the VM type (eg. VMDescriptor.INT)
Determine if this type is compatible to some other type (e.g. COALESCE(thistype, othertype)). Determine if this type can be CONVERTed to some other type Generate the code necessary to produce a SQL value based on a value.  The value's type is assumed to match the type of this TypeId.  For example, a TypeId for the SQL int type should be given an value that evaluates to a Java int or Integer. If the type of the value is incorrect, the generated code will not work. The stack must contain data value factory value. Generate the code necessary to produce a SQL null of the appropriate type. The stack must contain a DataValueFactory and a null or a value of the correct type (interfaceName()). Return the maximum width for this data type when cast to a char type. Get the name of the corresponding Java type.  For numerics and booleans we will get the corresponding Java primitive type. e Each SQL type has a corresponding Java type.  When a SQL value is passed to a Java method, it is translated to its corresponding Java type.  For example, a SQL Integer will be mapped to a Java int, but a SQL date will be mapped to a java.sql.Date. Get the method name for getting out the corresponding primitive Java type from a DataValueDescriptor. Get the name of the interface for this type.  For example, the interface for a SQLInteger is NumberDataValue.  The full path name of the type is returned. Type resolution methods on binary operators Determine if this type can have a value of another type stored into it. Note that direction is relevant here: the test is that the otherType is storable into this type.
Get a TypeCompiler corresponding to the given TypeId.
Check whether the given TypeCompiler has been allocated yet. If so, just return it, otherwise allocate a new instance given its class. Get a TypeCompiler corresponding to the given TypeId
Get the collation type for this type. This api applies only to character string types. And its return value is valid only if the collation derivation  of this type is "implicit" or "explicit". (In Derby 10.3, collation derivation can't be "explicit". Hence in Derby 10.3, this api should be used only if the collation derivation is "implicit". /////////////////////////////////////////////////////////////////////  METHODS  ///////////////////////////////////////////////////////////////////// Get the jdbc type id for this type.  JDBC type can be found in java.sql.Types. Returns the maximum width of the type.  This may have different meanings for different types.  For example, with char, it means the maximum number of characters, while with int, it is the number of bytes (i.e. 4). Returns the maximum width of the type IN BYTES.  This is the maximum number of bytes that could be returned for this type if the corresponding getXXX() method is used.  For example, if we have a CHAR type, then we want the number of bytes that would be returned by a ResultSet.getString() call. Returns the number of decimal digits for the type, if applicable. If this catalog type is a row multi-set type then return its array of column names. If this catalog type is a row multi-set type then return its array of catalog types. Converts this type descriptor (including length/precision) to a string suitable for appearing in a SQL type specifier.  E.g. VARCHAR(30) or java.util.Hashtable Returns the number of digits to the right of the decimal for the type, if applicable. Gets the name of this type. Gets the nullability that values of this type have. Return true if this is a Row Multiset type Return true if this is a user defined type
copy an array of type descriptors Compare if two TypeDescriptors are exactly the same  Get the jdbc type id for this type.  JDBC type can be found in java.sql.Types.  Return the length of this type in bytes.  Note that while the JDBC API _does_ define a need for returning length in bytes of a type, it doesn't state clearly what that means for the various types.  We assume therefore that the values here are meant to match those specified by the ODBC specification (esp. since ODBC clients are more likely to need this value than a Java client). The ODBC spec that defines the values we use here can be found at the following link: http://msdn.microsoft.com/library/default.asp?url=/library/ en-us/odbc/htm/odbctransfer_octet_length.asp Returns the number of decimal digits for the datatype, if applicable. Converts this data type descriptor (including length/precision) to a string. E.g. VARCHAR(30) or java.util.Hashtable Returns the number of digits to the right of the decimal for the datatype, if applicable. Get the formatID which corresponds to this class. Get the type Id stored within this type descriptor. Gets the name of this datatype. Returns TRUE if the datatype can contain NULL, FALSE if not. JDBC supports a return value meaning "nullability unknown" - I assume we will never have columns where the nullability is unknown.  Report whether this type is a string type.  Formatable methods Read this object from a stream of stored objects. Write this object to a stream of stored objects.
* Static methods to obtain TypeIds Create a TypeId for the given format identifiers using a BaseTypeIdImpl. Used to create the static final variables of this class. we want equals to say if these are the same type id or not. Return all of the builtin type ids.  Class methods  Get the approximate length of this type in bytes. For most datatypes this is just going to be dts.getMaximumWidth().  Some types, such as bit, will override this. Get the base type id that is embedded in this type id.  The base type id is an object with a minimal implementation of TypeId that is intended to be usable on the client side. Get a TypeId of the given JDBC type.  This factory method is intended to be used for built-in types.  For user-defined types, we will need a factory method that takes a Java type name. Given a SQL type name return the corresponding TypeId. Get the name of the corresponding Java type. Each SQL type has a corresponding Java type.  When a SQL value is passed to a Java method, it is translated to its corresponding Java type.  For example, when a SQL date column is passed to a method, it is translated to a java.sql.Date. JDBC has its own idea of type identifiers which is different from the Derby internal type ids.  The JDBC type ids are defined as public final static ints in java.sql.Types.  This method translates a Derby internal TypeId to a JDBC type id. For java objects this returns JAVA_OBJECT in Java2 and OTHER in JDK 1.1. For Boolean datatypes, this returns Type.BOOLEAN in JDK1.4 and Type.BIT for jdks prior to 1.4 Get the maximum maximum width of the type (that's not a typo).  For types with variable length, this is the absolute maximum for the type. Get the maximum precision of the type.  For types with variable precision, this is an arbitrary high precision. Get the maximum scale of the type.  For types with variable scale, this is an arbitrary high scale. Get SQL null value. Get the precision of the merge of two Decimals Get the name of the corresponding Java type. This method is used directly from EmbedResultSetMetaData (jdbc) to return the corresponding type (as choosen by getObject). It solves a specific problem for BLOB types where the getCorrespondingJavaTypeName() is used internall for casting which doesn't work if changed from byte[] to java.sql.Blob. So we do it here instread, to avoid unexpected sideeffects. Get a TypeId for the class that corresponds to the given Java type name. Returns the SQL name of the datatype. If it is a user-defined type, it returns the full Java path name for the datatype, meaning the dot-separated path including the package names. Get the scale of the merge of two decimals Get the formatID which corresponds to this class. Get the TypeId (fundemental type information) for a catalog type. This factory  method is used for ANSI UDTs. If the className argument is null, then this TypeId will have to be bound. Hashcode which works with equals. Is this a type id for a bit type? Is this a Blob? Is this a type id for a boolean type? Is this a Clob? Is this a type id for a concatable type? Is this DATE/TIME or TIMESTAMP? Is this a TypeId for DATE/TIME/TIMESTAMP Is this a type id for a decimal type? Is this a TypeId for DOUBLE Is this a fixed string type? Is this a TypeId for floating point (REAL/DOUBLE) Is this a type id for a LOB type? Is this a type id for a long concatable type? Is this a LongVarbinary? Is this a LongVarchar? Is this a type id for a numeric type? Is this a TypeId for REAL Is this a type id for a ref type? Does this TypeId represent a TypeId for a StringDataType. Is this a TIMESTAMP? Is this a type id for a user defined type? Is this an XML doc? Tell whether this type is orderable, that is, can participate in comparisons. Is this type StreamStorable? Converts this TypeId, given a data type descriptor (including length/precision), to a string. E.g. VARCHAR(30) For most data types, we just return the SQL type name. Each built-in type in JSQL has a precedence.  This precedence determines how to do type promotion when using binary operators.  For example, float has a higher precedence than int, so when adding an int to a float, the result type is float. The precedence for some types is arbitrary.  For example, it doesn't matter what the precedence of the boolean type is, since it can't be mixed with other types.  But the precedence for the number types is critical.  The SQL standard requires that exact numeric types be promoted to approximate numeric when one operator uses both.  Also, the precedence is arranged so that one will not lose precision when promoting a type. NOTE: char, varchar, and longvarchar must appear at the bottom of the hierarchy, but above USER_PRECEDENCE, since we allow the implicit conversion of those types to any other built-in system type. Tell whether this is a built-in type. NOTE: There are 3 "classes" of types: built-in                - system provided types which are implemented internally (int, smallint, etc.) system built-in - system provided types, independent of implementation (date, time, etc.) user types              - types implemented outside of the system (java.lang.Integer, asdf.asdf.asdf, etc.) Does type hava a declared variable length (defined by the application). Examples are CHAR(10), CLOB(1M). Unbounded long types, like LONG VARCHAR return false here.
Get a universally unique identifier for the type of this object.

Method to adjust line and column numbers for the start of a token.<BR> This method was added to support ability to get the input between two tokens.  This method was added to support ability to get the input between two tokens.
Get the formatID which corresponds to this class. /////////////////////////////////////////////////////////////////////////////////  AliasInfo BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  Formatable BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Read this object from a stream of stored objects. This is used by dblook to reconstruct the UDT-specific parts of the ddl needed to recreate this alias. Write this object to a stream of stored objects.
end of getValidDerbyProps
Tests whether the named file exists. end of exists Creates an input stream from a file name. end of getInputStream Get the parent of this file.
end of doInit Construct a persistent StorageFile from a path name. Construct a StorageFile from a directory and file name. Construct a StorageFile from a directory and file name.
The following methods implement the Orderable protocol.
Calculates an optimized buffer size. <p> The maximum size allowed is returned if the specified values don't give enough information to say a smaller buffer size is preferable. Close the reader, disallowing further reads. internal implementation Close the underlying stream if it is open. Fills the internal character buffer by decoding bytes from the stream. @GuardedBy("lock") Skips the requested number of characters. Reader implemention. Reads a single character from the stream. Reads characters into an array. Reads characters into an array as ASCII characters. <p> Due to internal buffering a smaller number of characters than what is requested might be returned. To ensure that the request is fulfilled, call this method in a loop until the requested number of characters is read or <code>-1</code> is returned. <p> Characters outside the ASCII range are replaced with an out of range marker. Methods just for Derby's JDBC driver Reads characters from the stream. <p> Due to internal buffering a smaller number of characters than what is requested might be returned. To ensure that the request is fulfilled, call this method in a loop until the requested number of characters is read or <code>-1</code> is returned. Repositions the stream so that the next character read will be the character at the requested position. <p> There are three types of repositioning, ordered after increasing cost: <ol> <li>Reposition within current character buffer (small hops forwards and potentially backwards - in range 1 char to {@code MAXIMUM_BUFFER_SIZE} chars)</li> <li>Forward stream from current position (hops forwards)</li> <li>Reset stream and skip data (hops backwards)</li> </ol> Resets the reader. <p> This method is used internally to achieve better performance. Skips characters. Convenience method generating an {@link UTFDataFormatException} and cleaning up the reader state.
Skip characters in the stream. <p> Note that a smaller number than requested might be skipped if the end-of-stream is reached before the specified number of characters has been decoded. It is up to the caller to decide if this is an error or not. For instance, when determining the character length of a stream, <code>Long.MAX_VALUE</code> could be passed as the requested number of characters to skip. Skip the requested number of characters from the stream. <p> Skip until the end-of-stream is reached.
Clear the DataValueDescriptor cache, if one exists. (The DataValueDescriptor can be 1 of 3 types: o  VARIANT		  - cannot be cached as its value can vary within a scan o  SCAN_INVARIANT - can be cached within a scan as its value will not change within a scan o  QUERY_INVARIANT- can be cached across the life of the query as its value will never change o  CONSTANT	      - can be cached across executions * Qualifier interface Get the id of the column to be qualified. * Get the operator to use in the comparison. Get the value that the column is to be compared to.  Get the getOrderedNulls argument to use in the comparison. Determine if the result from the compare operation is to be negated. <p> If true then only rows which fail the compare operation will qualify. This method reinitializes all the state of the Qualifier.  It is used to distinguish between resetting something that is query invariant and something that is constant over every execution of a query.  Basically, clearOrderableCache() will only clear out its cache if it is a VARIANT or SCAN_INVARIANT value.  However, each time a query is executed, the QUERY_INVARIANT qualifiers need to be reset.
Clone this UUID. Produce a string representation of this UUID which is suitable for use as a unique ANSI identifier.
Create a new UUID.  The resulting object is guaranteed to be unique "across space and time". Recreate a UUID from a string produced by UUID.toString.

Bind this operator For SQRT and ABS the parameter becomes a DOUBLE. For unary + and - no change is made to the underlying node. Once this node's type is set using setType, then the underlying node will have its type set. Bind SQRT or ABS Only called for Unary +/-. Do code generation for this unary plus operator A +? or a -? is considered a parameter. Unary + and - require their type to be set if they wrap another node (e.g. a parameter) that requires type from its context. We are overwriting this method here because for -?/+?, we now know the type of these dynamic parameters and hence we can do the parameter binding. The setType method will call the binding code after setting the type of the parameter
Set the type info for this node.  This method is useful both during binding and when we generate nodes within the language module outside of the parser. Bind this comparison operator.  All that has to be done for binding a comparison operator is to bind the operand and set the result type to SQLBoolean. Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree.      Get the absolute 0-based column position of the ColumnReference from the conglomerate for this Optimizable.  RelationalOperator interface   Negate the comparison.
Called by UnaryOperatorNode.bindExpression. If the operand is a constant then evaluate the function at compile time. Otherwise, if the operand input type is the same as the output type then discard this node altogether. If the function is "date" and the input is a timestamp then change this node to a cast. end of bindUnaryOperator Do code generation for this unary operator. end of generateExpression
Bind this logical operator.  All that has to be done for binding a logical operator is to bind the operand, check that the operand is SQLBoolean, and set the result type to SQLBoolean. Set all of the type info (nullability and DataTypeServices) for this node.  Extracts out tasks that must be done by both bind() and post-bind() AndNode generation.
Accept the visitor for all visitable children of this node. Add some additional arguments to our method call for XML related operations like XMLPARSE and XMLSERIALIZE. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. This method is the implementation for XMLPARSE and XMLSERIALIZE. Sub-classes need to implement their own bindExpression() method for their own specific rules. Bind the operand for this unary operator. Binding the operator may change the operand node. Sub-classes bindExpression() methods need to call this method to bind the operand. By default unary operators don't accept ? parameters as operands. This can be over-ridden for particular unary operators. We throw an exception if the parameter doesn't have a datatype assigned to it yet. Bind an XMLPARSE operator.  Makes sure the operand type is correct, and sets the result type. Bind an XMLSERIALIZE operator.  Makes sure the operand type and target type are both correct, and sets the result type. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views.  Do code generation for this unary operator. Get the operand of this unary operator. Get the operator of this unary operator. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) CONSTANT			- immutable Get the parameter operand of this unary operator. For the example below, for abs unary operator node, we want to get ? select * from t1 where -? = max_cni(abs(-?), sqrt(+?)) This gets called when ParameterNode is needed to get parameter specific information like getDefaultValue(), getParameterNumber() etc Determine the type the binary method is called on. By default, based on the receiver. Override in nodes that use methods on super-interfaces of the receiver's interface, such as comparisons. Return whether or not this expression tree represents a constant expression.  Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set the methodName. Set the operator. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
************************************************************************ Fields of the class ************************************************************************* ************************************************************************ Constructors for This class: ************************************************************************* ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods of This class: ************************************************************************* Interface to be called when an undo of an insert is processed. <p> Implementer of this class provides interface to be called by the raw store when an undo of an insert is processed.  Initial implementation will be by Access layer to queue space reclaiming events if necessary when a rows is logically "deleted" as part of undo of the original insert.  This undo can happen a lot for many applications if they generate expected and handled duplicate key errors. <p> It may be useful at some time to include the recordId of the deleted row, but it is not used currently by those notified.  The post commit work ultimately processes all rows on the table while it has the latch which is more efficient than one row at time per latch. <p> It is expected that notifies only happen for pages that caller is interested in.  Currently only the following aborted inserts cause a notify: o must be on a non overflow page o if all "user" rows on page are deleted a notify happens (page 1 has a system row so on page one notifies happen if all but the first row is deleted). o if the aborted insert row has either an overflow row or column component then the notify is executed.
Generate a loggable which will undo this change, using the optional input if necessary. <P><B>NOTE</B><BR>Any logical undo logic must be hidden behind generateUndo. During recovery redo, it should not depend on any logical undo logic. <P> There are 3 ways to implement a redo-only log record: <NL> <LI>Make the log record a Loggable instead of an Undoable, this is the cleanest method. <LI>If you want to extend a log operation class that is an Undoable, you can then either have generateUndo return null - this is preferred - (the log operation's undoMe should never be called, so you can put a null body there if the super class you are extending does not implement a undoMe). <LI>Or, have undoMe do nothing - this is least preferred. </NL> <P>Any resource (e.g., latched page) that is needed for the undoable.undoMe() must be acquired in undoable.generateUndo(). Moreover, that resource must be identified in the compensation operation, and reacquired in the compensation.needsRedo() method during recovery redo. <BR><B>If you do write your own generateUndo or needsRedo, any resource you latch or acquire, you must release them in Compensation.doMe() or in Compensation.releaseResource().</B> <P> To write a generateUndo operation, find the object that needs to be rolled back.  Assuming that it is a page, latch it, put together a Compensation operation with the undoOp set to this operation, and save the page number in the compensation operation, then return the Compensation operation to the logging system. <P> The sequence of events in a rollback of a undoable operation is <NL> <LI> The logging system calls undoable.generateUndo.  If this returns null, then there is nothing to undo. <LI> If generateUndo returns a Compensation operation, then the logging system will log the Compensation log record and call Compenstation.doMe().  (Hopefully, this just calls the undoable's undoMe) <LI> After the Compensation operation has been applied, the logging system will call compensation.releaseResource(). If you do overwrite a super class's releaseResource(), it would be prudent to call super.releaseResource() first. </NL> <P> The available() method of in indicates how much data can be read, i.e. how much was originally written.
Add any new ResultSetNodes that are necessary to the tree. We wait until after optimization to do this in order to make it easier on the optimizer. Bind the expressions under this TableOperatorNode.  This means binding the sub-expressions, as well as figuring out what the return type is for each expression. Make the RCL of this node match the target node for the insert. If this node represents a table constructor (a VALUES clause), we replace the RCL with an enhanced one if necessary, and recursively enhance the RCL of each child node. For table constructors, we also need to check that we don't attempt to override auto-increment columns in each child node (checking the top-level RCL isn't sufficient since a table constructor may contain the DEFAULT keyword, which makes it possible to specify a column without overriding its value). If this node represents a regular UNION, put a ProjectRestrictNode on top of this node and enhance the RCL in that node. Generate the code for this UnionNode.  Mark this as the top node of a table constructor.   Optimizable interface  DERBY-649: Handle pushing predicates into UnionNodes. It is possible to push single table predicates that are binaryOperations or inListOperations. Predicates of the form <columnReference> <RELOP> <constant> or <columnReference> IN <constantList> are currently handled. Since these predicates would allow optimizer to pick available indices, pushing them provides maximum benifit. It should be possible to expand this logic to cover more cases. Even pushing expressions (like a+b = 10) into SELECTs would improve performance, even if they don't allow use of index. It would mean evaluating expressions closer to data and hence could avoid sorting or other overheads that UNION may require. Note that the predicates are not removed after pushing. This is to ensure if pushing is not possible or only partially feasible. Check for (and reject) ? parameters directly under the ResultColumns. This is done for SELECT statements.  Don't reject parameters that are in a table constructor - these are allowed, as long as the table constructor is in an INSERT statement or each column of the table constructor has at least one non-? column.  The latter case is checked below, in bindExpressions(). Set the type of column in the result column lists of each source of this union tree to the type in the given result column list (which represents the result columns for an insert). This is only for table constructors that appear in insert statements. Tell whether this is a UNION for a table constructor. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
If the result set has been opened, close the currently open source. A union has a single underlying row at a time, although from one of several sources. RESOLVE - this should return activation.getCurrentRow(resultSetNumber), once there is such a method.  (currentRow is redundant) If there are rows still on the first source, return the next one; otherwise, switch to the second source and return a row from there.  ResultSet interface (leftover from NoPutResultSet)  Returns the description of the first source. Assumes the compiler ensured both sources had the same description.  CursorResultSet interface  A union has a single underlying row at a time, although from one of several sources. Return the total amount of time spent in this ResultSet open the first source.

Check whether there are more numbers in the sequence. Fetch the next number from the sequence.
Get the name of this object.  E.g. for a table descriptor, this will be the table name. Get the objects schema descriptor
Return the UUID for this Descriptor

Methods to check if the duplicate key can be inserted or not. It throws exception if the duplicates has no null part in the key.
Compares two keys. If all the parts of the keys are not null then the leading (keys.length - 1) parts are compared, else if a part of the key is null then all parts of the key are compared (keys.length). This behavior is useful for implementing unique constraints where multiple null values are allowed, but uniqueness must still be guaranteed for keys with no null values.   In this case the leading parts of the key are the user key columns, while the last column is a system provided column which is guaranteed unique per base row.

Find all the methods for java.sql objects in the Connection which raise SQLFeatureNotSupportedException.  Takes an array of classes and returns an array of objects with null values compatible with the classes. Helper method for converting a parameter list to an argument list. Returns a null value compatible with the class. For instance, return <code>Boolean.FALSE</code> for primitive booleans, 0 for primitive integers and <code>null</code> for non-primitive types. ///////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////  Initialize the hashtable of methods which are allowed to raise SQLFeatureNotSupportedException.   Returns true if this method is allowed to raise SQLFeatureNotSupportedException.   Make an MD if the JVM level supports the indicated classes. Returns null if the JVM doesn't.  Debug print the list of method failures which we don't understand debug print the list of methods which throw SQLFeatureNotSupportedException debug print the list of methods which have disappeared from the SQL interface  Record an unexpected error.   Return the methods of an interface in a deterministic order. Class.getMethods() does not do us this favor.  ///////////////////////////////////////////////////////////  ENTRY POINTS  /////////////////////////////////////////////////////////// <p> Find all methods in this framework which raise SQLFeatureNotSupportedException. </p>  Find all the objects inside the ConnectionPooledDataSource and vet them.   Find all the objects inside the DataSource and vet them.   Examine all the methods in an interface to determine which ones raise SQLFeatureNotSupportedException.   Find all the java.sql interfaces implemented by a class and find the methods in those interfaces which raise SQLFeatureNotSupportedException when called on the passed-in candidate object.   Examine BLOBs and CLOBs.   Examine a single method to see if it raises SQLFeatureNotSupportedException.   Find all the methods from java.sql interfaces which are implemented by an object and which raise SQLFeatureNotSupportedException.   Examine Savepoints.   Find all the objects inside the XADataSource and vet them.
Translate a Default node into a default value, given a type descriptor. Return the length public int	getLength() { if (SanityManager.DEBUG) SanityManager.ASSERT(false, "Unimplemented method - should not be called on UntypedNullConstantNode"); return 0; } Should never be called for UntypedNullConstantNode because we shouldn't make it to generate
Reads the next byte of data from the input stream. The value byte is returned as an <code>int</code> in the range <code>0</code> to <code>255</code>. If no byte is available because the end of the stream has been reached, the value <code>-1</code> is returned. This method blocks until input data is available, the end of the stream is detected, or an exception is thrown. <p> A subclass must provide an implementation of this method. <p> Note that this stream will reflect changes made to the underlying Blob at positions equal to or larger then the current position. Reads some number of bytes from the input stream and stores them into the buffer array <code>b</code>. The number of bytes actually read is returned as an integer.  This method blocks until input data is available, end of file is detected, or an exception is thrown. <p> Note that this stream will reflect changes made to the underlying Blob at positions equal to or larger then the current position . Reads up to <code>len</code> bytes of data from the input stream into an array of bytes.  An attempt is made to read as many as <code>len</code> bytes, but a smaller number may be read. The number of bytes actually read is returned as an integer. <p> Note that this stream will reflect changes made to the underlying Blob at positions equal to or larger then the current position . Skips over and discards <code>n</code> bytes of data from this input stream. The <code>skip</code> method may, for a variety of reasons, end up skipping over some smaller number of bytes, possibly <code>0</code>. This may result from any of a number of conditions; reaching end of file before <code>n</code> bytes have been skipped is only one possibility. The actual number of bytes skipped is returned.  If <code>n</code> is negative, no bytes are skipped. <p> Note that this stream will reflect changes made to the underlying Blob at positions equal to or larger then the current position . Checks if this object is using materialized blob if not it checks if the blob was materialized since this stream was last access. If the blob was materialized (due to one of the set methods) it gets the stream again and sets the position to current read position.
INTERFACE METHODS Get the formatID which corresponds to this class.
java.sql.PreparedStatement calls, passed through to our preparedStatement.
gets the increment value for a column. gets the row location gets the name of the desired column in the taget table. get the array of column names in the target table. CLASS METHODS Get the formatID which corresponds to this class. Does the target table has autoincrement columns. INTERFACE METHODS Formatable methods
Loggable methods Change the value of a field. methods to support prepared log the following two methods should not be called during recover Return my format identifier. Read this in LogicalUndoable methods Restore the row stored in the optional data of the log record. method to support BeforeImageLogging restore the before image of the page DEBUG: Print self. Undoable methods Restore field to its old value. Write the old column value and and new column value as optional data. If logical undo, writes out the entire row's before image.
Privileged lookup of a Context. Must be private so that user code can't call this entry point. Privileged module lookup. Must be private so that user code can't call this entry point. Load the class from the class path. Called by JarLoader when it has a request to load a class to fulfill the sematics of derby.database.classpath. <P> Enforces two restrictions: <UL> <LI> Do not allow classes in certain name spaces to be loaded from installed jars, see RESTRICTED_PACKAGES for the list. <LI> Referencing Derby's internal classes (those outside the public api) from installed is disallowed. This is to stop user defined routines bypassing security or taking advantage of security holes in Derby. E.g. allowing a routine to call a public method in derby would allow such routines to call public static methods for system procedures without having been granted permission on them, such as setting database properties. </UL> Tell the lock manager that we don't want timed waits to time out immediately.
Add all of the columns mentioned by the generation clauses of generated columns. The generated columns were added when we called addGeneratedColumns earlier on. Add generated columns to the update list as necessary. We add any column whose generation clause mentions columns already in the update list. We fill in a list of all generated columns affected by this update. We also fill in a list of all generated columns which we added to the update list. Add UPDATE_PRIV on all columns on the left side of SET operators. Associate all added columns with the TARGET table of the enclosing MERGE statement. Bind this UpdateNode.  This means looking up tables and columns and getting their types, and figuring out the result types of all expressions, as well as doing view resolution, permissions checking, etc. <p> Binding an update will also massage the tree so that the ResultSetNode has a set of columns to contain the old row value, followed by a set of columns to contain the new row value, followed by a column to contain the RowLocation of the row to be updated. end of bind() Check table name and then clear it from the result set columns. Collect all of the CastNodes in the WHERE clause and on the right side of SET operators. Later on, we will need to add permissions for all UDTs mentioned by these nodes. Collect all of the result set columns. Add to an evolving list all of the nodes under an expression which may require privilege checks. Do not allow generation clauses to be overriden. Throws an exception if the user attempts to override the value of a generated column.  The only value allowed in a generated column is DEFAULT. We will use addedGeneratedColumns list to pass through the generated columns which have already been added to the update list. Code generation for update. The generated code will contain: o  A static member for the (xxx)ResultSet with the RowLocations	and new update values o  The static member will be assigned the appropriate ResultSet within the nested calls to get the ResultSets.  (The appropriate cast to the (xxx)ResultSet will be generated.) o  The CurrentRowLocation() in SelectNode's select list will generate a new method for returning the RowLocation as well as a call to that method when generating the (xxx)ResultSet. Construct the changedColumnIds array. Note we sort its entries by columnId. Get the names of the explicitly set columns, that is, the columns on the left side of SET operators. Gets the map of all columns which must be read out of the base table. These are the columns needed to<UL>: <LI>maintain indices</LI> <LI>maintain foreign keys</LI> <LI>maintain generated columns</LI> <LI>support Replication's Delta Optimization</LI></UL> <p> The returned map is a FormatableBitSet with 1 bit for each column in the table plus an extra, unsued 0-bit. If a 1-based column id must be read from the base table, then the corresponding 1-based bit is turned ON in the returned FormatableBitSet. <p> <B>NOTE</B>: this method is not expected to be called when all columns are being updated (i.e. updateColumnList is null). Return the type of statement, something from StatementType. Builds a bitmap of all columns which should be read from the Store in order to satisfy an UPDATE statement. Is passed a list of updated columns. Does the following: 1)	finds all indices which overlap the updated columns 2)	adds the index columns to a bitmap of affected columns 3)	adds the index descriptors to a list of conglomerate descriptors. 4)	finds all constraints which overlap the updated columns and adds the constrained columns to the bitmap 5)	finds all triggers which overlap the updated columns. 6)	Go through all those triggers from step 5 and for each one of those triggers, follow the rules below to decide which columns should be read. Rule1)If trigger column information is null, then read all the columns from trigger table into memory irrespective of whether there is any trigger action column information. 2 egs of such triggers create trigger tr1 after update on t1 for each row values(1); create trigger tr1 after update on t1 referencing old as oldt for each row insert into t2 values(2,oldt.j,-2); Rule2)If trigger column information is available but no trigger action column information is found and no REFERENCES clause is used for the trigger, then read all the columns identified by the trigger column. eg create trigger tr1 after update of c1 on t1 for each row values(1); Rule3)If trigger column information and trigger action column information both are not null, then only those columns will be read into memory. This is possible only for triggers created in release 10.9 or higher(with the exception of 10.7.1.1 where we did collect that information but because of corruption caused by those changes, we do not use the information collected by 10.7). Starting 10.9, we are collecting trigger action column informatoin so we can be smart about what columns get read during trigger execution. eg create trigger tr1 after update of c1 on t1 referencing old as oldt for each row insert into t2 values(2,oldt.j,-2); Rule4)If trigger column information is available but no trigger action column information is found but REFERENCES clause is used for the trigger, then read all the columns from the trigger table. This will cover soft-upgrade scenario for triggers created pre-10.9. eg trigger created prior to 10.9 create trigger tr1 after update of c1 on t1 referencing old as oldt for each row insert into t2 values(2,oldt.j,-2); 7)	adds the triggers to an evolving list of triggers 8)	finds all generated columns whose generation clauses mention the updated columns and adds all of the mentioned columns Compile constants that Execution will use Force correlated column references in the SET clause to have the name of the base table. This dances around the problem alluded to in scrubResultColumn(). Normalize synonym column references to have the name of the base table. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Add privilege checks for UDTs referenced by this statement. Return true if the node references SESSION schema tables (temporary or permanent) Updates are deferred if they update a column in the index used to scan the table being updated. Tag the original columns mentioned in the result list. Tag all of the nodes which may require privilege checks. These are various QueryTreeNodes in the WHERE clause and on the right side of SET operators. Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.
Loggable methods Store the new record directly over the old record, the implementation of storeRecord is responsible for removing any old data. Return the last column of the row this operation logged methods to support prepared log the following two methods should not be called during recover return RecordHandle of the update row. <p> Return the RecordHandle that should be locked when updating the row in this UpdateOperation. <p> Return my format identifier. Read this in ************************************************************************ Public Methods of RePreparable Interface: ************************************************************************* reclaim locks associated with the changes in this log record. <p> PageBasicOperation restore the before image of the page PhysicalPageOperation methods Store the old record directly over the new record, the implementation of storeRecord is responsible for removing any new data. Write out the changed colums of new record (from the row) followed by changed columns of the old record (from the page).
Following 2 methods are for checking and make sure we don't have one un-objectified stream to be inserted into 2 temp table rows for deferred update.  Otherwise it would cause problem when writing to disk using the stream a second time.  In other cases we don't want to unnecessarily objectify the stream. beetle 4896.  Run check constraints against the current row. Raise an error if a check constraint is violated, unless all the offending checks are deferred, in which case a false value will be returned. A NULL value will be interpreted as success (not violation). getSetAutoincrementValue will get the autoincrement value of the columnPosition specified for the target table. If increment is non-zero we will also update the autoincrement value. The implementation of this method is slightly different than the one in InsertResultSet. This code was originally written for insert but with DERBY-6414, we have started supporting update of auto generated column with keyword DEFAULT. The reason of different implementation is that the array used in InsertResultSet's implementation of this method, ColumnDescriptors in resultDescription hold different entries for insert and update case. For insert case, the array holds the column descriptors of all the columns in the table. This is because all the columns in the table are going to get some value into them whether or not they were included directly in the actual INSERT statement. The 2nd array, rla has a spot for each of the columns in the table, with non null value for auto generated column. But in case of Update, resultDescription does not include all the columns in the table. It only has the columns being touched by the Update statement(the rest of the columns in the table will retain their original values), and for each of those touched columns, it has a duplicate entry in resultDescription in order to have before and after values for the changed column values. Lastly, it has a row location information for the row being updated. This difference in array content of resultDescription requires us to have separate implementation of this method for insert and update. beetle 3865, updateable cursor use index. If the row we are updating has new value that falls into the direction of the index scan of the cursor, we save this rid into a hash table (for fast search), so that when the cursor hits it again, it knows to skip it.  <p> Special handling if this is an UPDATE action of a MERGE statement. </p>
Re-creates the underlying Locator stream with the current position and the length values if specified.
Re-creates the underlying Locator stream with the current position and the length values if specified.
Verifies whether the current updateCount matches the updateCount of the Clob object and if it does not it recreates the stream. Re-creates the underlying Locator stream with the current position and the length values if specified.
Verifies whether the current updateCount matches the updateCount of the LOB object and if it does not it recreates the stream. Abstract method that will be implemented by the underlying streams specific to Clob and Blob.
* Methods of Hashtable (overridden) Put the key-value pair in the Properties set and mark this set as modified. Remove the key-value pair from the Properties set and mark this set as modified. Saves the service properties to the disk. look at the comments for serviceBooted at the top to understand this. * Class specific methods.
end of openCore end of updateVTI
Get the fixpack number of the old version being upgraded from. Get the major number of the old version being upgraded from. Get the minor number of the old version being upgraded from. Get the point number of the old version being upgraded from. Returns a {@code DerbyVersion} object describing the old version. Pretty-print the original version number. Get the phase of the upgrade sequence we are running. One of PH_CREATE, PH_SOFT_UPGRADE, PH_POST_SOFT_UPGRADE, PH_HARD_UPGRADE, PH_POST_HARD_UPGRADE. Pretty-print the phase. Return true if the old version is equal to or more recent that the passed in major and minor version. Return true if the old version is equal the passed in major and minor version. Return true if and only if the old version is equal to the passed major, minor, fixpack and point version specified version. Return true if and only if the old version is less than the specified version.
Create a class loader using jars in the specified location. Add all jars specified in jarFiles and the testing jar. Get a byte array with the contents of the class file for the {@code DriverUnloader} class. Get the location of jars of old release. The location is specified in the property derbyTesting.oldReleasePath. If derbyTesting.oldReleasePath is set to the empty string it is ignored. Get the location of jars of old release, using the url for svn at apache. <p> Wrap a class loader around the given version. </p>
Add the tests from the various Changes classes (sub-classes of UpgradeChange) to the base suite which corresponds to a single phase of a run against an old database version. <BR> Changes are only added if the old version is older than then version the changes represent. Thus Changes10_2 is not added if the old database (upgrade from) is already at 10.2, since Changes10_2 is intended to test upgrade from an older version to 10.2. <BR> This is for two reasons: <OL> <LI> Prevents an endless increase in number of test cases that do no real testing. <LI> Simplifies test fixtures by allowing them to focus on cases where testing is required, and not handling all future situations. </OL> When running against certains old releases in Java SE 6 we need to setup the connections to the old database to not use the specific JDBC 4 datasources (e.g. EmbeddedDataSource40). (Since they don't exist in the old release). Return true if and only if the left version is less than the right version. Adds a subset of the tests from DatabaseMetaDataTest to the test suite. <p> We want to run DatabaseMetaDataTest, but it includes some features not supported in older versions, so we cannot just add the DatabaseMetaDataTest.class as is. Note also, that this does not execute fixture initialCompilationTest. Check if a version suffers from DERBY-4835 or DERBY-5289.
Returns the name of this taglet upgrade not expected to be used in constructor documentation. upgrade not expected to be used in field documentation. upgrade not expected to be used in method documentation. upgrade can be used in overview documentation. upgrade can be used in package documentation. upgrade can be used in type documentation. upgrade is not an inline tag. Register this Taglet. Embed the contents of the upgrade tag as a row in the disk format table. Embed multiple upgrade tags as cells in the disk format table.
Make a "Could not instantiate aggregator" exception. Wrap the input operand in an implicit CAST node as necessary in order to coerce it the correct type for the aggregator. Return null if no cast is necessary. Determines the result datatype and verifies that the input datatype is correct. /////////////////////////////////////////////////////////////////////////////////  BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// Get the wrapped alias descriptor Get the Java class corresponding to a Derby datatype. Verify that an actual type is compatible with the expected type.
Authenticate a user's credentials. <BR> E.g. if connection url is <code>jdbc:derby:testdb;user=Fred;password=ScT7dmM2</code> then the userName will be Fred and within the Derby user authorization system, Fred becomes a case-insensitive authorization identifier and is known as FRED <BR> if connection url is <code>jdbc:derby:testdb;user="Fred";password=ScT7dmM2</code> then the userName will be "Fred" and within the Derby user authorization system, Fred becomes a case-sensitive authorization identifier and is known as Fred <BR>
Set the value of this UserDataValue to the given Object
Return the result of the aggregation. . Get the formatID which corresponds to this class. /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// Record an instantiation error trying to load the aggregator class.   Initialization logic shared by setup() and newAggregator() /////////////////////////////////////////////////////////////////////////////////  ExecAggregator BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// ///////////////////////////////////////////////////////////  FORMATABLE INTERFACE  ///////////////////////////////////////////////////////////
TypeCompiler methods Right now, casting is not allowed from one user defined type to another.    ANSI UDTs can only be stored into values of exactly their own type. This restriction can be lifted when we implement the ANSI subclassing clauses. Old-style User types are storable into other user types that they are assignable to. The other type must be a subclass of this type, or implement this type as one of its interfaces.
Return the java class name for this type Get the formatID which corresponds to this class. Has this user type been bound? Formatable interface. Read this object from a stream of stored objects. Does this type id represent a user type? Write this object to a stream of stored objects.
<p> Zero the password after getting it so that the char[] can't be memory-sniffed. </p>   class interface
DataValueDescriptor interface   Orderable interface * SQL Operators The = operator as called from the language module, as opposed to the storage module. end of estimateMemoryUsage             Storable interface, implies Externalizable, TypedFormat Return my format identifier. this is for DataType's error generator Get the type name of this value,  overriding with the passed in class name (for user/java types). Hash code Check if the value is null. The &lt;&gt; operator as called from the language module, as opposed to the storage module.     DataValueDescriptor interface  * String display of value
Should never be called for UserTypeConstantNode because we have our own generateExpression(). For a UserTypeConstantNode, we have to store away the object somewhere and have a way to get it back at runtime. These objects are serializable.  This gives us at least two options: 1) serialize it out into a byte array field, and serialize it back in when needed, from the field. 2) have an array of objects in the prepared statement and a #, to find the object directly. Because it is serializable, it will store with the rest of the executable just fine. Choice 2 gives better performance -- the ser/deser cost is paid on database access for the statement, not for each execution of it. However, it requires some infrastructure support from prepared statements.  For now, we take choice 3, and make some assumptions about available methods on the user type.  This choice has the shortcoming that it will not work for arbitrary user types. REVISIT and implement choice 2 when a general solution is needed. <p> A null is generated as a Null value cast to the type of the constant node. Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null. This is useful for bind time resolution of VTIs. RESOLVE: What do we do for primitives? Return the object value of this user defined type. Return whether or not this node represents a typed null constant.
Keeping this method out for now. The CcsidManager for **client** has this method as abstract but the one for the server doesn't seem to have it. int maxBytesPerChar() { return 4; } Offset and numToConvert are given in terms of bytes! Not characters!
<p> Get the int type id from java.sql.Types which corresponds to the SQLType. </p>
Convert a byte array to a hex string suitable for insert. Function to eliminate known package prefixes given a class full path <p>Calls the public method {@code getSysInfo} of the Network Server instance associated with the current test configuration and returns the result as a BufferedReader, making it easy to analyse the output line by line.</p> <p>This is useful for obtaining system information that could be used to verify, for example, values returned by Derby MBeans.</p> Calls the public method {@code getInfo} of the sysinfo tool within this JVM and returns a {@code BufferedReader} for reading its output. This is useful for obtaining system information that could be used to verify, for example, values returned by Derby MBeans. Creates a string with the specified length. <p> Called from various tests to test edge cases and such. Print out resultSet in two dimensional array format, for use by JDBC.assertFullResultSet(rs,expectedRows) expectedRows argument. Useful while converting tests to get output in correct format. Sleeps the specified number of milliseconds. Splits a string around matches of the given delimiter character. Copied from org.apache.derby.iapi.util.StringUtil Where applicable, this method can be used as a substitute for {@code String.split(String regex)}, which is not available on a JSR169/Java ME platform. Converts a string to a hex literal to assist in converting test cases that used to insert strings into bit data tables. <p> Converts using UTF-16BE just like the old casts used to.
latestException is assumed to be non-null, accumulatedExceptions can be null Used only by computeBigDecimalPrecision() Quote an SQL identifier by enclosing it in double-quote characters and escaping any double-quote characters with an extra double-quote character. Squash an array of longs into an array of ints
<p> Get the int type id from java.sql.Types which corresponds to the SQLType. </p>
Creates a handle to a temporary file. Cleans up the internal dummy data store after a database has been dropped. Returns the path separator used by this storage factory. Returns the temporary directory for this storage factory instance. Initializes the storage factory instance by setting up a temporary directory, the database directory and checking if the database being named already exists. The service is fast and supports random access. The service supports writes. Returns a handle to the specific storage file. Returns a handle to the specified storage file. Returns a handle to the specified storage file. Returns a normalized absolute path. Returns a normalized absolute path. Set the canonicalName. May need adjustment due to DERBY-5096 Normally does nothing, but if the database is in a state such that it should be deleted this will happen here. The service supports random access. The sync method is a no-op for this storage factory.


Cacheable interface   * Class specific methods. Get the VM Type name (java/lang/Object) that is associated with this Cacheable
Return the text of the statement which invoked the table function ///////////////////////////////////////////////////////////////  PUBLIC BEHAVIOR  /////////////////////////////////////////////////////////////// Return the name of the schema holding the table function Return the unqualified table function name
Get the estimated cost for a single instantiation of a Table Function. Get the estimated row count for a single scan of a Table Function. Find out if the ResultSet of the Table Function can be instantiated multiple times.
See if a VTI modification statement should be deferred. end of deferIt end of skipChildren end of stopTraversal end of visit
Return the SQL text of the original SQL statement. Get an object associated with a key from set of objects maintained with the statement plan. Get the  specific JDBC isolation of the statement. If it returns Connection.TRANSACTION_NONE then no isolation was specified and the connection's isolation level is implied. Return true if this instance of the Table Function has been created for compilation, false if it is for runtime execution. Saves an object associated with a key that will be maintained for the lifetime of the statement plan. Any previous value associated with the key is discarded. Any saved object can be seen by any JDBC Connection that has a Statement object that references the same statement plan.
What's a column's table's catalog name? * JDBC 2.0 Returns the fully-qualified name of the Java class whose instances are manufactured if the method <code>ResultSet.<!-- -->getObject</code> is called to retrieve a value from the column. JDBC 2.0. What's the column's normal maximum width in chars? What's the suggested column title for use in printouts and displays? What's a column's name? What's a column's data source specific type name? How many decimal digits are in the column? What's a column's number of digits to the right of the decimal point? What's a column's table's schema? What's a column's table name? Is the column automatically numbered, and thus read-only? Does a column's case matter? Is the column a cash value? Will a write on the column definitely succeed? Can you put a NULL in this column? Is a column definitely not writable? Can the column be used in a WHERE clause? Is the column a signed number? Is it possible for a write on the column to succeed? ///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// <p> Create a SQLException saying that the calling method is not implemented. </p>
<p> Cast the value coming out of the user-coded ResultSet. The rules are described in CastNode.getDataValueConversion(). </p> <p> Set the correct precision and scale for a decimal value. </p> <p> Truncate long varbinary values to the legal maximum. </p> <p> Truncate long varchars to the legal maximum. </p> Clone the restriction for a Restricted VTI, filling in parameter values as necessary.  Cache the ExecRow for this result set. This is not used in positioned update and delete, so just return a null. If open and not returned yet, returns the row after plugging the parameters into the expressions. <p> Get the types of the columns returned by a Derby-style table function. </p>  CursorResultSet interface  This is not operating against a stored table, so it has no row location to report. Return the total amount of time spent in this ResultSet Class implementation Return the GeneratedMethod for instantiating the VTI. * VTIEnvironment  ResultSet interface (leftover from NoPutResultSet)  Sets state to 'open'.  If the VTI is a version2 vti that does not need to be instantiated multiple times then we simply close the current ResultSet and create a new one via a call to PreparedStatement.executeQuery().
///////////////////////////////////////////////////////////////////////  AwareVTI BEHAVIOR  /////////////////////////////////////////////////////////////////////// <p> Get an array of descriptors for the return table shape declared for this AwareVTI by its CREATE FUNCTION statement. </p> If you implement findColumn() yourself, then the following overrides mean that you only have to implement the getXXX(int) methods. You don't have to also implement the getXXX(String) methods. ///////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////// <p> Create a SQLException saying that the calling method is not implemented. </p>  java.sql.ResultSet calls, passed through to our result set.

Bind this expression.  This is a place-holder method - it should never be called. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. Finish putting an expression into conjunctive normal form.  An expression tree in conjunctive normal form meets the following criteria: o  If the expression tree is not null, the top level will be a chain of AndNodes terminating in a true BooleanConstantNode. o  The left child of an AndNode will never be an AndNode. o  Any right-linked chain that includes an AndNode will be entirely composed of AndNodes terminated by a true BooleanConstantNode. o  The left child of an OrNode will never be an OrNode. o  Any right-linked chain that includes an OrNode will be entirely composed of OrNodes terminated by a false BooleanConstantNode. o  ValueNodes other than AndNodes and OrNodes are considered leaf nodes for purposes of expression normalization. In other words, we won't do any normalization under those nodes. In addition, we track whether or not we are under a top level AndNode. SubqueryNodes need to know this for subquery flattening. Bind time logic. Raises an error if this ValueNode does not resolve to a boolean value. This method is called by WHERE clauses. Update the array of columns in = conditions with expressions without column references from the same table.  This is useful when doing subquery flattening on the basis of an equality condition. eqOuterCols or tableColMap may be null if the calling routine doesn't need the information provided Return whether or not this expression tree represents a constant value. In this case, "constant" means that it will always evaluate to the same thing, even if it includes columns.  A column is constant if it is compared to a constant expression. Copy all of the "appropriate fields" for a shallow copy. Eliminate NotNodes in the current query block.  We traverse the tree, inverting ANDs and ORs and eliminating NOTs as we go.  We stop at ComparisonOperators and boolean expressions.  We invert ComparisonOperators and replace boolean expressions with boolean expression = false. NOTE: Since we do not recurse under ComparisonOperators, there still could be NotNodes left in the tree. If this node is known to always evaluate to the same value, return a node that represents that known value as a constant. Typically used to transform operators with constant operands into constants. Transform this into this = false.  Useful for NOT elimination. Transform this into this IS NULL or IS NOT NULL. Generate a SQL-&gt;Java-&gt;SQL conversion tree above the current node and bind the new nodes individually. This is useful when doing comparisons, built-in functions, etc. on java types which have a direct mapping to system built-in types. ///////////////////////////////////////////////////////////////////////  The ValueNode defers its generate() work to a method that works on ExpressionClassBuilders rather than ActivationClassBuilders. This is so that expression generation can be shared by the Core compiler AND the Replication Filter compiler.  /////////////////////////////////////////////////////////////////////// Do the code generation for this node.  Call the more general routine that generates expressions. Generate code for this calculation.  This is a place-holder method - it should not be called. Return a clone of this node. This is null so that the caller will substitute in the resultset generated name as needed. Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null. This is useful for bind time resolution of VTIs. RESOLVE: What do we do for primitives? Return the DataValueFactory Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) This returns the user-supplied schema name of the column. At this class level, it simply returns null. But, the subclasses of ValueNode will overwrite this method to return the user-supplied schema name. When the value node is in a result column of a select list, the user can request metadata information. The result column won't have a column descriptor, so we return some default information through the expression. This lets expressions that are simply columns return all of the info, and others use this supertype's default values. Get the source for this ValueNode. This returns the user-supplied table name of the column. At this class level, it simply returns null. But, the subclasses of ValueNode will overwrite this method to return the user-supplied table name. When the value node is in a result column of a select list, the user can request metadata information. The result column won't have a column descriptor, so we return some default information through the expression. This lets expressions that are simply columns return all of the info, and others use this supertype's default values. Get a bit map of table references in this expression Return whether or not this predicate has been transformed. Get the TypeCompiler from this ValueNode, based on its TypeId using getTypeId(). Get the TypeId from this ValueNode. Get the DataTypeServices from this ValueNode. Returns true if this value node is a <em>equals</em> operator. Does this represent a false constant. Does this represent a true constant. Return whether or not this expression tree is cloneable. Return whether or not this expression tree represents a constant expression. Tests if this node is equivalent to the specified ValueNode. Two ValueNodes are considered equivalent if they will evaluate to the same value during query execution. <p> This method provides basic expression matching facility for the derived class of ValueNode and it is used by the language layer to compare the node structural form of the two expressions for equivalence at bind phase. <p> Note that it is not comparing the actual row values at runtime to produce a result; hence, when comparing SQL NULLs, they are considered to be equivalent and not unknown. <p> One usage case of this method in this context is to compare the select column expression against the group by expression to check if they are equivalent.  e.g.: <p> SELECT c1+c2 FROM t1 GROUP BY c1+c2 <p> In general, node equivalence is determined by the derived class of ValueNode.  But they generally abide to the rules below: <ul> <li>The two ValueNodes must be of the same node type to be considered equivalent.  e.g.:  CastNode vs. CastNode - equivalent (if their args also match), ColumnReference vs CastNode - not equivalent. <li>If node P contains other ValueNode(s) and so on, those node(s) must also be of the same node type to be considered equivalent. <li>If node P takes a parameter list, then the number of arguments and its arguments for the two nodes must also match to be considered equivalent.  e.g.:  CAST(c1 as INTEGER) vs CAST(c1 as SMALLINT), they are not equivalent. <li>When comparing SQL NULLs in this context, they are considered to be equivalent. <li>If this does not apply or it is determined that the two nodes are not equivalent then the derived class of this method should return false; otherwise, return true. </ul> Returns true if this value node is an operator created for optimized performance of an IN list. Or more specifically, returns true if this value node is an equals operator of the form "col = ?" that we generated during preprocessing to allow index multi-probing. Returns TRUE if this is a parameter node. We do lots of special things with Parameter Nodes. Returns true if this ValueNode is a relational operator. Relational Operators are &lt;, &lt;=, =, &gt;, &gt;=, &lt;&gt; as well as IS NULL and IS NOT NULL. This is the preferred way of figuring out if a ValueNode is relational or not. Some node classes represent several logical node types (to reduce footprint), which we call <em>kinds</em>. This means that implementations of {@link #isEquivalent} cannot always just use {@code instanceof} to check if the other node represents the same kind. Hence this method needs to be overridden by all node classes that represent several kinds. This default implementation does not look at kinds. It is only called from implementations of {@code isEquivalent}. Return true if the predicate represents an optimizable equality node. an expression is considered to be an optimizable equality node if all the following conditions are met: <ol> <li> the operator is an <em>=</em> or <em>IS NULL</em> operator </li> <li> one of the operands is a column specified by optTable/columnNumber</li> <li> Both operands are not the same column; i.e tab.col = tab.col </li> <li> There are no implicit varchar comparisons of the operands; i.e either both operands are string like (varchar, char, longvarchar) or neither operand is string like </li> </ol> Preprocess an expression tree.  We do a number of transformations here (including subqueries, IN lists, LIKE and BETWEEN) plus subquery flattening. NOTE: This is done before the outer ResultSetNode is preprocessed. Do the 1st step in putting an expression into conjunctive normal form.  This step ensures that the top level of the expression is a chain of AndNodes. Remap all ColumnReferences in this tree to be clones of the underlying expression. Returns TRUE if the type of this node will be determined from the context in which it is getting used. If true is returned then after bindExpression() is called on the node, its type must be set (from the relevant context) using setType(). The default selectivity for value nodes is 50%.  This is overridden in specific cases, such as the RelationalOperators. Set the collation type and derivation of this node based upon the collation information passed in. This may result in a different object being returned from getTypeServices(). Set the collation type and derivation of this node based upon the collation information in the passed in type. Note that the base type of this node is not changed (e.g. INTEGER), only its collation settings. This may result in a different object being returned from getTypeServices(). Set the collation based upon the current schema with derivation type implicit. There are many subclasses of ValueNode where we want the DataTypeDescriptor of the node to have the same collation type as the compilation schema's collation type. For that purpose, this method in the baseclass here can be utilized by the subclasses. In addition, the subclasses can pass the collationDerivation that they expect the DataTypeDescriptor to have. Set the nullability of this value. Mark this predicate has having been transformed (other predicates were generated from it).  This will help us with ensure that the predicate does not get calculated into the selectivity multiple times. Set the DataTypeServices for this ValueNode.  This method is overridden in ParameterNode. Set this node's type from type components. Set this node's type from type components. Set eqOuterCols and the column in all the tables for constants, parmeters and correlation columns The column in the tableColMap is set only for the current table if the table is the result column table.  For other tables in the query we set the column for all the tables since the constant will reduced the number of columns required in a unique multicolumn index for distinctness. For example, given an unique index on t1(a,b), setting b=1 means that t1(a) is unique since there can be no duplicates for a where b=1 without destroying the uniqueness of t1(a,b).  However, for the result columns setting b=1, does not mean that a select list of t1.a is distinct if t1.a is the only column used in joining with another table e.g. select t1.a from t1, t2 where t1.a = t2.a and t1.b = 1; t1			t2			result a	b		a			a 1	1		1			1 1 	2		2			1 2	1 Convert this object to a String.  See comments in QueryTreeNode.java for how this should be done for tree printing.  Set the correct bits in tableColMap and set the boolean value in eqOuterCols given two arguments to an = predicate tableColMap[t] - bit is set if the column is in an = predicate with a column in table t, or a bit is set if the column is in an = predicate with a constant,parameter or correlation variable (for all table t, if this tableColMap is not for the table with the result columns) eqOuterCols[c] - is true if the column is in an = predicate with a constant, parameter or correlation variable Verify that changeToCNF() did its job correctly.  Verify that: o  AndNode  - rightOperand is not instanceof OrNode leftOperand is not instanceof AndNode o  OrNode	- rightOperand is not instanceof AndNode leftOperand is not instanceof OrNode Verify that eliminateNots() did its job correctly.  Verify that there are no NotNodes above the top level comparison operators and boolean expressions. Verify that putAndsOnTop() did its job correctly.  Verify that the top level of the expression is a chain of AndNodes.
Add a ValueNode to the list. Return whether or not all of the entries in the list have the same type precendence as the specified value. Bind this expression.  This means binding the sub-expressions, as well as figuring out what the return type is for this expression. Categorize this predicate.  Initially, this means building a bit map of the referenced tables for each predicate. If the source of this ColumnReference (at the next underlying level) is not a ColumnReference or a VirtualColumnNode then this predicate will not be pushed down. For example, in: select * from (select 1 from s) a (x) where x = 1 we will not push down x = 1. NOTE: It would be easy to handle the case of a constant, but if the inner SELECT returns an arbitrary expression, then we would have to copy that tree into the pushed predicate, and that tree could contain subqueries and method calls. RESOLVE - revisit this issue once we have views. Determine whether or not the leftOperand is comparable() with all of the elements in the list. Throw an exception if any of them are not comparable. Make sure that passed ValueNode's type is compatible with the non-parameter elements in the ValueNodeList.  Does this list contain all ConstantNodes? Does this list contain all ParameterNodes? Does this list *only* contain constant and/or parameter nodes? Does this list contain a ParameterNode? Eliminate NotNodes in all the nodes in this list. Generate a SQL-&gt;Java-&gt;SQL conversion tree any node in the list which is not a system built-in type. This is useful when doing comparisons, built-in functions, etc. on java types which have a direct mapping to system built-in types. Get the dominant DataTypeServices from the elements in the list. This method will also set the correct collation information on the dominant DataTypeService if we are dealing with character string datatypes. Algorithm for determining collation information This method will check if it is dealing with character string datatypes. If yes, then it will check if all the character string datatypes have the same collation derivation and collation type associated with them. If not, then the resultant DTD from this method will have collation derivation of NONE. If yes, then the resultant DTD from this method will have the same collation derivation and collation type as all the character string datatypes. Note that this method calls DTD.getDominantType and that method returns the dominant type of the 2 DTDs involved in this method. That method sets the collation info on the dominant type following the algorithm mentioned in the comments of Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query CONSTANT			- constant Get the first non-null DataTypeServices from the elements in the list. Return whether or not this expression tree represents a constant expression. Check if all the elements in this list are equivalent to the elements in another list. The two lists must have the same size, and the equivalent nodes must appear in the same order in both lists, for the two lists to be equivalent. Determine whether or not any of the elements in the list are nullable. Preprocess a ValueNodeList.  For now, we just preprocess each ValueNode in the list. Remap all ColumnReferences in this tree to be clones of the underlying expression. Set the descriptor for every ParameterNode in the list. Sort the entries in the list in ascending order. (All values are assumed to be constants.)
position is 1-based * ExecRow interface position is 1-based Row interface position is 1-based position is 1-based get a new Object[] for the row Get the array form of the row that Access expects. Get a clone of the array form of the row that Access expects. /////////////////////////////////////////////////////////////////////  EXECROW INTERFACE  ///////////////////////////////////////////////////////////////////// this is the actual current # of columns Set the number of columns in the row to ncols, preserving the existing contents. Reset all columns in the row array to null values. position is 1-based. Set the row array class interface


//////////////////////  DATATYPE COVERAGE  ////////////////////// ////////////////////////  PRIMITIVES VS OBJECTS  //////////////////////// Format a message ////////////////////////  IN, OUT, IN/OUT PARAMETERS  //////////////////////// ////////////////////////  LEADING NON-VARARGS  //////////////////////// /////////////////////////////////////////////////////////////////////////////////  CONSTANTS  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  STATE  ///////////////////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////////////////  SQL ROUTINES  ///////////////////////////////////////////////////////////////////////////////// ////////////////  SIMPLE ROUTINES  //////////////// Compute the maximum of a series of ints ////////////////////////  NON-VARARGS METHODS  //////////////////////// ////////////////////////  TABLE FUNCTIONS  //////////////////////// <p> This is a table function which creates a StringArrayVTI out of a space separated list of column names, and a varargs of rows. Each row is a space separated list of column values. Here is a sample usage: </p> <pre> connect 'jdbc:derby:memory:db;create=true'; create function leftTable ( columnNames varchar( 32672 ), rowContents varchar( 32672 ) ... ) returns table ( a   varchar( 5 ), b   varchar( 5 ) ) language java parameter style derby_jdbc_result_set no sql external name 'org.apache.derbyTesting.functionTests.tests.lang.VarargsRoutines.stringArrayTable'; select * from table( leftTable( 'A B', 'APP T', 'APP S' ) ) l; </pre> ////////////////////////  VARARGS & NON-VARARGS RESOLUTIONS  ////////////////////////

Set the width and scale (if relevant).  Sort of a poor man's normalize.  Used when we need to normalize a datatype but we don't want to use a NormalizeResultSet (e.g. for an operator that can change the width/scale of a datatype, namely CastNode). @param desiredWidth width @param desiredScale scale, if relevant (ignored for strings) @param errorOnTrunc	throw an error on truncation of value @exception StandardException		Thrown on error
Don't visit children under an aggregate, subquery or any node which is equivalent to any of the group by expressions. //////////////////////////////////////////////  VISITOR INTERFACE  ////////////////////////////////////////////// Verify that this expression is ok for an aggregate query.
Adds a <code>ClassInfo</code> object to a set. checks that a class implements a specific method. Perform JDBC operations on a <code>CallableStatement</code>. Collect the classes of all JDBC objects that are found. Perform JDBC operations on a <code>Connection</code>. Collect the classes of all JDBC objects that are found. Obtain a connection from a <code>ConnectionPoolDataSource</code> object and perform JDBC operations on it. Collect the classes of all JDBC objects that are found. Obtain a connection from a <code>DataSource</code> object and perform JDBC operations on it. Collect the classes of all JDBC objects that are found. Perform JDBC operations on a <code>PreparedStatement</code>. Collect the classes of all JDBC objects that are found. Perform JDBC operations on a <code>Statement</code>. Collect the classes of all JDBC objects that are found. Obtain a connection from an <code>XADataSource</code> object and perform JDBC operations on it. Collect the classes of all JDBC objects that are found. Search an array of classes for a class that is identical to or a super-class of the specified exception class. Get the set consisting of an interface and all its super-interfaces. Returns the declared set of JDBC interfaces that Derby implements. Build a suite of tests to be run.
printWriter synchronized by caller Not an external, just a helper method for DatabaseMetaData.getDriverVersion() Same as java.sql.Driver.getMajorVersion(), getMinorVersion() -------------------------- configuration print stream ---------------------
Adds compatibility tests to the specified suite. <p> The following determines how many tests are added: <ul> <li>available distributions locally (release repository)</li> <li>list of includes and/or excludes (by default empty)</li> <li>the configurator's current settings</li> </ul> Filters Derby distributions available in the distribution repository. Returns the URI of the source for the specified class. Returns a configuration that will test trunk against all other available releases. Returns the default configuration intended to be run as part of <tt>suites.all</tt>, which is a kind of minimal acceptance test (MATS). <p> The default configuration is defined to be all combinations that have trunk as either the server or the client. Returns a configuration where the newest releases within each major-minor version are tested against each other. <p> Given releases designated <tt>M.m.f.p</tt> (i.e. 10.8.1.2), this configuration will include all major-minor releases with the highest <ff>f</ff>. Returns a configuration where all versions found are tested against each other. Returns the directory for the JAR file containing the given class. Returns the running distribution, which is typically trunk. Forwarding convenience methods Check if a certain server version should be skipped due to bugs that prevent it from working in the current environment. Sorts and filters out distributions based on the configurator settings.
attributes Return the full version string.
Drop this descriptor, if not already done. Drop this descriptor, if not already done, due to action. If action is not {@code DependencyManager.DROP_VIEW}, the descriptor is dropped due to dropping some other object, e.g. a table column. Gets an identifier telling what type of check option is on this view. Get the provider's type. Get the compilation type schema id when this view was first bound.  Provider interface   Get the provider's UUID Return the name of this Provider.  (Useful for errors.)  ViewDescriptor interface  Gets the UUID of the view. Gets the text of the view definition.  Dependent Inteface  Check that all of the dependent's dependencies are valid. Mark the dependent as invalid (due to at least one of its dependencies being invalid). Prepare to mark the dependent as invalid (due to at least one of its dependencies being invalid). Sets the UUID of the view. Sets the name of the view.  class interface  Prints the contents of the ViewDescriptor
ColumnNode's are against the current row in the system. This lets us generate a faster get that simply returns the column from the current row, rather than getting the value out and returning that, only to have the caller (in the situations needed) stuffing it back into a new column holder object. We will assume the general generate() path is for getting the value out, and use generateColumn() when we want to keep the column wrapped. Return whether or not this VCN is a correlated reference. Return the variant type for the underlying expression. The variant type can be: VARIANT				- variant within a scan (method calls and non-static field access) SCAN_INVARIANT		- invariant within a scan (column references from outer tables) QUERY_INVARIANT		- invariant within the life of a query (constant expressions) Get the name of the schema the ResultColumn's table is in, if any. The return value will be null if the user did not supply a schema name (for example, select t.a from t). Another example for null return value (for example, select b.a from t as b). But for following query select app.t.a from t, this will return APP Return the ResultColumn that is the source of this VirtualColumnNode. Return the ResultColumn that is the source of this VirtualColumnNode. Return the ResultSetNode that is the source of this VirtualColumnNode. Get the name of the table the ResultColumn is in, if any.  This will be null if the user did not supply a name (for example, select a from t). The method will return B for this example, select b.a from t as b The method will return T for this example, select t.a from t Get the DataTypeServices from this Node. Return whether or not this expression tree is cloneable. Prints the sub-nodes of this object.  See QueryTreeNode.java for how tree printing is supposed to work. Mark this VCN as a reference to a correlated column. (It's source resultSet is an outer ResultSet. Return whether or not the ResultColumn is wirtable by a positioned update.
Tells if this file can be written to. Creates the the file denoted by this virtual file object. Deletes this file, of if exists. Deletes the path denoted by this file and all its contents, including sub directories. Tells if this file exists. Returns the data store entry denoted by this file, if it exists. Returns an input stream for the file denoted. Obtains an output stream for the file denoted. <p> If the file already exists, it will be truncated. Obtains an output stream for the file denoted. Returns the path of this file. Creates a random access file that can be used to read and write from/into the file. Tells if this file is a directory. <p> Note that {@code false} is returned if this path doesn't exist. Returns the length of the file. <p> If the file doesn't exists, or is a directory, {@code 0} is returned. Returns the contents of the directory denoted by this file, including any sub directories and their contents. Creates the directory denoted by this virtual file if it doesn't exist. <p> For the directory to be created, it cannot exist already (either as a file or a directory), and any parent directories must exist. Creates the directory and any parent directories denoted by this virtual file. <p> For the directory to be created, it cannot exist already (either as a file or a directory), and all the parent elements most denote either existing directories or non-existing paths. Renames the file denoted by this handle. Returns a textual representation of this file.


Accept a visitor, and call v.visit() on child nodes as necessary. Add a tag to this Visitable. Return true if this Visitable is tagged with the indicated tag.
Return true if the Visitable passes the filter.
Method that is called to indicate whether we should skip all nodes below this node for traversal.  Useful if we want to effectively ignore/prune all branches under a particular node. <p> Differs from stopTraversal() in that it only affects subtrees, rather than the entire traversal. Method that is called to see if query tree traversal should be stopped before visiting all nodes. Useful for short circuiting traversal if we already know we are done. This is the default visit operation on a QueryTreeNode.  It just returns the node.  This will typically suffice as the default visit operation for most visitors unless the visitor needs to count the number of nodes visited or something like that. <p> Visitors will overload this method by implementing a version with a signature that matches a specific type of node.  For example, if I want to do something special with aggregate nodes, then that Visitor will implement a <I> visit(AggregateNode node)</I> method which does the aggregate specific processing. Method that is called to see if {@code visit()} should be called on the children of {@code node} before it is called on {@code node} itself. If this method always returns {@code true}, the visitor will walk the tree bottom-up. If it always returns {@code false}, the tree is visited top-down.


Clear all information to allow object re-use.

Used to merge equivalent window definitions.   QueryTreeNode override. Prints the sub-nodes of this object. java.lang.Object override.
ValueNode override. ValueNode override.  Get the generated ColumnReference to this window function after the parent called replaceCallsWithColumnReferences(). <p/> There are cases where this will not have been done because the tree has been re-written to eliminate the window function, e.g. for this query: <p/><pre> {@code SELECT * FROM t WHERE EXISTS (SELECT ROW_NUMBER() OVER () FROM t)} </pre><p/> in which case the top PRN of the subquery sitting over a WindowResultSetNode just contains a RC which is boolean constant {@code true}. This means that the replaceCallsWithColumnReferences will not have been called for {@code this}, so the returned {@code generatedRef} is null. Get the null result expression column.  ValueNode override. QueryTreeNode override. Replace window function calls in the expression tree with a ColumnReference to that window function, append the aggregate to the supplied RCL (assumed to be from the child ResultSetNode) and return the ColumnReference. Set window associated with this window function call.



If the result set has been opened, close the open scan, else throw. Cache the ExecRow for this result set. Return the requested values computed from the next row (if any) for which the restriction evaluates to true. <p> Restriction and projection parameters are evaluated for each row. Return the total amount of time spent in this ResultSet Open this ResultSet. Copy columns from srcrow into destrow, or insert ROW_NUMBER. <p/> <b>FIXME</b> This is temporary. Window function treatment needs to generalized to work for other window functions. Reopen this ResultSet.
Substitute new result columns for window function calls and add the result columns to childResult's list of columns. Add a new PR node.  Put the new PR under any sort.   QueryTreeNode override
Helper method which drops a table if it exists. Nothing happens if the table doesn't exist.
Get the RuleBasedCollator which is getting used for collation sensitive methods. Check if the string consists of a single collation element. This method implements the like function for char (with no escape value). The difference in this method and the same method in SQLChar is that here we use special Collator object to do the comparison rather than using the Collator object associated with the default jvm locale. This method implements the like function for char with an escape value.
/////////////////////////////////////////////////////////////////////  JDBC 4.1 BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  OTHER PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////  JDBC 4.1 BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  OTHER PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////  JDBC 4.1 BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  OTHER PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////  JDBC 4.1 BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  OTHER PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////  JDBC 4.1 BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  OTHER PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////  JDBC 4.1 BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  OTHER PUBLIC BEHAVIOR  ///////////////////////////////////////////////////////////////////// /////////////////////////////////////////////////////////////////////  MINIONS  /////////////////////////////////////////////////////////////////////
/////////////////////////////////////////////////////////////////////  JDBC 4.2 BEHAVIOR  /////////////////////////////////////////////////////////////////////
This method tests whether the StorageRandomAccessFile "rws" and "rwd" modes are implemented. If the "rws" and "rwd" modes are supported then the database engine will conclude that the write methods of "rws"/"rwd" mode StorageRandomAccessFiles are slow but the sync method is fast and optimize accordingly. Force the data of an output stream out to the underlying storage. That is, ensure that it has been made persistent. If the database is to be transient, that is, if the database does not survive a restart, then the sync method implementation need not do anything.
/////////////////////////////////////////////////////////////////  INTERFACE METHODS  ///////////////////////////////////////////////////////////////// NOP routine. The work is done in InsertResultSet. Get the conglomerate id for the changed heap. /////////////////////////////////////////////////////////////////  ACCESSORS  ///////////////////////////////////////////////////////////////// Gets the foreign key information for this constant action. A full list of foreign keys was compiled into this constant action. get the index name given the conglomerate id of the index. The the value of the specified key, if it exists, from the targetProperties. Get the targetProperties from the constant action. Basically, the same as getFKInfo but for triggers. Formatable methods Read this object from a stream of stored objects. ACCESSORS Return true if this is an action of a MERGE statement Write this object to a stream of stored objects.


*************** *  Asks user to enter a wish list item or 'exit' to exit the loop - returns *       the string entered - loop should exit when the string 'exit' is returned **************** * END wwdInitTable  * END  getWishItem  ** *      Check for  WISH_LIST table    ***
Get a connection from a single use XADataSource configured from the configuration but with the passed in property set.
get XA Resource for this connection Make a new connection using the database name and set the connection in the database SetXAResource
************************************************************************ Public Methods of This class: ************************************************************************* This method is called to commit the global transaction specified by xid. <p> RESOLVE - how do we map to the "right" XAExceptions. <p> Find the given Xid in the transaction table. <p> This routine is used to find a in-doubt transaction from the list of Xid's returned from the recover() routine. <p> In the current implementation it is up to the calling routine to make the returned ContextManager the "current" ContextManager before calls to commit,abort, or forget.  The caller is responsible for error handling, ie. calling cleanupOnError() on the correct ContextManager. <p> If the Xid is not in the system, "null" is returned. RESOLVE - find out from sku if she wants a exception instead? <p> This method is called to remove the given transaction from the transaction table/log. <p> Used to let the store remove all record from log and transaction table of the given transaction.  This should only be used to clean up heuristically completed transactions, otherwise commit or abort should be used to act on other transactions. <p> If forget() is called on a transaction which has not be heuristically completed then it will throw an exception: SQLState.STORE_XA_PROTOCOL_VIOLATION. This method is called to obtain a list of prepared transactions. <p> This call returns a complete list of global transactions which are either prepared or heuristically complete. <p> The XAResource interface expects a scan type interface, but our implementation only returns a complete list of transactions.  So to simulate the scan the following state is maintained.  If TMSTARTSCAN is specified the complete list is returned.  If recover is called with TMNOFLAGS is ever called a 0 length array is returned. rollback the transaction identified by Xid. <p> The given transaction is roll'ed back and it's history is not maintained in the transaction table or long term log. <p>
Can cursors be held across commits. Close the realCallableStatement within this control. Close the realPreparedStatement within this control. Close the realStatement within this control. Don't need to wrap the ResultSet but do need to update its application Statement reference to be the one the application used to create the ResultSet.
This method is called to commit the current XA global transaction. <p> Once this call has been made all other calls on this controller other than destroy will throw exceptions. <p> This method is called to ask the resource manager to prepare for a transaction commit of the transaction specified in xid. <p> If XA_OK is returned then any call other than xa_commit() or xa_abort() will throw exceptions.  If XA_RDONLY is returned then any call other than destroy() will throw exceptions. rollback the current global transaction. <p> The given transaction is roll'ed back and it's history is not maintained in the transaction table or long term log. <p> Once this call has been made all other calls on this controller other than destroy will throw exceptions. <p>
This function is called from the timer task when the transaction times out. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Schedule a timeout task which will rollback the global transaction after the specified time will elapse. Commit the global transaction and cancel the timeout task. This method cancels timeoutTask and assigns 'performTimeoutRollback = false'. Prepare the global transaction for commit. Rollback the global transaction and cancel the timeout task.
************************************************************************ Constructors for This class: ************************************************************************* initialize by making array copies of appropriate fields. <p> Obtain the transaction branch qualifier part of the Xid in a byte array. <p> ************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods implementing the Xid interface: ************************************************************************* Obtain the format id part of the Xid. <p> Obtain the global transaction identifier part of XID as an array of bytes. <p>
The SQL/XML XMLExists operator. Checks to see if evaluation of the query expression contained within the received util object against this XML value returns at least one item. NOTE: For now, the query expression must be XPath only (XQuery not supported) because that's what Xalan supports. **** XMLDataValue interface. Method to parse an XML string and, if it's valid, store the _serialized_ version locally and then return this XMLDataValue. Evaluate the XML query expression contained within the received util object against this XML value and store the results into the received XMLDataValue "result" param (assuming "result" is non-null; else create a new XMLDataValue). The SQL/XML XMLSerialize operator. Serializes this XML value into a string with a user-specified character type, and returns that string via the received StringDataValue (if the received StringDataValue is non-null and of the correct type; else, a new StringDataValue is returned). Check if we have a JAXP implementation installed. See if the required JAXP and Xalan classes are in the user's classpath.  Assumption is that we will always call this method before instantiating an instance of SqlXmlUtil, and thus we will never get a ClassNotFound exception caused by missing JAXP/Xalan classes.  Instead, if either is missing we should throw an informative error indicating what the problem is. NOTE: This method only does the checks necessary to allow successful instantiation of the SqlXmlUtil class.  Further checks (esp. the presence of a JAXP _implementation_ in addition to the JAXP _interfaces_) are performed in the SqlXmlUtil constructor. Check if XPath is supported on this platform. **** DataValueDescriptor interface.  Compare two XML DataValueDescriptors.  NOTE: This method should only be used by the database store for the purpose of index positioning--comparisons of XML type are not allowed from the language side of things.  That said, all store wants to do is order the NULLs, so we don't actually have to do a full comparison.  Just return an order value based on whether or not this XML value and the other XML value are null.  As mentioned in the "compare" method of DataValueDescriptor, nulls are considered equal to other nulls and less than all other values. An example of when this method might be used is if the user executed a query like: select i from x_table where x_col is not null     **** Storable interface, implies Externalizable, TypedFormat   Retrieve this XML value's qualified type. Return whether or not this XML value represents a sequence that has one or more top-level attribute nodes.   Take note of the fact this XML value represents an XML sequence that has one or more top-level attribute nodes. Normalization method - this method will always be called when storing an XML value into an XML column, for example, when inserting/updating.  We always force normalization in this case because we need to make sure the qualified type of the value we're trying to store is XML_DOC_ANY--we don't allow anything else. Read an XML value from an input stream.   **** StreamStorable interface     **** Helper classes and methods. Set this XML value's qualified type.  Write an XML value.
This generates the proper constant.  For an XML value, this constant value is simply the XML string (which is just null because null values are the only types of XML constants we can have). Return an Object representing the bind time value of this expression tree.  If the expression tree does not evaluate to a constant at bind time then we return null.
The SQL/XML XMLExists operator. Checks to see if evaluation of the query expression contained within the received util object against this XML value returns at least one item. NOTE: For now, the query expression must be XPath only (XQuery not supported) because that's what Xalan supports. Method to parse an XML string and, if it's valid, store the _serialized_ version locally and then return this XMLDataValue. Evaluate the XML query expression contained within the received util object against this XML value and store the results into the received XMLDataValue "result" param (assuming "result" is non-null; else create a new XMLDataValue). The SQL/XML XMLSerialize operator. Serializes this XML value into a string with a user-specified character type, and returns that string via the received StringDataValue (if the received StringDataValue is non-null and of the correct type; else, a new StringDataValue is returned). Retrieve this XML value's qualified type. Return whether or not this XML value represents a sequence that has one or more top-level attribute nodes. Take note of the fact this XML value represents an XML sequence that has one or more top-level attribute nodes. **** Helper classes and methods. Set this XML value's qualified type.
Create an element and add it to a parent Format a CostEstimate as subelements of a parent Format a join order list Turn a CostEstimate for a join order into a human-readable element <p> Produce a string representation of the plan being considered now. The string has the following grammar: </p> <pre> join :== factor OP factor OP :== "*" | "#" factor :== factor | conglomerateName </pre> Format selectivity subelement Create an element explaining that we're skipping some processing Turn a timestamp into a human-readable string //////////////////////////////////////////////////////////////////////  MINIONS  ////////////////////////////////////////////////////////////////////// Get the Optimizable with the given tableNumber Get the name of an optimizable Return true if the optimizable is a base table Return true if the join order has been completely filled in Return true if the optimizable is a FromTable Make a TableName Print an exception to the log file //////////////////////////////////////////////////////////////////////  BEHAVIOR  //////////////////////////////////////////////////////////////////////
Return the suite that runs the XML tests.
Tell whether this type (XML) is compatible with the given type. Tell whether this type (XML) can be converted to the given type. An XML value can't be converted to any other type, per SQL/XML[2003] 6.3 <cast specification>      Tell whether this type (XML) can be stored into from the given type. Only XML values can be stored into an XML type, per SQL/XML spec: 4.2.2 XML comparison and assignment Values of XML type are assignable to sites of XML type.

uncache the visitor and reset the factory state the factory method, which gets called to determine and return an appropriate XPLAINVisitor instance
This method gets called when the user switches off the explain facility. The factory destroys for example the cached visitor implementation(s) or releases resources to save memory. This method returns an appropriate visitor to traverse the ResultSetStatistics. Depending on the current configuration, the perfect visitor will be chosen, created and cached by this factory method.
Builds a list of columns suitable for creating this Catalog.
Builds a list of columns suitable for creating this Catalog.
Builds a list of columns suitable for creating this Catalog.
Builds a list of columns suitable for creating this Catalog.
Builds a list of columns suitable for creating this Catalog.
Builds a list of columns suitable for creating this Catalog.
This method writes the created descriptor arrays to the cooresponding system catalogs. This method writes only the stmt and its timing descriptor to the dataDictionary --------------------------------------------------------- helper methods --------------------------------------------------------- This method cleans up things after explanation. It frees kept resources and still holded references. the interface method, which gets called by the Top-ResultSet, which starts the tree traversal. Open a nested Connection with which to execute INSERT statements. helper method, which pushes the UUID, "number of Children" times onto the UUIDStack. This method resets the visitor. Gets called right before explanation to make sure all needed objects exist and are up to date and the lists are cleared --------------------------------------------------------- XPLAINVisitor Implementation --------------------------------------------------------- this method only stores the current number of children of the current explained node. The child nodes then can re-use this information. Visit this node, calling back to it to get details. This method visits the RS Statisitcs node, calling back to the node to get detailed descriptor information about it.

helper method which extracts the right (non-internationalzed) scan properties of the scan info properties helper method which extracts the right (non-internationalzed) sort properties of the sort info properties object Compute average, avoiding divide-by-zero problems. util function, to resolve the isolation level and return a isolation level code util function, to resolve the lock granularity and return a lock granularity code util function, to resolve the lock mode, and return a lock mode code This method helps to figure out the statement type and returns an appropriate return code, characterizing the stmt type. --------------------------------------------- utility functions ---------------------------------------------
This method is the hook method which is called from the TopResultSet. It starts the explanation of the current ResultSetStatistics tree and keeps the information during one explain run. Call this method to reset the visitor for a new run over the statistics. A default implementation should call this method automatically at first of a call of doXPLAIN(). This method informs the visitor about the number of children. It has to be called first! by the different explainable nodes before the visit method of the visitor gets called. Each node knows how many children he has. The visitor can use this information to resolve the relationship of the current explained node to above nodes. Due to the top-down, pre-order, depth-first traversal of the tree, this information can directly be exploited. This is the Visitor hook method, which gets called from each ResultSetStatistics. It depends on the sub-class implementation of this interface, to describe the behaviour of the explanation facility. <br/> To be easily extendable with new explain representation methods, just implement this interface and provide the new behaviour.
This method gets called to let a visitor visit this XPLAINable object. The general contract is to implement pre-order, depth-first traversal to produce a predictable traversal behaviour. The methods below return descriptive information about the particular result set. There are a few common implementations, and the various ResultSetStatistics sub-classes override these methods when they have more detailed information to provide. The visitor calls these methods during xplain tree visiting.

Add to the list of post abort work that may be processed after this transaction aborts. Add this to the xactFactory list of update transaction. Return true if any transaction is currently blocked, even if not by this transaction. Assume complete identity of the given Transaction Table Entry. <p> Used by the final phase of the recovery to create new real transactions to take on the identity of in-doubt prepared transactions found during redo.  Need to assume the globalId. Transform this identity to the one stored in transaction table entry. Used by recovery only! Make the transaction block the online backup. Perform a checkpoint during rollforward recovery.     Do work to complete a commit which is not just a prepare. <p> Releases locks, does post commit work, and moves the state of the transaction to IDLE. <p> Convert a local transaction to a global transaction. <p> Must only be called a previous local transaction was created and exists in the context.  Can only be called if the current transaction is in the idle state, and no current global id. <p> Simply call setTransactionId() which takes care of error checking. If this transaction is not idle, abort it.  After this call close(). If this is a user transaction (not an internal or nested top transaction), and this is not already taking care of post commit work, and not an XA transaction, then take care of hi prioirty work right now using this thread and this context manager. Otherwise, leave it to the post commit daemon.   Get string id of the transaction that would be when the Transaction is IN active state. This transaction "name" will be the same id which is returned in the TransactionInfo information if Tx is already in Active State. If the Transaction is in IDLE state, Transaction ID is incremented when getActiveStateTxIdString() on raw transaction is called, instead of the Tx ID being incremented when Transaction gets into active state. The reason for incrementing the Tx ID earlier than when Tx is actually goes into active state is some debug statement cases like log statement text. SQL statements are written to log before they are actually executed; In such cases we would like to display the actual TX ID on which locks are acquired when the statement is executed. Get the compatibility space of the transaction. <p> Returns an object that can be used with the lock manager to provide the compatibility space of a transaction.  2 transactions with the same compatibility space will not conflict in locks.  The usual case is that each transaction has it's own unique compatibility space. <p> Get my transaction context Id Get DataValueFactory. <p> Return a DataValueFactory that can be used to allocate objects.  Used to make calls to: DataValueFactory.getInstanceUsingFormatIdAndCollationType() Get the current default locking policy for all operations within this transaction. The transaction is initially started with a default locking policy equivalent to <PRE> newLockingPolicy( LockingPolicy.MODE_RECORD, TransactionController.ISOLATION_SERIALIZABLE, true); </PRE> This default can be changed by subsequent calls to setDefaultLockingPolicy(LockingPolicy policy). Get the log instant for the first log record written by this transaction. get the Global (external to raw store) transaction id that is unique across all raw stores get the short (internal to raw store) transaction id that is unique only for this raw store Get the transaction id without sanity check, this should only be called by a cloned TransactionTableEntry Get the log instant for the last log record written by this transaction. * Methods of RawTransaction  JIRA-606. As a part of this fix, it was required that LogFactory.checkVersion method to be exposed for any possible Version checks in the Transaction processing module. *	Implementation specific methods Get the Logger object used to write log records to the transaction log. Does a save point exist in the stack with the given name. Returns the position of the savepoint in the array package Is the transaction in rollforward recovery see if this transaction has ever done anything. MT - single thread through synchronizing this.  This method may be called by other thread to test the state of this transaction.  That's why we need to synchronize with all methods which enters or exits the Idle state. Local method which read the state need not be synchronized because the other thread may look at the state but it may not change it. Check if the transaction is blocking the backup ? See if this transaction is in the idle state, called by other thread to test the state of this transaction.  That's why we need to synchronzied with all methods whcih enters or exits the idle state see if this transaction is in PREPARED state. MT - single thread through synchronizing this.  This method may be called by other thread to test the state of this transaction. see if this transaction is in a pristine state. <BR>MT - called only by the same thread that owns the xact, no need to synchronize. see if this transaction is a user transaction. Log the operation and do it. If this transaction has not generated any log records prior to this, then log a beginXact log record. If the passed in operation is null, then do nothing (after logging the beginXact if needed). Log and apply a compensation operation. Only need to write out the compensation op itself, the optional data has already been written by the rollforward operation this is attempting to undo.  Tells whether lock requests should time out immediately if they can't be granted without waiting. Only works if this object is the owner of the compatibility space used in the request. Return a record handle that is initialized to the given page number and record id. public RecordHandle makeRecordHandle(long segmentId, long containerId, long pageNumber, int recordId) throws	StandardException { return(this.dataFactory.makeRecordHandle( segmentId, containerId, pageNumber, recordId)); }   Open a container that may already have been dropped.  Pop all savepoints upto the one with the given name and rollback all changes made since this savepoint was pushed. If release is true then this savepoint is popped as well, otherwise it is left in the stack (at the top).  Do work of commit that is common to xa_prepare and commit. <p> Do all the work necessary as part of a commit up to and including writing the commit log record.  This routine is used by both prepare and commit.  The work post commit is done by completeCommit(). <p> Remove this from the xactFactory list of update transaction. Recreate a container during redo recovery. Used only during redo recovery while processing log records which are trying to create a container, and no valid container is found in the database. * Lock escalation related * Methods of Limit  Remove this from the xactFactory list of update transaction. During recovery re-prepare a transaction. <p> After redo() and undo(), this routine is called on all outstanding in-doubt (prepared) transactions.  This routine re-acquires all logical write locks for operations in the xact, and then modifies the transaction table entry to make the transaction look as if it had just been prepared following startup after recovery. <p> This routine is only called during Recovery.   Set the log instant for the first log record written by this transaction. Set the log instant for the last log record written by this transaction. {@inheritDoc } <p> This only works if this transaction is the owner of the compatibility space used in the request. If this transaction has inherited the compatibility space from its parent, the call to this method has no effect (except in debug builds, where an error will be raised). Set the transaction to issue pre complete work at postComplete time, instead of preComplete time. This means that latches and containers will be held open until after a commit or an abort. Move the state of the transaction from UPDATE to PREPARE. <p> The state transition should only be from UPDATE to PREPARE.  Read-only transactions (IDLE and ACTIVE) will never be prepared, they will be commited when the prepare is requested.  Only Update transactions will be allowed to go to prepared state. <p>  Set my transaction identifier. Move the transaction into the update state. * Methods of Transaction The default value for LOCKS_ESCALATION_THRESHOLD  put this into the beginXact log record to help recovery if we needs to rolled back first, put that in put this into the endXact log record to help recovery, nothing to add SQL savepoint can't be nested inside other user defined savepoints. To enforce this, we check if there are already user savepoint(SQL/JDBC) defined in the transaction. If yes, then throw an exception  Return the xid as a string. <p> The virtual lock table depends on this routine returning just the local transaction id as a string, even if it is a global transaction. Joins between the lock table and the transaction table will not work if this routine returns anything other than myId.toString(). <p> Unblock the backup, if it was blocked by some operation in this transaction. Unblocking is done at commit/abort of this transaction. This method is called to commit the current XA global transaction. <p> RESOLVE - how do we map to the "right" XAExceptions. <p> This method is called to ask the resource manager to prepare for a transaction commit of the transaction specified in xid. <p> rollback the current global transaction. <p> The given transaction is roll'ed back and it's history is not maintained in the transaction table or long term log. <p>
* Context methods (most are implemented by super-class)
Add a transaction to the list of transactions that has updated the raw store. <P> This is called underneath the BeginXact log operation's doMe method. The logging system must guarentee that transactions are added in the true order they are started, as defined by the order of beginXact log record in the log. Block the online backup. Backup needs to be blocked while executing any unlogged operations or any opearation that prevents from  making a consistent backup. Checks if there are any backup blocking operations in progress and prevents new ones from starting until the backup is finished. If backup blocking operations are in progress and  <code> wait </code> parameter value is <tt>true</tt>, then it will wait for the current backup blocking operations to finish. A Consistent backup can not be made if there are any backup blocking operations (like unlogged operations) are in progress Privileged startup. Must be private so that user code can't call this entry point. * Methods of ModuleControl Database creation finished Privileged startup. Must be private so that user code can't call this entry point. the following TransactionFactory methods are to support recovery and should only be used by recovery! Find the TransactionTableEntry with the given ID and make the passed in transaction assume the identity and properties of that TransactionTableEntry. Used in recovery only. Get the earliest log instant that is still active, ie, the first log record logged by the earliest transaction that is still active. <BR> The logging system must guarentee that the transaction table is populated in the order transactions are started. Used in recovery only. Privileged lookup of the ContextService. Private so that user code can't call this entry point. * Methods of TransactionFactory Get the LockFactory to use with this store. Get a locking policy for a transaction. Privileged Monitor lookup. Must be private so that user code can't call this entry point. Return the transaction table to be logged with the checkpoint operation Return the module providing XAresource interface to the transaction table. XAResourceManager Run through all prepared transactions known to this factory and restore their state such that they remain after recovery, and can be found and handled by a XA transaction manager.  This includes creating a context manager for each, pushing a xact context, and reclaiming update locks on all data changed by the transaction. Expected to be called just after the redo and undo recovery loops, where the transaction table should be empty except for prepared xacts. Used only in recovery. Check if there are any prepared transanctions in the transaction table. Caller must be aware that if there is no other mechanism to stop transactions from starting and ending, then this information is outdated as soon as it is reported.  Make a new UUID for whomever that wants it * Methods of Corruptable Really this is just a convience routine for callers that might not have access to a log factory. Returns true if there is no in flight updating tranasaction. Caller must be aware that if there is no other mechanism to stop transactions from starting and ending, then this information is outdated as soon as it is reported. Only call this function in special times - e.g, during recovery Change state of transaction to prepared.  Used by recovery to update the transaction table entry to prepared state. Create a new RawTransaction, a context for it and push the context onto the current context manager.  Then add the transacion to the transaction table. remove the transaction Id an return false iff the transaction is found in the table and it doesn't need exclusion from quiesce state Remove a transaction from the list of transactions that has updated the raw store. *	Set the shortTranId, this is called by the log factory after recovery Rollback all active transactions that has updated the raw store. Use the recovery Transaction that is passed in to do all the work. Used in recovery only. <P> Transactions are rolled back in the following order: <OL> <LI>internal transactions in reversed beginXact chronological order, <LI>all other transactions in reversed beginXact chronological order, </NL> *		Implementation specific methods. Common work done to create local or global transactions. Submit this post commit work to the post commit daemon Unblock the backup, a backup blocking operation finished. Backup completed. Allow backup blocking operations. Use this transaction table, which is gotten from a checkpoint operation.  Use ONLY during recovery.
Methods specific to this class Return	0 if a == b, +ve number if a &gt; b -ve number if a &lt; b TransactionId method Return my format identifier. Read this in Write this out.
************************************************************************ Private/Protected methods of This class: ************************************************************************* ************************************************************************ Public Methods implementing XAResourceManager interface ************************************************************************* This method is called to commit the global transaction specified by xid. <p> RESOLVE - how do we map to the "right" XAExceptions. <p> Find the given Xid in the transaction table. <p> This routine is used to find a in-doubt transaction from the list of Xid's returned from the recover() routine. <p> In the current implementation it is up to the calling routine to make the returned ContextManager the "current" ContextManager before calls to commit,abort, or forget.  The caller is responsible for error handling, ie. calling cleanupOnError() on the correct ContextManager. <p> If the Xid is not in the system, "null" is returned. RESOLVE - find out from sku if she wants a exception instead? <p> This method is called to remove the given transaction from the transaction table/log. <p> Used to let the store remove all record from log and transaction table of the given transaction.  This should only be used to clean up heuristically completed transactions, otherwise commit or abort should be used to act on other transactions. <p> This method is called to obtain a list of prepared transactions. <p> This call returns a complete list of global transactions which are either prepared or heuristically complete. <p> The XAResource interface expects a scan type interface, but our implementation only returns a complete list of transactions.  So to simulate the scan the following state is maintained.  If TMSTARTSCAN is specified the complete list is returned.  If recover is called with TMNOFLAGS is ever called a 0 length array is returned. rollback the transaction identified by Xid. <p> The given transaction is roll'ed back and it's history is not maintained in the transaction table or long term log. <p>
Factory method to create an ArrayList<String> /////////////////////////////////////////////////////////////////////////////////  ResultSet BEHAVIOR  ///////////////////////////////////////////////////////////////////////////////// <p> Find the value of a column inside an element. The columnNumber is 0-based. </p> /////////////////////////////////////////////////////////////////////////////////  StringColumnVTI BEHAVIOR TO BE IMPLEMENTED BY SUBCLASSES  ///////////////////////////////////////////////////////////////////////////////// <p> Get the string value of the column in the current row identified by the 1-based columnNumber. </p> <p> Parse a row into columns. </p> /////////////////////////////////////////////////////////////////////////////////  MINIONS  ///////////////////////////////////////////////////////////////////////////////// ////////////////////////  XML MINIONS  //////////////////////// <p> Fault in the list of rows. </p> <p> Squeeze the text out of an Element. </p> This is the static method for creating functions from an URL and both parent and child tags /////////////////////////////////////////////////////////////////////////////////  ENTRY POINTS (SQL FUNCTIONS)  ///////////////////////////////////////////////////////////////////////////////// This is the static method for creating functions from a file name and child tags This is the static method for creating functions from a file name and both parent and child tags This is the static method for creating functions from an url and child tags This is the static method for creating functions from an URL and both parent and child tags
Method to get only the "interesting" pieces of information for the customer, namely the version number (2.0.1) and the beta status and the build number


Returns an extended suite of compatibility tests.
Returns an extended suite of compatibility tests.



setup for restart recovery test which will require the use of correct Collator object during recovery of territory based database that will be created and crashed in this test and later will be recovered in col_rec2.
setup for restart recovery test which will require the use of correct Collator object during recovery of territory based database that was created and crashed in this col_rec1

return the command line to invoke this VM.  The caller then adds the class and program arguments.

not running indexes test because it doesn't finish even after running for over 2 hours ALSO, IF WE EVER ENABLE THIS TEST IN FUTURE, WE NEED TO REWRITE THE TEST SO THAT WE TRY TO CREATE OVER 32767 *DIFFERENT* INDEXES. AS PART OF DB2 COMPATIBILITY WORK, BUG - 5685 DISALLOWS CREATION OF AN INDEX ON A COLUMN THAT ALREADY HAS A PRIMARY KEY OR UNIQUE CONSTRAINT ON IT.

jarname - jarname to use path - path to database dbname - database name in archive
************************************************ addQuotes: Add quotes to the received object name, and return the result. @param name the name to which to add quotes. @return the name with double quotes around it. ** Return true if we are at 10.6 or later. ************************************************ expandDoubleQuotes: If the received SQL id contains a quote, we have to expand it into TWO quotes so that it can be treated correctly at parse time. @param name Id that we want to print. ** ************************************************ extractDBNameFromUrl: Given a database url, parse out the actual name of the database.  This is required for creation the DB2JJARS directory (the database name is part of the path to the jar). @param dbUrl The database url from which to extract the the database name. @return the name of the database (including its path, if provided) that is referenced by the url. ** ************************************************ extractTableNamesFromList: Given an array of command line arguments containing a list of table names beginning at start'th position, read the list of table names and store them as our target table list.  Names without quotes are turned into ALL CAPS and then double quotes are added; names whcih already have double quotes are stored exactly as they are. NOTE: DB2 enforces maximum of 30 tables, and ignores the rest; so do we. @param args Array of command line arguments. @start Position of the start of the list of tables with the args array. @return The position of the last table name in the list of table names. ** ************************************************ getColNameFromNumber: Takes a tableid and a column number colNum, and returns the name of the colNum'th column in the table with tableid. @param tableid id of the table. @param colNum number of the column for which we want the name. @return The name of the colNum'th column in the table with tableid. ** ************************************************ getColumnListFromDescription: Takes string description of column numbers in the form of "(2, 1, 3...)" and the id of the table having those columns, and then returns a string with the column numbers replaced by their actual names ('2' is replaced with the 2nd column in the table, '1' with the first column, etc.). @param tableId the id of the table to which the column numbers should be applied. @param description a string holding a list of column numbers, enclosed in parentheses and separated by commas. @return a new string with the column numbers in 'description' replaced by their column names; also, the parentheses have been stripped off. ** ************************************************ go: Connect to the source database, prepare statements, and load a list of table id-to-name mappings.  Then, generate the DDL for the various objects in the database by making calls to static methods of helper classes (one helper class for each type of database object).  If a particular object type should not be generated (because of the user-specified command- line), then we enforce that here. @precondition all user-specified parameters have been loaded. @return DDL for the source database has been generated and printed to output, subject to user-specified restrictions. *** ************************************************ initState: Initialize class variables. ** ************************************************ isExcludedTable: Takes a table name and determines whether or not the DDL for objects related to that table should be generated. @param tableName name of the table to check. @return true if 1) the user specified a table list and that list does NOT include the received name; or 2) if the user specified a schema restriction and the received name does NOT have that schema; false otherwise. ** ************************************************ loadDriver: Load derby driver. @param precondition sourceDBUrl has been loaded. @return false if anything goes wrong; true otherwise. ** ************************************************ loadParam: Read in a flag and its corresponding values from list of command line arguments, starting at the start'th argument. @return The position of the argument that was most recently processed. ** ************************************************ lookupMessage: Retrieve a localized message. @param key The key for the localized message. @return the message corresponding to the received key. ** ************************************************ lookupMessage: Retreive a localized message. @param key The key for the localized message. @param vals Array of values to be used in the message. @return the message corresponding to the received key, with the received values substituted where appropriate. ** ************************************************ lookupSchemaId: Return the schema name corresponding to the received schema id. @param schemaId The id to look up. @return the schema name. ** ************************************************ lookupTableId: Return the table name corresponding to the received table id. @param tableId The id to look up. @return the table name. ** ************************************************ main: Initialize program state by creating a dblook object, and then start the DDL generation by calling "go". *** ************************************************ parseArgs: Parse the command-line arguments. @param args args[0] is the url for the source database. @return true if all parameters were loaded and the output files were successfully created; false otherwise. ** ************************************************ partOfWord: Returns true if the part of the string given by str.substring(pos, pos + nameLen) is part of another word. @param str The string in which we're looking. @param pos The position at which the substring in question begins. @param nameLen the length of the substring in question. @param strLen The length of the string in which we're looking. @return true if the substring from pos to pos+nameLen is part of larger word (i.e. if it has a letter/digit immediately before or after); false otherwise. ** ************************************************ prepForDump: Prepare any useful statements (i.e. statements that are required by more than one helper class) and load the id-to-name mappings for the source database. ** ************************************************ removeNewlines: Remove any newline characters from the received string (replace them with spaces). @param str The string from which we are removing all newline characters. @return The string, with all newline characters replaced with spaces. ** ************************************************ showVariables: Echo primary variables to output, so user can see what s/he specified. ** ************************************************ Takes a string and determines whether or not that string makes reference to any of the table names in the user-specified table list. @param str The string in which to search for table names. @return true if 1) the user didn't specify a target table list, or 2) the received string contains at least one of the table names in the user-specified target list; false otherwise. ** ************************************************ stripQuotes: Takes a name and, if the name is enclosed in quotes, strips the quotes off.  This method assumes that the received String either has no quotes, or has a quote (double or single) as the very first AND very last character. @param quotedName a name with quotes as the first and last character, or else with no quotes at all. @return quotedName, without the quotes. ** inverse of expandDoubleQuotes ************************************************ writeVerboseOutput: Writes the received string as "verbose" output, meaning that we write it to System.err.  We choose System.err so that the string doesn't show up if the user pipes dblook output to a file (unless s/he explicitly pipes System.err output to that file, as well). @param key Key for the message to be printed as verbose output. @param value Value to be substituted into the message. @return message for received key has been printed to System.err. **
********************************************** columnHoldsObjectName: Return true if the received column, which is from some system table, holds the _name_ of a database object (table, constraint, etc.).  Typically, we can just look for the keyword "NAME"; the exception is aliases, where the name is held in a column called ALIAS. @param colName Name of the column in question. @return True if the column name indicates that it holds the _name_ of a database object; false if the column name indicates that it holds something else. ** ********************************************** createDBFromDDL: Read from the given script and use it to create a new database of the given name. @param newDBName Name of the database to be created. @param scriptName Name of the script containing the DDL from which the new database will be created. @return New database has been created from the script; any commands in the script that failed to execute have been echoed to output. ** ********************************************** createTestDatabase: Using the creation script created as part of the test package, create the database that will be used as the basis for all dblook tests. @param scriptName The name of the sql script to use for creating the test database. @return The test database has been created in the current test directory, which is "./dblook/" (as created by the harness). ** ********************************************** deleteDB: Deletes the database with the received name from the test directory. @param dbName Name of the database to be deleted. @return Database has been completely deleted; if deletion failed for any reason, a message saying so has been printed to output. ** ********************************************** deleteFile: Delete everything in a given directory, then delete the directory itself (recursive). @param aFile File object representing the directory to be deleted. @return the directory corresponding to aFile has been deleted, as have all of its contents. ** ********************************************** doTest Run a full test of the dblook utility. ** ********************************************** dumpColumnData: Stores the value for a specific column of some result set.  If the value needs to be filtered (to remove system-generated ids that would otherwise cause diffs with the master), that filtering is done here. @param colName Name of the column whose value we're writing. @param value Value that we're writing. @param mappedName: Name corresponding to the value, for cases where the value is actually an object id (then we want to write the name instead). rowValues a list of column values for the current row of the result set. @return The (possibly filtered) value of the received column has been added to the "rowVals" array list, and the corresponding piece of the row's unique name has been returned, if one exists. ** ********************************************** dumpFileToSysOut: Checks to see if the received file is empty, and prints a message saying so. @param fName Name of the file to be written to output. @return The contents of the specified file have been written to System.out. ** ********************************************** dumpResultSet: Iterates through the received result set and dumps ALL columns in ALL rows of that result set to output.  Since no order is guaranteed in the received result set, we have to generate unique "ids" for each row in the result, and then use those ids to determine what order the rows will be output.  Failure to do so will lead to diffs in the test for rows that occur out of order.  The unique id's must NOT depend on system-generated id's, as the latter will vary for every run of the test, and thus will lead to different orderings every time (which we don't want). @param rs The result set that is being dumped. @param idToNameMap Mapping of various ids to object names; used in forming unique ids. @param conn Connection from which the result set originated. ** ********************************************** dumpSysCatalogs: Takes a database name and dumps ALL of the system catalogs for that database, with the exception of SYSSTATISTICS.  This allows us to look at the full contents of a database's schema (without using dblook, of course) so that we can see if the databases created from the DDL generated by dblook have been built correctly--if they have all of the correct system catalog information, then the databases themselves must be correct. @param dbName The name of the database for which we are dumping the system catalogs. @return All of the system catalogs for the received database have been dumped to output. ** ********************************************** getDependsData: Forms a string containing detailed information about a row in the SYSDEPENDS table, and returns that string. @param rs Result set with SYSDEPENDS rows; current row is the one for which we're getting the data. @param conn Connection to the database being examined. @param idToNameMap mapping of object ids to names for the database in question. @return Schema, type and name of both the Provider and the Dependent for the current row of SYSDEPENDS have been returned as a string. ** ********************************************** getHiddenDependsData: Returns a string containing the schema and name of the object having the received id. All object ids received by this message come from rows of the SYSDEPENDS table. @param type Type of the object that has the received object id. @param id Id of the object in question. @param stmt Statement from the database in question. @param idToNameMap mapping of ids to names for the database in question. @isProvider True if we're getting data for a Provider object; false if we're getting data for a Dependent object. @return Schema, type, and name for the object with the received id have been returned as a string. ** ********************************************** go: Makes the call to execute the dblook command using the received arguments. @param dbName The name of the source database (i.e. the database for which the DDL is generated). @args The list of arguments with which to execute the dblook command. ** ********************************************** handleDuplicateRow: If we get here, then despite our efforts (while dumping the system catalogs for a database), we still have a duplicate row id.  So, as a last resort we just use the ENTIRE row as a 'row id'. In the rare-but-possible case that the entire row is a duplicate (as can happen with the SYSDEPENDS table), then we tag a simple number onto the latest row's id, so that the row will still show up multiple times--and since the rows are identical, it doesn't matter which comes 'first'. @param newRow The most recently-fetched row from the database system catalogs. @param oldRow The row that was replaced when the newRow was inserted (because they had the same row id), or "null" if we were already here once for this row id, and so just want insert a new row. @param orderedRows The ordered set of rows, into which oldRow and newRow need to be inserted. @return oldRow and newRow have been inserted into orderedRows, and each has a (truly) unique id with it. ** ********************************************** isIgnorableSchema: Returns true if the the schema is a "system" schema, vs. a user schema. @param schemaName name of schema to check. ** ********************************************** isSystemGenerated: Returns true if the received string looks like it is a system-generated string.  We assume it's system-generated if either 1) it starts with the letters "SQL", in which case it's a system-name, or 2) it has a dash in it, in which case it's a system id. @param str The string to check. @return True if we assume the string is system- generated, false otherwise. ** ********************************************** loadIdMappings: Load mappings of object ids to object names for purposes of having meaningful output and for creating unique ids on the rows of the system catalogs. @param stmt Statement on a connection to the database being examined. @param conn Connection to the database being examined. @return A HashMap with all relevant id-to- name mappings has been returned. ** ********************************************** lookFive: Use dblook to generate DDL for all objects in the source database (with any schema) that are related to table 'T1' and 'TWITHKEYS' (with no matches existing for the latter). -t t1 "tWithKeys" @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The appropriate DDL has been generated and written to a file called <dbName + ".sql">. ** ********************************************** lookFour: Use dblook to generate DDL for all objects in the source database with schema 'BAR' that are related to tables 'T3', 'tWithKeys', and 'MULTI WORD NAME'. -z bar -t t3 "\"tWithKeys\"" "Multi word name" @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The appropriate DDL has been generated and written to a file called <dbName + ".sql">. ** ********************************************** lookOne: Use dblook to generate FULL DDL for a given database. @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The full DDL for the source database has been generated and written to a file called <dbName + ".sql">. ** ********************************************** lookSeven: Use dblook to generate DDL for all objects in the source database with schema '"Quoted"Schema"'. -z \"\"Quoted\"Schema\"\" @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The appropriate DDL has been generated and written to a file called <dbName + ".sql">. ** ********************************************** lookSix: Call dblook with an invalid url, to make sure that errors are printed to log. -d <dbName> // missing protocol. @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The appropriate DDL has been generated and written to a file called <dbName + ".sql">. ** ********************************************** lookThree: Use dblook to generate DDL for all objects in the source database, using Network Server. @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The appropriate DDL has been generated and written to a file called <dbName + ".sql">. ** ********************************************** lookTwo: Use dblook to generate DDL for all objects in the source database with schema 'BAR', excluding views: -z bar -noview @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The appropriate DDL has been generated and written to a file called <dbName + ".sql">. ** ********************************************** looksLikeSysGenId: See if the received string looks like it is a system-generated id (i.e. contains a dash (-)). NOTE: This test assumes that none of object names provided in "dblook_makeDB.sql" will contain dashes.  If they do, then they will be filtered out in the test output. @param val The string value in question. @return True if the value looks like it is a system- generated id; false otherwise. ** ********************************************** looksLikeSysGenName: See if the received string looks like it is a system-generated name.  There are two types of system-generated names: 1) visible names, which start with "SQL", and 2) hidden names, which exist for Stored Statements that are used to back triggers; these names start with "TRIGGERACTN_" and then have a UUID. NOTE: This test assumes that none of object names provided in "dblook_makeDB.sql" satisfy either of these conditions.  If they do, they will be filtered out of the test output. @param val The string value in question. @return True if the value looks like it is a system- generated name; false otherwise. ** ********************************************** main: ** ********************************************** printAsHeader: Print the received string to output as a header. @param str String to print. ** ********************************************** renameDbLookLog: Checks if the logfile of dblook exists and tries to rename it to prevent possible next tests from failing. The log should not be deleted because the output may be examined in case a test fails. The new name of dblook.log should be dblook_testname#.log, where # is a 'version' number. The 'version' number is needed because the same test may be run multiple times with different parameters. @param nameOfTest Name of the finished test. ** ********************************************** runAllTests: Makes the call to execute each of the desired tests. @param dbName The name of the database on which to run the tests. @param newDBName The name of the database to be created from the DDL that is generated (by dblook) for the source database. ** ********************************************** runDBLook: Runs a series of tests using dblook on the received database. @param dbName The name of the database on which to run the tests. @return A series of tests intended to verify the full functionality of the dblook utility has been run. ** ********************************************** runDDL: Run an sql script. @param conn database connection @param scriptName Name of the script ** ********************************************** runMessageCheckTest Run dblook and verify that all of the dblook messages are correctly displayed. @param dbName The name of the source database (i.e. the database for which the DDL is generated). @return The DDL for a simple database, plus all dblook messages, have been generated and written to System.out. ** ********************************************** runTest: Runs dblook on the source database with a specific set of parameters, then uses the resultant DDL to create a new database, and dumps the system catalogs for that database to file.  Finally, the new database is deleted in preparation for subsequent calls to this method. @param whichTest An indication of which test to run; each test number has a different set of parameters. @param dbName The name of the source database. @param newDBName The name of the database to be created from the DDL that is generated (by dblook) for the source database. @return dblook has been executed using the parameters associated with the given test, and that DDL has been written to a ".sql" file named after the source database; a new database has been created from the ".sql" generated by dblook; the system catalogs for that new database have been dumped to output; and the new database has been deleted. ** Regression test case for DERBY-6387. Verify that triggers are returned in the order in which they were created. ********************************************** writeOut: Write the received string to some output. @param str String to write. **
********************************************** doTest Run a test of the dblook utility using Network Server. ** This test runs dblook on a test database using a connection to the Network Server.
********************************************** doTest Run a test of the dblook utility using Network Server. ** This test runs dblook on a test database using a connection to the Network Server.
********************************************** main: **
Invoke {@code org.apache.derby.iapi.tools.run} in a sub-process.
Reproduce JIRA DERBY-662 <p> Find the conglomerate with number 2080, and drop it.  The bug is that during redo the system, on windows, will incorrectly delete C2080.dat because it did not do the hex conversion on the conglomerate number.  This will result in conglomerate 8320 not having it's associate data file c2080.dat. create tables, commit, and cause checkpoint of db.


return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
Run a SQL script from an InputStream and write the resulting output to the provided PrintStream. SQL commands are separated by a semi-colon ';' character. Run a SQL script from an InputStream and write the resulting output to the provided PrintStream. SQL commands are separated by a semi-colon ';' character.





Clears the warnings in all resultsets
Since they will all need to do warning calls/clears, may as well stick it here.








return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.
return the command line to invoke this VM.  The caller then adds the class and program arguments.




Get the base file name from a resource name string Get the current JVM using the normal test harness rules for finding a JVM. <OL> <LI> If the sytem property 'jvm' use this name. <LI> else if the java version starts with 1.2 use "jdk12". <LI> else use "currentjvm". return the property definition introducer, with a space if a separator is needed. utility for locating a jvm. pass in class name for JVM.  If we can't find it, try also org.apache.derbyTesting.functionTests.harness.<jvmName> Return the major version number Return the major version number Find $WS based on the assumption that JAVA_HOME is $WS/<jvm_name> or $WS/<jvm_name>/jre set up security properties for server command line. Get the current JVM using the normal test harness rules for finding a JVM.
Check the data on the positioned row against what we inserted. Prepares and executes query against table t0 with n parameters The assumption is that the query will always return our one row of data inserted into the t0 table. Test in clause with many parameters Create a large insert statement with rowCount rows all with constants. Prepare and execute it and then rollback to leave the table unchanged. Tests numParam parameter markers in a where clause Test an INSERT statement with a large number of rows in the VALUES clause. Reported as DERBY-1714. Test many logical operators in the where clause.



for use in test getbestrowidentifier.sql

public static Connection conn; ///////////////////////////////////////////////////////////  RETURN PARAMETER METHODS  /////////////////////////////////////////////////////////// should do get setString() to use a string that is appropriate for the target type these come from the performance test JDBC.Parameters that was failing ///////////////////////////////////////////////////////////  OUTPUT PARAMETER METHODS  /////////////////////////////////////////////////////////// test that everything works ok when we regsiter the param as type OTHER. should be able to get/setXXX of the appropriate type test: do we get an appropriate update count when using ?=call?

For each module with a particular tag in derby.module.<tag>, see if there is any configuration restriction.  If there is no cloudscape.config.<tag> property, then this module should be present in all configurations.  If there is a cloudscape.config.<tag>, then this module should only be present in the configurations listed. <br>If this module should be present or this configuration, then gather up all the properties belong to this module and send it to the output file.


Switch on the first argument to choose the tool, pass the remaining arguments to the tool. Print the usage statement if the user didn't enter a valid choice of tool. Utility method to trim one element off of the argument array.



Determine if this is a message that the client is using There are some classes of ids that we assume are client messages (see code below for the definitive list). All other shared message ids should be added to the static array clientMessageIds, defined at the top of this class

Run two threads, where thread 1 first reads from table A and then inserts a row into table B, and thread 2 first reads from table B and then inserts a row into table A. This should cause a deadlock in one of the threads. Before DERBY-715, sometimes a timeout would be raised instead of a deadlock.
Invoke SYSCS_DIAG.SPACE_TABLE on the specified table in the current schema. Print the value of all columns in the current row of the specified result set, if debugging is enabled. Test reclaim of a single deleted blob on a page with non-deleted rows. <p> loops through inserting alternating long and short column rows resulting in pages with 1 short and one long.  Deletes the long column row and tests that space from the long column row is reclaimed even though there are non-deleted rows on the page. DERBY-1913 <p> test2 is too sensitive to machine speed and background thread processing.  It would be better suited as a long running stress test if someone has the inclination.  Disabling this test for now.  test1 covers the original intent to test that blobs are immediately marked for post commit on individual delete, rather than waiting for all rows on a page to be deleted. wait for background thread to convert allocated pages to free pages <p> Wait until the total number of allocated pages is &lt;= alloc_wait_count. The expectation is that the test has performed some deletes and committed allowing the background task converted empty allocated pages with only deleted rows into free pages. On an machine with some idle processors only a short wait should be necessary.  But on machines with lots of load, possibly other tests running just sleeping does not guarantee background thread an immediate chance to run.  Without this extra wait some nightly's were seeing failures, see DERBY-1913.
gets the build number for the Apache Derby embedded library gets the build number for the specified library gets the major version of the Apache Derby embedded code. gets the major version of the specified code library. gets the minor version of the Apache Derby embedded code. gets the minor version of the specified code library. gets the product name for the Apache Derby embedded library gets the external name for the specified code library. Return the version information string for the specified library including alpha or beta indicators. Return the version information string for the Apache Derby embedded library including alpha or beta indicators.
method that does a delete of rows on table t1 based on values from triggers.
class interface Mimic SanityManager.ASSERT in a JDBC-friendly way, and providing system cleanup for JDBC failures. We need the connection to do cleanup... Checks whether a data type is supported and raises a SQLException if it isn't. Checks whether a data type is supported and raises a StandardException if it isn't. * There is at least one static method for each message id. * Its parameters are specific to its message. * These will throw SQLException when the message repository * cannot be located. * Note that these methods use the SQL exception factory, * they don't directly do a new Util. Returns false if a data type is not supported for: <code>setObject(int, Object, int)</code> and <code>setObject(int, Object, int, int)</code>. * Methods of Throwable class implementation Log SQLException to the error log if the severity exceeds the logSeverityLevel  and then throw it.  This method can be used for logging JDBC exceptions to derby.log DERBY-1191. Log an SQLException to the error log or to the console if there is no error log available. This method could perhaps be optimized to have a static shared ErrorStringBuilder and synchronize the method, but this works for now. Create an {@code IOException} that wraps another {@code Throwable}. Generate an <code>SQLException</code> which points to another <code>SQLException</code> nested within it with <code>setNextException()</code>. Squash an array of longs into an array of ints
Position on the specified row of the specified ResultSet. Position after the last row of the specified ResultSet and return NULL to the user. Position before the first row of the specified ResultSet and return NULL to the user. Check that the cursor is scrollable. Perform cleanup after a script has been run. Close the input streams if required and shutdown derby on an exit. catch processing on failed commands. This really ought to be in ij somehow, but it was easier to catch in Main. REMIND: eventually this might be part of StatementFinder, used at each carriage return to show that it is still "live" when it is reading multi-line input. stack trace dumper Position on the first row of the specified ResultSet and return that row to the user. Get the current row number run ij over the specified input, sending output to the specified output. Any prior input and output will be lost. Support to run a script. Performs minimal setup to set the passed in connection into the existing ij setup, ConnectionEnv. This routine displays SQL exceptions and decides whether they are fatal or not, based on the ignoreErrors field. If they are fatal, an ijFatalException is thrown. Lifted from ij/util.java:ShowSQLException Initialize the connections from the environment. Position on the last row of the specified ResultSet and return that row to the user. Position on the previous row of the specified ResultSet and return that row to the user. Move the cursor position by the specified amount. Run the guts of the script. Split out to allow calling from the full ij and the minimal goScript.
Obtain the URL for a test resource, e.g. a policy file or a SQL script. Open the URL for a a test resource, e.g. a policy file or a SQL script.

Obtain the transaction branch qualifier part of the Xid in a byte array. <p> Obtain the format id part of the Xid. <p> Obtain the global transaction identifier part of XID as an array of bytes. <p> non-xa stuff. DataSource and ConnectionPoolDataSource Get a DataSource that supports distributed transactions. Handles the given throwable. <p> If possible, an {@code SQLException} is returned. Otherwise the appropriate actions are taken and a {@code RuntimeException} is thrown.
