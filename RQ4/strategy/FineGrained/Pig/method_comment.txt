java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()





(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
Replaces the given entries in the configuration by clearing the Configuration and re-adding the elements that aren't in the Map of entries to unset Method to allow specific implementations to add more elements to the Job for reading data from Accumulo. Method to allow specific implementations to add more elements to the Job for writing data to Accumulo. Extract arguments passed into the constructor to avoid the URI Extract elements from the Configuration whose keys match the given prefix Returns UDFProperties based on <code>contextSignature</code>. Ensure that Accumulo's dependent jars are added to the Configuration to alleviate the need for clients to REGISTER dependency jars. Initializes {@link #columnDefs} and splits columns on {@link #COMMA} StoreFunc methods Unsets elements in the Configuration using the unset method Removes the given values from the configuration, accounting for changes in the Configuration API given the version of Hadoop being used.
{@inheritDoc } {@inheritDoc } {@inheritDoc }


Clear internal buffer, this should be called after all data are retreived Get iterator of tuples in the buffer Whether there are more tuples to pull out of iterator Pull next batch of tuples from iterator and put them into this buffer
Pass tuples to the UDF. Called after getValue() to prepare processing for next key. Called when all tuples from current key have been passed to accumulate.






Check if an operator is qualified to be under POForEach to turn on accumulator. The operator must be in the following list or an <code>POUserFunc</code>. If the operator has sub-operators, they must also belong to this list. <li>ConstantExpression</li> <li>POProject, whose result type is not BAG, or TUPLE and overloaded</li> <li>POMapLookup</li> <li>POCase</li> <li>UnaryExpressionOperator</li> <li>BinaryExpressionOperator</li> <li>POBinCond</li> If the operator is <code>POUserFunc</code>, it must implement <code>Accumulator</code> interface and its inputs pass the check by calling <code>checkUDFInput()</code> Check operators under POUserFunc to verify if this is a valid UDF to run as accumulator. The inputs to <code>POUserFunc</code> must be in the following list. If the operator has sub-operators, they must also belong to this list. <li>PORelationToExprProject</li> <li>ConstantExpression</li> <li>POProject</li> <li>POCase</li> <li>UnaryExpressionOperator</li> <li>BinaryExpressionOperator</li> <li>POBinCond</li> <li>POSortedDistinct</li> <li>POForEach</li>
NOT IMPLEMENTED NOT IMPLEMENTED NOT IMPLEMENTED NOT IMPLEMENTED Not implemented! NOT IMPLEMENTED NOT IMPLEMENTED NOT IMPLEMENTED
Adds the given column family, column qualifier and value to the given mutation

This method is used to invoke the appropriate addition method, as Java does not provide generic dispatch for it.



Get the final function. Get the initial function. Get the intermediate function.




This is the free accumulate implementation based on the static classes provided by the Algebraic static classes. This implemention works by leveraging the initial, intermediate, and final classes provided by the algebraic interface. The exec function of the Initial EvalFunc will be called on every Tuple of the input and the output will be collected in an intermediate state. Periodically, this intermediate state will have the Intermediate EvalFunc called on it 1 or more times. The Final EvalFunc is not called until getValue() is called. Per the Accumulator interface, this clears all of the variables used in the implementation. This must be implement as per a normal Algebraic interface. See {@link Algebraic} for more information. This must be implement as per a normal Algebraic interface. See {@link Algebraic} for more information. This must be implement as per a normal Algebraic interface. See {@link Algebraic} for more information. This function returns the ultimate result. It is when getValue() is called that the Final EvalFunc's exec function is called on the accumulated data. This helper function instantiates an EvalFunc given its String class name.







$ANTLR start "alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:113:1: alias : IDENTIFIER ; $ANTLR start "alias_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:582:1: alias_col_ref : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "as_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:189:1: as_clause : ^( AS field_def_list ) ; $ANTLR start "assert_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:307:1: assert_clause : ^( ASSERT alias cond ( comment )? ) ; $ANTLR start "assert_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:94:1: assert_statement : assert_clause ; $ANTLR start "bag" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:627:1: bag : ^( BAG_VAL ( tuple )* ) ; $ANTLR start "bag_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:226:1: bag_type : ^( BAG_TYPE ( IDENTIFIER )? ( tuple_type )? ) ; $ANTLR start "bag_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:363:1: bag_type_cast : ^( BAG_TYPE_CAST ( tuple_type_cast )? ) ; $ANTLR start "bin_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:399:1: bin_expr : ^( BIN_EXPR cond expr expr ) ; $ANTLR start "cache_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:159:1: cache_clause : ^( CACHE path_list ) ; $ANTLR start "case_cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:407:1: case_cond : ^( CASE_COND ^( WHEN ( cond )+ ) ^( THEN ( expr )+ ) ) ; $ANTLR start "case_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:403:1: case_expr : ^( CASE_EXPR ( ^( CASE_EXPR_LHS expr ) ( ^( CASE_EXPR_RHS expr ) )+ )+ ) ; $ANTLR start "cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:146:1: cmd : ^( EXECCOMMAND ( ship_clause | cache_clause | input_clause | output_clause | error_clause )* ) ; $ANTLR start "col_alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:382:1: col_alias : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "col_alias_or_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:379:1: col_alias_or_index : ( col_alias | col_index ); $ANTLR start "col_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:388:1: col_index : DOLLARVAR ; $ANTLR start "col_range" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:392:1: col_range : ^( COL_RANGE ( col_ref )? DOUBLE_PERIOD ( col_ref )? ) ; $ANTLR start "col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:579:1: col_ref : ( alias_col_ref | dollar_col_ref ); $ANTLR start "comment" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:311:1: comment : QUOTEDSTRING ; $ANTLR start "cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:318:1: cond : ( ^( OR cond cond ) | ^( AND cond cond ) | ^( NOT cond ) | ^( NULL expr ( NOT )? ) | ^( rel_op expr expr ) | in_eval | func_eval | ^( BOOL_COND expr ) ); $ANTLR start "const_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:604:1: const_expr : literal ; $ANTLR start "cross_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:457:1: cross_clause : ^( CROSS rel_list ( partition_clause )? ) ; $ANTLR start "cube_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:254:1: cube_by_clause : ^( BY cube_or_rollup ) ; $ANTLR start "cube_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:270:1: cube_by_expr : ( col_range | expr | STAR ); $ANTLR start "cube_by_expr_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:266:1: cube_by_expr_list : ( cube_by_expr )+ ; $ANTLR start "cube_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:246:1: cube_clause : ^( CUBE cube_item ) ; $ANTLR start "cube_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:250:1: cube_item : rel ( cube_by_clause ) ; $ANTLR start "cube_or_rollup" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:258:1: cube_or_rollup : ( cube_rollup_list )+ ; $ANTLR start "cube_rollup_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:262:1: cube_rollup_list : ^( ( CUBE | ROLLUP ) cube_by_expr_list ) ; $ANTLR start "define_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:142:1: define_clause : ^( DEFINE IDENTIFIER ( cmd | func_clause ) ) ; $ANTLR start "distinct_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:449:1: distinct_clause : ^( DISTINCT rel ( partition_clause )? ) ; $ANTLR start "dollar_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:600:1: dollar_col_ref : DOLLARVAR ; $ANTLR start "dot_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:375:1: dot_proj : ^( PERIOD ( col_alias_or_index )+ ) ; $ANTLR start "eid" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:636:1: eid : ( rel_str_op | IMPORT | RETURNS | DEFINE | LOAD | FILTER | FOREACH | CUBE | ROLLUP | MATCHES | ORDER | RANK | DISTINCT | COGROUP | JOIN | CROSS | UNION | SPLIT | INTO | IF | ALL | AS | BY | USING | INNER | OUTER | PARALLEL | PARTITION | GROUP | AND | OR | NOT | GENERATE | FLATTEN | EVAL | ASC | DESC | BOOLEAN | INT | LONG | FLOAT | DOUBLE | DATETIME | CHARARRAY | BIGINTEGER | BIGDECIMAL | BYTEARRAY | BAG | TUPLE | MAP | IS | NULL | TRUE | FALSE | STREAM | THROUGH | STORE | MAPREDUCE | SHIP | CACHE | INPUT | OUTPUT | STDERROR | STDIN | STDOUT | LIMIT | SAMPLE | LEFT | RIGHT | FULL | IDENTIFIER | TOBAG | TOMAP | TOTUPLE | ASSERT ); $ANTLR start "error_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:177:1: error_clause : ^( STDERROR ( QUOTEDSTRING ( INTEGER )? )? ) ; $ANTLR start "expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:341:1: expr : ( ^( PLUS expr expr ) | ^( MINUS expr expr ) | ^( STAR expr expr ) | ^( DIV expr expr ) | ^( PERCENT expr expr ) | ^( CAST_EXPR type expr ) | const_expr | var_expr | ^( NEG expr ) | ^( CAST_EXPR type_cast expr ) | ^( EXPR_IN_PAREN expr ) ); $ANTLR start "field_def" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:199:1: field_def : ( ^( FIELD_DEF IDENTIFIER ( type )? ) | ^( FIELD_DEF_WITHOUT_IDENTIFIER type ) ); $ANTLR start "field_def_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:211:1: field_def_list : ( field_def )+ ; $ANTLR start "filename" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:185:1: filename : QUOTEDSTRING ; $ANTLR start "filter_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:314:1: filter_clause : ^( FILTER rel cond ) ; $ANTLR start "flatten_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:299:1: flatten_clause : ^( FLATTEN expr ) ; $ANTLR start "flatten_generated_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:289:1: flatten_generated_item : ( flatten_clause | col_range | expr | STAR ) ( field_def_list )? ; $ANTLR start "foreach_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:496:1: foreach_clause : ^( FOREACH rel foreach_plan ) ; $ANTLR start "foreach_plan" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:500:1: foreach_plan : ( ^( FOREACH_PLAN_SIMPLE generate_clause ) | ^( FOREACH_PLAN_COMPLEX nested_blk ) ); $ANTLR start "func_args" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:242:1: func_args : ( QUOTEDSTRING )+ ; $ANTLR start "func_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:233:1: func_clause : ( ^( FUNC_REF func_name ) | ^( FUNC func_name ( func_args )? ) ); $ANTLR start "func_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:333:1: func_eval : ^( FUNC_EVAL func_name ( real_arg )* ) ; $ANTLR start "func_name" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:238:1: func_name : eid ( ( PERIOD | DOLLAR ) eid )* ; $ANTLR start "general_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:101:1: general_statement : ^( STATEMENT ( alias )? op_clause ( parallel_clause )? ) ; $ANTLR start "generate_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:509:1: generate_clause : ^( GENERATE ( flatten_generated_item )+ ) ; delegates $ANTLR start "group_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:274:1: group_clause : ^( ( GROUP | COGROUP ) ( group_item )+ ( group_type )? ( partition_clause )? ) ; $ANTLR start "group_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:281:1: group_item : rel ( join_group_by_clause | ALL | ANY ) ( INNER | OUTER )? ; $ANTLR start "group_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:278:1: group_type : QUOTEDSTRING ; $ANTLR start "in_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:329:1: in_eval : ^( IN ( ^( IN_LHS expr ) ^( IN_RHS expr ) )+ ) ; $ANTLR start "input_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:163:1: input_clause : ^( INPUT ( stream_cmd )+ ) ; $ANTLR start "join_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:465:1: join_clause : ^( JOIN join_sub_clause ( join_type )? ( partition_clause )? ) ; $ANTLR start "join_group_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:484:1: join_group_by_clause : ^( BY ( join_group_by_expr )+ ) ; $ANTLR start "join_group_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:488:1: join_group_by_expr : ( col_range | expr | STAR ); $ANTLR start "join_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:480:1: join_item : ^( JOIN_ITEM rel join_group_by_clause ) ; $ANTLR start "join_sub_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:472:1: join_sub_clause : ( join_item ( LEFT | RIGHT | FULL ) ( OUTER )? join_item | ( join_item )+ ); $ANTLR start "join_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:469:1: join_type : QUOTEDSTRING ; $ANTLR start "keyvalue" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:620:1: keyvalue : ^( KEY_VAL_PAIR map_key const_expr ) ; $ANTLR start "limit_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:411:1: limit_clause : ^( LIMIT rel ( INTEGER | LONGINTEGER | expr ) ) ; $ANTLR start "literal" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:607:1: literal : ( scalar | map | bag | tuple ); $ANTLR start "load_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:181:1: load_clause : ^( LOAD filename ( func_clause )? ( as_clause )? ) ; $ANTLR start "map" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:616:1: map : ^( MAP_VAL ( keyvalue )* ) ; $ANTLR start "map_key" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:624:1: map_key : QUOTEDSTRING ; $ANTLR start "map_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:230:1: map_type : ^( MAP_TYPE ( IDENTIFIER )? ( type )? ) ; $ANTLR start "mr_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:563:1: mr_clause : ^( MAPREDUCE QUOTEDSTRING ( path_list )? store_clause load_clause ( EXECCOMMAND )? ) ; $ANTLR start "nested_blk" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:505:1: nested_blk : ( nested_command )* generate_clause ; $ANTLR start "nested_command" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:513:1: nested_command : ( ^( NESTED_CMD IDENTIFIER nested_op ) | ^( NESTED_CMD_ASSI IDENTIFIER expr ) ); $ANTLR start "nested_cross" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:547:1: nested_cross : ^( CROSS nested_op_input_list ) ; $ANTLR start "nested_distinct" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:539:1: nested_distinct : ^( DISTINCT nested_op_input ) ; $ANTLR start "nested_filter" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:531:1: nested_filter : ^( FILTER nested_op_input cond ) ; $ANTLR start "nested_foreach" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:550:1: nested_foreach : ^( FOREACH nested_op_input generate_clause ) ; $ANTLR start "nested_limit" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:543:1: nested_limit : ^( LIMIT nested_op_input ( INTEGER | expr ) ) ; $ANTLR start "nested_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:518:1: nested_op : ( nested_proj | nested_filter | nested_sort | nested_distinct | nested_limit | nested_cross | nested_foreach ); $ANTLR start "nested_op_input" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:556:1: nested_op_input : ( col_ref | nested_proj ); $ANTLR start "nested_op_input_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:553:1: nested_op_input_list : ( nested_op_input )+ ; $ANTLR start "nested_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:527:1: nested_proj : ^( NESTED_PROJ col_ref ( col_ref )+ ) ; $ANTLR start "nested_sort" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:535:1: nested_sort : ^( ORDER nested_op_input order_by_clause ( func_clause )? ) ; $ANTLR start "num_scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:613:1: num_scalar : ( MINUS )? ( INTEGER | LONGINTEGER | FLOATNUMBER | DOUBLENUMBER | BIGINTEGERNUMBER | BIGDECIMALNUMBER ) ; $ANTLR start "op_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:121:1: op_clause : ( define_clause | load_clause | group_clause | store_clause | filter_clause | distinct_clause | limit_clause | sample_clause | order_clause | rank_clause | cross_clause | join_clause | union_clause | stream_clause | mr_clause | split_clause | foreach_clause | cube_clause | assert_clause ); $ANTLR start "order_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:440:1: order_by_clause : ( STAR ( ASC | DESC )? | ( order_col )+ ); $ANTLR start "order_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:436:1: order_clause : ^( ORDER rel order_by_clause ( func_clause )? ) ; $ANTLR start "order_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:445:1: order_col : ( col_range | col_ref ) ( ASC | DESC )? ; $ANTLR start "output_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:173:1: output_clause : ^( OUTPUT ( stream_cmd )+ ) ; $ANTLR start "parallel_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:109:1: parallel_clause : ^( PARALLEL INTEGER ) ; $ANTLR start "partition_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:453:1: partition_clause : ^( PARTITION func_name ) ; $ANTLR start "path_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:155:1: path_list : ( QUOTEDSTRING )+ ; $ANTLR start "pound_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:395:1: pound_proj : ^( POUND ( QUOTEDSTRING | NULL ) ) ; $ANTLR start "projectable_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:371:1: projectable_expr : ( func_eval | col_ref | bin_expr | case_expr | case_cond ); $ANTLR start "query" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:82:1: query : ^( QUERY ( statement )* ) ; $ANTLR start "rank_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:427:1: rank_by_clause : ( STAR ( ASC | DESC )? | ( rank_col )+ ); $ANTLR start "rank_by_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:423:1: rank_by_statement : ^( BY rank_by_clause ( DENSE )? ) ; $ANTLR start "rank_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:419:1: rank_clause : ^( RANK rel ( rank_by_statement )? ) ; $ANTLR start "rank_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:432:1: rank_col : ( col_range | col_ref ) ( ASC | DESC )? ; $ANTLR start "real_arg" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:337:1: real_arg : ( expr | STAR ); $ANTLR start "realias_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:106:1: realias_clause : ^( REALIAS alias IDENTIFIER ) ; $ANTLR start "realias_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:97:1: realias_statement : realias_clause ; $ANTLR start "rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:285:1: rel : ( alias | ( op_clause ( parallel_clause )? ) ); $ANTLR start "rel_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:461:1: rel_list : ( rel )+ ; $ANTLR start "rel_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:714:1: rel_op : ( rel_op_eq | rel_op_ne | rel_op_gt | rel_op_gte | rel_op_lt | rel_op_lte | STR_OP_MATCHES ); $ANTLR start "rel_op_eq" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:724:1: rel_op_eq : ( STR_OP_EQ | NUM_OP_EQ ); $ANTLR start "rel_op_gt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:734:1: rel_op_gt : ( STR_OP_GT | NUM_OP_GT ); $ANTLR start "rel_op_gte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:739:1: rel_op_gte : ( STR_OP_GTE | NUM_OP_GTE ); $ANTLR start "rel_op_lt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:744:1: rel_op_lt : ( STR_OP_LT | NUM_OP_LT ); $ANTLR start "rel_op_lte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:749:1: rel_op_lte : ( STR_OP_LTE | NUM_OP_LTE ); $ANTLR start "rel_op_ne" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:729:1: rel_op_ne : ( STR_OP_NE | NUM_OP_NE ); $ANTLR start "rel_str_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:754:1: rel_str_op : ( STR_OP_EQ | STR_OP_NE | STR_OP_GT | STR_OP_LT | STR_OP_GTE | STR_OP_LTE | STR_OP_MATCHES ); $ANTLR start "sample_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:415:1: sample_clause : ^( SAMPLE rel ( DOUBLENUMBER | expr ) ) ; $ANTLR start "scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:610:1: scalar : ( num_scalar | QUOTEDSTRING | NULL | TRUE | FALSE ); $ANTLR start "ship_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:151:1: ship_clause : ^( SHIP ( path_list )? ) ; $ANTLR start "simple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:218:1: simple_type : ( BOOLEAN | INT | LONG | FLOAT | DOUBLE | DATETIME | BIGINTEGER | BIGDECIMAL | CHARARRAY | BYTEARRAY ); $ANTLR start "split_branch" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:571:1: split_branch : ^( SPLIT_BRANCH alias cond ) ; $ANTLR start "split_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:567:1: split_clause : ^( SPLIT rel ( split_branch )+ ( split_otherwise )? ) ; $ANTLR start "split_otherwise" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:575:1: split_otherwise : ^( OTHERWISE alias ) ; $ANTLR start "split_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:91:1: split_statement : split_clause ; $ANTLR start "statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:85:1: statement : ( general_statement | split_statement | realias_statement | assert_statement ); $ANTLR start "store_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:303:1: store_clause : ^( STORE alias filename ( func_clause )? ) ; $ANTLR start "stream_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:559:1: stream_clause : ^( STREAM rel ( EXECCOMMAND | IDENTIFIER ) ( as_clause )? ) ; $ANTLR start "stream_cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:167:1: stream_cmd : ( ^( STDIN ( func_clause )? ) | ^( STDOUT ( func_clause )? ) | ^( QUOTEDSTRING ( func_clause )? ) ); $ANTLR end "rel_str_op" $ANTLR start synpred102_AliasMasker $ANTLR end synpred102_AliasMasker $ANTLR start synpred106_AliasMasker $ANTLR end synpred106_AliasMasker $ANTLR start synpred107_AliasMasker $ANTLR end synpred107_AliasMasker $ANTLR start synpred110_AliasMasker $ANTLR end synpred110_AliasMasker $ANTLR start synpred133_AliasMasker $ANTLR end synpred133_AliasMasker $ANTLR start synpred134_AliasMasker $ANTLR end synpred179_AliasMasker Delegated rules $ANTLR end synpred134_AliasMasker $ANTLR start synpred135_AliasMasker $ANTLR end synpred135_AliasMasker $ANTLR start synpred161_AliasMasker $ANTLR end synpred161_AliasMasker $ANTLR start synpred179_AliasMasker $ANTLR start "tuple" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:631:1: tuple : ^( TUPLE_VAL ( literal )* ) ; $ANTLR start "tuple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:222:1: tuple_type : ^( TUPLE_TYPE ( field_def_list )? ) ; $ANTLR start "tuple_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:359:1: tuple_type_cast : ^( TUPLE_TYPE_CAST ( type_cast )* ) ; $ANTLR start "type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:215:1: type : ( simple_type | tuple_type | bag_type | map_type ); $ANTLR start "type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:355:1: type_cast : ( simple_type | map_type | tuple_type_cast | bag_type_cast ); $ANTLR start "union_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:492:1: union_clause : ^( UNION ( ONSCHEMA )? rel_list ) ; $ANTLR start "var_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AliasMasker.g:367:1: var_expr : projectable_expr ( dot_proj | pound_proj )* ;
Get a new instance of the expression visitor to apply to a given expression.
@Override
Reads the partition columns Tries to determine the LoadFunc by using the LoadFuncHelper to identify a loader for the first file in the location directory.<br/> If no LoadFunc can be determine ad FrontendException is thrown.<br/> If the LoadFunc implements the LoadMetadata interface and returns a non null schema this schema is returned. --------------- Save Signature and PartitionFilter Expression ----------------- //
Method to call on every node in the logical expression plan.
Method to call on every node in the logical plan.



This adds the additional overhead of the append Tuple

$ANTLR start "alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:109:1: alias : IDENTIFIER ; $ANTLR start "alias_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:610:1: alias_col_ref : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "as_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:187:1: as_clause : ^( AS field_def_list ) ; $ANTLR start "assert_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:314:1: assert_clause : ^( ASSERT rel cond ( comment )? ) ; $ANTLR start "assert_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:80:1: assert_statement : assert_clause ; $ANTLR start "bag" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:655:1: bag : ( ^( BAG_VAL tuple ( tuple )* ) | ^( BAG_VAL ) ); $ANTLR start "bag_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:221:1: bag_type : ^( BAG_TYPE ( ( IDENTIFIER )? tuple_type )? ) ; $ANTLR start "bag_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:371:1: bag_type_cast : ^( BAG_TYPE_CAST ( tuple_type_cast )? ) ; $ANTLR start "bin_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:408:1: bin_expr : ^( BIN_EXPR cond expr expr ) ; $ANTLR start "cache_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:153:1: cache_clause : ^( CACHE path_list ) ; $ANTLR start "case_cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:418:1: case_cond : ^( CASE_COND ^( WHEN cond ( cond )* ) ^( THEN expr ( expr )* ) ) ; $ANTLR start "case_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:412:1: case_expr : ^( CASE_EXPR ( ^( CASE_EXPR_LHS expr ) ( ^( CASE_EXPR_RHS expr ) )+ ) ( ^( CASE_EXPR_LHS expr ) ( ^( CASE_EXPR_RHS expr ) )+ )* ) ; $ANTLR start "cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:139:1: cmd : ^( EXECCOMMAND ( ship_clause | cache_clause | input_clause | output_clause | error_clause )* ) ; $ANTLR start "col_alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:390:1: col_alias : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "col_alias_or_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:387:1: col_alias_or_index : ( col_alias | col_index ); $ANTLR start "col_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:396:1: col_index : DOLLARVAR ; $ANTLR start "col_range" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:400:1: col_range : ^( COL_RANGE ( col_ref )? DOUBLE_PERIOD ( col_ref )? ) ; $ANTLR start "col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:607:1: col_ref : ( alias_col_ref | dollar_col_ref ); $ANTLR start "comment" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:310:1: comment : QUOTEDSTRING ; $ANTLR start "cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:322:1: cond : ( ^( OR cond cond ) | ^( AND cond cond ) | ^( NOT cond ) | ^( NULL expr ( NOT )? ) | ^( rel_op expr expr ) | in_eval | func_eval | ^( BOOL_COND expr ) ); $ANTLR start "const_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:620:1: const_expr : literal ; $ANTLR start "cross_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:473:1: cross_clause : ^( CROSS rel_list ( partition_clause )? ) ; $ANTLR start "cube_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:256:1: cube_by_clause : ^( BY cube_or_rollup ) ; $ANTLR start "cube_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:272:1: cube_by_expr : ( col_range | expr | STAR ); $ANTLR start "cube_by_expr_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:268:1: cube_by_expr_list : ( cube_by_expr ( cube_by_expr )* ) ; $ANTLR start "cube_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:248:1: cube_clause : ^( CUBE cube_item ) ; $ANTLR start "cube_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:252:1: cube_item : rel ( cube_by_clause ) ; $ANTLR start "cube_or_rollup" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:260:1: cube_or_rollup : cube_rollup_list ( cube_rollup_list )* ; $ANTLR start "cube_rollup_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:264:1: cube_rollup_list : ^( ( CUBE | ROLLUP ) cube_by_expr_list ) ; $ANTLR start "define_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:134:1: define_clause : ^( DEFINE IDENTIFIER ( cmd | func_clause ) ) ; $ANTLR start "distinct_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:465:1: distinct_clause : ^( DISTINCT rel ( partition_clause )? ) ; $ANTLR start "dollar_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:616:1: dollar_col_ref : DOLLARVAR ; $ANTLR start "dot_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:383:1: dot_proj : ^( PERIOD col_alias_or_index ( col_alias_or_index )* ) ; $ANTLR start "eid" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:666:1: eid : ( rel_str_op | IMPORT | RETURNS | DEFINE | LOAD | FILTER | FOREACH | CUBE | ROLLUP | MATCHES | ORDER | RANK | DISTINCT | COGROUP | JOIN | CROSS | UNION | SPLIT | INTO | IF | ALL | AS | BY | USING | INNER | OUTER | PARALLEL | PARTITION | GROUP | AND | OR | NOT | GENERATE | FLATTEN | EVAL | ASC | DESC | BOOLEAN | INT | LONG | FLOAT | DOUBLE | BIGINTEGER | BIGDECIMAL | DATETIME | CHARARRAY | BYTEARRAY | BAG | TUPLE | MAP | IS | NULL | TRUE | FALSE | STREAM | THROUGH | STORE | MAPREDUCE | SHIP | CACHE | INPUT | OUTPUT | STDERROR | STDIN | STDOUT | LIMIT | SAMPLE | LEFT | RIGHT | FULL | IDENTIFIER | TOBAG | TOMAP | TOTUPLE | IN | CASE | ASSERT ); $ANTLR start "error_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:173:1: error_clause : ^( STDERROR ( QUOTEDSTRING ( INTEGER )? )? ) ; $ANTLR start "expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:348:1: expr : ( ^( PLUS expr expr ) | ^( MINUS expr expr ) | ^( STAR expr expr ) | ^( DIV expr expr ) | ^( PERCENT expr expr ) | ^( CAST_EXPR type expr ) | const_expr | var_expr | ^( NEG expr ) | ^( CAST_EXPR type_cast expr ) | ^( EXPR_IN_PAREN expr ) ); $ANTLR start "field_def" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:191:1: field_def : ( ^( FIELD_DEF IDENTIFIER ( type )? ) | ^( FIELD_DEF type ) ); $ANTLR start "field_def_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:196:1: field_def_list : ( field_def ( field_def )+ | field_def ); $ANTLR start "filename" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:183:1: filename : QUOTEDSTRING ; $ANTLR start "filter_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:318:1: filter_clause : ^( FILTER rel cond ) ; $ANTLR start "flatten_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:302:1: flatten_clause : ^( FLATTEN expr ) ; $ANTLR start "flatten_generated_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:298:1: flatten_generated_item : ( flatten_clause | col_range | expr | STAR ) ( field_def_list )? ; $ANTLR start "foreach_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:514:1: foreach_clause : ^( FOREACH rel foreach_plan ) ; $ANTLR start "foreach_plan" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:518:1: foreach_plan : ( ^( FOREACH_PLAN_SIMPLE generate_clause ) | ^( FOREACH_PLAN_COMPLEX nested_blk ) ); $ANTLR start "func_args" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:237:1: func_args : func_first_arg_clause ( func_next_arg_clause )* ; $ANTLR start "func_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:228:1: func_clause : ( ^( FUNC_REF func_name ) | ^( FUNC func_name ( func_args )? ) ); $ANTLR start "func_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:339:1: func_eval : ( ^( FUNC_EVAL func_name real_arg ( real_arg )* ) | ^( FUNC_EVAL func_name ) ); $ANTLR start "func_first_arg_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:240:1: func_first_arg_clause : ( QUOTEDSTRING | MULTILINE_QUOTEDSTRING ); $ANTLR start "func_name" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:233:1: func_name : eid ( ( PERIOD | DOLLAR ) eid )* ; $ANTLR start "func_next_arg_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:244:1: func_next_arg_clause : ( QUOTEDSTRING | MULTILINE_QUOTEDSTRING ); $ANTLR start "general_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:97:1: general_statement : ^( STATEMENT ( alias )? op_clause ( parallel_clause )? ) ; $ANTLR start "generate_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:527:1: generate_clause : ^( GENERATE flatten_generated_item ( flatten_generated_item )* ) ; delegates $ANTLR start "group_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:276:1: group_clause : ^( ( GROUP | COGROUP ) group_item ( group_item )* ( group_type )? ( partition_clause )? ) ; $ANTLR start "group_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:287:1: group_item : rel ( join_group_by_clause | ALL | ANY ) ( INNER | OUTER )? ; $ANTLR start "group_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:284:1: group_type : QUOTEDSTRING ; $ANTLR start "import_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:70:1: import_statement : ^( IMPORT QUOTEDSTRING ) ; $ANTLR start "in_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:333:1: in_eval : ^( IN ( ^( IN_LHS expr ) ^( IN_RHS expr ) ) ( ^( IN_LHS expr ) ^( IN_RHS expr ) )* ) ; $ANTLR start "input_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:157:1: input_clause : ^( INPUT stream_cmd ( stream_cmd )* ) ; $ANTLR start "join_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:481:1: join_clause : ^( JOIN join_sub_clause ( join_type )? ( partition_clause )? ) ; $ANTLR start "join_group_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:501:1: join_group_by_clause : ^( BY join_group_by_expr ( join_group_by_expr )* ) ; $ANTLR start "join_group_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:506:1: join_group_by_expr : ( col_range | expr | STAR ); $ANTLR start "join_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:497:1: join_item : ^( JOIN_ITEM rel join_group_by_clause ) ; $ANTLR start "join_sub_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:489:1: join_sub_clause : ( join_item ( LEFT | RIGHT | FULL ) ( OUTER )? join_item | join_item ( join_item )* ); $ANTLR start "join_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:486:1: join_type : QUOTEDSTRING ; $ANTLR start "keyvalue" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:648:1: keyvalue : ^( KEY_VAL_PAIR map_key const_expr ) ; $ANTLR start "limit_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:424:1: limit_clause : ^( LIMIT rel ( INTEGER | LONGINTEGER | expr ) ) ; $ANTLR start "literal" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:623:1: literal : ( scalar | map | bag | tuple ); $ANTLR start "load_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:178:1: load_clause : ^( LOAD filename ( func_clause )? ( as_clause )? ) ; $ANTLR start "map" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:643:1: map : ( ^( MAP_VAL keyvalue ( keyvalue )* ) | ^( MAP_VAL ) ); $ANTLR start "map_key" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:652:1: map_key : QUOTEDSTRING ; $ANTLR start "map_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:225:1: map_type : ^( MAP_TYPE ( IDENTIFIER )? ( type )? ) ; $ANTLR start "mr_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:588:1: mr_clause : ^( MAPREDUCE QUOTEDSTRING ( path_list )? store_clause load_clause ( EXECCOMMAND )? ) ; $ANTLR start "nested_blk" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:523:1: nested_blk : ( nested_command )* generate_clause ; $ANTLR start "nested_command" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:532:1: nested_command : ( ^( NESTED_CMD IDENTIFIER nested_op ) | ^( NESTED_CMD_ASSI IDENTIFIER expr ) ); $ANTLR start "nested_cross" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:567:1: nested_cross : ^( CROSS nested_op_input_list ) ; $ANTLR start "nested_distinct" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:559:1: nested_distinct : ^( DISTINCT nested_op_input ) ; $ANTLR start "nested_filter" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:550:1: nested_filter : ^( FILTER nested_op_input cond ) ; $ANTLR start "nested_foreach" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:571:1: nested_foreach : ^( FOREACH nested_op_input generate_clause ) ; $ANTLR start "nested_limit" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:563:1: nested_limit : ^( LIMIT nested_op_input ( INTEGER | expr ) ) ; $ANTLR start "nested_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:537:1: nested_op : ( nested_proj | nested_filter | nested_sort | nested_distinct | nested_limit | nested_cross | nested_foreach ); $ANTLR start "nested_op_input" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:575:1: nested_op_input : ( col_ref | nested_proj ); $ANTLR start "nested_op_input_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:578:1: nested_op_input_list : nested_op_input ( nested_op_input )* ; $ANTLR start "nested_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:546:1: nested_proj : ^( NESTED_PROJ col_ref col_ref ( col_ref )* ) ; $ANTLR start "nested_sort" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:554:1: nested_sort : ^( ORDER nested_op_input order_by_clause ( func_clause )? ) ; $ANTLR start "num_scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:633:1: num_scalar : ( MINUS )? ( INTEGER | LONGINTEGER | FLOATNUMBER | DOUBLENUMBER | BIGINTEGERNUMBER | BIGDECIMALNUMBER ) ; $ANTLR start "op_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:113:1: op_clause : ( define_clause | load_clause | group_clause | store_clause | filter_clause | distinct_clause | limit_clause | sample_clause | order_clause | rank_clause | cross_clause | join_clause | union_clause | stream_clause | mr_clause | split_clause | foreach_clause | cube_clause | assert_clause ); $ANTLR start "order_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:456:1: order_by_clause : ( STAR ( ASC | DESC )? | order_col ( order_col )* ); $ANTLR start "order_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:450:1: order_clause : ^( ORDER rel order_by_clause ( func_clause )? ) ; $ANTLR start "order_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:461:1: order_col : ( col_range | col_ref ) ( ASC | DESC )? ; $ANTLR start "output_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:168:1: output_clause : ^( OUTPUT stream_cmd ( stream_cmd )* ) ; $ANTLR start "parallel_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:105:1: parallel_clause : ^( PARALLEL INTEGER ) ; $ANTLR start "partition_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:469:1: partition_clause : ^( PARTITION func_name ) ; $ANTLR start "path_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:148:1: path_list : a= QUOTEDSTRING (b= QUOTEDSTRING )* ; $ANTLR start "pound_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:403:1: pound_proj : ^( POUND ( QUOTEDSTRING | NULL ) ) ; $ANTLR start "projectable_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:379:1: projectable_expr : ( func_eval | col_ref | bin_expr | case_expr | case_cond ); $ANTLR start "query" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:53:1: query : ^( QUERY ( statement )* ) ; $ANTLR start "rank_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:441:1: rank_by_clause : ( STAR ( ASC | DESC )? | rank_col ( rank_col )* ); $ANTLR start "rank_by_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:437:1: rank_by_statement : ^( BY rank_by_clause ( DENSE )? ) ; $ANTLR start "rank_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:433:1: rank_clause : ^( RANK rel ( rank_by_statement )? ) ; $ANTLR start "rank_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:446:1: rank_col : ( col_range | col_ref ) ( ASC | DESC )? ; $ANTLR start "real_arg" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:344:1: real_arg : ( expr | STAR | col_range ); $ANTLR start "realias_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:102:1: realias_clause : ^( REALIAS alias IDENTIFIER ) ; $ANTLR start "realias_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:67:1: realias_statement : realias_clause ; $ANTLR start "register_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:75:1: register_statement : ^( REGISTER QUOTEDSTRING ( scripting_udf_clause )? ) ; $ANTLR start "rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:293:1: rel : ( alias | ( op_clause ( parallel_clause )? ) ); $ANTLR start "rel_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:477:1: rel_list : rel ( rel )* ; $ANTLR start "rel_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:746:1: rel_op returns [String result] : ( rel_op_eq | rel_op_ne | rel_op_gt | rel_op_gte | rel_op_lt | rel_op_lte | STR_OP_MATCHES ); $ANTLR start "rel_op_eq" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:756:1: rel_op_eq returns [String result] : ( STR_OP_EQ | NUM_OP_EQ ); $ANTLR start "rel_op_gt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:766:1: rel_op_gt returns [String result] : ( STR_OP_GT | NUM_OP_GT ); $ANTLR start "rel_op_gte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:771:1: rel_op_gte returns [String result] : ( STR_OP_GTE | NUM_OP_GTE ); $ANTLR start "rel_op_lt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:776:1: rel_op_lt returns [String result] : ( STR_OP_LT | NUM_OP_LT ); $ANTLR start "rel_op_lte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:781:1: rel_op_lte returns [String result] : ( STR_OP_LTE | NUM_OP_LTE ); $ANTLR start "rel_op_ne" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:761:1: rel_op_ne returns [String result] : ( STR_OP_NE | NUM_OP_NE ); $ANTLR start "rel_str_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:786:1: rel_str_op : ( STR_OP_EQ | STR_OP_NE | STR_OP_GT | STR_OP_LT | STR_OP_GTE | STR_OP_LTE | STR_OP_MATCHES ); $ANTLR start "sample_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:429:1: sample_clause : ^( SAMPLE rel ( DOUBLENUMBER | expr ) ) ; $ANTLR start "scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:626:1: scalar : ( num_scalar | QUOTEDSTRING | NULL | TRUE | FALSE ); $ANTLR start "scripting_language_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:86:1: scripting_language_clause : ( USING IDENTIFIER ) ; $ANTLR start "scripting_namespace_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:91:1: scripting_namespace_clause : ( AS IDENTIFIER ) ; $ANTLR start "scripting_udf_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:83:1: scripting_udf_clause : scripting_language_clause scripting_namespace_clause ; $ANTLR start "ship_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:144:1: ship_clause : ^( SHIP ( path_list )? ) ; $ANTLR start "simple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:204:1: simple_type : ( BOOLEAN | INT | LONG | FLOAT | DOUBLE | BIGINTEGER | BIGDECIMAL | DATETIME | CHARARRAY | BYTEARRAY ); $ANTLR start "split_branch" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:599:1: split_branch : ^( SPLIT_BRANCH alias cond ) ; $ANTLR start "split_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:594:1: split_clause : ^( SPLIT rel split_branch ( split_branch )* ( split_otherwise )? ) ; $ANTLR start "split_otherwise" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:603:1: split_otherwise : ^( OTHERWISE alias ( ALL )? ) ; $ANTLR start "split_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:64:1: split_statement : split_clause ; $ANTLR start "statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:56:1: statement : ( general_statement | split_statement | import_statement | register_statement | assert_statement | realias_statement ); $ANTLR start "store_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:306:1: store_clause : ^( STORE rel filename ( func_clause )? ) ; $ANTLR start "stream_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:582:1: stream_clause : ^( STREAM rel ( EXECCOMMAND | IDENTIFIER ) ( as_clause )? ) ; $ANTLR start "stream_cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:162:1: stream_cmd : ( ^( STDIN ( func_clause )? ) | ^( STDOUT ( func_clause )? ) | ^( QUOTEDSTRING ( func_clause )? ) ); $ANTLR end synpred50_AstPrinter $ANTLR start synpred110_AstPrinter $ANTLR end synpred110_AstPrinter $ANTLR start synpred114_AstPrinter $ANTLR end synpred114_AstPrinter $ANTLR start synpred115_AstPrinter $ANTLR end synpred190_AstPrinter Delegated rules $ANTLR end synpred115_AstPrinter $ANTLR start synpred118_AstPrinter $ANTLR end synpred118_AstPrinter $ANTLR start synpred123_AstPrinter $ANTLR end synpred123_AstPrinter $ANTLR start synpred144_AstPrinter $ANTLR end synpred144_AstPrinter $ANTLR start synpred145_AstPrinter $ANTLR end synpred145_AstPrinter $ANTLR start synpred146_AstPrinter $ANTLR end synpred146_AstPrinter $ANTLR start synpred172_AstPrinter $ANTLR end synpred172_AstPrinter $ANTLR start synpred190_AstPrinter $ANTLR end "rel_str_op" $ANTLR start synpred50_AstPrinter $ANTLR start "tuple" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:660:1: tuple : ( ^( TUPLE_VAL literal ( literal )* ) | ^( TUPLE_VAL ) ); $ANTLR start "tuple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:217:1: tuple_type : ^( TUPLE_TYPE ( field_def_list )? ) ; $ANTLR start "tuple_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:366:1: tuple_type_cast : ( ^( TUPLE_TYPE_CAST type_cast ( type_cast )* ) | ^( TUPLE_TYPE_CAST ( type_cast )? ) ); $ANTLR start "type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:201:1: type : ( simple_type | tuple_type | bag_type | map_type ); $ANTLR start "type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:362:1: type_cast : ( simple_type | map_type | tuple_type_cast | bag_type_cast ); $ANTLR start "union_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:510:1: union_clause : ^( UNION ( ONSCHEMA )? rel_list ) ; $ANTLR start "var_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstPrinter.g:375:1: var_expr : projectable_expr ( dot_proj | pound_proj )* ;
$ANTLR start "alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:147:1: alias returns [String name, CommonTree node] : IDENTIFIER ; $ANTLR start "alias_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:623:1: alias_col_ref : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "as_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:233:1: as_clause : ^( AS field_def_list ) ; $ANTLR start "assert_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:363:1: assert_clause : ^( ASSERT rel cond ( comment )? ) ; $ANTLR start "assert_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:132:1: assert_statement : assert_clause ; $ANTLR start "bag" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:650:1: bag : ^( BAG_VAL ( tuple )* ) ; $ANTLR start "bag_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:276:1: bag_type : ^( BAG_TYPE ( IDENTIFIER )? ( tuple_type )? ) ; $ANTLR start "bag_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:410:1: bag_type_cast : ^( BAG_TYPE_CAST ( tuple_type_cast )? ) ; $ANTLR start "bin_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:438:1: bin_expr : ^( BIN_EXPR cond expr expr ) ; $ANTLR start "cache_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:210:1: cache_clause : ^( CACHE path_list ) ; $ANTLR start "case_cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:444:1: case_cond : ^( CASE_COND ^( WHEN ( cond )+ ) ^( THEN ( expr )+ ) ) ; $ANTLR start "case_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:441:1: case_expr : ^( CASE_EXPR ( ^( CASE_EXPR_LHS expr ) ( ^( CASE_EXPR_RHS expr ) )+ )+ ) ; $ANTLR start "cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:187:1: cmd : ^( EXECCOMMAND ( ship_clause | cache_clause | input_clause | output_clause | error_clause )* ) ; $ANTLR start "col_alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:425:1: col_alias : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "col_alias_or_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:422:1: col_alias_or_index : ( col_alias | col_index ); $ANTLR start "col_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:428:1: col_index : DOLLARVAR ; $ANTLR start "col_range" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:431:1: col_range : ^( COL_RANGE ( col_ref )? DOUBLE_PERIOD ( col_ref )? ) ; $ANTLR start "col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:620:1: col_ref : ( alias_col_ref | dollar_col_ref ); $ANTLR start "comment" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:366:1: comment : QUOTEDSTRING ; $ANTLR start "cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:372:1: cond : ( ^( OR cond cond ) | ^( AND cond cond ) | ^( NOT cond ) | ^( NULL expr ( NOT )? ) | ^( rel_op expr expr ) | in_eval | func_eval | ^( BOOL_COND expr ) ); $ANTLR start "const_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:629:1: const_expr : literal ; $ANTLR start "cross_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:484:1: cross_clause : ^( CROSS rel_list ( partition_clause )? ) ; $ANTLR start "cube_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:303:1: cube_by_clause : ^( BY cube_or_rollup ) ; $ANTLR start "cube_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:319:1: cube_by_expr : ( col_range | expr | STAR ); $ANTLR start "cube_by_expr_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:315:1: cube_by_expr_list : ( cube_by_expr )+ ; $ANTLR start "cube_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:295:1: cube_clause : ^( CUBE cube_item ) ; $ANTLR start "cube_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:299:1: cube_item : rel ( cube_by_clause ) ; $ANTLR start "cube_or_rollup" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:307:1: cube_or_rollup : ( cube_rollup_list )+ ; $ANTLR start "cube_rollup_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:311:1: cube_rollup_list : ^( ( CUBE | ROLLUP ) cube_by_expr_list ) ; $ANTLR start "define_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:184:1: define_clause : ^( DEFINE alias ( cmd | func_clause ) ) ; $ANTLR start "distinct_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:478:1: distinct_clause : ^( DISTINCT rel ( partition_clause )? ) ; $ANTLR start "dollar_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:626:1: dollar_col_ref : DOLLARVAR ; $ANTLR start "dot_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:419:1: dot_proj : ^( PERIOD ( col_alias_or_index )+ ) ; $ANTLR start "eid" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:657:1: eid : ( rel_str_op | IMPORT | RETURNS | DEFINE | LOAD | FILTER | FOREACH | CUBE | ROLLUP | MATCHES | ORDER | RANK | DISTINCT | COGROUP | JOIN | CROSS | UNION | SPLIT | INTO | IF | ALL | AS | BY | USING | INNER | OUTER | PARALLEL | PARTITION | GROUP | AND | OR | NOT | GENERATE | FLATTEN | EVAL | ASC | DESC | BOOLEAN | INT | LONG | FLOAT | DOUBLE | BIGINTEGER | BIGDECIMAL | DATETIME | CHARARRAY | BYTEARRAY | BAG | TUPLE | MAP | IS | NULL | TRUE | FALSE | STREAM | THROUGH | STORE | MAPREDUCE | SHIP | CACHE | INPUT | OUTPUT | STDERROR | STDIN | STDOUT | LIMIT | SAMPLE | LEFT | RIGHT | FULL | IDENTIFIER | TOBAG | TOMAP | TOTUPLE | ASSERT ); $ANTLR start "error_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:224:1: error_clause : ^( STDERROR ( QUOTEDSTRING ( INTEGER )? )? ) ; $ANTLR start "expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:391:1: expr : ( ^( PLUS expr expr ) | ^( MINUS expr expr ) | ^( STAR expr expr ) | ^( DIV expr expr ) | ^( PERCENT expr expr ) | ^( CAST_EXPR type expr ) | const_expr | var_expr | ^( NEG expr ) | ^( CAST_EXPR type_cast expr ) | ^( EXPR_IN_PAREN expr ) ); $ANTLR start "field_def" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:236:1: field_def[Set<String> fieldNames, NumValCarrier nvc] : ( ^( FIELD_DEF IDENTIFIER ( type )? ) | ^( FIELD_DEF_WITHOUT_IDENTIFIER type ) ); $ANTLR start "field_def_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:241:1: field_def_list : ( field_def[$field_def_list::fieldNames, $field_def_list::nvc] )+ ; $ANTLR start "filename" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:230:1: filename : QUOTEDSTRING ; $ANTLR start "filter_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:369:1: filter_clause : ^( FILTER rel cond ) ; $ANTLR start "flatten_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:357:1: flatten_clause : ^( FLATTEN expr ) ; $ANTLR start "flatten_generated_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:354:1: flatten_generated_item : ( flatten_clause | col_range | expr | STAR ) ( field_def_list )? ; $ANTLR start "foreach_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:534:1: foreach_clause : ^( FOREACH rel foreach_plan ) ; $ANTLR start "foreach_plan" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:537:1: foreach_plan : ( ^( FOREACH_PLAN_SIMPLE generate_clause ) | ^( FOREACH_PLAN_COMPLEX nested_blk ) ); $ANTLR start "func_args" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:292:1: func_args : ( func_args_string )+ ; $ANTLR start "func_args_string" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:289:1: func_args_string : ( QUOTEDSTRING | MULTILINE_QUOTEDSTRING ); $ANTLR start "func_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:282:1: func_clause : ( ^( FUNC_REF func_name ) | ^( FUNC func_name ( func_args )? ) ); $ANTLR start "func_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:385:1: func_eval : ( ^( FUNC_EVAL func_name ( real_arg )* ) | ^( INVOKER_FUNC_EVAL func_name IDENTIFIER ( real_arg )* ) ); $ANTLR start "func_name" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:286:1: func_name : eid ( ( PERIOD | DOLLAR ) eid )* ; $ANTLR start "general_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:135:1: general_statement : ^( STATEMENT ( alias )? op_clause ( parallel_clause )? ) ; $ANTLR start "generate_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:547:1: generate_clause : ^( GENERATE ( flatten_generated_item )+ ) ; delegates $ANTLR start "group_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:323:1: group_clause : ^( ( GROUP | COGROUP ) ( group_item )+ ( group_type )? ( partition_clause )? ) ; $ANTLR start "group_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:336:1: group_item : rel ( join_group_by_clause | ALL | ANY ) ( INNER | OUTER )? ; $ANTLR start "group_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:333:1: group_type : QUOTEDSTRING ; $ANTLR start "in_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:382:1: in_eval : ^( IN ( ^( IN_LHS expr ) ^( IN_RHS expr ) )+ ) ; $ANTLR start "input_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:213:1: input_clause : ^( INPUT ( stream_cmd )+ ) ; $ANTLR start "join_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:490:1: join_clause : ^( JOIN join_sub_clause ( join_type )? ( partition_clause )? ) ; $ANTLR start "join_group_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:521:1: join_group_by_clause returns [int exprCount] : ^( BY ( join_group_by_expr )+ ) ; $ANTLR start "join_group_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:528:1: join_group_by_expr : ( col_range | expr | STAR ); $ANTLR start "join_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:508:1: join_item : ^( JOIN_ITEM rel join_group_by_clause ) ; $ANTLR start "join_sub_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:503:1: join_sub_clause : ( join_item ( LEFT | RIGHT | FULL ) ( OUTER )? join_item | ( join_item )+ ); $ANTLR start "join_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:500:1: join_type : QUOTEDSTRING ; $ANTLR start "keyvalue" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:644:1: keyvalue : ^( KEY_VAL_PAIR map_key const_expr ) ; $ANTLR start "limit_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:447:1: limit_clause : ^( LIMIT rel ( INTEGER | LONGINTEGER | expr ) ) ; $ANTLR start "literal" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:632:1: literal : ( scalar | map | bag | tuple ); $ANTLR start "load_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:227:1: load_clause : ^( LOAD filename ( func_clause )? ( as_clause )? ) ; $ANTLR start "map" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:641:1: map : ^( MAP_VAL ( keyvalue )* ) ; $ANTLR start "map_key" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:647:1: map_key : QUOTEDSTRING ; $ANTLR start "map_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:279:1: map_type : ^( MAP_TYPE ( IDENTIFIER )? ( type )? ) ; $ANTLR start "mr_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:601:1: mr_clause : ^( MAPREDUCE QUOTEDSTRING ( path_list )? store_clause load_clause ( EXECCOMMAND )? ) ; $ANTLR start "nested_blk" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:541:1: nested_blk : ( nested_command )* generate_clause ; $ANTLR start "nested_command" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:550:1: nested_command : ( ^( NESTED_CMD IDENTIFIER nested_op ) | ^( NESTED_CMD_ASSI IDENTIFIER expr ) ); $ANTLR start "nested_cross" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:586:1: nested_cross : ^( CROSS nested_op_input_list ) ; $ANTLR start "nested_distinct" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:580:1: nested_distinct : ^( DISTINCT nested_op_input ) ; $ANTLR start "nested_filter" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:573:1: nested_filter : ^( FILTER nested_op_input cond ) ; $ANTLR start "nested_foreach" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:589:1: nested_foreach : ^( FOREACH nested_op_input generate_clause ) ; $ANTLR start "nested_limit" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:583:1: nested_limit : ^( LIMIT nested_op_input ( INTEGER | expr ) ) ; $ANTLR start "nested_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:561:1: nested_op : ( nested_proj | nested_filter | nested_sort | nested_distinct | nested_limit | nested_cross | nested_foreach ); $ANTLR start "nested_op_input" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:592:1: nested_op_input : ( col_ref | nested_proj ); $ANTLR start "nested_op_input_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:595:1: nested_op_input_list : ( nested_op_input )+ ; $ANTLR start "nested_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:570:1: nested_proj : ^( NESTED_PROJ col_ref ( col_ref )+ ) ; $ANTLR start "nested_sort" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:577:1: nested_sort : ^( ORDER nested_op_input order_by_clause ( func_clause )? ) ; $ANTLR start "num_scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:638:1: num_scalar : ( MINUS )? ( INTEGER | LONGINTEGER | FLOATNUMBER | DOUBLENUMBER | BIGINTEGERNUMBER | BIGDECIMALNUMBER ) ; $ANTLR start "op_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:163:1: op_clause : ( define_clause | load_clause | group_clause | store_clause | filter_clause | distinct_clause | limit_clause | sample_clause | order_clause | rank_clause | cross_clause | join_clause | union_clause | stream_clause | mr_clause | split_clause | foreach_clause | cube_clause | assert_clause ); $ANTLR start "order_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:470:1: order_by_clause : ( STAR ( ASC | DESC )? | ( order_col )+ ); $ANTLR start "order_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:467:1: order_clause : ^( ORDER rel order_by_clause ( func_clause )? ) ; $ANTLR start "order_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:474:1: order_col : ( col_range ( ASC | DESC )? | col_ref ( ASC | DESC )? ); $ANTLR start "output_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:221:1: output_clause : ^( OUTPUT ( stream_cmd )+ ) ; $ANTLR start "parallel_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:144:1: parallel_clause : ^( PARALLEL INTEGER ) ; $ANTLR start "partition_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:481:1: partition_clause : ^( PARTITION func_name ) ; $ANTLR start "path_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:207:1: path_list : ( QUOTEDSTRING )+ ; $ANTLR start "pound_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:435:1: pound_proj : ^( POUND ( QUOTEDSTRING | NULL ) ) ; $ANTLR start "previous_rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:155:1: previous_rel returns [String name, CommonTree node] : ARROBA ; $ANTLR start "projectable_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:416:1: projectable_expr : ( func_eval | col_ref | bin_expr | case_expr | case_cond ); $ANTLR start "query" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:113:1: query : ^( QUERY ( statement )* ) ; $ANTLR start "rank_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:459:1: rank_by_clause : ( STAR ( ASC | DESC )? | ( rank_col )+ ); $ANTLR start "rank_by_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:456:1: rank_by_statement : ^( BY rank_by_clause ( DENSE )? ) ; $ANTLR start "rank_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:453:1: rank_clause : ^( RANK rel ( rank_by_statement )? ) ; $ANTLR start "rank_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:463:1: rank_col : ( col_range ( ASC | DESC )? | col_ref ( ASC | DESC )? ); $ANTLR start "real_arg" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:388:1: real_arg : ( expr | STAR | col_range ); $ANTLR start "realias_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:138:1: realias_clause : ^( REALIAS alias IDENTIFIER ) ; $ANTLR start "realias_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:126:1: realias_statement : realias_clause ; $ANTLR start "register_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:129:1: register_statement : ^( REGISTER QUOTEDSTRING ( USING IDENTIFIER AS IDENTIFIER )? ) ; $ANTLR start "rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:349:1: rel : ( alias | previous_rel | op_clause ( parallel_clause )? ); $ANTLR start "rel_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:487:1: rel_list : ( rel )+ ; $ANTLR start "rel_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:735:1: rel_op : ( rel_op_eq | rel_op_ne | rel_op_gt | rel_op_gte | rel_op_lt | rel_op_lte | STR_OP_MATCHES ); $ANTLR start "rel_op_eq" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:744:1: rel_op_eq : ( STR_OP_EQ | NUM_OP_EQ ); $ANTLR start "rel_op_gt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:750:1: rel_op_gt : ( STR_OP_GT | NUM_OP_GT ); $ANTLR start "rel_op_gte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:753:1: rel_op_gte : ( STR_OP_GTE | NUM_OP_GTE ); $ANTLR start "rel_op_lt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:756:1: rel_op_lt : ( STR_OP_LT | NUM_OP_LT ); $ANTLR start "rel_op_lte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:759:1: rel_op_lte : ( STR_OP_LTE | NUM_OP_LTE ); $ANTLR start "rel_op_ne" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:747:1: rel_op_ne : ( STR_OP_NE | NUM_OP_NE ); $ANTLR start "rel_str_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:762:1: rel_str_op : ( STR_OP_EQ | STR_OP_NE | STR_OP_GT | STR_OP_LT | STR_OP_GTE | STR_OP_LTE | STR_OP_MATCHES ); $ANTLR start "sample_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:450:1: sample_clause : ^( SAMPLE rel ( DOUBLENUMBER | expr ) ) ; $ANTLR start "scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:635:1: scalar : ( num_scalar | QUOTEDSTRING | NULL | TRUE | FALSE ); $ANTLR start "ship_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:204:1: ship_clause : ^( SHIP ( path_list )? ) ; $ANTLR start "simple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:260:1: simple_type returns [byte typev] : ( BOOLEAN | INT | LONG | FLOAT | DOUBLE | BIGINTEGER | BIGDECIMAL | DATETIME | CHARARRAY | BYTEARRAY ); $ANTLR start "split_branch" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:607:1: split_branch : ^( SPLIT_BRANCH alias cond ) ; $ANTLR start "split_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:604:1: split_clause : ^( SPLIT rel ( split_branch )+ ( split_otherwise )? ) ; $ANTLR start "split_otherwise" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:614:1: split_otherwise : ^( OTHERWISE alias ( ALL )? ) ; $ANTLR start "split_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:123:1: split_statement : split_clause ; $ANTLR start "statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:116:1: statement : ( general_statement | split_statement | realias_statement | register_statement | assert_statement ); $ANTLR start "store_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:360:1: store_clause : ^( STORE rel filename ( func_clause )? ) ; $ANTLR start "stream_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:598:1: stream_clause : ^( STREAM rel ( EXECCOMMAND | IDENTIFIER ) ( as_clause )? ) ; $ANTLR start "stream_cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:216:1: stream_cmd : ( ^( STDIN ( func_clause )? ) | ^( STDOUT ( func_clause )? ) | ^( QUOTEDSTRING ( func_clause )? ) ); $ANTLR end "rel_str_op" $ANTLR start synpred109_AstValidator $ANTLR end synpred109_AstValidator $ANTLR start synpred113_AstValidator $ANTLR end synpred113_AstValidator $ANTLR start synpred114_AstValidator $ANTLR end synpred114_AstValidator $ANTLR start synpred117_AstValidator $ANTLR end synpred190_AstValidator Delegated rules $ANTLR end synpred117_AstValidator $ANTLR start synpred140_AstValidator $ANTLR end synpred140_AstValidator $ANTLR start synpred141_AstValidator $ANTLR end synpred141_AstValidator $ANTLR start synpred142_AstValidator $ANTLR end synpred142_AstValidator $ANTLR start synpred172_AstValidator $ANTLR end synpred172_AstValidator $ANTLR start synpred190_AstValidator $ANTLR start "tuple" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:653:1: tuple : ^( TUPLE_VAL ( literal )* ) ; $ANTLR start "tuple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:273:1: tuple_type : ^( TUPLE_TYPE ( field_def_list )? ) ; $ANTLR start "tuple_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:407:1: tuple_type_cast : ^( TUPLE_TYPE_CAST ( type_cast )* ) ; $ANTLR start "type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:253:1: type returns [byte typev] : ( simple_type | tuple_type | bag_type | map_type ); $ANTLR start "type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:404:1: type_cast : ( simple_type | map_type | tuple_type_cast | bag_type_cast ); $ANTLR start "union_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:531:1: union_clause : ^( UNION ( ONSCHEMA )? rel_list ) ; $ANTLR start "var_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/AstValidator.g:413:1: var_expr : projectable_expr ( dot_proj | pound_proj )* ;
generate a constraint tuple that conforms to the constraint and passes the predicate (or null if unable to find such a tuple)  for now, constraint tuples are tuples whose fields are a blend of actual data values and nulls, where a null stands for "don't care"  in the future, may want to replace "don't care" with a more rich constraint language; this would help, e.g. in the case of two filters in a row (you want the downstream filter to tell the upstream filter what predicate it wants satisfied in a given field)  generate a constraint tuple that conforms to the schema and passes the predicate (or null if unable to find such a tuple)
Returns current value.


Returns current value.
copied from org.apache.avro.mapred.AvroOutputFormat
Add a field schema to a bag schema Convert an Avro schema to a Pig schema Wrap a pig type to a field schema Convert a schema with field name to a pig schema
Look up schema using type name or field name Initialize given a schema
@see org.apache.pig.StoreFuncInterface#checkSchema(org.apache.pig.ResourceSchema ) @see org.apache.pig.StoreFuncInterface#cleanupOnFailure(java.lang.String, org.apache.hadoop.mapreduce.Job) Reads the avro schema at the specified location. Reads the avro schemas at the specified location. Helper function reads the input avro schema from the UDF Properties.  @see org.apache.pig.LoadFunc#getNext() Utility function that gets the output schema from the udf properties for this instance of the store function. @see org.apache.pig.StoreFuncInterface#getOutputFormat() @see org.apache.pig.LoadMetadata#getPartitionKeys(java.lang.String, org.apache.hadoop.mapreduce.Job) Internal function for getting the Properties object associated with this UDF instance. Internal function for getting the Properties object associated with this UDF instance. @see org.apache.pig.LoadMetadata#getSchema(java.lang.String, org.apache.hadoop.mapreduce.Job) @see org.apache.pig.LoadMetadata#getStatistics(java.lang.String, org.apache.hadoop.mapreduce.Job) @see org.apache.pig.LoadFunc#prepareToRead(org.apache.hadoop.mapreduce.RecordReader , org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigSplit) @see org.apache.pig.StoreFuncInterface#prepareToWrite(org.apache.hadoop.mapreduce .RecordWriter) @see org.apache.pig.LoadPushDown#pushProjection(org.apache.pig.LoadPushDown. RequiredFieldList) @see org.apache.pig.StoreFuncInterface#putNext(org.apache.pig.data.Tuple) @see org.apache.pig.StoreFuncInterface#relToAbsPathForStoreLocation(java.lang .String, org.apache.hadoop.fs.Path) Sets the input avro schema to {@s }. @see org.apache.pig.LoadFunc#setLocation(java.lang.String, org.apache.hadoop.mapreduce.Job) Sets the output avro schema to {@s }. @see org.apache.pig.LoadMetadata#setPartitionFilter(org.apache.pig.Expression) @see org.apache.pig.StoreFuncInterface#setStoreFuncUDFContextSignature(java. lang.String) @see org.apache.pig.StoreFuncInterface#setStoreLocation(java.lang.String, org.apache.hadoop.mapreduce.Job) Utility function that gets the input avro schema from the udf properties and updates schema for this instance.
Packs a Pig DataBag into an Avro array. Packs a Pig Tuple into an Avro record.


Translates an Avro schema to a Resource Schema (for Pig). Translates an Avro schema to a Resource Schema (for Pig). Internal method. Returns a Union Schema composed of {@in } and null. Returns a Union Schema composed of {@in } and null. Translates a field schema (avro) to a resource field schema (pig). Determines the pig object type of the Avro schema. Checks to see if an avro schema is a combination of null and another object. Takes an Avro Schema and a Pig RequiredFieldList and returns a new schema with only the required fields, or no if the function can't extract only those fields. Useful for push down projections. Takes an Avro Schema and a Pig RequiredFieldList and returns a new schema with only the requried fields, or no if the function can't extract only those fields. Useful for push down projections. Given an input schema that is a union of an avro schema and null (or just a union with one type), return the avro schema. Creates a new Avro schema object from the input ResouceSchema. Translated a ResourceSchema to an Avro Schema. Translates a name in a pig schema to an acceptable Avro name, or throws an error if the name can't be translated.
determine whether the input schema contains generic unions Called by {@link #containsGenericUnion(Schema)} and it recursively checks whether the input schema contains generic unions. determine whether the input schema contains recursive records Called by {@link #containsRecursiveRecord(Schema)} and it recursively checks whether the input schema contains recursive records. create  an avro field using the given schema create an avro field with null schema (it is a space holder) extract schema from a nullable union Returns all non-hidden files recursively inside the base paths given get last file of a hdfs path if it is  a directory; or return the file itself if path is a file Gets the list of paths from the pathString specified which may contain comma-separated paths and glob style path This method is called by {@link #getAvroSchema}. The default implementation returns the schema of an avro file; or the schema of the last file in a first-level directory (it does not contain sub-directories). When merging multiple avro record schemas, we build a map (schemaToMergedSchemaMap) to associate each input record with a remapping of its fields relative to the merged schema. Take the following two schemas for example: // path1 { "type": "record", "name": "x", "fields": [ { "name": "xField", "type": "string" } ] } // path2 { "type": "record", "name": "y", "fields": [ { "name": "yField", "type": "string" } ] } The merged schema will be something like this: // merged { "type": "record", "name": "merged", "fields": [ { "name": "xField", "type": "string" }, { "name": "yField", "type": "string" } ] } The schemaToMergedSchemaMap will look like this: // schemaToMergedSchemaMap { path1 : { 0 : 0 }, path2 : { 0 : 1 } } The meaning of the map is: - The field at index '0' of 'path1' is moved to index '0' in merged schema. - The field at index '0' of 'path2' is moved to index '1' in merged schema. With this map, we can now remap the field position of the original schema to that of the merged schema. This is necessary because in the backend, we don't use the merged avro schema but embedded avro schemas of input files to load them. Therefore, we must relocate each field from old positions in the original schema to new positions in the merged schema. get field schema given index number determine whether a union is a nullable union; note that this function doesn't check containing types of the input union recursively. check whether it is just a wrapped tuple check whether a schema is a space holder (using field name) This method merges two avro schemas into one. Note that not every avro schema can be merged. For complex types to be merged, they must be the same type. For primitive types to be merged, they must meet certain conditions. For schemas that cannot be merged, an exception is thrown. This method merges two primitive avro types into one. This method must be used only to merge two primitive types. For complex types, null will be returned unless they are both the same type. Also note that not every primitive type can be merged. For types that cannot be merged, null is returned. check whether there is NO directory in the input file (status) list wrap a pig schema as tuple Wrap an avro schema as a nullable union if needed. For instance, wrap schema "int" as ["null", "int"]
Required for Java serialization used by Spark: PIG-5134 Required for Java serialization used by Spark: PIG-5134
For historical reasons, Pig supports .bz and .bz2 for bzip2 extension


@Override
@Override





Get a reference to the singleton factory. Get a default (unordered, not distinct) data bag. Get a default (unordered, not distinct) data bag with an existing list of tuples inserted into the bag. Get a distinct data bag.  Distinct bags guarantee that when an iterator is opened on the bag, no two tuples returned from the iterator will be equal. Get a limited sorted data bag.  Limited sorted bags are sorted bags with number of elements no more than limit. Get a sorted data bag.  Sorted bags guarantee that when an iterator is opened on the bag the tuples will be returned in sorted order.  Provided for testing purposes only.  This function should never be called by anybody but the unit tests.



Calculate the size of the output tuple based on the sum of the size of each tuple in the input bag

Add a new operator to the plan.  It will not be connected to any existing operators. Connect two operators in the plan, controlling which position in the edge lists that the from and to edges are placed. Connect two operators in the plan. Create an soft edge between two nodes. Disconnect two operators in the plan. For a given operator, get all operators immediately before it in the plan. Get all operators in the plan that have no successors. For a given operator, get all operators softly immediately before it in the plan. For a given operator, get all operators softly immediately after it. Get all operators in the plan that have no predecessors. For a given operator, get all operators immediately after it. Check if given two operators are directly connected. Move everything below a given operator to the new operator plan.  The specified operator will be moved and will be the root of the new operator plan A method to check if there is a path from a given node to another node Remove an operator from the plan. We assume if node has multiple inputs, it only has one output; if node has multiple outputs, it only has one input. Otherwise, we don't know how to connect inputs to outputs. This assumption is true for logical plan/physical plan, and most MR plan Remove an soft edge Get number of nodes in the plan. Trim everything below a given operator.  The specified operator will NOT be removed.
















Returns the operator which handles this condition Get the left hand side of this expression. Get the right hand side of this expression.
(non-Javadoc) @see org.apache.pig.data.InterSedes#addColsToTuple(java.io.DataInput, org.apache.pig.data.Tuple) (non-Javadoc) @see org.apache.pig.data.InterSedes#readDatum(java.io.DataInput) Expects binInterSedes data types (NOT DataType types!) <p> (non-Javadoc) @see org.apache.pig.data.InterSedes#writeDatum(java.io.DataOutput, java.lang.Object)



(non-Javadoc) @see org.apache.hadoop.mapreduce.InputFormat#createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)
(non-Javadoc) @see org.apache.hadoop.mapreduce.lib.output.FileOutputFormat#getRecordWriter(org.apache.hadoop.mapreduce.TaskAttemptContext)
Get the progress within the split
(non-Javadoc) @see org.apache.hadoop.mapreduce.RecordWriter#close(org.apache.hadoop.mapreduce.TaskAttemptContext) (non-Javadoc) @see org.apache.hadoop.mapreduce.RecordWriter#write(java.lang.Object, java.lang.Object)

Get the left hand side of this binary expression. Get the right hand side of this binary expression.
Get the child expressions of this expression


For testing only, do not use directly.

Describe the schema of an alias in this pipeline. Results will be printed to stdout. ------------------------------------------------------------------------- Explain this pipeline.  Results will be printed to stdout. Run illustrate for this pipeline.  Results will be printed to stdout. Run multiple instances of bound pipeline on Hadoop in parallel. If there are no stores in this pipeline then nothing will be run. Bind is called first with the list of maps of variables to bind. Run multiple instances of bound pipeline on Hadoop in parallel. Run multiple instances of bound pipeline on Hadoop in parallel. Run a pipeline on Hadoop. If there are no stores in this pipeline then nothing will be run. Run a pipeline on Hadoop. If there are no stores in this pipeline then nothing will be run. Run a pipeline on Hadoop. If there are no stores in this pipeline then nothing will be run.

Returns the current position in the tracked InputStream.





getPos is used by the caller to know when the processing of the current {@link InputSplit} is complete. In this method, as we read each bzip block, we keep returning the beginning of the {@link InputSplit} as the return value until we hit a block  which starts at a position >= end of current split. At that point we should set up retpos such that after a record is read, future getPos() calls will get a value > end of current split - this way we will read only one record out of that bzip block - the rest of the records from that bzip block should be read by the next map task while processing the next split
modified by Oliver Merkel, 010128

(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
combine results of different data chunk compute sum(XY), sum(X), sum(Y), sum(XX), sum(YY) from given data sets Function to compute correlation between data sets. used to pass schema name to Final class constructor Function to return argument of constructor as string. It append ( and ) at starting and end or argument respectively. If default constructor is called is returns empty string.


(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()

combine results of different data chunk compute sum(XY), sum(X), sum(Y) from given data sets Function to compute covariance between data sets. used to pass schema name to Final class constructor Function to return argument of constructor as string. It append ( and ) at starting and end or argument respectively. If default constructor is called is returns empty string.

---------------------------------------- STORAGE ----------------------------- ---------------------------------------- LOADING  ----------------------------- (non-Javadoc) @see org.apache.pig.builtin.PigStorage#getNext() Service method for getNext(). Looks at char after char in the input record, that was previously pulled in by getNext(), and fills the fieldBuffer with those chars. <p> If multilineTreatment is Multiline.YES, then the return value indicates whether an embedded newline was found in a field, and that newline was in a field that opened with a double quote that was not closed before the end of the record was reached. If multilineTreatment is Multine.NO, then the return value is always false. <p> A return value of true will cause the calling method to continue pulling records from the input stream, until a closing quote is found. <p> Note that the recordReader that delivers records to out getNext() method above considers record boundaries to be newlines. We therefore never see an actual newline character embedded in a field. We just run out of record. For Multiline.NO we just take such an end of record at face value; the final resulting tuple will contain information only up to the first newline that was found. <p> For Multiline.YES, when we run out of record in an open double quote, our return of true from this method will cause the caller getNext() to do its additional readings of records from the stream, until the closing double quote is found. <p> @param evenQuotesSeen @param sawEmbeddedRecordDelimiter @param buf @param recordLen @param fieldBuffer @return (non-Javadoc) @see org.apache.pig.builtin.PigStorage#putNext(org.apache.pig.data.Tuple) Given a tuple that corresponds to one record, write it out as CSV, converting among Unix/Windows line breaks as requested in the instantiation. Also take care of escaping field delimiters, double quotes, and linebreaks embedded within fields,


Get the <code>FuncSpec</code> that performs the casting functionality Set the <code>FuncSpec</code> that performs the casting functionality





If file size has changed, or if destination does not exist yet, copy it.
Get the next option. Get any remaining arguments. Get the value, as an Integer. Get the value, as a string. Register a command line option.
When this method is called, Pig is communicating to the Loader that it must load data such that all instances of a key are in same split. Pig will make no further checks at runtime to ensure whether the contract is honored or not.



Insert new ColumnInfo for a project-star or project-range-to-end In reduce, the input#1 represent the first input, put 0 instead of 1, so that we can match the sort information collected from POLocalRearrange



get a set of column indexes from a set of uids
Add ForEach after op to prune unnecessary columns remove all the operators starting from an operator

Checks for algebraic operations and if they exist. Replaces global rearrange (cogroup) with reduceBy as follows: Input: foreach (using algebraicOp) -> packager -> globalRearrange -> localRearrange Output: foreach (using algebraicOp.Final) -> reduceBy (uses algebraicOp.Intermediate) -> foreach (using algebraicOp.Initial) -> CombinerRearrange Modifies the map side of foreach (before reduce). Modifies the input plans of the post reduce foreach to match the output of reduce stage. Look for a algebraic POUserFunc that is the leaf of an input plan. Update the ReduceBy Operator with the packaging used by Local rearrange.
add algebraic functions with appropriate projection to new foreach in combiner Algebraic functions and distinct in nested plan of a foreach are partially computed in the map and combine phase. A new foreach statement with initial and intermediate forms of algebraic functions are added to map and combine plans respectively. If bag portion of group-by result is projected or a non algebraic expression/udf has bag as input, combiner will not be used. This is because the use of combiner in such case is likely to degrade performance as there will not be much reduction in data size in combine stage to offset the cost of the additional number of times (de)serialization is done. Major areas for enhancement: 1. use of combiner in cogroup 2. queries with order-by, limit or sort in a nested foreach after group-by 3. case where group-by is followed by filter that has algebraic expression Recursively clone op and its predecessors from pplan and add them to newplan Change the algebriac function type for algebraic functions in map and combine In map and combine the algebraic functions will be leaf of the plan Create a new foreach with same scope,alias as given foreach add an inner plan that projects the group column, which is going to be the first input Translate POForEach in combiner into a POPartialAgg Create new plan and  add to it the clones of operator algeOp  and its predecessors from the physical plan pplan . find algebraic operators and also check if the foreach statement is suitable for combiner use Look for a algebraic POUserFunc as successor to this project, called recursively to skip any other projects seen on the way. create new Local rearrange by cloning existing rearrange and add plan for projecting the key  Replace old POLocalRearrange with new pre-combine LR, add new map foreach, new map-local-rearrange, and connect them



Compare two tuples.  Note that even though both args are given type of WritableComparable to match the WritableComparable interface, they must both be tuples. This callback method must be implemented by all subclasses. Compares its two arguments for order.  The order of elements of the tuples correspond to the fields specified in the order by clause. Same semantics as {@link java.util.Comparator}. Set the reporter.  If the comparison takes a long time the reporter should be called occasionally to avoid Hadoop timing out underneath.  The default Hadoop timeout is 600 seconds.
Stupid java doesn't allow multiple inheritence, so I have to duplicate all the getNext functions here so that comparitors can have them. Determine the type of the operand(s) of this comparator. Set the type of the operand(s) of this comparator.




Returns Properties containing alternative names of given property and same values - can be used to solve deprecations
Validate properties which need to be validated and return *ONLY* those * All pig configurations should be validated in here before use
Restore the original {@link System#in} input stream.

Get the value of this constant.


Get Counter for a given counterName and Store Signature


@Override
@Override
if the dimension values contain null then replace it with "unknown" value since null will be used for rollups


Initialise the database connection and prepared statement here. Write the tuple to Database directly here.

Compares a tuple with two fields. Emits any differences.
Reinitialise. Reinitialise. Reinitialise. Reinitialise. Disable tracing. Enable tracing. Generate ParseException. Get the next Token. Get the specific Token.

Reinitialise parser. Reinitialise parser. Switch to specified lex state. Get the next Token. Set debug output.


Add a tuple to the bag. Add contents of a bag to the bag. Clear out the contents of the bag, both on disk and in memory. Any attempts to read after this is called will produce undefined results. Find out if the bag is distinct. Find out if the bag is sorted. Get an iterator to the bag. For default and distinct bags, no particular order is guaranteed. For sorted bags the order is guaranteed to be sorted according to the provided comparator. This is used by FuncEvalSpec.FakeDataBag. Get the number of elements in the bag, both in memory and on disk.
Append given byte array to the internal byte array. Compare two byte arrays. Comparison is done first using byte values then length. So "g" will be greater than "abcdefg", but "hello worlds" is greater than "hello world". If the other object is not a DataByteArray, {@link DataType#compare} will be called. Get the underlying byte array.  This is the real thing, not a copy, so don't mess with it! Set the internal byte array.  This should not be called unless the default constructor was used. Set the internal byte array.  This should not be called unless the default constructor was used. Find the size of the byte array. Convert the byte array to a string.  UTF8 encoding will be assumed.


Created an entity handle for a container. Creates an entity handle for an object (no containment relation) from a String Clean-up and releasing of resources. Provides configuration information about the storage itself. For instance global data-replication policies if any, default values, ... Some of such values could be overridden at a finer granularity (e.g. on a specific object in the Data Storage) Provides statistics on the Storage: capacity values, how much storage is in use...  TODO: more keys  Place holder for possible initialization activities. Provides a way to change configuration parameters at the Data Storage level. For instance, change the data replication policy.

Test if one type can cast to the other. Compare two objects to each other.  This function is necessary because there's no super class that implements compareTo.  This function provides an (arbitrary) ordering of objects of different types as follows:  NULL &lt; BOOLEAN &lt; BYTE &lt; INTEGER &lt; LONG &lt; FLOAT &lt; DOUBLE &lt; DATETIME &lt; BYTEARRAY &lt; STRING &lt; MAP &lt; TUPLE &lt; BAG.  No other functions should implement this cross object logic.  They should call this function for it instead. Same as {@link #compare(Object, Object)}, but does not use reflection to determine the type of passed in objects, relying instead on the caller to provide the appropriate values, as determined by {@link DataType#findType(Object)}. Use this version in cases where multiple objects of the same type have to be repeatedly compared. Utility method that determines the schema from the passed in dataType. If the dataType is Bag or Tuple, then we need to determine the schemas inside this dataType; for this we iterate through the fields inside this field. This method works both for raw objects and ResourceSchema.ResourceFieldSchema field descriptions; the specific behavior is determined by the klass parameter. * Determine the field schema of an object * Determine the field schema of an ResourceFieldSchema Test whether two byte arrays (Java byte arrays not Pig byte arrays) are equal.  I have no idea why we have this function. Determine the datatype of an object. Given a Type object determine the data type it represents.  This isn't cheap, as it uses reflection, so use sparingly. Get the type code from the type name Get the type name from the type byte code Get the type name. Get an array of all type values. Get a map of type names to type values. Get a map of type values to type names. Determine whether the this data type is atomic. Determine whether the this data type is atomic. Determine whether the this data type is complex. Determine whether the object is complex or atomic. Determine if this type is a numeric type. Determine whether the this data type can have a schema. Determine whether the this object can have a schema. Determine if this is a type that can work can be done on. Given a map, turn it into a String. Merge types if possible.  Merging types means finding a type that one or both types can be upcast to. Return the number of types Pig knows about. Purely for debugging If this object is a bag, return it as a bag. This isn't particularly efficient, so if you already <b>know</b> that the object you have is a bag you should just cast it. Force a data object to a Boolean, if possible. Any numeric type can be forced to a Boolean, as well as CharArray, ByteArray. Complex types cannot be forced to a Boolean. This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Boolean you should just cast it. Force a data object to a DateTime, if possible. Only CharArray, ByteArray can be forced to a DateTime. Numeric types and complex types cannot be forced to a DateTime. This isn't particularly efficient, so if you already <b>know</b> that the object you have is a DateTime you should just cast it. Force a data object to a Double, if possible.  Any numeric type can be forced to a Double, as well as CharArray, ByteArray, or Boolean.  Complex types cannot be forced to an Double.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Double you should just cast it.  Unlike {@link #toDouble(Object, byte)} this method will first determine the type of o and then do the cast. Use {@link #toDouble(Object, byte)} if you already know the type. Force a data object to a Double, if possible.  Any numeric type can be forced to a Double, as well as CharArray, ByteArray.  Complex types cannot be forced to a Double.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Double you should just cast it. Force a data object to a Float, if possible.  Any numeric type can be forced to a Float (though precision may be lost), as well as CharArray, ByteArray, or Boolean.  Complex types cannot be forced to an Float.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Float you should just cast it.  Unlike {@link #toFloat(Object, byte)} this method will first determine the type of o and then do the cast. Use {@link #toFloat(Object, byte)} if you already know the type. Force a data object to a Float, if possible.  Any numeric type can be forced to a Float (though precision may be lost), as well as CharArray, ByteArray.  Complex types cannot be forced to a Float.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Float you should just cast it. Force a data object to an Integer, if possible.  Any numeric type can be forced to an Integer (though precision may be lost), as well as CharArray, ByteArray, or Boolean.  Complex types cannot be forced to an Integer.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is an Integer you should just cast it.  Unlike {@link #toInteger(Object, byte)} this method will first determine the type of o and then do the cast. Use {@link #toInteger(Object, byte)} if you already know the type. Force a data object to an Integer, if possible.  Any numeric type can be forced to an Integer (though precision may be lost), as well as CharArray, ByteArray, or Boolean.  Complex types cannot be forced to an Integer.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is an Integer you should just cast it. Force a data object to a Long, if possible.  Any numeric type can be forced to a Long (though precision may be lost), as well as CharArray, ByteArray, or Boolean.  Complex types cannot be forced to an Long.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Long you should just cast it.  Unlike {@link #toLong(Object, byte)} this method will first determine the type of o and then do the cast. Use {@link #toLong(Object, byte)} if you already know the type. Force a data object to a Long, if possible.  Any numeric type can be forced to a Long (though precision may be lost), as well as CharArray, ByteArray, or Boolean.  Complex types cannot be forced to a Long.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Long you should just cast it. If this object is a map, return it as a map. This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Map you should just cast it. Force a data object to a String, if possible.  Any simple (atomic) type can be forced to a String including ByteArray.  Complex types cannot be forced to a String.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a String you should just cast it.  Unlike {@link #toString(Object, byte)} this method will first determine the type of o and then do the cast. Use {@link #toString(Object, byte)} if you already know the type. Force a data object to a String, if possible.  Any simple (atomic) type can be forced to a String including ByteArray.  Complex types cannot be forced to a String.  This isn't particularly efficient, so if you already <b>know</b> that the object you have is a String you should just cast it. If this object is a tuple, return it as a tuple. This isn't particularly efficient, so if you already <b>know</b> that the object you have is a Tuple you should just cast it.



Returns true iff <code>o</code> is a DateTimeWritable with the same value.

Add a tuple to the bag. Add contents of an iterable (a collection or a DataBag) Clear out the contents of the bag, both on disk and in memory. Any attempts to read after this is called will produce undefined results. This method is potentially very expensive since it may require a sort of the bag; don't call it unless you have to. Return the size of memory usage. Get a file to spill contents to.  The file will be registered in the mSpillFiles array. All bag implementations that can get big enough to be spilled should call this method after every time they add an element. This is used by FuncEvalSpec.FakeDataBag. Read a bag from disk. Report progress to HDFS. Memory size of objects are rounded to multiple of 8 bytes Sample every SPILL_SAMPLE_FREQUENCYth tuple until we reach a max of SPILL_SAMPLE_SIZE to get an estimate of the tuple sizes. Get the number of elements in the bag, both in memory and on disk. Write the bag into a string. Write a bag's contents to disk.
Get a default (unordered, not distinct) data bag. Get a default (unordered, not distinct) data bag from an existing list of tuples. Note that the bag does NOT copy the tuples but uses the provided list as its backing store. So it takes ownership of the list. Get a distinct data bag. Get a limited sorted data bag. Get a sorted data bag.




Append a field to a tuple. This method is not efficient as it may force copying of existing data in order to grow the data structure. Whenever possible you should construct your Tuple with the newTuple(int) method and then fill in the values with set(), rather than construct it with newTuple() and append values. Get the value in a given field. Get all of the fields in the tuple as a list. Determine the size of tuple in memory. This is used by data bags to determine their memory size. This need not be exact, but it should be a decent estimation. Set the value in a given field. Find the size of the tuple. Used to be called arity().

Begin traversing the graph.
Begin traversing the graph.



print out nested gen block in ForEach




This method is used to invoke the appropriate method, as Java does not provide generic dispatch for it. This method is used to invoke the appropriate method, as Java does not provide generic dispatch for it.



* Load Dot graph from string * Convenient method for loading Dot graph from text file




Used to generate the the attributes of a node Used to generate the label for an operator.



each derived class provides the computation here java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API
each derived class provides the computation here java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API


java level API
java level API
java level API
java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API

java level API
Returns true iff <code>o</code> is a DoubleWritable with the same value.







moved from newplan/logical/rules/DuplicateForEachColumnRewrite.check()





Copy entity from an existing one, possibly residing in a different Data Storage. Opens a stream onto which an entity can be written to. Remove entity from the Data Storage. Checks whether the entity exists or not Retrieve configuration information for entity  TODO: more keys  List entity statistics Open for read a given entity Changes the name of an entity in the Data Storage Open an element in the Data Storage with support for random access (seek operations). Defines whether the element is visible to users or contains system's metadata Update configuration information for this entity







Method invoked when an error occurs processing of tuple Method invoked on success processing of tuple
This method is called to determine the ErrorHandler implementation that to handle errors in {@code StoreFunc#putNext(Tuple)}
Whether the UDF should be evaluated at compile time if all inputs are constant. This is applicable for most UDF, however, if a UDF will access hdfs file which is not available at compile time, it has to be false This callback method must be implemented by all subclasses. This is the method that will be invoked on every Tuple of a given dataset. Since the dataset may be divided up in a variety of ways the programmer should not make assumptions about state that is maintained between invocations of this method. Placeholder for cleanup to be performed at the end. User defined functions can override. Default implementation is a no-op. Allow a UDF to specify type specific implementations of itself.  For example, an implementation of arithmetic sum might have int and float implementations, since integer arithmetic performs much better than floating point arithmetic.  Pig's typechecker will call this method and using the returned list plus the schema of the function's input data, decide which implementation of the UDF to use. Allow a UDF to specify a list of hdfs files it would like placed in the distributed cache.  These files will be put in the cache for every job the UDF is used in. The default implementation returns null. This method is intended to be called by the user in {@link EvalFunc} to get the input schema of the EvalFunc This will be called on both the front end and the back end during execution. Get the Type that this EvalFunc returns. Returns the {@link SchemaType} of the EvalFunc. User defined functions can override this method to return {@link SchemaType#VARARG}. In this case the last FieldSchema added to the Schema in {@link #getArgToFuncMapping()} will be considered as a vararg field. Allow a UDF to specify a list of local files it would like placed in the distributed cache. These files will be put in the cache for every job the UDF is used in. Check for {@link FuncUtils} for utility function to facilitate it The default implementation returns null. This function should be overriden to return true for functions that return their values asynchronously.  Currently pig never attempts to execute a function asynchronously. Report the schema of the output of this UDF.  Pig will make use of this in error checking, optimization, and planning.  The schema of input data to this UDF is provided. <p> The default implementation interprets the {@link OutputSchema} annotation, if one is present. Otherwise, it returns <code>null</code> (no known output schema). report that progress is being made (otherwise hadoop times out after 600 seconds working on one outer tuple) Utility method to allow UDF to report progress.  If exec will take more than a a few seconds {@link PigProgressable#progress} should be called occasionally to avoid timeouts.  Default Hadoop timeout is 600 seconds. This method is for internal use. It is called by Pig core in both front-end and back-end to setup the right input schema for EvalFunc Set the PigLogger object.  Called by Pig to provide a reference to the UDF. Set the reporter.  Called by Pig to provide a reference of the reporter to the UDF. This method will be called by Pig both in the front end and back end to pass a unique signature to the {@link EvalFunc}. The signature can be used to store into the {@link UDFContext} any information which the {@link EvalFunc} needs to store between various method invocations in the front end and back end. Issue a warning.  Warning messages are aggregated and reported to the user.
//////////// String Formatting Helpers ////////////// * Report plan1.OperatorSet - plan2.OperatorSet

Writable methods:
Performs a unit of work on item, possibly throwing {@code E} in the process.

hook for asynchronous notification of job completion pushed from the back-end Returns the alias of relation generated by this job Get configuration information Get exceptions that happened during execution Collecting various forms of outputs  if query has executed successfully we want to retrieve the results via iterating over them. Can be information about the state (not submitted, e.g. the execute method has not been called yet; not running, e.g. execute has been issued, but job is waiting; running...; completed; aborted...; progress information true is the physical plan has executed successfully and results are ready to be retrieved Kills current job.
An ExecType is selected based off the Properties for the given script. There may be multiple settings that trigger the selection of a given ExecType. For example, distributed MR mode is currently triggered if the user specifies "mapred" or "mapreduce". It is desirable to override the toString method of the given ExecType to uniquely identify the ExecType. The initialize method should return true if it accepts the properties or false if it does not. The Java ServiceLoader framework will be used to iterate through and select the correct ExecType. Returns the Execution Engine that this ExecType is associated with. Once the ExecType the script is running in is determined by the PigServer, it will then call this method to get an instance of the ExecutionEngine associated with this ExecType to delegate all further execution to on the backend. Returns the Execution Engine class that this ExecType is associated with. This method simply returns the class of the ExecutionEngine associated with this ExecType, and not an instance of it. An ExecType is classified as local if it runs in-process and through the local filesystem. While an ExecutionEngine may have a more nuanced notion of local mode, these are the qualifications Pig requires. ExecutionEngines can extend the ExecType interface to additionally differentiate between ExecTypes as necessary. Returns the canonical name for this ExecType.
This method attempts to return a singleton instance of the given exec type. Only works for MR ExecTypes as these are the only ExecTypes that we have constants in the Pig codebase for.
Close and cleanup the {@link ExecutableManager}. Configure and initialize the {@link ExecutableManager}. Start execution of the external process. This takes care of setting up the environment of the process and also starts ProcessErrorThread to process the <code>stderr</code> of the managed process. Helper function to close input and output streams to the process and kill it Workhorse to process the stderr stream of the managed process. By default <code>ExecuatbleManager</code> just sends out the received error message to the <code>stderr</code> of itself. Start execution of the {@link ExecutableManager}.
Perform any cleanup operation This method handles the backend processing of the Explain command. Once again, no assumptions is made about the architecture of the ExecutionEngine, except that it is capable of "explaining" the LogicalPlan representation of a script. The ExecutionEngine should print all of it's explain statements in the PrintStream provided UNLESS the File object is NOT null. In that case, the file is the directory for which the ExecutionEngine must write out the explain statements into semantically distinct files. For example, if the ExecutionEngine operates on a PhysicalPlan and an ExecutionPlan then there would be two separate files detailing each. The suffix param indicates the suffix of each of these file names. Returns the Properties representation of the ExecutionEngine configuration. The Properties object returned does not have to be the same object between distinct calls to getConfiguration(). The ExecutionEngine may create a new Properties object populated with all the properties each time. Returns the DataStorage the ExecutionEngine is using. Returns the ExecutableManager to be used in Pig Streaming. This method is responsible for the initialization of the ExecutionEngine. All necessary setup tasks and configuration should execute in the init() method. This method will be called via the PigContext object. Creates a PigStats object which will be accessible as a ThreadLocal variable inside the PigStats class. This method is called when first initializing the PigStats. Creates a ScriptState object which will be accessible as a ThreadLocal variable inside the ScriptState class. This method is called when first initializing the ScriptState as to delegate to the ExecutionEngine the version of ScriptState to use to manage the execution at hand. This method is called when user requests to kill all jobs associated with the execution engine This method is called when a user requests to kill a job associated with the given job id. If it is not possible for a user to kill a job, throw a exception. It is imperative for the job id's being displayed to be unique such that the correct jobs are being killed when the user supplies the id. This method is responsible for the actual execution of a LogicalPlan. No assumptions is made about the architecture of the ExecutionEngine, except that it is capable of executing the LogicalPlan representation of a script. The ExecutionEngine should take care of all cleanup after executing the logical plan and returns an implementation of PigStats that contains the relevant information/statistics of the execution of the script. Responsible for updating the properties for the ExecutionEngine. The update may require reinitialization of the engine, perhaps through another call to init() if appropriate. This decision is delegated to the ExecutionEngine -- that is, the caller will not call init() after updating the properties. The Properties passed in should replace any configuration that occurred from previous Properties object. The Properties object should also be updated to reflect the deprecation/modifications that the ExecutionEngine may trigger. Responsible for setting a specific property and value. This method may be called as a result of a user "SET" command in the script or elsewhere in Pig to set certain properties. The properties object of the PigContext should be updated with the property and value with deprecation/other configuration done by the ExecutionEngine reflected. The ExecutionEngine should also update its internal configuration view as well.


Drive all the UDFs in accumulative mode Make a deep copy of this operator.  This is declared here to make it possible to call clone on ExpressionOperators. check whether this expression contains any UDF this is called if reducer is run as accumulative mode in this case, all UDFs must be called Get the sub-expressions of this expression. This is called if reducer is run as accumulative mode, all the child expression must be called if they have any UDF to drive the UDF.accumulate()
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping() This is needed to make sure that both bytearrays and chararrays can be passed as arguments This method gives a name to the column.
The EvalFunc interface Algebraic interface







Creates an empty MR plan Runs the fetch task by executing chain of calls on the PhysicalPlan from the leaf up to the LoadFunc
Checks whether the plan fulfills the prerequisites needed for fetching. Checks whether the fetch is enabled Visits the plan with {@link FetchablePlanVisitor} and checks whether the plan is fetchable.




Returns the uri of output file in string Returns the total size of output files in bytes Returns whether the given POStore is supported by this output size reader or not. We check whether the uri scheme of output file is one of hdfs, local, and s3.



Ensures that the passed path is on the local file system, fetching it to the java.io.tmpdir if necessary. If pig.jars.relative.to.dfs is true and dfs is not null, then a relative path is assumed to be relative to the passed dfs active directory. Else they are assumed to be relative to the local working directory. Ensures that the passed files pointed to by path are on the local file system, fetching them to the java.io.tmpdir if necessary. If pig.jars.relative.to.dfs is true and dfs is not null, then a relative path is assumed to be relative to the passed dfs active directory. Else they are assumed to be relative to the local working directory. Copies the files from remote to local filesystem. When 'multipleFiles' is set the path could point to multiple files through globs or a directory. In this case, return array contains multiple files, otherwise a single file is returned. If pig.jars.relative.to.dfs is true then a relative path is assumed to be relative to the default filesystem's active directory. Else they are assumed to be relative to the local working directory. Ensures that the passed resource is available from the local file system, fetching it to a temporary directory.   recursively get all "File" element descriptors present in the input element descriptor Accessor method to get the resource ContainerDescriptor used for tez resource path bound to this thread. Calling this method lazy-initialized the resourcePath object. This path is different than relativeRoot in that calling PigServer.shutdown will only remove relativeRoot but not resourthPath since resourthPath should be available in the entire session    This function returns an input stream to a local file system file or a file residing on Hadoop's DFS This function is meant to be used if the mappers/reducers want to access any HDFS file Convert path from Windows convention to Unix convention. Invoked under cygwin. Accessor method to get the root ContainerDescriptor used for temporary files bound to this thread. Calling this method lazy-initialized the relativeRoot object. This method is only used by test code to reset state.


(non-Javadoc) @see java.lang.Object#equals(java.lang.Object) (non-Javadoc) @see java.lang.Object#hashCode()



Assume that the given operator is already disconnected from its predecessors.
Placeholder for cleanup to be performed at the end. User defined functions can override. Default implementation is a no-op.
first field in the input tuple is the number of quantiles to generate second field is the *sorted* bag of samples We need to instantiate any user defined comparison function on the backend when the FindQuantiles udf is deserialized
the last field of the tuple is a tuple for memory size and disk size

Methods called on the frontend Methods called on the backend
Methods called on the frontend Methods called on the backend
java level API

java level API
java level API


java level API
java level API
java level API
java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API

java level API



@Override

Get the argument values passed to the func spec.  Parse the class name out of a function specification string.   Parse the argument values out of a function specification string.    (non-Javadoc) @see java.lang.Object#toString()
Utility function to get a list of containing jars via classes


For a given class that implements the parameterized interface <code>ExceptionalFunction</code>, return the type class at the <code>index</code> position. If the Function class, is <code>GoogleFunctionBridge</code>, return the type class for the wrapped function.




creates the POForEach operator for foreach A generate flatten(field) creates the POForEach operator for foreach A generate field creates the POForEach operator for foreach A generate field[0] field[1] creates the POForEach operator for foreach A generate field[0] field[1] creates the PlansAndFlattens struct for generate grpCol, *. creates the PlansAndFlattens struct for 'generate field'. creates the PlansAndFlattens struct for 'generate field[0] field[1] ...'. creates the PlansAndFlattens struct for 'generate field[0] field[1] ...'. with the flatten list as specified creates the PlansAndFlattens struct for 'generate flatten(field)'. creates the PlansAndFlattens struct for generate grpCol, *. creates the POLocalRearrange operator with the given index for group by grpCol creates the POLocalRearrange operator with the given index for group by grpCol













@Override








Converts an object created on the Groovy side to its Pig counterpart. The conversions are as follow: Groovy Pig Object[] Tuple groovy.lang.Tuple Tuple org.apache.pig.data.Tuple Tuple org.apache.pig.data.DataBag DataBag java.util.Map Map java.util.List DataBag Byte/Short/Integer int Long long Float float Double double BigInteger BigInteger BigDecimal BigDecimal String chararray byte[] DataByteArray (copy) Boolean boolean org.joda.time.DateTime org.joda.time.DateTime null null anything else raises an exception Converts an object created on the Pig side to its Groovy counterpart. The conversions are as follow: Pig Groovy Tuple groovy.lang.tuple DataBag groovy.lang.Tuple containing the bag's size and an iterator on its content Map java.util.Map int/long/float/double as is chararray String bytearray byte[] (copy) boolean boolean BigInteger BigInteger BigDecimal BigDecimal org.joda.time.DateTime org.joda.time.DateTime null null anything else raises an exception



parseOnly method added for supporting penny Parses Pig commands in either interactive mode or batch mode. In interactive mode, executes the plan right away whenever a STORE command is encountered. For Testing Only
NOT IMPLEMENTED Not implemented! Not implemented! NOT IMPLEMENTED NOT IMPLEMENTED Not implemented! Not implemented! NOT IMPLEMENTED NOT IMPLEMENTED NOT IMPLEMENTED NOT IMPLEMENTED
If we have a qualifier with a prefix and a wildcard (i.e. cf:foo*), we need a filter on every possible column to be returned as shown below. This will become very inneficient for long lists of columns mixed with a prefixed wildcard. FilterList - must pass ALL of - FamilyFilter - AND a must pass ONE FilterList of - either Qualifier - or ColumnPrefixFilter If we have only column family filters (i.e. cf:*) or explicit column descriptors (i.e., cf:foo) or a mix of both then we don't need filters, since the scan will take care of that. If there is no column with a prefix, we don't need filters, we can just call addColumn and addFamily on the scan Get delegation token from hbase and add it to the Job Public method to initialize a Delete. Public method to initialize a Put. Used to allow assertions of how Puts are initialized by unit tests. Returns the ColumnInfo list so external objects can inspect it. LoadPushDown Methods. Set up the caster to use for reading values out of, and writing to, HBase. StoreFunc Methods @see org.apache.pig.StoreFuncInterface#getOutputFormat() Returns UDFProperties based on <code>contextSignature</code>. Group the list of ColumnInfo objects by their column family and returns a map of CF to its list of ColumnInfo objects. Using String as key since it implements Comparable. Increments the byte array by one for use with setting stopRow. If all bytes in the array are set to the maximum byte value, then the original array will be returned with a 0 byte appended to it. This is because HBase compares bytes from left to right. If byte array B is equal to byte array A, but with an extra byte appended, A will be < B. For example {@code }A = byte[] {-1}{@code } increments to {@code }B = byte[] {-1, 0}{@code } and {@code }A < B{@code }  Suppressing unchecked warnings for RecordWriter, which is not parameterized by StoreFuncInterface  Suppressing unchecked warnings for RecordWriter, which is not parameterized by StoreFuncInterface Updates the ColumnInfo List. Use this if you need to implement custom projections Stores the requiredFieldsList as a serialized object so it can be fetched on the cluster. If you plan to overwrite pushProjection, you need to call this with the requiredFieldList so it they can be accessed on the cluster.





Loads S3 properties from core-site.xml including aws keys that are needed for both local and non-local mode.






Should the stderr data of this task be persisted on HDFS?
Returns whether the give path has a FileSystem implementation.
Create an <code>InputHandler</code> for the given input specification of the <code>StreamingCommand</code>. Create an <code>OutputHandler</code> for the given output specification of the <code>StreamingCommand</code>.






Reads the partition columns Will parse the required columns from the UDFContext properties if the requiredColumns[] variable is null, or else just return the requiredColumns. Uses the ColumnarSerde to deserialize the buff:BytesRefArrayWritable into a ColumnarStruct instance. Only read the columns that were requested in the constructor.<br/> Does the configuration setup and schema parsing and setup.

If the date range was supplied in the loader constructor we need to build our own filter expression.<br/> Initialises an instance of HiveRCRecordReader. The input split size should never be smaller than the RCFile.SYNC_INTERVAL Parse a date string with format yyyy-MM-dd.
Returns the number of columns set in the conf for writers. set number of columns into the given configuration.

Returns a set of columns, with the column names strimmed Extract the date from the hive file names e.g /user/hive/warehouse/table/daydate=2009-10-01/upload001/0002.dat<br/> This method will extract the 2009-10-01 from this name. Converts from a hive type to a pig type Returns the pig DataType for the hive type Trims items in the list. Converts the LazyArray to a Tuple.<br/> Converts the LazyMap to a InternalMap. General schema parsing method, is used to parse the column names. Parses the schema types and returns a List of these.








Method invoked on every tuple during foreach evaluation



















input tuple mark up to be illustrate-able


revisit an enhanced physical plan from MR compilation

Look to see if this is a non-split node with two outputs.  If so it matches.
//////// Helper ///////////
A method called by the Pig runtime to give an opportunity for implementations to perform cleanup actions like closing the underlying input stream. This is necessary since while performing a join the Pig run time may determine than no further join is possible with remaining records and may indicate to the IndexableLoader to cleanup by calling this method. This method is called by Pig run time to allow the IndexableLoadFunc to perform any initialization actions This method is called by the Pig runtime to indicate to the LoadFunc to position its underlying input stream near the keys supplied as the argument. Specifically: 1) if the keys are present in the input stream, the loadfunc implementation should position its read position to a record where the key(s) is/are the biggest key(s) less than the key(s) supplied in the argument OR to the record with the first occurrence of the keys(s) supplied. 2) if the key(s) are absent in the input stream, the implementation should position its read position to a record where the key(s) is/are the biggest key(s) less than the key(s) supplied OR to the first record where the key(s) is/are the smallest key(s) greater than the keys(s) supplied. The description above holds for descending order data in a similar manner with "biggest" and "less than" replaced with "smallest" and "greater than" and vice versa.
firstly compare the index secondly compare the key (both first and secondary key) If key is empty, we'd like compute equality based on key and index. If key is not empty, we'd like to compute equality based on just the key (like we normally do). There are two possible cases when two tuples are compared: 1) Compare tuples of same table (same index) 2) Compare tuples of different tables (different index values) In 1) key1    key2    equal? null    null      Y foo     null      N null    foo       N foo     foo       Y (1,1)   (1,1)     Y (1,)    (1,)      Y (1,2)   (1,2)     Y <p/> <p/> In 2) key1    key2    equal? null    null     N foo     null     N null    foo      N foo     foo      Y (1,1)   (1,1)    Y (1,)    (1,)     N (1,2)   (1,2)    Y Calculate hashCode by index and key if key is empty, return index value if key is not empty, return the key.hashCode()
IndexableLoadFunc interface implementation The list of readers is always sorted before and after this call. Assumes this list of readers is already sorted except for the provided element. This element is bubbled up the array to its appropriate sort location (faster than doing a Utils sort).
Bind the <code>InputHandler</code> to the <code>OutputStream</code> from which it reads input and sends it to the managed process. Close the <code>InputHandler</code> since there is no more input to be sent to the managed process. Get the handled <code>InputType</code> Send the given input <code>Tuple</code> to the managed executable.


Determines the number of reducers to be used. Get the total input size in bytes by looking at statistics provided by loaders that implement @{link LoadMetadata}. Get the input size for as many inputs as possible. Inputs that do not report their size nor can pig look that up itself are excluded from this size.

java level API




Get the progress within the split
(non-Javadoc) @see org.apache.hadoop.mapreduce.RecordWriter#close(org.apache.hadoop.mapreduce.TaskAttemptContext) (non-Javadoc) @see org.apache.hadoop.mapreduce.RecordWriter#write(java.lang.Object, java.lang.Object)
The type of next object has been determined to be of type Tuple, add the columns that belong to the tuple to given tuple argument t Get the next object from DataInput in Get the next object from DataInput in of the type of type argument The type information has been read from DataInput. Write given object val to DataOutput out Write given object val of DataType type to DataOutput out





















Loads the key distribution file obtained from the sampler











A conditional node is constructed if its condition is true.  All the nodes that have been pushed since the node was opened are made children of the conditional node, which is then pushed on to the stack.  If the condition is false the node is not constructed and they are left on the stack. A definite node is constructed from a specified number of children.  That number of nodes are popped from the stack and made the children of the definite node.  Then the definite node is pushed on to the stack. Returns the number of children on the stack in the current node scope. Determines whether the current node was actually closed and pushed.  This should only be called in the final user action of a node scope. Returns the node currently on the top of the stack. Returns the node on the top of the stack, and remove it from the stack. Pushes a node on to the stack. Call this to reinitialize the node stack.  It is called automatically by the parser's ReInit() method. Returns the root node of the AST.  It only makes sense to call this after a successful parse.
See: {@link Pig#bind()} javascript helper for binding parameters. See: {@link Pig#bind(Map)} ------------------------------------------------------------------------- Define a Pig pipeline. Define a named portion of a Pig pipeline.  This allows it to be imported into another pipeline. Define a Pig pipeline based on Pig Latin in a separate file. Define a named Pig pipeline based on Pig Latin in a separate file. This allows it to be imported into another pipeline. See {@link Pig} See {@link Pig} See {@link Pig} See {@link Pig} See {@link Pig}


Add the jars containing the given classes to the job's configuration such that JobClient will ship them to the cluster and add them to the DistributedCache Add the qualified path name of jars containing the given classes Adds a stream to a Jar file. Creates a Classloader based on the passed jarFile and any extra jar files. Find a jar that contains a class of the same name, if any. It will return a jar file, even if that is not the first thing on the class path that has a class with the same name.
Set buffers back to null when finished.   Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Method to adjust line and column numbers for the start of a token. Retreat.    Get end column. Get end line.  Read a character.

Adjust the number of reducers based on the default_parallel, requested parallel and estimated parallel. For sampler jobs, we also adjust the next job in advance to get its runtime parallel as the number of partitions used in the sampler. Calculate the runtime #reducers based on the default_parallel, requested parallel and estimated parallel, and save it to MapReduceOper's runtimeParallelism. Compiles all jobs that have no dependencies removes them from the plan and returns. Should be called with the same plan until exhausted. Looks up the estimator from REDUCER_ESTIMATOR_KEY and invokes it to find the number of reducers to use. If REDUCER_ESTIMATOR_KEY isn't set, defaults to InputSizeReducerEstimator. The method that creates the Job corresponding to a MapReduceOper. The assumption is that every MapReduceOper will have a load and a store. The JobConf removes the load operator and serializes the input filespec so that PigInputFormat can take over the creation of splits. It also removes the store operator and serializes the output filespec so that PigOutputFormat can take over record writing. The remaining portion of the map plan and reduce plans are serialized and stored for the PigMapReduce or PigMapOnly objects to take over the actual running of the plans. The Mapper &amp; Reducer classes and the required key value formats are set. Checks if this is a map only job and uses PigMapOnly class as the mapper and uses PigMapReduce otherwise. If it is a Map Reduce job, it is bound to have a package operator. Remove it from the reduce plan and serializes it so that the PigMapReduce class can use it to package the indexed tuples received by the reducer. Gets the map of Job and the MR Operator Returns all store locations of a previously compiled job Moves all the results of a collection of MR jobs to the final output directory. Some of the results may have been put into a temp location to work around restrictions with multiple output from a single map reduce job. This method should always be called after the job execution completes. Walks the temporary directory structure to move (rename) files to their final location. if url is not in HDFS will copy the path to HDFS from local before adding to distributed cache Resets the state Reads the global counters produced by a job on the group labeled with PIG_MAP_RANK_NAME. Then, it is calculated the cumulative sum, which consists on the sum of previous cumulative sum plus the previous global counter value. copy the file to hdfs in a temporary path Ensure that 'src' is a valid URI Update Map-Reduce plan with the execution status of the jobs. If one job completely fail (the job has only one store and that job fail), then we remove all its dependent jobs. This method will return the number of MapReduceOper removed from the Map-Reduce plan

deal special cases containing operators with multiple predecessors when multiquery is enabled to get the predecessors of specified physicalOp in previous SparkOp(see PIG-4675) if the parallelism of skewed join is NOT specified by user in the script when sampling, set a default parallelism for sampling Calling EvalFunc.finish() In Spark, currently only async actions return job id. There is no async equivalent of actions like saveAsNewAPIHadoopFile() <p/> The only other way to get a job id is to register a "job group ID" with the spark context and request all job ids corresponding to that job group via getJobIdsForGroup. <p/> However getJobIdsForGroup does not guarantee the order of the elements in it's result. <p/> This method simply returns the previously unseen job ids. get all rdds of predecessors sorted by the OperatorKey

Calculate the median value from the given array   Returns the total bytes written to user specified HDFS locations of this job.            Looks up the output size reader from OUTPUT_SIZE_READER_KEY and invokes it to get the size of output. If OUTPUT_SIZE_READER_KEY is not set, defaults to FileBasedOutputSizeReader.   Returns the total number of records in user specified output locations of this job.
collapse LRA,GRA,PKG to POJoinGroupSpark

Calls getNext to get next ForEach result. The input for POJoinPackage is a (key, NullableTuple) pair. We will materialize n-1 inputs into bags, feed input#n one tuple a time to the delegated ForEach operator, the input for ForEach is (input#1, input#2, input#3....input#n[i]), i=(1..k), suppose input#n consists of k tuples. For every ForEach input, pull all the results from ForEach. getNext will be called multiple times for a particular input, it returns one output tuple from ForEach every time we call getNext, so we need to maintain internal status to keep tracking of where we are.
This uses the "exec" method required of AccumulatorPigUdf Ruby classes. It streams the data bags it receives through the exec method defined on the registered class. This method calls "get" on the AccumulatorPigUdf Ruby class that was specified. This function intializes the object that receives method calls, and the class object that has schema information. While this is only 3 lines, it is split out so that the schema function can initialize it if necessary. The class object is pulled from the ruby script registered at the path per the RubyFunctions helper methods. This provides the Schema of the output, and leverages the get_output_schema function on the class object that is defined on the ruby side.

The exec method passes the tuple argument to the Ruby function, and converts the result back to Pig. This method initializes the objects necessary to evaluate the Ruby class on the pig side. Using the object that was saved to the functionName key by Ruby, this class gets an instance of the class that will receive method calls, as well as information on the arity. This method uses the schema method of the function encapsulation object to get the Schema information for the Ruby method.
Consults the scripting container, after the script has been evaluated, to determine what dependencies to ship. <p> FIXME: Corner cases like the following: "def foobar; require 'json'; end" are NOT dealt with using this method Evaluates the script containing ruby udfs to determine what udfs are defined as well as what libaries and other external resources are necessary. These libraries and resources are then packaged with the job jar itself.
debug utility to display a JS object ///////////////////// Debugging functions ///////////////////// ///////////////////////// EvalFunc implementation ///////////////////////// generates a String of white spaces for debugging indentation /////////////////////// Conversion JS to Pig /////////////////////// converts a bag to javascript object based on a schema converts a map to javascript object based on a schema /////////////////////// Conversion Pig to JS /////////////////////// converts a tuple to javascript object based on a schema helper function for debugging helper function for debugging debug utility to display a JS object
call a javascript function evaluate javascript from a reader evaluate a javascript String creates a new javascript array creates a new JavaScript object put a value in the current scope
------------------------------------------------------------------------
. Given a path, which may represent a glob pattern, a directory, comma separated files/glob patterns or a file, this method finds the set of relevant metadata files on the storage system. The algorithm for finding the metadata file is as follows: <p> For each object represented by the path (either directly, or via a glob): If object is a directory, and path/metaname exists, use that as the metadata file. Else if parentPath/metaname exists, use that as the metadata file. <p> Resolving conflicts, merging the metadata, etc, is not handled by this method and should be taken care of by downstream code. <p> ------------------------------------------------------------------------ Implementation of LoadMetaData interface For JsonMetadata schema is considered optional This method suppresses (and logs) errors if they are encountered. Read the schema from json metadata file If isSchemaOn parameter is false, the errors are suppressed and logged For JsonMetadata stats are considered optional This method suppresses (and logs) errors if they are encountered. ------------------------------------------------------------------------ Implementation of StoreMetaData interface
Methods called on the front end Methods called on the back end

Gets the Python function object. execs the script text using the interpreter. populates the pig context with necessary module paths. does not close the inputstream.






















Finds the last location of a substring in a given string. (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()

Returns an Unmodifiable Map of Input Number to Uid Given an expression plan this function returns a LogicalFieldSchema that can be generated using this expression plan




Find the LOInnerLoad of the inner plan corresponding to the project, and also find whether there is a relational operator in inner plan along the way



Get the output schema corresponding to each input expression plan
Get the LOForEach operator that contains this operator as part of inner plan
Get all of the expressions plans that are in this join.

Get the schema for this load.  The schema will be either be what was given by the user in the script or what the load functions getSchema call returned.  Otherwise null will be returned, indicating that the schema is unknown. This method will store the scriptSchema:Schema using ObjectSerializer to the current configuration.<br/> The schema can be retrieved by load functions or UDFs to know the schema the user entered in the as clause.<br/> The name format is:<br/> <pre> ${UDFSignature}.scriptSchema = ObjectSerializer.serialize(scriptSchema) </pre> <p/> Note that this is not the schema the load function returns but will always be the as clause schema.<br/> That is a = LOAD 'input' as (a:chararray, b:chararray)<br/> The schema wil lbe (a:chararray, b:chararray)<br/> <p/> TODO Find better solution to make script schema available to LoadFunc see https://issues.apache.org/jira/browse/PIG-1717

Get the schema for the output of LORank. Composed by long value prepended to the rest of the input schema Get if it is a dense RANK BY Get if it is a simple RANK operation. Which means a row number attached to each tuple. Set if it is a dense RANK BY Set if it is a simple RANK operation.




Get the StreamingCommand object associated with this operator
create schema for union-onschema Get input uids mapping to the output uid
Method invoked on every tuple during foreach evaluation (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping() This method gives a name to the column.

(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
Compute the progress of the current job submitted through the JobControl object jc to the JobClient jobClient Explain how a pig job will be executed on the underlying infrastructure.    Method to launch pig for hadoop either for a cluster's job tracker or for a local job runner. THe only difference between the two is the job client. Depending on the pig context the job client will be initialize to one of the two. Launchers for other frameworks can overide these methods. Given an input PhysicalPlan, it compiles it to get a MapReduce Plan. The MapReduce plan which has multiple MapReduce operators each one of which has to be run as a map reduce job with dependency information stored in the plan. It compiles the MROperPlan into a JobControl object. Each Map Reduce operator is converted into a Job and added to the JobControl object. Each Job also has a set of dependent Jobs that are created using the MROperPlan. The JobControl object is obtained from the JobControlCompiler Then a new thread is spawned that submits these jobs while respecting the dependency information. The parent thread monitors the submitted jobs' progress and after it is complete, stops the JobControl thread. Returns the progress of a Job j which is part of a submitted JobControl object. The progress is for this Job. So it has to be scaled down by the num of jobs that are present in the JobControl. Resets the state after a launch





Move all operators between POLimit and POStore in reducer plan from firstMROp to the secondMROp


Add a tuple to the bag. Add contents of a bag to the bag. Clear out the contents of the bag, both on disk and in memory. Any attempts to read after this is called will produce undefined results. Not implemented. Since comparator in Java 1.7 does not have reversed(), we need this method to get a reversed comparator for a given comparator Find out if the bag is distinct. Find out if the bag is sorted. Get an iterator to the bag. For default and distinct bags, no particular order is guaranteed. For sorted bags the order is guaranteed to be sorted according to the provided comparator. Not implemented. This is used by FuncEvalSpec.FakeDataBag. Read a bag from disk. Report progress to HDFS. Get the number of elements in the bag in memory. Not implemented. Write the bag into a string. Write a bag's contents to disk.
Add given uid, loadFuncSpec to mapping Find single load func spec associated with this relation. If the relation has schema, all uids in schema should be associated with same load func spec. if it does not have schema check the existing mapping Copied from POCast.instantiateFunc if uid in input field schemas or their inner schemas map to same load function, then map the new uid in bincond also to same load function in uid2LoadFuncMap If all predecessors of relOp are associated with same load func , then map reOp to it. map all uids in schema to funcSpec
get the cardinality of each tuple set (identified by a representative tuple) get all members of the set containing t get a mapping from set representatives to members find the set representative of a given tuple get the cardinality of each tuple set, weighted in a special way weighting works like this: if a tuple set contains one or more tuples from the "specialTuples" set, we multiply its value by "multiplier" public IdentityHashMap<Tuple, Integer> getWeightedCounts(IdentityHashSet<Tuple> specialTuples, int multiplier) { IdentityHashMap<Tuple, Integer> repCounts = new IdentityHashMap<Tuple, Integer>(); IdentityHashSet<Tuple> specialSets = new IdentityHashSet<Tuple>();  for (IdentityHashMap.Entry<Tuple, Integer> e : counts.entrySet()) { Tuple t = e.getKey();  int newCount = counts.get(t); Tuple rep = getRepresentative(t); int oldCount = (repCounts.containsKey(rep))? repCounts.get(rep) : 0; repCounts.put(rep, oldCount + newCount); if (specialTuples.contains(t)) specialSets.add(rep); }  for (IdentityHashMap.Entry<Tuple, Integer> e : repCounts.entrySet()) { if (specialSets.contains(e.getKey())) e.setValue(e.getValue() * multiplier); }  return repCounts; } insert a new tuple (if a tuple is inserted multiple times, it gets a count > 1) union two tuple sets


Make a deep copy of this operator. Similar to POPackage.getNext except that only one input is expected with index 0 and ReadOnceBag is used instead of DefaultDataBag. Makes use of the superclass method, but this requires an additional parameter key passed by ReadOnceBag. key of this instance will be set to null in detachInput call, but an instance of ReadOnceBag may have the original key that it uses. Therefore this extra argument is taken to temporarily set it before the call to the superclass method and then restore it.
Cast data from bytearray to bag value. Cast data from bytearray to BigDecimal value. Cast data from bytearray to BigInteger value. Cast data from bytearray to boolean value. Cast data from bytearray to chararray value. Cast data from bytearray to datetime value. Cast data from bytearray to double value. Cast data from bytearray to float value. Cast data from bytearray to integer value. Cast data from bytearray to long value. Cast data from bytearray to map value. Cast data from bytearray to tuple value.
stolen from JobControlCompiler TODO: refactor it to share this
Construct the absolute path from the file location and the current directory. The current directory is either of the form {code}hdfs://<nodename>:<nodeport>/<directory>{code} in Hadoop MapReduce mode, or of the form {code}file:///<directory>{code} in Hadoop local mode. Allow a LoadFunc to specify a list of files it would like placed in the distributed cache. The default implementation returns null. This will be called during planning on the front end. This is the instance of InputFormat (rather than the class name) because the load function may need to instantiate the InputFormat in order to control how it is constructed. This will be called on both the front end and the back end during execution. Retrieves the next tuple to be processed. Implementations should NOT reuse tuple objects (or inner member objects) they return across calls and should return a different tuple object in each call. Parse comma separated path strings into a string array. This method escapes commas in the Hadoop glob pattern of the given paths. This method is borrowed from {@link org.apache.hadoop.mapreduce.lib.input.FileInputFormat}. A jira (MAPREDUCE-1205) is opened to make the same name method there accessible. We'll use that method directly when the jira is fixed. Allow a LoadFunc to specify a list of files located locally and would like to ship to backend (through distributed cache). Check for {@link FuncUtils} for utility function to facilitate it The default implementation returns null. ------------------------------------------------------------------------ Join multiple strings into a string delimited by the given delimiter. Initializes LoadFunc for reading data.  This will be called during execution before any calls to getNext.  The RecordReader needs to be passed here because it has been instantiated for a particular InputSplit. This method is called by the Pig runtime in the front end to convert the input location to an absolute path if the location is relative. The loadFunc implementation is free to choose how it converts a relative location to an absolute location since this may depend on what the location string represent (hdfs path or some other data source) Communicate to the loader the location of the object(s) being loaded. The location string passed to the LoadFunc here is the return value of {@link LoadFunc#relativeToAbsolutePath(String, Path)}. Implementations should use this method to communicate the location (and any other information) to its underlying InputFormat through the Job object. This method will be called in the frontend and backend multiple times. Implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls. This method will be called by Pig both in the front end and back end to pass a unique signature to the {@link LoadFunc}. The signature can be used to store into the {@link UDFContext} any information which the {@link LoadFunc} needs to store between various method invocations in the front end and back end. A use case is to store {@link RequiredFieldList} passed to it in {@link LoadPushDown#pushProjection(RequiredFieldList)} for use in the back end before returning tuples in {@link LoadFunc#getNext()}. This method will be call before other methods in {@link LoadFunc} Issue a warning.  Warning messages are aggregated and reported to the user.
If location is a directory the first file found is returned If location is a directory the first file found in the directory is used.<br/> The file extension of the file will be searched against the file.extension.loaders mappings. If none found null is returned. The file extension of the file will be searched against the file.extension.loaders mappings. If none found null is returned. Searches in the path for the first occurrence of the tags associated with the extension.<br/> If this extension has no tags an empty string is returned.<br/> If it has tags and no tag is found in the path null is returned.<br/>  Looks for and returns the first file it can find. Tries to identify the extension and there by the loader from the content type. Search for the correct loader based on the extension and tags mappings. Open a SequenceFile.Reader instance and return the keyClassName
The wrapped LoadMetadata object must be set before method calls are made on this object. Typically, this is done with via constructor, but often times the wrapped object can not be properly initialized until later in the lifecycle of the wrapper object.
Returns a method in the call stack at the given depth. Depth 0 will return the method that called this getMethodName, depth 1 the method that called it, etc... The wrapped LoadFunc object must be set before method calls are made on this object. Typically, this is done with via constructor, but often times the wrapped object can not be properly initialized until later in the lifecycle of the wrapper object.
Find what columns are partition keys for this input. Get a schema for the data to be loaded. Get statistics about the data to be loaded.  If no statistics are available, then null should be returned. If the implementing class also extends {@link LoadFunc}, then {@link LoadFunc#setLocation(String, org.apache.hadoop.mapreduce.Job)} is guaranteed to be called before this method. Set the filter for partitioning.  It is assumed that this filter will only contain references to fields given as partition keys in getPartitionKeys. So if the implementation returns null in {@link #getPartitionKeys(String, Job)}, then this method is not called by Pig runtime. This method is also not called by the Pig runtime if there are no partition filter conditions.
Find what fields of the data can support predicate pushdown. Indicate operations on fields supported by the loader for predicate pushdown Push down expression to the loader
Determine the operators that can be pushed to the loader. Note that by indicating a loader can accept a certain operator (such as selection) the loader is not promising that it can handle all selections.  When it is passed the actual operators to push down it will still have a chance to reject them. Indicate to the loader fields that will be needed.  This can be useful for loaders that access data that is stored in a columnar format where indicating columns to be accessed a head of time will save scans.  This method will not be invoked by the Pig runtime if all fields are required. So implementations should assume that if this method is not invoked, then all fields from the input are required. If the loader function cannot make use of this information, it is free to ignore it by returning an appropriate Response

if we are inserting casts in a load and if the loader implements determineSchema(), insert casts only where necessary Note that in this case, the data coming out of the loader is not a BYTEARRAY but is whatever determineSchema() says it is.
Do the final configuration of LoadFuncs and store what goes where. This will need to be changed as the inputs get un-bundled





This function takes in a List of LogicalExpressionPlan and converts them to a list of PhysicalPlans updates plan with check for empty bag and if bag is empty to flatten a bag with as many null's as dictated by the schema Transformation from Logical to Physical Plan involves the following steps: First, it is generated a random number which will link a POCounter within a PORank. On this way, avoiding possible collisions on parallel rank operations. Then, if it is row number mode: <pre> In case of a RANK operation (row number mode), are used two steps: 1.- Each tuple is counted sequentially on each mapper, and are produced global counters 2.- Global counters are gathered and summed, each tuple calls to the respective counter value in order to calculate the corresponding rank value. </pre> or not: <pre> In case of a RANK BY operation, then are necessary five steps: 1.- Group by the fields involved on the rank operation: POPackage 2.- In case of multi-fields, the key (group field) is flatten: POForEach 3.- Sort operation by the fields available after flattening: POSort 4.- Each group is sequentially counted on each mapper through a global counter: POCounter 5.- Global counters are summed and passed to the rank operation: PORank </pre>

Create the deep copy of this expression and add that into the passed LogicalExpressionPlan Return the copy of this expression with updated logical expression plan. Get the field schema for the output of this expression operator.  This does not merely return the field schema variable.  If schema is not yet set, this will attempt to construct it.  Therefore it is abstract since each operator will need to construct its field schema differently. Get the data type for this expression. This is a convenience method to avoid the side-effectful nature of getFieldSchema(). It simply returns whether or not fieldSchema is currently null. used for junit test, should not be called elsewhere Erase all cached uid, regenerate uid when we regenerating schema. This process currently only used in ImplicitSplitInsert, which will insert split and invalidate some uids in plan
Merge all nodes in lgExpPlan, keep the connections

Returns the signature of the LogicalPlan. The signature is a unique identifier for a given plan generated by a Pig script. The same script run multiple times with the same version of Pig is guaranteed to produce the same signature, even if the input or output locations differ. Equality is checked by calling equals on every leaf in the plan.  This assumes that plans are always connected graphs.  It is somewhat inefficient since every leaf will test equality all the way to every root.  But it is only intended for use in testing, so that should be ok.  Checking predecessors (as opposed to successors) was chosen because splits (which have multiple successors) do not depend on order of outputs for correctness, whereas joins (with multiple predecessors) do.  That is, reversing the outputs of split in the graph has no correctness implications, whereas reversing the inputs of join can.  This method of doing equals will detect predecessors in different orders but not successors in different orders. It will return false if either plan has non deterministic EvalFunc.
Build operator for foreach inner plan. Build a project expression for a projection present in global plan (not in nested foreach plan). Build a project expression in foreach inner plan. The only difference here is that the projection can be for an expression alias, for which we will return whatever the expression alias represents. Build a project expression that projects a range of columns This methods if the dimensions specified by the user has duplicates if multiple CUBE operations occur continuously then it can be combined together CUBE rel BY CUBE(a,b), CUBE(c,d); => CUBE rel BY CUBE(a,b,c,d) This function creates logical plan for foreach and groupby operators. It connects the predecessors of cube operator with foreach plan and disconnects cube operator from the logical plan. It also connects foreach plan with groupby plan. User defined schema for generate operator. If not specified output schema of UDF will be used which will prefix "dimensions" namespace to all fields This method connects the predecessors of cube operator with foreach operator and disconnects the cube operator from its predecessors performs merging of dimensions of merged cube operation Ex: CUBE(a,b), CUBE(c,d) ==> CUBE(a,b,c,d) in the above example CUBE operator and dimensions are merged Parse big decimal formatted string (e.g. "123456.7890123BD") into BigDecimal object Parse big integer formatted string (e.g. "1234567890123BI") into BigInteger object Parse the long given as a string such as "34L". Process expression plans of LOGenerate and set inputs relation for the ProjectExpression For any UNKNOWN type in the schema fields, set the type to BYTEARRAY
Returns the number of {@link LogicalRelationalOperator}s present in the pig script.
$ANTLR start "alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:224:1: alias returns [String name] : IDENTIFIER ; $ANTLR start "alias_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1735:1: alias_col_ref[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "as_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:350:1: as_clause returns [LogicalSchema logicalSchema] : ^( AS field_def_list ) ; $ANTLR start "assert_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:705:1: assert_clause returns [String alias] : ^( ASSERT rel cond[exprPlan] ( comment )? ) ; $ANTLR start "assert_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:186:1: assert_statement : assert_clause ; $ANTLR start "bag" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1915:1: bag returns [Object value] : ^( BAG_VAL ( tuple )* ) ; $ANTLR start "bag_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:430:1: bag_type returns [LogicalSchema logicalSchema] : ^( BAG_TYPE ( IDENTIFIER )? ( tuple_type )? ) ; $ANTLR start "bag_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:941:1: bag_type_cast returns [LogicalSchema logicalSchema] : ^( BAG_TYPE_CAST ( tuple_type_cast )? ) ; $ANTLR start "bin_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1091:1: bin_expr[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ^( BIN_EXPR cond[$plan] e1= expr[$plan] e2= expr[$plan] ) ; $ANTLR start "cache_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:281:1: cache_clause[List<String> paths] : ^( CACHE path_list[$paths] ) ; $ANTLR start "case_cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1127:1: case_cond[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ^( CASE_COND ^( WHEN ( cond[$plan] )+ ) ^( THEN ( expr[$plan] )+ ) ) ; $ANTLR start "case_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1099:1: case_expr[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ^( CASE_EXPR ( ( ^( CASE_EXPR_LHS lhs= expr[$plan] ) ) ( ^( CASE_EXPR_RHS rhs= expr[$plan] ) )+ )+ ) ; $ANTLR start "cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:259:1: cmd[String alias] returns [StreamingCommand command] : ^( EXECCOMMAND ( ship_clause[shipPaths] | cache_clause[cachePaths] | input_clause | output_clause | error_clause )* ) ; $ANTLR start "col_alias" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1063:1: col_alias returns [Object col] : ( GROUP | CUBE | IDENTIFIER ); $ANTLR start "col_alias_or_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1059:1: col_alias_or_index returns [Object col] : ( col_alias | col_index ); $ANTLR start "col_index" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1069:1: col_index returns [Integer col] : DOLLARVAR ; $ANTLR start "col_range" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1074:1: col_range[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ^( COL_RANGE (startExpr= col_ref[$plan] )? DOUBLE_PERIOD (endExpr= col_ref[$plan] )? ) ; $ANTLR start "col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1730:1: col_ref[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ( alias_col_ref[$plan] | dollar_col_ref[$plan] ); $ANTLR start "comment" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:719:1: comment returns [String comment] : QUOTEDSTRING ; $ANTLR start "cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:737:1: cond[LogicalExpressionPlan exprPlan] returns [LogicalExpression expr] : ( ^( OR left= cond[exprPlan] right= cond[exprPlan] ) | ^( AND left= cond[exprPlan] right= cond[exprPlan] ) | ^( NOT c= cond[exprPlan] ) | ^( NULL expr[$exprPlan] ( NOT )? ) | ^( rel_op_eq e1= expr[$exprPlan] e2= expr[$exprPlan] ) | ^( rel_op_ne e1= expr[$exprPlan] e2= expr[$exprPlan] ) | ^( rel_op_lt e1= expr[$exprPlan] e2= expr[$exprPlan] ) | ^( rel_op_lte e1= expr[$exprPlan] e2= expr[$exprPlan] ) | ^( rel_op_gt e1= expr[$exprPlan] e2= expr[$exprPlan] ) | ^( rel_op_gte e1= expr[$exprPlan] e2= expr[$exprPlan] ) | ^( STR_OP_MATCHES e1= expr[$exprPlan] e2= expr[$exprPlan] ) | in_eval[$exprPlan] | func_eval[$exprPlan] | ^( BOOL_COND e1= expr[$exprPlan] ) ); $ANTLR start "const_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1793:1: const_expr[LogicalExpressionPlan plan] returns [LogicalExpression expr] : literal ; $ANTLR start "cross_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1327:1: cross_clause returns [String alias] : ^( CROSS rel_list ( partition_clause )? ) ; $ANTLR start "cube_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:526:1: cube_by_clause returns [List<String> operations, MultiMap<Integer, LogicalExpressionPlan> plans] : ^( BY cube_or_rollup ) ; $ANTLR start "cube_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:561:1: cube_by_expr returns [LogicalExpressionPlan plan] : ( col_range[$plan] | expr[$plan] | STAR ); $ANTLR start "cube_by_expr_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:554:1: cube_by_expr_list returns [List<LogicalExpressionPlan> plans] : ( cube_by_expr )+ ; $ANTLR start "cube_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:496:1: cube_clause returns [String alias] : ^( CUBE cube_item ) ; $ANTLR start "cube_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:518:1: cube_item : rel ( cube_by_clause ) ; $ANTLR start "cube_or_rollup" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:534:1: cube_or_rollup returns [List<String> operations, MultiMap<Integer, LogicalExpressionPlan> plans] : ( cube_rollup_list )+ ; $ANTLR start "cube_rollup_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:547:1: cube_rollup_list returns [String operation, List<LogicalExpressionPlan> plans] : ^( ( CUBE | ROLLUP ) cube_by_expr_list ) ; $ANTLR start "define_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:248:1: define_clause : ( ^( DEFINE alias cmd[$alias.name] ) | ^( DEFINE alias func_clause[FunctionType.UNKNOWNFUNC] ) ); $ANTLR start "distinct_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1312:1: distinct_clause returns [String alias] : ^( DISTINCT rel ( partition_clause )? ) ; $ANTLR start "dollar_col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1784:1: dollar_col_ref[LogicalExpressionPlan plan] returns [LogicalExpression expr] : DOLLARVAR ; $ANTLR start "dot_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1052:1: dot_proj returns [List<Object> cols] : ^( PERIOD ( col_alias_or_index )+ ) ; $ANTLR start "eid" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1932:1: eid returns [String id] : ( rel_str_op | IMPORT | RETURNS | DEFINE | LOAD | FILTER | FOREACH | MATCHES | ORDER | DISTINCT | COGROUP | CUBE | ROLLUP | JOIN | CROSS | UNION | SPLIT | INTO | IF | ALL | AS | BY | USING | INNER | OUTER | PARALLEL | PARTITION | GROUP | AND | OR | NOT | GENERATE | FLATTEN | EVAL | ASC | DESC | BOOLEAN | INT | LONG | FLOAT | DOUBLE | BIGINTEGER | BIGDECIMAL | DATETIME | CHARARRAY | BYTEARRAY | BAG | TUPLE | MAP | IS | NULL | TRUE | FALSE | STREAM | THROUGH | STORE | MAPREDUCE | SHIP | CACHE | INPUT | OUTPUT | STDERROR | STDIN | STDOUT | LIMIT | SAMPLE | LEFT | RIGHT | FULL | IDENTIFIER | TOBAG | TOMAP | TOTUPLE | ASSERT ); $ANTLR start "error_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:319:1: error_clause returns [String dir, Integer limit] : ^( STDERROR ( QUOTEDSTRING ( INTEGER )? )? ) ; $ANTLR start "expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:865:1: expr[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ( ^( PLUS left= expr[$plan] right= expr[$plan] ) | ^( MINUS left= expr[$plan] right= expr[$plan] ) | ^( STAR left= expr[$plan] right= expr[$plan] ) | ^( DIV left= expr[$plan] right= expr[$plan] ) | ^( PERCENT left= expr[$plan] right= expr[$plan] ) | const_expr[$plan] | var_expr[$plan] | ^( NEG e= expr[$plan] ) | ^( CAST_EXPR type_cast e= expr[$plan] ) | ^( EXPR_IN_PAREN e= expr[$plan] ) ); $ANTLR start "field_def" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:358:1: field_def[NumValCarrier nvc] returns [LogicalFieldSchema fieldSchema] : ( ^( FIELD_DEF IDENTIFIER ( type )? ) | ^( FIELD_DEF_WITHOUT_IDENTIFIER ( type ) ) ); $ANTLR start "field_def_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:375:1: field_def_list returns [LogicalSchema schema] : ( field_def[nvc] )+ ; $ANTLR start "filename" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:346:1: filename returns [String filename] : QUOTEDSTRING ; $ANTLR start "filter_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:723:1: filter_clause returns [String alias] : ^( FILTER rel cond[exprPlan] ) ; $ANTLR start "flatten_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:692:1: flatten_clause[LogicalExpressionPlan plan] : ^( FLATTEN expr[$plan] ) ; $ANTLR start "flatten_generated_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:676:1: flatten_generated_item returns [LogicalExpressionPlan plan, boolean flattenFlag, LogicalSchema schema] : ( flatten_clause[$plan] | col_range[$plan] | expr[$plan] | STAR ) ( field_def_list )? ; $ANTLR start "foreach_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1427:1: foreach_clause returns [String alias] : ^( FOREACH rel foreach_plan ) ; $ANTLR start "foreach_plan" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1444:1: foreach_plan returns [LogicalPlan plan] : ( ^( FOREACH_PLAN_SIMPLE generate_clause ) | ^( FOREACH_PLAN_COMPLEX nested_blk ) ); $ANTLR start "func_args" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:483:1: func_args returns [List<String> args] : ( QUOTEDSTRING | MULTILINE_QUOTEDSTRING )+ ; $ANTLR start "func_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:451:1: func_clause[byte ft] returns [FuncSpec funcSpec] : ( ^( FUNC_REF func_name ) | ^( FUNC func_name ( func_args )? ) ); $ANTLR start "func_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:839:1: func_eval[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ( ^( FUNC_EVAL func_name ( real_arg[$plan] )* ) | ^( INVOKER_FUNC_EVAL package_name= IDENTIFIER function_name= IDENTIFIER is_static= IDENTIFIER ( real_arg[$plan] )* ) ); $ANTLR start "func_name" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:473:1: func_name returns [String funcName] : p1= eid ( ( PERIOD | DOLLAR ) p2= eid )* ; $ANTLR start "general_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:197:1: general_statement : ^( STATEMENT ( alias )? oa= op_clause ( parallel_clause )? ) ; $ANTLR start "generate_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1608:1: generate_clause : ^( GENERATE ( flatten_generated_item )+ ) ; delegates $ANTLR start "group_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:573:1: group_clause returns [String alias] : ( ^( GROUP ( group_item )+ ( group_type )? ( partition_clause )? ) | ^( COGROUP ( group_item )+ ( group_type )? ( partition_clause )? ) ); $ANTLR start "group_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:612:1: group_item : rel ( join_group_by_clause | ALL | ANY ) ( INNER | OUTER )? ; $ANTLR start "group_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:605:1: group_type returns [GROUPTYPE type] : QUOTEDSTRING ; $ANTLR start "in_eval" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:812:1: in_eval[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ^( IN ( ^( IN_LHS lhs= expr[$plan] ) ^( IN_RHS rhs= expr[$plan] ) )+ ) ; $ANTLR start "inline_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:660:1: inline_op : op_clause ( parallel_clause )? ; $ANTLR start "input_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:285:1: input_clause returns [List<HandleSpec> inputHandleSpecs] : ^( INPUT ( stream_cmd[true] )+ ) ; $ANTLR start "join_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1340:1: join_clause returns [String alias] : ^( JOIN join_sub_clause ( join_type )? ( partition_clause )? ) ; $ANTLR start "join_group_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1396:1: join_group_by_clause returns [List<LogicalExpressionPlan> plans] : ^( BY ( join_group_by_expr )+ ) ; $ANTLR start "join_group_by_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1403:1: join_group_by_expr returns [LogicalExpressionPlan plan] : ( col_range[$plan] | expr[$plan] | STAR ); $ANTLR start "join_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1386:1: join_item : ^( JOIN_ITEM rel join_group_by_clause ) ; $ANTLR start "join_sub_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1374:1: join_sub_clause : ( join_item ( LEFT | RIGHT | FULL ) ( OUTER )? join_item | ( join_item )+ ); $ANTLR start "join_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1367:1: join_type returns [JOINTYPE type] : QUOTEDSTRING ; $ANTLR start "keyvalue" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1903:1: keyvalue returns [String key, Object value] : ^( KEY_VAL_PAIR map_key literal ) ; $ANTLR start "limit_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1156:1: limit_clause returns [String alias] : ^( LIMIT rel ( INTEGER | LONGINTEGER | expr[exprPlan] ) ) ; $ANTLR start "literal" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1801:1: literal returns [Object value, byte type] : ( scalar | map | bag | tuple ); $ANTLR start "load_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:337:1: load_clause returns [String alias] : ^( LOAD filename ( func_clause[FunctionType.LOADFUNC] )? ( as_clause )? ) ; $ANTLR start "map" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1895:1: map returns [Object value] : ^( MAP_VAL ( keyvalue )* ) ; $ANTLR start "map_key" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1911:1: map_key returns [String value] : QUOTEDSTRING ; $ANTLR start "map_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:439:1: map_type returns [LogicalSchema logicalSchema] : ^( MAP_TYPE ( IDENTIFIER )? ( type )? ) ; $ANTLR start "mr_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1674:1: mr_clause returns [String alias] : ^( MAPREDUCE QUOTEDSTRING ( path_list[paths] )? store_clause load_clause ( EXECCOMMAND )? ) ; $ANTLR start "nested_blk" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1464:1: nested_blk : ( nested_command )* generate_clause ; $ANTLR start "nested_command" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1467:1: nested_command : ( ^( NESTED_CMD IDENTIFIER nested_op[$IDENTIFIER.text] ) | ^( NESTED_CMD_ASSI IDENTIFIER expr[exprPlan] ) ); $ANTLR start "nested_cross" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1579:1: nested_cross[String alias] returns [Operator op] : ^( CROSS nested_op_input_list ) ; $ANTLR start "nested_distinct" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1546:1: nested_distinct[String alias] returns [Operator op] : ^( DISTINCT nested_op_input ) ; $ANTLR start "nested_filter" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1516:1: nested_filter[String alias] returns [Operator op] : ^( FILTER nested_op_input cond[plan] ) ; $ANTLR start "nested_foreach" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1590:1: nested_foreach[String alias] returns [Operator op] : ^( FOREACH nested_op_input generate_clause ) ; $ANTLR start "nested_limit" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1557:1: nested_limit[String alias] returns [Operator op] : ^( LIMIT nested_op_input ( INTEGER | expr[exprPlan] ) ) ; $ANTLR start "nested_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1487:1: nested_op[String alias] returns [Operator op] : ( nested_proj[$alias] | nested_filter[$alias] | nested_sort[$alias] | nested_distinct[$alias] | nested_limit[$alias] | nested_cross[$alias] | nested_foreach[$alias] ); $ANTLR start "nested_op_input" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1631:1: nested_op_input returns [Operator op] : ( col_ref[plan] | nested_proj[null] ); $ANTLR start "nested_op_input_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1647:1: nested_op_input_list returns [List<Operator> opList] : ( nested_op_input )+ ; $ANTLR start "nested_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1497:1: nested_proj[String alias] returns [Operator op] : ^( NESTED_PROJ cr0= col_ref[plan] (cr= col_ref[new LogicalExpressionPlan()] )+ ) ; $ANTLR start "nested_sort" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1531:1: nested_sort[String alias] returns [Operator op] : ^( ORDER nested_op_input order_by_clause ( func_clause[FunctionType.COMPARISONFUNC] )? ) ; $ANTLR start "num_scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1851:1: num_scalar returns [Object value, byte type] : ( MINUS )? ( INTEGER | LONGINTEGER | FLOATNUMBER | DOUBLENUMBER | BIGINTEGERNUMBER | BIGDECIMALNUMBER ) ; $ANTLR start "op_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:227:1: op_clause returns [String alias] : ( define_clause | load_clause | group_clause | store_clause | filter_clause | distinct_clause | limit_clause | sample_clause | order_clause | rank_clause | cross_clause | join_clause | union_clause | stream_clause | mr_clause | foreach_clause | cube_clause | assert_clause ); $ANTLR start "order_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1284:1: order_by_clause returns [List<LogicalExpressionPlan> plans, List<Boolean> ascFlags] : ( STAR ( ASC | DESC )? | ( order_col )+ ); $ANTLR start "order_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1270:1: order_clause returns [String alias] : ^( ORDER rel order_by_clause ( func_clause[FunctionType.COMPARISONFUNC] )? ) ; $ANTLR start "order_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1303:1: order_col returns [LogicalExpressionPlan plan, Boolean ascFlag] : ( col_range[$plan] ( ASC | DESC )? | col_ref[$plan] ( ASC | DESC )? ); $ANTLR start "output_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:312:1: output_clause returns [List<HandleSpec> outputHandleSpecs] : ^( OUTPUT ( stream_cmd[false] )+ ) ; $ANTLR start "parallel_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:217:1: parallel_clause : ^( PARALLEL INTEGER ) ; $ANTLR start "partition_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1320:1: partition_clause returns [String partitioner] : ^( PARTITION func_name ) ; $ANTLR start "path_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:277:1: path_list[List<String> paths] : ( QUOTEDSTRING )+ ; $ANTLR start "pound_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1087:1: pound_proj returns [String key] : ^( POUND ( QUOTEDSTRING | NULL ) ) ; $ANTLR start "previous_rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:657:1: previous_rel returns [String name] : ARROBA ; $ANTLR start "projectable_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1029:1: projectable_expr[LogicalExpressionPlan plan] returns [LogicalExpression expr] : ( func_eval[$plan] | col_ref[$plan] | bin_expr[$plan] | case_expr[$plan] | case_cond[$plan] ); $ANTLR start "query" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:159:1: query : ^( QUERY ( statement )* ) ; $ANTLR start "rank_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1243:1: rank_by_clause returns [List<LogicalExpressionPlan> plans, List<Boolean> ascFlags] : ( STAR ( ASC | DESC )? | ( rank_col )+ ); $ANTLR start "rank_by_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1230:1: rank_by_statement returns [List<LogicalExpressionPlan> plans, List<Boolean> ascFlags, Boolean isDenseRank] : ^( BY rank_by_clause ( DENSE )? ) ; $ANTLR start "rank_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1200:1: rank_clause returns [String alias] : ^( RANK rel ( rank_by_statement )? ) ; $ANTLR start "rank_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1261:1: rank_col returns [LogicalExpressionPlan plan, Boolean ascFlag] : ( col_range[$plan] ( ASC | DESC )? | col_ref[$plan] ( ASC | DESC )? ); $ANTLR start "real_arg" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:855:1: real_arg[LogicalExpressionPlan plan] returns [LogicalExpression expr] : (e= expr[$plan] | STAR |cr= col_range[$plan] ); $ANTLR start "realias_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:205:1: realias_clause : ^( REALIAS alias IDENTIFIER ) ; $ANTLR start "realias_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:183:1: realias_statement : realias_clause ; $ANTLR start "register_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:189:1: register_statement : ^( REGISTER QUOTEDSTRING ( USING IDENTIFIER AS IDENTIFIER )? ) ; $ANTLR start "rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:645:1: rel : ( alias | previous_rel | inline_op ); $ANTLR start "rel_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1335:1: rel_list returns [List<String> aliasList] : ( rel )+ ; $ANTLR start "rel_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2009:1: rel_op : ( rel_op_eq | rel_op_ne | rel_op_gt | rel_op_gte | rel_op_lt | rel_op_lte | STR_OP_MATCHES ); $ANTLR start "rel_op_eq" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2018:1: rel_op_eq : ( STR_OP_EQ | NUM_OP_EQ ); $ANTLR start "rel_op_gt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2024:1: rel_op_gt : ( STR_OP_GT | NUM_OP_GT ); $ANTLR start "rel_op_gte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2027:1: rel_op_gte : ( STR_OP_GTE | NUM_OP_GTE ); $ANTLR start "rel_op_lt" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2030:1: rel_op_lt : ( STR_OP_LT | NUM_OP_LT ); $ANTLR start "rel_op_lte" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2033:1: rel_op_lte : ( STR_OP_LTE | NUM_OP_LTE ); $ANTLR start "rel_op_ne" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2021:1: rel_op_ne : ( STR_OP_NE | NUM_OP_NE ); $ANTLR start "rel_str_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:2036:1: rel_str_op returns [String id] : ( STR_OP_EQ | STR_OP_NE | STR_OP_GT | STR_OP_LT | STR_OP_GTE | STR_OP_LTE | STR_OP_MATCHES ); $ANTLR start "sample_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1180:1: sample_clause returns [String alias] : ^( SAMPLE rel ( DOUBLENUMBER | expr[exprPlan] ) ) ; $ANTLR start "scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1824:1: scalar returns [Object value, byte type] : ( num_scalar | QUOTEDSTRING | NULL | TRUE | FALSE ); $ANTLR start "ship_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:273:1: ship_clause[List<String> paths] : ^( SHIP ( path_list[$paths] )? ) ; $ANTLR start "simple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:406:1: simple_type returns [byte datatype] : ( BOOLEAN | INT | LONG | FLOAT | DOUBLE | BIGINTEGER | BIGDECIMAL | DATETIME | CHARARRAY | BYTEARRAY ); $ANTLR start "split_branch" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1702:1: split_branch : ^( SPLIT_BRANCH alias cond[splitPlan] ) ; $ANTLR start "split_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1691:1: split_clause : ^( SPLIT rel ( split_branch )+ ( split_otherwise )? ) ; $ANTLR start "split_otherwise" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1716:1: split_otherwise : ^( OTHERWISE alias ( ALL )? ) ; $ANTLR start "split_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:180:1: split_statement : split_clause ; $ANTLR start "statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:162:1: statement : ( general_statement | split_statement | realias_statement | assert_statement | register_statement ); $ANTLR start "store_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:696:1: store_clause returns [String alias] : ^( STORE rel filename ( func_clause[FunctionType.STOREFUNC] )? ) ; $ANTLR start "stream_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1652:1: stream_clause returns [String alias] : ^( STREAM rel ( EXECCOMMAND | IDENTIFIER ) ( as_clause )? ) ; $ANTLR start "stream_cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:292:1: stream_cmd[boolean in] returns [HandleSpec handleSpec] : ( ^( STDIN ( func_clause[ft] )? ) | ^( STDOUT ( func_clause[ft] )? ) | ^( QUOTEDSTRING ( func_clause[ft] )? ) ); $ANTLR end "rel_str_op" $ANTLR start synpred147_LogicalPlanGenerator $ANTLR end synpred147_LogicalPlanGenerator $ANTLR start synpred148_LogicalPlanGenerator $ANTLR end synpred196_LogicalPlanGenerator Delegated rules $ANTLR end synpred148_LogicalPlanGenerator $ANTLR start synpred149_LogicalPlanGenerator $ANTLR end synpred149_LogicalPlanGenerator $ANTLR start synpred179_LogicalPlanGenerator $ANTLR end synpred179_LogicalPlanGenerator $ANTLR start synpred196_LogicalPlanGenerator $ANTLR start "tuple" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1923:1: tuple returns [Tuple value] : ^( TUPLE_VAL ( literal )* ) ; $ANTLR start "tuple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:419:1: tuple_type returns [LogicalSchema logicalSchema] : ^( TUPLE_TYPE ( field_def_list )? ) ; $ANTLR start "tuple_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:934:1: tuple_type_cast returns [LogicalSchema logicalSchema] : ^( TUPLE_TYPE_CAST ( type_cast )* ) ; $ANTLR start "type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:384:1: type returns [Byte datatype, LogicalSchema logicalSchema] : ( simple_type | tuple_type | bag_type | map_type ); $ANTLR start "type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:915:1: type_cast returns [LogicalFieldSchema fieldSchema] : ( simple_type | map_type | tuple_type_cast | bag_type_cast ); $ANTLR start "union_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:1416:1: union_clause returns [String alias] : ^( UNION ( ONSCHEMA )? rel_list ) ; $ANTLR start "var_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/LogicalPlanGenerator.g:951:1: var_expr[LogicalExpressionPlan plan] returns [LogicalExpression expr] : projectable_expr[$plan] ( dot_proj | pound_proj )* ;
Add rule to ruleSet if its mandatory, or has not been disabled.

Validates logical operators as defined in the logical plan of a pig script.

Do some basic equality checks on two relational operators.  Equality is defined here as having equal schemas and  predecessors that are equal. This is intended to be used by operators' equals methods. In the case of an operation which manipualtes columns (such as a foreach or a join) it is possible for multiple columns to have been derived from the same column and thus have duplicate UID's. This detects that case and resets the uid. See PIG-3020 and PIG-3093 for more information. Get the alias of this operator.  That is, if the Pig Latin for this operator was 'X = sort W by $0' then the alias will be X.  For store and split it will be the alias being stored or split.  Note that because of this this alias is not guaranteed to be unique to a single operator. Get the line number in the submitted Pig Latin script where this operator occurred. Get the requestedParallelism for this operator. Get the schema for the output of this relational operator.  This does not merely return the schema variable.  If schema is not yet set, this will attempt to construct it.  Therefore it is abstract since each operator will need to construct its schema differently. Only to be used by unit tests.  This is a back door cheat to set the schema without having to calculate it.  This should never be called by production code, only by tests. Reset the schema to null so that the next time getSchema is called the schema will be regenerated from scratch. Erase all cached uid, regenerate uid when we regenerating schema. This process currently only used in ImplicitSplitInsert, which will insert split and invalidate some uids in plan
Add a field to this schema. Recursively compare two schemas to check if the input schema can be cast to the cast schema Recursively compare two schemas for equality Look for the index of the field that contains the specified uid Fetch a field by field number Fetch a field by alias Given an alias name, find the associated LogicalFieldSchema. If exact name is not found see if any field matches the part of the 'namespaced' alias. eg. if given alias is nm::a , and schema is (a,b). It will return FieldSchema of a. if given alias is nm::a and schema is (nm2::a, b), it will return null Get all fields Two schemas are equal if they are of equal size and their fields schemas considered in order are equal. This function does not compare the alias of the fields. Two schemas are equal if they are of equal size and their fields schemas considered in order are equal. If compareAlias argument is set to true, the alias of the fields are also compared. Merge two schemas. If one of the aliases is of form 'nm::str1', and other is of the form 'str1', this returns str1 Merges two schemas using their column aliases (unlike mergeSchema(..) functions which merge using positions) Schema will not be merged if types are incompatible, as per DataType.mergeType(..) For Tuples and Bags, SubSchemas have to be equal be considered compatible Merges collection of schemas using their column aliases (unlike mergeSchema(..) functions which merge using positions) Schema will not be merged if types are incompatible, as per DataType.mergeType(..) For Tuples and Bags, SubSchemas have to be equal be considered compatible * Old Pig schema does not require a tuple schema inside a bag; Now it is required to have that; this method is to fill the gap Reset uids of all fieldschema that the schema contains Get the size of the schema.

java level API





(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
Used for compiling blocking operators. If there is a single input and its map phase is still open, then close it so that further operators can be compiled into the reduce phase. If its reduce phase is open, add a store and close it. Start a new map MROper into which further operators can be compiled into. If there are multiple inputs, the logic is to merge all map MROpers into one map MROper and retain the reduce MROpers. Since the operator is blocking, it has to be a Global Rerrange at least now. This operator need not be inserted into our plan as it is implemented by hadoop. But this creates the map-reduce boundary. So the merged map MROper is closed and its reduce phase is started. Depending on the number of reduce MROpers and the number of pipelines in the map MRoper a Union operator is inserted whenever necessary. This also leads to the possibility of empty map plans. So have to be careful while handling it in the PigMapReduce class. If there are no map plans, then a new one is created as a side effect of the merge process. If there are no reduce MROpers, and only a single pipeline in the map, then no union oper is added. Otherwise a Union oper is added to the merged map MROper to which all the reduce MROpers are connected by store-load combinations. Care is taken to connect the MROpers in the MRPlan. The front-end method that the user calls to compile the plan. Assumes that all submitted plans have a Store operators as the leaf. Compiles the plan below op into a MapReduce Operator and stores it in curMROp. Connect the reduce MROpers to the leaf node in the map MROper mro by adding appropriate loads Force an end to the current map reduce job with a store into a temporary file. Use Mult File Combiner to concatenate small input files Sets up the indexing job for map-side cogroups. Used to get the compiled plan Used to get the plan that was compiled Create a sampling job to collect statistics by sampling an input file. The sequence of operations is as following: <li>Transform input sample tuples into another tuple.</li> <li>Add an extra field &quot;all&quot; into the tuple </li> <li>Package all tuples into one bag </li> <li>Add constant field for number of reducers. </li> <li>Sorting the bag </li> <li>Invoke UDF with the number of reducers and the sorted bag.</li> <li>Data generated by UDF is stored into a file.</li> Create Sampling job for skewed join. Returns a temporary DFS Path Merges the map MROpers in the compiledInputs into a single merged map MRoper and returns a List with the merged map MROper as the first oper and the rest being reduce MROpers. Care is taken to remove the map MROpers that are merged from the MRPlan and their connections moved over to the merged map MROper. Merge is implemented as a sequence of binary merges. merge(PhyPlan finPlan, List<PhyPlan> lst) := finPlan,merge(p) foreach p in lst The merge of a list of map plans A map MROper is an MROper whose map plan is still open for taking more non-blocking operators. A reduce MROper is an MROper whose map plan is done but the reduce plan is open for taking more non-blocking opers. Used for compiling non-blocking operators. The logic here is simple. If there is a single input, just push the operator into whichever phase is open. Otherwise, we merge the compiled inputs into a list of MROpers where the first oper is the merged oper consisting of all map MROpers and the rest are reduce MROpers as reduce plans can't be merged. Then we add the input oper op into the merged map MROper's map plan as a leaf and connect the reduce MROpers using store-load combinations to the input operator which is the leaf. Also care is taken to connect the MROpers according to the dependencies. Starts a new MRoper and connects it to the old one by load-store. The assumption is that the store is already inserted into the old MROper. For the counter job, it depends if it is row number or not. In case of being a row number, any previous jobs are saved and POCounter is added as a leaf on a map task. If it is not, then POCounter is added as a leaf on a reduce task (last sorting phase). This is an operator which will have multiple inputs(= to number of join inputs) But it prunes off all inputs but the fragment input and creates separate MR jobs for each of the replicated inputs and uses these as the replicated files that are configured in the POFRJoin operator. It also sets that this is FRJoin job and some parametes associated with it. Leftmost relation is referred as base relation (this is the one fed into mappers.) First, close all MROpers except for first one (referred as baseMROPer) Then, create a MROper which will do indexing job (idxMROper) Connect idxMROper before the mappedMROper in the MRPlan. Since merge-join works on two inputs there are exactly two MROper predecessors identified  as left and right. Instead of merging two operators, both are used to generate a MR job each. First MR oper is run to generate on-the-fly index on right side. Second is used to actually do the join. First MR oper is identified as rightMROper and second as curMROper. 1) RightMROper: If it is in map phase. It can be preceded only by POLoad. If there is anything else in physical plan, that is yanked and set as inner plans of joinOp. If it is reduce phase. Close this operator and start new MROper. 2) LeftMROper:  If it is in map phase, add the Join operator in it. If it is in reduce phase. Close it and start new MROper. In case of PORank, it is closed any other previous job (containing POCounter as a leaf) and PORank is added on map phase. The visitOp methods that decide what to do with the current operator Compiles a split operator. The logic is to close the split job by replacing the split oper by a store and creating a new Map MRoper and return that as the current MROper to which other operators would be compiled into. The new MROper would be connected to the split job by load-store. Also add the split oper to the splitsSeen map.




Get all paths for intermediate data. visit() must be called before this.


(non-Javadoc) @see java.lang.Object#toString()
Updates the statistics after a patch of jobs is done Add stats for a new Job, which doesn't yet need to be completed. Logs the statistics in the Pig log file at INFO level Returns the count for the given counter name in the counter group 'MultiStoreCounters' Starts collecting statistics for the given MR plan Stops collecting statistics for a MR plan Updates the {@link JobGraph} of the {@link PigStats}. The initial {@link JobGraph} is created without job ids using {@link MROperPlan}, before any job is submitted for execution. The {@link JobGraph} then is updated with job ids after jobs are executed.



Convert MR settings to Tez settings and set on conf. Process the mapreduce configuration settings and - copy as is the still required ones (like those used by FileInputFormat/FileOutputFormat) - convert and set equivalent tez runtime settings - handle compression related settings Set config with Scope.Vertex in TezConfiguration on the vertex Exact copy of private method from from org.apache.tez.mapreduce.hadoop.MRInputHelpers Update provided localResources collection with the required local resources needed by MapReduce tasks with respect to Input splits. Write input splits (job.split and job.splitmetainfo) to disk
Get a simple POForEach: ForEach X generate flatten($1) simpleConnectMapToReduce is a utility to end a map phase and start a reduce phase in a mapreduce operator: 1. mro only contains map plan 2. need to add POLocalRearrange to end map plan, and add POPackage to start a reduce plan 3. POLocalRearrange/POPackage are trivial
TODO jz: log4j.properties should be used instead Returns the built time of the Pig build being run. Returns the major version of Pig being run. Returns the major version of the Pig build being run. Returns the patch version of the Pig build being run. Returns the svn revision number of the Pig build being run. Returns a version string formatted similarly to that of svn. <pre> Apache Pig version 0.11.0-SNAPSHOT (r1202387) compiled Nov 15 2011, 15:22:09 </pre> The Main-Class for the Pig Jar that will provide a shell and setup a classpath appropriate for executing Jar files.  Warning, this method calls System.exit(). returns the stream of final pig script to be passed to Grunt Print usage string.

This function returns a set of Uids corresponding to map datatype in the first level of this schema This function checks if the schema has a map. We dont check for a nested structure.


Get all files recursively from the given list of files Returns the total number of bytes for this file, or if a directory all files in the directory. Loads the key distribution sampler file Sets up output and log dir paths for a multi-store streaming job Sets up output and log dir paths for a single-store streaming job
Get the exception that caused a failure on the backend for a store location (if any). Log the progress and notify listeners if there is sufficient progress If stop_on_failure is enabled and any job has failed, it stops other jobs.
Uses the string representation of the component plans to identify itself.   @Override public String name() { return "MapReduce - " + mKey.toString(); }




Accumulate implementation - calls max() on the incoming tuple set including intermediate tuple if already exists




(non-Javadoc) @see org.apache.pig.LoadFunc#getInputFormat() (non-Javadoc) @see org.apache.pig.LoadFunc#getLoadCaster() (non-Javadoc) @see org.apache.pig.LoadFunc#prepareToRead(org.apache.hadoop.mapreduce.RecordReader, org.apache.hadoop.mapreduce.InputSplit) (non-Javadoc) @see org.apache.pig.LoadFunc#setLocation(java.lang.String, org.apache.hadoop.mapreduce.Job)
Given a method and a class, this will return true if the method is declared in the class, and if it is, if the NotImplemented annotation is present. This method will recurse through the parent class hierarchy until it finds the first instance of the method at hand, and then it will return accordingly. This implements a stripped down version of method equality. method.equals(method) checks to see whether the declaring classes are equal, which we do not want. Instead, we just want to know if the methods are equal assuming that they come from the same class hierarchy (ie generated code which extends SchemaTuple).




Returns the single instance of class MiniGenericCluster that represents the resources for a mini dfs cluster and a mini mr (or tez) cluster. The system property "test.exec.type" is used to decide whether a mr or tez mini cluster will be returned. Throw RunTimeException if isSetup is false



Number of time units after which the execution should be halted and default returned. UDF author can implement a static extension of MonitoredUDFExecutor.ErrorCallback and provide its class to the annotation in order to perform custom error handling. Time Units in which to measure timeout value.
This method *MUST* be called in the finish by POUserFunc. Though we do use an ExitingExecutorService just in case.




Get the collection of values associated with a given key. Get a set of all the keys in this map. Add an element to the map. Add a key to the map with a collection of elements. Remove one value from an existing key.  If that is the last value for the key, then remove the key too. Remove all the values associated with the given key Get the number of keys in the map. Get a single collection of all the values in the map.  All of the values in the map will be conglomerated into one collection.  There will not be any duplicate removal.
Removes the specified MR operator from the plan after the merge. Connects its predecessors and successors to the merged MR operator
Merge every operators in planB to operator "to" of planA

Adds a list of IsKeyWrapped boolean values Appends the specified package object to the end of the package list. Appends the specified package object to the end of the package list. Returns the list of booleans that indicates if the key needs to unwrapped for the corresponding plan. Constructs the output tuple from the inputs. <p> The output is consumed by for the demultiplexer operator (PODemux) in the format (key, {bag of tuples}) where key is an indexed WritableComparable, not the wrapped value as a pig type. Returns the list of packages.






java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping() This is needed to make sure that both bytearrays and chararrays can be passed as arguments This method gives a name to the column.





Find single relational operator in plan that is instance of class c Find relational operators that are instance of class c
@Override
This method tells the node to add its argument to the node's list of children. This method is called after all the child nodes have been added. This method returns a child node.  The children are numbered from zero, left to right. Return the number of children the node has. This method is called after the node has been made the current node.  It indicates that child nodes can now be added to it. This pair of methods are used to inform the node of its parent.
Returns the NodeIdGenerator singleton. Returns the next ID to be used for the current Thread. Reset all scope IDs to 0 for the current Thread. Reset the given scope IDs to 0 for the current Thread.
* This method does matching between vertices in two given plans.


(non-Javadoc) @see org.apache.pig.data.BagFactory#newDefaultBag(java.util.List)


(non-Javadoc) @see java.lang.Object#equals(java.lang.Object) Read a bag from disk. Report progress to HDFS. Write the bag into a string. Write a bag's contents to disk.
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping() This is needed to make sure that both bytearrays and chararrays can be passed as arguments






















Convenience method to cache objects in ObjectRegistry for a vertex Returns the tez ObjectRegistry which allows caching of objects at the Session, DAG and Vertex level on container reuse for better performance and savings Convenience method to retrieve objects cached for the vertex from ObjectRegistry For internal use only. This method to be called only by PigProcessor

Accept a visitor at this node in the graph. Add an annotation to a node in the plan. Look to see if a node is annotated. Get the plan associated with this operator. This is like a shallow equals comparison. It returns true if two operators have equivalent properties even if they are different objects. Here properties mean equivalent plan and equivalent name. Remove an annotation
Utility function for creating operator keys.
Add a new operator to the plan.  It will not be connected to any existing operators. Connect two operators in the plan, controlling which position in the edge lists that the from and to edges are placed. Connect two operators in the plan. Create an soft edge between two nodes. Disconnect two operators in the plan. Get an iterator of all operators in this plan For a given operator, get all operators immediately before it in the plan. Get all operators in the plan that have no successors. For a given operator, get all operators softly immediately before it in the plan. For a given operator, get all operators softly immediately after it. Get all operators in the plan that have no predecessors. For a given operator, get all operators immediately after it. This method insert node operatorToInsert between pred and succ. Both pred and succ cannot be null This is like a shallow comparison. Two plans are equal if they have equivalent operators and equivalent structure. check if there is a path in the plan graph between the load operator to the store operator. Remove an operator from the plan. This method remove a node operatorToRemove. It also Connect all its successors to predecessor/connect all it's predecessors to successor Remove an soft edge This method replace the oldOperator with the newOperator, make all connection to the new operator in the place of old operator Get number of nodes in the plan.
////////////// Helpers ////////////////// * Construct the plan based on the given Dot graph * This method has be overridden to instantiate the correct vertex type * Helper for retrieving operator key from encoded attributes. By default, it will look for "key" in attributes. If no key is found, an arbitrary one will be generated. * This method is used for loading an operator plan encoded in Dot format * Convenient method for loading directly from file

* This method has to be called after the visit is totally finished only

Find generate op from the foreach operator. Helper method to find if a given LOForEach instance contains any flatten fields. Check if a given LOGenerate operator has any flatten fields. Helper method to determine if the logical expression plan for a Filter contains non-deterministic operations and should therefore be treated extra carefully during optimization.


this is a simple example - more complex comparison will require breakout of the individual values. I suggest you'll have to convert "catch(IOException e) to RuntimeException('msg', e)"
this is a simple example - more complex comparison will require breakout of the individual values. I suggest you'll have to convert "catch(IOException e) to RuntimeException('msg', e)"


The WritableComparable object returned will be used to compare the position of different splits in an ordered stream
Certain operators may buffer the output. We need to flush the last set of records from such operators, when we encounter the last input record, before calling getNextTuple() for the last time.
Bind the <code>OutputHandler</code> to the <code>InputStream</code> from which to read the output data of the managed process. Close the <code>OutputHandler</code>. Get the next output <code>Tuple</code> of the managed process. Get the handled <code>OutputType</code>.




This method is called to cleanup the store/output location of this {@link StoreFunc}. This method is called by the Pig runtime to determine whether to ignore output validation problems (see {@link PigOutputFormat#checkOutputSpecs} and {@link InputOutputFileValidator#validate}) and to delete the existing output.

Get child expressions of this expression Get condition Get left expression Get right expression

Set your broadcast variable name so that BroadcastConverter can put this broadcasted variable in a map which can be referenced by other functions / closures in Converters

Get child expression of this expression

Overridden since the attachment of the new input should cause the old processing to end.
Add current task id and local counter value. Sequential counter used at ROW NUMBER and RANK BY DENSE mode Initialization step into the POCounter is to set up local counter to 1. Dense Rank flag Row number flag Operation ID: identifier shared within the corresponding PORank Task ID: identifier of the task (map or reducer)



Appends the specified plan at the end of the list. Returns the list of inner plans. Returns a flag indicating if this operator is in a combiner. Sets a flag indicating if this operator is in a combiner.
(non-Javadoc) @see org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator#clone()
Configures the Local Rearrange operators & the foreach operator Extracts the value tuple from the LR operator's output tuple Builds the HashMaps by reading each replicated input from the DFS using a Load operator

Builds the HashMaps by reading replicated inputs from broadcast edges
Attaches the proccesed input tuple to the expression plan and checks if comparison operator returns a true. If so the tuple is not filtered and let to pass through. Else, further input is processed till a tuple that can be passed through is found or EOP is reached.
Make a deep copy of this operator.  Calls getNext on the generate operator inside the nested physical plan and returns it maintaining an additional state to denote the begin and end of the nested plan processing.





Counts the number of tuples processed into static variable soFar, if the number of tuples processed reach the limit, return EOP; Otherwise, return the tuple
The main method used by this operator's successor to read tuples from the specified file using the specified load function. Set up the loader by 1) Instantiating the load func 2) Opening an input stream to the specified file and 3) Binding to the input stream at the specified offset. At the end of processing, the inputstream is closed using this method
Overridden since the attachment of the new input should cause the old processing to end. Make a deep copy of this operator. Calls getNext on the generate operator inside the nested physical plan. Converts the generated tuple into the proper format, i.e, (key,indexedTuple(value))        Sets the co-group index of this operator Sets the multi-query index of this operator



Set to POStatus.STATUS_EOP (default) for MR and POStatus.STATUS_NULL for Tez. This is because: For MR, we send EOP at the end of every record For Tez, we only use a global EOP, so send NULL for end of record
Configures the Local Rearrange operators to get keys out of tuple. This provides a List to store Tuples in. The implementation of that list depends on whether or not there is a TupleFactory available. This is a helper method that sets up all of the TupleFactory members.  (non-Javadoc) @see org.apache.pig.impl.plan.Operator#supportsMultipleOutputs()
if there is a argument that starts with "-D", unquote the value part to support use case in PIG-1917



Make a deep copy of this operator. Calls getNext on the generate operator inside the nested physical plan and returns it maintaining an additional state to denote the begin and end of the nested plan processing.

Attaches the required inputs Make a deep copy of this operator. attachInput's better half! From the inputs, constructs the output tuple for this co-group in the required format which is (key, {bag of tuples from input 1}, {bag of tuples from input 2}, ...)

For each entry in rawInputMap, feed the list of tuples into the aggregator funcs and add the results to processedInputMap. Remove the entries from rawInputMap as we go. Runs the provided key-value pair through the aggregator plans.
Make a deep copy of this operator. Returns bag of tuples Calls getNext on the generate operator inside the nested physical plan. Converts the generated tuple into the proper format, i.e, (key,indexedTuple(value))  Loads the key distribution file obtained from the sampler
Returns bag of tuples Calls getNext on the generate operator inside the nested physical plan. Converts the generated tuple into the proper format, i.e, (key,indexedTuple(value))
Update the average tuple size base on newly sampled tuple t and recalculate skipInterval

Overridden since the attachment of the new input should cause the old processing to end. Calls getNext on the generate operator inside the nested physical plan. Converts the generated tuple into the proper format, i.e, (key,indexedTuple(value))

Add i'th column from inpValue to objList Overridden since the attachment of the new input should cause the old processing to end.  Fetches the input tuple and returns the requested column Asked for Tuples. Check if the input is a bag. If so, stream the tuples in the bag instead of the entire bag.
Reads the output tuple from POCounter and the cumulative sum previously calculated. Here is read the task identifier in order to get the corresponding cumulative sum, and the local counter at the tuple. These values are summed and prepended to the tuple. Operation ID: identifier shared within the corresponding POCounter




See PIG-4644 (non-Javadoc) @see org.apache.pig.backend.hadoop.executionengine.physicalLayer.PhysicalOperator#reset()




Previously, we reused the same Result object for all results, but we found certain operators (e.g. POStream) save references to the Result object and expect it to be constant.



Appends the specified plan to the end of the nested input plan list Returns the list of nested plans. Returns the name of the file associated with this operator Removes plan from the nested input plan list Sets the name of the file associated with this operator

To perform cleanup when there is an error.    Set up the storer Called at the end of processing for clean up.
To perform cleanup when there is an error. Uses the FileLocalizer method which only deletes the file but not the dirs created with it. Set up the storer At the end of processing, the outputstream is closed using this method

Get the {@link StreamingCommand} for this <code>StreamSpec</code>.
The code below, tries to follow our single threaded shared execution model with execution being passed around each non-drained input
Get child expressions of this expression
Get child expression of this expression Sets EvalFunc's inputschema based on the signature

Sets tuple with task index and record index as the key. For eg: (0,1), (0,2), etc Default is empty key



(non-Javadoc) @see java.lang.Object#toString()


Reinitialise. Reinitialise. Reinitialise. Reinitialise. Disable tracing. Enable tracing. Generate ParseException. Get the next Token. Get the specific Token.

Reinitialise parser. Reinitialise parser. Switch to specified lex state. Get the next Token. Set debug output.

This is the main API that takes script template and produces pig script Kept for compatibility with old interface

{@inheritDoc }
Used to convert raw characters to their escaped version when these raw version cannot be used as part of an ASCII string literal. It uses "currentToken" and "expectedTokenSequences" to generate a parse error message and returns it.  If this object has been created due to a parse error, and you do not catch it (it gets thrown from the parser) the correct error message gets displayed.





first field in the input tuple is the number of reducers second field is the *sorted* bag of samples this should be called only once the last field of the tuple is a tuple for memory size and disk size


Evaluates the partitionExpression set in the HiveColumnarLoader.setPartitionExpression. * @ Returns the partition keys for a location.<br/> The work is delegated to the PathPartitioner class Recursively works through all directories, skipping filtered partitions. Returns the Partition keys and each key's value for a single location.<br/> That is the location must be something like mytable/partition1=a/partition2=b/myfile.<br/> This method will return a map with [partition1='a', partition2='b']<br/> The work is delegated to the PathPartitioner class This method is called by the FileInputFormat to find the input paths for which splits should be calculated.<br/> If applyDateRanges == true: Then the HiveRCDateSplitter is used to apply filtering on the input files.<br/> Else the default FileInputFormat listStatus method is used. Sets the PARITITION_FILTER_EXPRESSION property in the UDFContext identified by the loaderClass. Reads the partition keys from the location i.e the base directory
Searches for the key=value pairs in the path pointer by the location parameter. Note: this must be the path lowes in the Searches for the key=value pairs in the path pointer by the location parameter. Will look for key=value pairs in the path for example: /user/hive/warehouse/mylogs/year=2010/month=07
Set isLeafNode to true if the node must be a source  Set isSourceNode to true if the node must be a source
This function can be used to create a new PatternPlan if the pattern nodes have at most one parent/child, and they are connected to each other. The PatternNode corresponding to the i'th class in classList will be the predecessor of the one corresponding to i+1'th class. try matching list of pattern nodes with list of plan nodes . these are either predecessors or successors of a matching node if pattern nodes is a ordered subset of plan nodes, return true Check if the pattern node ptNode matches given Operator plOp to suppress warnings from currentPlan.getPredecessors and getSuccessors. Return true if the given plan has nodes that match the pattern represented by this class If a match is found, the PatterNodes in the plan will return non null node for getMatch(). Reset the matching information if the pattern has been used to find a match
Dump the total time, total number of starts and stops, and average run time of the timer to an output stream. Start the timer. Stop the timer.
Call print on all of the known performance timers. Get the timer factory. Get a performance timer.  If the indicated timer does not exist, it will be created.  If a timer of that name already exists, it will be returned.
@Override public void visitPartitionRearrange(POPartitionRearrange lrfi) throws VisitorException { super.visitPartitionRearrange(lrfi); lrfi.setParentPlan(parent); }

Shorts the input path of this operator by providing the input tuple directly Make a copy of this operator. This function is blank, however, we should leave a place holder so that the subclasses can clone to make deep copy as this one creates a shallow copy of non-primitive types (objects, arrays and lists) Detaches any tuples that are attached Implementations that call into the different versions of getNext are often identical, differing only in the signature of the getNext() call they make. This method allows to cut down on some of the copy-and-paste.  A blocking operator should override this to return true. Blocking operators are those that need the full bag before operate on the tuples inside the bag. Example is the Global Rearrange. Non-blocking or pipeline operators are those that work on a tuple by tuple basis. A generic method for parsing input that either returns the attached input if it exists or fetches it from its predecessor. If special processing is required, this method should be overridden. Reset internal state in an operator.  For use in nested pipelines where operators like limit and sort may need to reset their state. Limit needs it because it needs to know it's seeing a fresh set of input.  Blocking operators like sort and distinct need it because they may not have drained their previous input due to a limit and thus need to be told to drop their old input and start over.   @StaticDataCleanup
Write a visual representation of the Physical Plan into the given output stream Write a visual representation of the Physical Plan into the given output stream Write a visual representation of the Physical Plan into the given printstream (non-Javadoc) @see org.apache.pig.impl.plan.OperatorPlan#add(org.apache.pig.impl.plan.Operator) @Override public void add(PhysicalOperator op) { // attach this plan as the plan the operator is part of //op.setParentPlan(this); super.add(op); } (non-Javadoc) @see org.apache.pig.impl.plan.OperatorPlan#replace(org.apache.pig.impl.plan.Operator, org.apache.pig.impl.plan.Operator)

Bind a Pig object to variables in the host language (optional operation).  This does an implicit mapping of variables in the host language to parameters in Pig Latin.  For example, if the user provides a Pig Latin statement <tt> p = Pig.compile("A = load '$input';");</tt> and then calls this function it will look for a variable called <tt>input</tt> in the host language.  Scoping rules of the host language will be followed in selecting which variable to bind.  The variable bound must contain a string value.  This method is optional because not all host languages may support searching for in scope variables. Bind this to multiple sets of variables.  This will cause the Pig Latin script to be executed in parallel over these sets of variables. ------------------------------------------------------------------------- Bind this to a set of variables. Values must be provided for all Pig Latin parameters. Define a Pig pipeline. Define a named portion of a Pig pipeline.  This allows it to be imported into another pipeline. Define a Pig pipeline based on Pig Latin in a separate file. Define a named Pig pipeline based on Pig Latin in a separate file. This allows it to be imported into another pipeline. Define an alias for a UDF or a streaming command.  This definition will then be present for <b>all subsequent</b> Pig pipelines defined in this script.  If you wish to define it for only a single Pig pipeline, use define within that definition. Escape the $ so that we can use the parameter substitution to perform bind operation. Parameter substitution will un-escape $ Run a filesystem command.  Any output from this command is written to stdout or stderr as appropriate. ------------------------------------------------------------------------- Register a jar for use in Pig.  Once this is done this jar will be registered for <b>all subsequent</b> Pig pipelines in this script. If you wish to register it for only a single Pig pipeline, use register within that definition. Register scripting UDFs for use in Pig. Once this is done all UDFs defined in the file will be available for <b>all subsequent</b> Pig pipelines in this script. If you wish to register UDFS for only a single Pig pipeline, use register within that definition. Replaces the $<identifier> with their actual values Set a variable for use in Pig Latin.  This set will then be present for <b>all subsequent</b> Pig pipelines defined in this script.  If you wish to set it for only a single Pig pipeline, use set within that definition. Run a sql command.  Any output from this command is written to stdout or stderr as appropriate.

Called by the default implementation of {@link #readArray} to add a value. Overridden to append to pig bag. Called to create an enum value. Overridden to create a pig string. Called to create new array instances. Overridden to return a new bag. Called to create new record instances. Overridden to return a new tuple. Called by the default implementation of {@link #readArray} to retrieve a value from a reused instance. Called to read byte arrays. Overridden to return a pig byte array. Called to read a fixed value. Overridden to read a pig byte array. Called to read a map instance. Overridden to read a pig map. Called to read a record instance. Overridden to read a pig tuple. Called to read strings. Overridden to return a pig string.
Called by the implementation of {@link #writeArray} to enumerate array elements. Called by the implementation of {@link #writeArray} to get the size of an array. Called by the implementation of {@link #writeRecord} to retrieve a record field value. Recursively check whether "datum" is an instance of "schema" and called by {@link #resolveUnionSchema(Schema,Object)}, {@link #unwrappedInstanceOf(Schema,Object)}. As of Avro 1.5.1 this method is now in the superclass so it's no longer needed here, but leaving here for backward compatibility with Avro 1.4.1. Called to resolve union. Check whether "datum" is an instance of "schema" after stripping the tuple wrapper. Write boolean. Users can cast an integer into boolean. Called to write a bytes. Write double. Users can cast long, float and integer to double. Called to write a fixed value. Write float. Users can cast long and integer into float. Write long. Users can cast integer into long. Overriding to fetch the field value from the Tuple. Called to write union.
Create and return an avro record reader. It uses the input schema passed in to the constructor. This is to support multi-level/recursive directory listing until MAPREDUCE-1577 is fixed.
Enable output compression using the deflate codec and specify its level.
Remap the position of fields to the merged schema Wrap non-tuple value as a tuple

Compare two NullableBigDecimalWritables as raw bytes.  If neither are null, then BigDecimalWritable.compare() is used.  If both are null then the indices are compared. Otherwise the null one is defined to be less.
Compare two NullableBigIntegerWritables as raw bytes.  If neither are null, then BigIntegerWritable.compare() is used.  If both are null then the indices are compared. Otherwise the null one is defined to be less.
Compare two NullableIntWritables as raw bytes.  If neither are null, then BooleanWritable.compare() is used.  If both are null then the indices are compared.  Otherwise the null one is defined to be less.
Compare two NullableBytesWritables as raw bytes. If both are null, then the indices are compared. If neither are null and both are bytearrays, then direct Writable.compareBytes is used. For non-bytearrays, we use BinInterSedesTupleRawComparator. If either is null, null one is defined to be less.

Validates a Pig command as defined by {@link Command}.




Add a path to be skipped while automatically shipping binaries for streaming. calls: addScriptFile(path, new File(path)), ensuring that a given path is added to the jar at most once. this method adds script files that must be added to the shipped jar named differently from their local fs path. Creates a Classloader based on the passed jarFile and any extra jar files. Create a new {@link ExecutableManager} depending on the ExecType. Get the {@link StreamingCommand} for the given alias.  Check the execution mode and return the appropriate error source Returns the type of execution currently in effect. Get paths which are to skipped while automatically shipping binaries for streaming. Provides configuration information. script files as name/file pairs to be added to the job jar This method is created with the aim of unifying the Grunt and PigServer approaches, so all common initializations can go in here. A common Pig pattern for initializing objects via system properties is to support passing something like this on the command line: <code>-Dpig.notification.listener=MyClass</code> <code>-Dpig.notification.listener.arg=myConstructorStringArg</code> This method will properly initialize the class with the args, if they exist. Adds the specified path to the predeployed jars list. These jars will never be included in generated job jar. <p> This can be called for jars that are pre-installed on the Hadoop cluster to reduce the size of the job jar. Defines an alias for the given function spec. This is useful for functions that require arguments to the constructor. Defines an alias for the given streaming command. This is useful for complicated streaming command specs. @StaticDataCleanup
Mocks the Reporter.incrCounter, but adds buffering. See org.apache.hadoop.mapred.Reporter's incrCounter. Mocks the Reporter.incrCounter, but adds buffering. See org.apache.hadoop.mapred.Reporter's incrCounter.

Compare two NullableIntWritables as raw bytes. If neither are null, then IntWritable.compare() is used. If both are null then the indices are compared. Otherwise the null one is defined to be less.
Compare two NullableIntWritables as raw bytes.  If neither are null, then IntWritable.compare() is used.  If both are null then the indices are compared.  Otherwise the null one is defined to be less.
A static method to determine the error source given the error code Returns the detailed message used by developers for debugging Returns the error code of the exception Returns the error source of the exception. Can be more than one source. Check if this PigException is marked as the ones whose message is to be displayed to the user. This can be used to indicate if the corresponding error message is a good candidate for displaying to the end user, instead of drilling down the stack trace further. Return the location in the source that generated the exception. A static method to query if an error source is due to a bug or not. A static method to query if an error source is due to an input or not. A static method to query if an error source is due to the remote environment or not. A static method to query if an error source is due to the user environment or not. Checks if the exception is retriable. Set the detailed message of the exception Set the error code of the exception Set the error source of the exception Mark this exception as a good candidate for showing its message to the pig user Set the retriable attribute of the exception Returns a short description of this throwable. The result is the concatenation of: <ul> <li> the {@linkplain Class#getName() name} of the class of this object <li> ": " (a colon and a space) <li> "ERROR " (the string ERROR followed by a a space) <li> the result of invoking this object's {@link #getErrorCode} method <li> ": " (a colon and a space) <li> the result of invoking {@link Throwable#getLocalizedMessage() getLocalizedMessage} method </ul> If <tt>getLocalizedMessage</tt> returns <tt>null</tt>, then just the class name is returned.

This is to support multi-level/recursive directory listing until MAPREDUCE-1577 is fixed.
Reinitialise. Reinitialise. Reinitialise. Reinitialise. Disable tracing. Enable tracing. Generate ParseException. Get the next Token. Get the specific Token. match the newlines,spaces and comments match the newlines,spaces and comments match others, see comments above on description of OTHER , NOT_OTHER_CHAR a string that can contain parameter write the newlines,spaces and comments to preserve formatting

Reinitialise parser. Reinitialise parser. Switch to specified lex state. Get the next Token. Set debug output.
Compare two NullableIntWritables as raw bytes.  If neither are null, then IntWritable.compare() is used.  If both are null then the indices are compared.  Otherwise the null one is defined to be less.
Will be called when all the tuples in the input are done. So reporter thread should be closed.  The map function that attaches the inpTuple appropriately and executes the map plan if its not empty. Collects the result of execution into oc or the input directly to oc if map plan empty. The collection is left abstract for the map-only or map-reduce job to implement. Map-only collects the tuple as-is whereas map-reduce collects it after extracting the key and indexed tuple.  for local map/reduce simulation Configures the mapper with the map plan and the reproter thread
@StaticDataCleanup

Get singleton instance of the context

(non-Javadoc) @see org.apache.hadoop.mapreduce.InputFormat#createRecordReader(org.apache.hadoop.mapreduce.InputSplit, org.apache.hadoop.mapreduce.TaskAttemptContext)  (non-Javadoc) @see org.apache.hadoop.mapreduce.InputFormat#getSplits(org.apache.hadoop.mapreduce.JobContext) get the corresponding configuration for the input on which the split is based and merge it with the Conf supplied package level access so that this is not publicly used elsewhere Pass loader signature to LoadFunc and to InputFormat through the conf
This is where we have to wrap PigSplits into SparkPigSplits

Compare two NullableIntWritables as raw bytes.  If neither are null, then IntWritable.compare() is used.  If both are null then the indices are compared.  Otherwise the null one is defined to be less.
The main loop for the thread. The loop does the following: Check the states of the running jobs Update the states of waiting jobs Submit the jobs in ready state
This is a hack to get around the fact that in JRuby 1.6.7, the enumeratorize method isn't public. In 1.7.0, it will be made public and this can be gotten rid of, but until then the Jruby API doesn't provide an easy way (or even a difficult way, really) to provide this functionality; thus, it felt much cleaner to use reflection to make public a method that will soon be public anyway instead of doing something much hairier. This method is called from JRuby to register any classes in the library. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. This is the method which provides conversion from Pig to Ruby. In this case, an instance of the Ruby runtime is necessary. This method provides the general detection of the type and then calls the more specific conversion methods. A type specific conversion routine. A type specific conversion routine for Pig Maps. This only deals with maps with String keys, which is all that Pig supports. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. TODO need to convert to output a String A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. A type specific conversion routine. This method facilitates conversion from Ruby objects to Pig objects. This is a general class which detects the subclass and invokes the appropriate conversion routine. It will fail on an unsupported datatype.
If you have warning messages that need aggregation
Compare two NullableIntWritables as raw bytes.  If neither are null, then IntWritable.compare() is used.  If both are null then the indices are compared.  Otherwise the null one is defined to be less.
check for multi-value return pattern: alias, alias, ..., alias = Macro inline nodes have the following form: (MACRO_INLINE <name> (RETURN_VAL <values>) (PARAMS <values>)) Child nodes: 0: macro name 1: list of return values 2: list of parameters Validates that return alias exists in the macro body.
Get mapper's illustrator context



Compare two nullable objects.  Step one is to check if either or both are null.  If one is null and the other is not, then the one that is null is declared to be less.  If both are null the indices are compared.  If neither are null the indices are again compared.  If these are equal, finally the values are compared. These comparators are used by hadoop as part of the post-map sort, when the data is still in object format.    (non-Javadoc) @see org.apache.hadoop.io.IntWritable#readFields(java.io.DataInput)   (non-Javadoc) @see org.apache.hadoop.io.IntWritable#write(java.io.DataOutput)
This method only be called in 20.203+/0.23 This method only be called in 20.203+/0.23
Check if given property prop is same in configurations conf1, conf2 Before delegating calls to underlying OutputFormat or OutputCommitter Pig needs to ensure the Configuration in the JobContext contains the output location and StoreFunc for the specific store - so set these up in the context for this specific store





Invoked before any Hadoop jobs (or a Tez DAG) are run with the plan that is to be executed. Invoked when a Hadoop job fails. Invoked just after a Hadoop job (or Tez DAG) is completed successfully. Invoked after a Hadoop job (or Tez DAG) is started. Invoked just before submitting a batch of Hadoop jobs (or Tez DAGs). Invoked just after all Hadoop jobs (Tez DAGs) spawned by the script are completed. Invoked just before launching Hadoop jobs (or tez DAGs) spawned by the script. Invoked just after an output is successfully written. Invoked to update the execution progress.
Report progress. Report progress with a message.

Get the record reader for the next chunk in this CombineFileSplit.
Estimate the number of reducers for a given job based on the collection of load funcs passed.

////////////////////////////////////////////////////////// Methods in Set 1: Convert Pig schema to Avro schema ////////////////////////////////////////////////////////// Convert a pig ResourceSchema to avro schema Convert a Pig ResourceFieldSchema to avro schema Convert Pig primitive type to Avro type Convert pig data to Avro record Check whether Avro type is compatible with Pig type This is a painful hack to make unit tests pass. The static counter holds state between unit tests, so it needs to be reset. Otherwise tests will fail if their order is swapped or a new test is added. ////////////////////////////////////////////////////////// Methods in Set 2: Validate whether a Pig schema is compatible with a given Avro schema. ////////////////////////////////////////////////////////// Validate whether pigSchema is compatible with avroSchema Validate whether pigSchema is compatible with avroSchema and convert those Pig fields with to corresponding Avro schemas. Validate a Pig tuple is compatible with Avro record. If the Avro schema is not complete (with uncovered fields), then convert those fields using methods in set 1. Notice that users can get rid of Pig tuple wrappers, e.g. an Avro schema "int" is compatible with a Pig schema "T:(int)"
Reinitialise. Reinitialise. Reinitialise. Reinitialise. Disable tracing. Enable tracing. Generate ParseException. Get the next Token. Get the specific Token.

Reinitialise parser. Reinitialise parser. Switch to specified lex state. Get the next Token. Set debug output.

IndexedKeyPartitioner will put the tuple with same mainKey together, in PigSecondaryKeyComparatorSpark#compare (Object o1, Object o2) we only compare the secondaryKey compare the mainKey and secondaryKey
This is to support multi-level/recursive directory listing until MAPREDUCE-1577 is fixed.
Add a path to be skipped while automatically shipping binaries for streaming. NOTE: For testing only. Don't use. Returns the unused byte capacity of an HDFS filesystem. This value does not take into account a replication factor, as that can vary from file to file. Thus if you are using this to determine if you data set will fit in the HDFS, you need to divide the result of this call by your specific replication setting. Set the logging level to the default. Set the logging level to DEBUG. Delete a file. Discards a batch of Pig commands. Write the schema for an alias to System.out. Write the schema for a nestedAlias to System.out. Denoted by alias::nestedAlias. Compile and execute the current plan. Submits a batch of Pig commands for execution. Submits a batch of Pig commands for execution. Parse and build of script should be skipped if user called {@link PigServer#parseAndBuild()} before. Pass false as an argument in which case. Test whether a file exists. Provide information on how a pig query will be executed.  For now this information is very developer focussed, and probably not very useful to the average user. Provide information on how a pig query will be executed. Returns the length of a file in bytes which exists in the HDFS (accounts for replication). Get the set of all current aliases. Return a map containing the logical plan associated with each alias. Creates a clone of the current DAG Current DAG Retrieves a list of Job objects from the PigStats object Returns data associated with LogicalPlan. It makes sense to call this method only after a query/script has been registered with one of the {@link #registerQuery(String)} or {@link #registerScript(InputStream)} methods. Returns whether there is anything to process in the current batch. Retrieve the current execution mode. A common method for launching the jobs according to the logical plan List the contents of a directory. Make a directory. Executes a Pig Latin script up to and including indicated alias.  That is, if a user does: <pre> PigServer server = new PigServer(); server.registerQuery("A = load 'foo';"); server.registerQuery("B = filter A by $0 &gt; 0;"); server.registerQuery("C = order B by $1;"); </pre> Then <pre> server.openIterator("B"); </pre> filtered but unsorted data will be returned.  If instead a user does <pre> server.openIterator("C"); </pre> filtered and sorted data will be returned. This method parses the scripts and builds the LogicalPlan. This method should be followed by {@link PigServer#executeBatch(boolean)} with argument as false. Do Not use {@link PigServer#executeBatch()} after calling this method as that will re-parse and build the script. Intended to be used by unit tests only. Print a list of all aliases in in the current Pig Latin script.  Output is written to System.out. Universal Scripting Language Support, see PIG-928 Defines an alias for the given function spec. This is useful for functions that require arguments to the constructor. Registers a jar file. Name of the jar file can be an absolute or relative path. If multiple resources are found with the specified name, the first one is registered as returned by getSystemResources. A warning is issued to inform the user. Register a query with the Pig runtime. The query is parsed and registered, but it is not executed until it is needed.  Equivalent to calling {@link #registerQuery(String, int)} with startLine set to 1. Register a query with the Pig runtime. The query is parsed and registered, but it is not executed until it is needed. Register a pig script from InputStream source which is more general and extensible the pig script can be from local file, then you can use FileInputStream. or pig script can be in memory which you build it dynamically, the you can use ByteArrayInputStream even pig script can be in remote machine, which you get wrap it as SocketInputStream Register a pig script from InputStream source which is more general and extensible the pig script can be from local file, then you can use FileInputStream. or pig script can be in memory which you build it dynamically, the you can use ByteArrayInputStream even pig script can be in remote machine, which you get wrap it as SocketInputStream The parameters in the pig script will be substituted with the values in the parameter files Register a pig script from InputStream source which is more general and extensible the pig script can be from local file, then you can use FileInputStream. or pig script can be in memory which you build it dynamically, the you can use ByteArrayInputStream even pig script can be in remote machine, which you get wrap it as SocketInputStream. The parameters in the pig script will be substituted with the values in params Register a pig script from InputStream.<br> The pig script can be from local file, then you can use FileInputStream. Or pig script can be in memory which you build it dynamically, the you can use ByteArrayInputStream Pig script can even be in remote machine, which you get wrap it as SocketInputStream.<br> The parameters in the pig script will be substituted with the values in the map and the parameter files. The values in params Map will override the value in parameter file if they have the same parameter Register a query with the Pig runtime.  The query will be read from the indicated file. Register a pig script file.  The parameters in the file will be substituted with the values in the parameter files Register a pig script file.  The parameters in the file will be substituted with the values in params Register a pig script file.  The parameters in the file will be substituted with the values in the map and the parameter files The values in params Map will override the value in parameter file if they have the same parameter Defines an alias for the given streaming command. Rename a file. Starts batch execution mode. Set the default parallelism for this job Set the name of the job.  This name will get translated to mapred.job.name. Set Hadoop job priority.  This value will get translated to mapred.job.priority. Set whether to skip parsing while registering the query in batch mode This can be called to indicate if the query is being parsed/compiled in a mode that expects each statement to be validated as it is entered, instead of just doing it once for whole script. Reclaims resources used by this instance of PigServer. This method deletes all temporary files generated by the current thread while executing Pig commands. Executes a Pig Latin script up to and including indicated alias and stores the resulting records into a file.  That is, if a user does: <pre> PigServer server = new PigServer(); server.registerQuery("A = load 'foo';"); server.registerQuery("B = filter A by $0 &gt; 0;"); server.registerQuery("C = order B by $1;"); </pre> Then <pre> server.store("B", "bar"); </pre> filtered but unsorted data will be stored to the file <tt>bar</tt>.  If instead a user does <pre> server.store("C", "bar"); </pre> filtered and sorted data will be stored to the file <tt>bar</tt>. Equivalent to calling {@link #store(String, String, String)} with <tt>org.apache.pig.PigStorage</tt> as the store function. Executes a Pig Latin script up to and including indicated alias and stores the resulting records into a file.  That is, if a user does: <pre> PigServer server = new PigServer(); server.registerQuery("A = load 'foo';"); server.registerQuery("B = filter A by $0 &gt; 0;"); server.registerQuery("C = order B by $1;"); </pre> Then <pre> server.store("B", "bar", "mystorefunc"); </pre> filtered but unsorted data will be stored to the file <tt>bar</tt> using <tt>mystorefunc</tt>.  If instead a user does <pre> server.store("C", "bar", "mystorefunc"); </pre> filtered and sorted data will be stored to the file <tt>bar</tt> using <tt>mystorefunc</tt>. <p>
package level access because we don't want LoadFunc implementations to get this information - this is to be used only from PigInputFormat Return the length of a wrapped split   This methods returns the actual InputSplit (as returned by the {@link InputFormat}) which this class is wrapping.  Returns true if the map has multiple inputs, else false (non-Javadoc) Indicates this map has multiple input (such as the result of a join operation).
Returns the total bytes written to user specified HDFS locations of this script. Returns the display message in pig grunt Returns the error code of {@link PigException} Returns error message string Returns the error code of {@link PigException} Returns the DAG of jobs spawned by the script Returns the number of bytes for the given output location, -1 for invalid location or name. Returns the number of jobs for this script Returns the number of records for the given output location, -1 for invalid location or name. Returns the alias associated with this output location Returns the list of output locations in the script Returns the list of output names in the script Returns the properties associated with the script Returns the total number of bags that spilled proactively Returns the total number of records that spilled proactively Returns the total number of records in user specified output locations of this script. Returns code are defined in {@link ReturnCode} Returns the total spill counts from {@link SpillableMemoryManager}. Returns the contents of the script that was run.
Returns the size of output in bytes. If the size of output cannot be computed for any reason, -1 should be returned. Returns whether the given PSStore is supported by this output size reader or not.
Returns an empty PigStats object Use of this method is not advised as it will return the MR execution engine version of PigStats by default, and is not necessarily empty depending on the timing. Returns the counter name for the given input file name Returns the counter name for the given {@link POStore} Returns the PigStats with the given return code
Get singleton instance of the context @StaticDataCleanup
------------------------------------------------------------------------ Implementation of LoadMetaData interface Read the bytes between start and end into a DataByteArray for inclusion in the return tuple. ------------------------------------------------------------------------ Implementation of StoreMetadata






Given a byte array from a streaming executable, produce a tuple. This will be called on the front end during planning and not on the back end during execution. Given a tuple, produce an array of bytes to be passed to the streaming executable.

This is to support multi-level/recursive directory listing until MAPREDUCE-1577 is fixed.

Compare two NullableTextWritables as raw bytes.  If neither are null, then IntWritable.compare() is used.  If both are null then the indices are compared.  Otherwise the null one is defined to be less.
Invoked just after the Tez DAGs is completed (successful or failed). Invoked just before launching a Tez DAG spawned by the script. Invoked to update the execution progress. Invoked after a Tez DAG is started.
Given a tuple, produce an array of bytes to be passed to the streaming executable.
Compare two NullableTuples as raw bytes. If neither are null, then IntWritable.compare() is used. If both are null then the indices are compared. Otherwise the null one is defined to be less.
Compare two NullableTuples as raw bytes. Tuples are compared field-wise. If both are null, then the indices are compared. Otherwise the null one is defined to be less.



This is the public interface. Dump writes the plan and nested plans to the stream. Will be called to dump the edges of the plan. Each edge results in one call. Will be called when an operator has nested plans, which are connected to one of the multiple inputs. Will be called for nested operators, where the plans represent how the output of the operator is processed. Will be called for nested operators. The operators are not specifically connected to any input or output operators of E Will be called to dump a simple operator Used to determine if an operator has nested plans, which are connected to specific input operators. Used to determine if an operator has nested output plans Used to determine if an operator has nested plans (without connections to in- or output operators. Helper function to print a string array. makeDumper is a factory method. Used by subclasses to specify what dumper should handle the nested plan.
Add an element to the map. Remove one value from an existing key and return which position in the arraylist the value was at..  If that is the last value for the key, then remove the key too.


Finds POLocalRearrange from POSplit sub-plan Returns a LinkedList of operators contained within the physical plan which implement the supplied class, in dependency order. Returns an empty LinkedList of no such operators exist. Creates a relative path that can be used to build a temporary place to store the output from a number of map-reduce tasks.
Adds a listener to the optimization.  This listener will be fired after each rule transforms a plan.  Listeners are guaranteed to be fired in the order they are added. Run the optimizer.  This method attempts to match each of the Rules against the plan.  If a Rule matches, it then calls the check method of the associated Transformer to give the it a chance to check whether it really wants to do the optimization.  If that returns true as well, then Transformer.transform is called.

//////////// String Formatting Helpers ////////////// * Report operator1.edges - operator2.edges  (Non-commutative) * Generate inverse map of the given map * This allows different implementation of node matcher * Same as above in case just want to compare but don't want to know the error messages * This method does structural comparison of two plans based on:- - Graph connectivity The current implementation is based on simple key-based vertex matching.
Notification that a plan has been transformed.  The listener is free in this method to make changes to the annotations on the plan now that it has been transformed.

Pop the next to previous walker off of the stack and set it as the current walker.  This will drop the reference to the current walker. Push the current walker onto the stack of saved walkers and begin using the newly passed walker as the current walker. Entry point for visiting the plan.
Set the plan for this walker to operate on. Return a new instance of this same type of walker for a subplan. When this method is called the same type of walker with the provided plan set as the plan, must be returned.  This can then be used to walk subplans.  This allows abstract visitors to clone walkers without knowning the type of walker their subclasses used. Begin traversing the graph.


Update the average tuple size base on newly sampled tuple t and recalculate skipInterval
Begin traversing the graph.


executes the 'cmd' in shell and returns result This method generates value for the specified key by performing substitution if needed within the value first. This method generates value for the specified key by performing substitution if needed within the value first. This method generates parameter value by running specified command This method generates parameter value by running specified command


LOForeach needs special handling because LOInnerLoad's inner ProjectExpression is the one that gets expanded
Find the LogicalRelationalOperator that this projection refers to. Column number this project references.  The column number is the column in the relational operator that contains this expression.  The count is zero based.  Input number this project references.  This is the input number for the relational operator that contains this expression.  The count is zero based.  Set the column number for this project.  This should only be called by ProjectionPatcher.  Stupid Java needs friends. If there is an alias, finds the column number from it.
Create new logical plan with a project that is attached to LogicalRelation attachRel and projects i'th column from input Expand plan into multiple plans if the plan contains a project star, if there is no project star the returned list contains the plan argument. expand this plan containing project star to multiple plans each projecting a single column Find project-star in foreach statement. The LOInnerLoad corresponding to the project-star also needs to have a project-star if LogicalExpressionPlan argument has a project star output then return it, otherwise return null
If the argument project is a project-star or project-range that can be expanded, find the position of first and last columns it should project


Loads default properties. Loads the default properties from pig-default.properties and pig.properties. Finds the file with the given file name in the classpath and loads the properties provided in it. Loads the properties from a given file. Sets properties to their default values if not set by Client



delegates delegators $ANTLR end "OTHERWISE" $ANTLR start "ALL" $ANTLR end "ARROBA" $ANTLR start "AMPERSAND" $ANTLR end "GROUP" $ANTLR start "AND" $ANTLR end "QMARK" $ANTLR start "ARROBA" $ANTLR end "ALL" $ANTLR start "AS" $ANTLR end "FLATTEN" $ANTLR start "ASC" $ANTLR end "STORE" $ANTLR start "ASSERT" $ANTLR end "BYTEARRAY" $ANTLR start "BAG" $ANTLR end "FLOAT" $ANTLR start "BIGDECIMAL" $ANTLR end "DOUBLENUMBER" $ANTLR start "BIGDECIMALNUMBER" $ANTLR end "BIGDECIMAL" $ANTLR start "BIGINTEGER" $ANTLR end "BIGDECIMALNUMBER" $ANTLR start "BIGINTEGERNUMBER" $ANTLR end "DESC" $ANTLR start "BOOLEAN" $ANTLR end "AS" $ANTLR start "BY" $ANTLR end "CHARARRAY" $ANTLR start "BYTEARRAY" $ANTLR end "SHIP" $ANTLR start "CACHE" $ANTLR end "FULL" $ANTLR start "CASE" $ANTLR end "DATETIME" $ANTLR start "CHARARRAY" $ANTLR end "DISTINCT" $ANTLR start "COGROUP" $ANTLR end "STAR" $ANTLR start "COLON" $ANTLR end "EQUAL" $ANTLR start "COMMA" $ANTLR end "JOIN" $ANTLR start "CROSS" $ANTLR end "DENSE" $ANTLR start "CUBE" $ANTLR end "DOUBLE" $ANTLR start "DATETIME" $ANTLR end "ID" $ANTLR start "DCOLON" $ANTLR end "RETURNS" $ANTLR start "DEFINE" $ANTLR end "RANK" $ANTLR start "DENSE" $ANTLR end "ASC" $ANTLR start "DESC" $ANTLR end "NUM_OP_NE" $ANTLR start "DIGIT" $ANTLR end "INVOKE" $ANTLR start "DISTINCT" $ANTLR end "DOUBLE_PERIOD" $ANTLR start "DIV" $ANTLR end "COLON" $ANTLR start "DOLLAR" $ANTLR end "LONGINTEGER" $ANTLR start "DOLLARVAR" $ANTLR end "BIGINTEGER" $ANTLR start "DOUBLE" $ANTLR end "DOLLARVAR" $ANTLR start "DOUBLENUMBER" $ANTLR end "PERIOD" $ANTLR start "DOUBLE_PERIOD" $ANTLR end "THEN" $ANTLR start "ELSE" $ANTLR end "ELSE" $ANTLR start "END" $ANTLR end "POUND" $ANTLR start "EQUAL" $ANTLR end "MULTILINE_QUOTEDSTRING" $ANTLR start "EXECCOMMAND" $ANTLR end "TRUE" $ANTLR start "FALSE" $ANTLR end "AMPERSAND" $ANTLR start "FAT_ARROW" $ANTLR end "LOAD" $ANTLR start "FILTER" $ANTLR end "GENERATE" $ANTLR start "FLATTEN" $ANTLR end "LONG" $ANTLR start "FLOAT" $ANTLR end "IDENTIFIER" $ANTLR start "FLOATINGPOINT" $ANTLR end "BIGINTEGERNUMBER" $ANTLR start "FLOATNUMBER" $ANTLR end "FILTER" $ANTLR start "FOREACH" $ANTLR end "RIGHT" $ANTLR start "FULL" $ANTLR end "NOT" $ANTLR start "GENERATE" $ANTLR end "PARTITION" $ANTLR start "GROUP" $ANTLR end "SPECIALCHAR" $ANTLR start "ID" $ANTLR end "DCOLON" $ANTLR start "IDENTIFIER" $ANTLR end "INTO" $ANTLR start "IF" $ANTLR end "NULL" $ANTLR start "IMPORT" $ANTLR end "STR_OP_MATCHES" $ANTLR start "IN" $ANTLR end "USING" $ANTLR start "INNER" $ANTLR end "CACHE" $ANTLR start "INPUT" $ANTLR end "BOOLEAN" $ANTLR start "INT" $ANTLR end "FLOATINGPOINT" $ANTLR start "INTEGER" $ANTLR end "SPLIT" $ANTLR start "INTO" $ANTLR end "ROLLUP" $ANTLR start "INVOKE" $ANTLR end "MAP" $ANTLR start "IS" $ANTLR end "COGROUP" $ANTLR start "JOIN" $ANTLR end "SAMPLE" $ANTLR start "LEFT" $ANTLR end "RIGHT_CURLY" $ANTLR start "LEFT_BRACKET" $ANTLR end "RIGHT_PAREN" $ANTLR start "LEFT_CURLY" $ANTLR end "SEMI_COLON" $ANTLR start "LEFT_PAREN" $ANTLR end "DIGIT" $ANTLR start "LETTER" $ANTLR end "STDOUT" $ANTLR start "LIMIT" $ANTLR end "DEFINE" $ANTLR start "LOAD" $ANTLR end "INT" $ANTLR start "LONG" $ANTLR end "INTEGER" $ANTLR start "LONGINTEGER" $ANTLR end "TUPLE" $ANTLR start "MAP" $ANTLR end "ASSERT" $ANTLR start "MAPREDUCE" $ANTLR end "PLUS" $ANTLR start "MINUS" $ANTLR end "SL_COMMENT" $ANTLR start "ML_COMMENT" $ANTLR end "QUOTEDSTRING" $ANTLR start "MULTILINE_QUOTEDSTRING" $ANTLR end "OR" $ANTLR start "NOT" $ANTLR end "VOID" $ANTLR start "NULL" $ANTLR end "FALSE" $ANTLR start "NUM_OP_EQ" $ANTLR end "NUM_OP_LTE" $ANTLR start "NUM_OP_GT" $ANTLR end "NUM_OP_GT" $ANTLR start "NUM_OP_GTE" $ANTLR end "NUM_OP_EQ" $ANTLR start "NUM_OP_LT" $ANTLR end "NUM_OP_LT" $ANTLR start "NUM_OP_LTE" $ANTLR end "NUM_OP_GTE" $ANTLR start "NUM_OP_NE" $ANTLR end "OUTER" $ANTLR start "ONSCHEMA" $ANTLR end "AND" $ANTLR start "OR" $ANTLR end "FOREACH" $ANTLR start "ORDER" $ANTLR end "IF" $ANTLR start "OTHERWISE" $ANTLR end "INNER" $ANTLR start "OUTER" $ANTLR end "INPUT" $ANTLR start "OUTPUT" $ANTLR end "ONSCHEMA" $ANTLR start "PARALLEL" $ANTLR end "PARALLEL" $ANTLR start "PARTITION" $ANTLR end "DIV" $ANTLR start "PERCENT" $ANTLR end "COMMA" $ANTLR start "PERIOD" $ANTLR end "PERCENT" $ANTLR start "PLUS" $ANTLR end "RIGHT_BRACKET" $ANTLR start "POUND" $ANTLR end "MINUS" $ANTLR start "QMARK" $ANTLR end "FLOATNUMBER" $ANTLR start "QUOTEDSTRING" $ANTLR end "ORDER" $ANTLR start "RANK" $ANTLR end "IMPORT" $ANTLR start "REGISTER" $ANTLR end "REGISTER" $ANTLR start "RETURNS" $ANTLR end "LEFT" $ANTLR start "RIGHT" $ANTLR end "LEFT_BRACKET" $ANTLR start "RIGHT_BRACKET" $ANTLR end "LEFT_CURLY" $ANTLR start "RIGHT_CURLY" $ANTLR end "LEFT_PAREN" $ANTLR start "RIGHT_PAREN" $ANTLR end "CUBE" $ANTLR start "ROLLUP" $ANTLR end "LIMIT" $ANTLR start "SAMPLE" $ANTLR end "ML_COMMENT" $ANTLR start "SEMI_COLON" $ANTLR end "MAPREDUCE" $ANTLR start "SHIP" $ANTLR end "WS" $ANTLR start "SL_COMMENT" $ANTLR end "LETTER" $ANTLR start "SPECIALCHAR" $ANTLR end "UNION" $ANTLR start "SPLIT" $ANTLR end "EXECCOMMAND" $ANTLR start "STAR" $ANTLR end "OUTPUT" $ANTLR start "STDERROR" $ANTLR end "STDERROR" $ANTLR start "STDIN" $ANTLR end "STDIN" $ANTLR start "STDOUT" $ANTLR end "THROUGH" $ANTLR start "STORE" $ANTLR end "IS" $ANTLR start "STREAM" $ANTLR end "END" $ANTLR start "STR_OP_EQ" $ANTLR end "STR_OP_EQ" $ANTLR start "STR_OP_GT" $ANTLR end "STR_OP_LT" $ANTLR start "STR_OP_GTE" $ANTLR end "STR_OP_GT" $ANTLR start "STR_OP_LT" $ANTLR end "STR_OP_GTE" $ANTLR start "STR_OP_LTE" $ANTLR end "STR_OP_NE" $ANTLR start "STR_OP_MATCHES" $ANTLR end "STR_OP_LTE" $ANTLR start "STR_OP_NE" $ANTLR end "WHEN" $ANTLR start "THEN" $ANTLR end "STREAM" $ANTLR start "THROUGH" $ANTLR end "IN" $ANTLR start "TRUE" $ANTLR end "BAG" $ANTLR start "TUPLE" $ANTLR end "FAT_ARROW" $ANTLR end "CROSS" $ANTLR start "UNION" $ANTLR end "BY" $ANTLR start "USING" $ANTLR start "VOID" $ANTLR end "CASE" $ANTLR start "WHEN" $ANTLR end "DOLLAR" $ANTLR start "WS" $ANTLR end synpred1_QueryLexer $ANTLR start synpred1_QueryLexer
$ANTLR start "after_left_paren" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:847:1: after_left_paren : ( explicit_type_cast RIGHT_PAREN cast_expr -> ^( CAST_EXPR explicit_type_cast cast_expr ) | RIGHT_PAREN ( projection )* -> ^( TUPLE_VAL ) ( projection )* | STAR ( COMMA real_arg )* RIGHT_PAREN ( projection )* -> ^( FUNC_EVAL TOTUPLE STAR ( real_arg )* ) ( projection )* | col_range ( COMMA real_arg )* RIGHT_PAREN ( projection )* -> ^( FUNC_EVAL TOTUPLE col_range ( real_arg )* ) ( projection )* | cond ( ( ( COMMA real_arg )+ RIGHT_PAREN ( projection )* -> ^( FUNC_EVAL TOTUPLE cond ( real_arg )+ ) ( projection )* ) | ( RIGHT_PAREN -> ^( EXPR_IN_PAREN cond ) ) | ( QMARK exp1= expr COLON exp2= expr RIGHT_PAREN -> ^( BIN_EXPR cond $exp1 $exp2) ) ) ); $ANTLR start "and_cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:626:1: and_cond : not_cond ( AND ^ not_cond )* ; $ANTLR start "as_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:458:1: as_clause : AS ^ ( explicit_field_def | ( LEFT_PAREN ! ( field_def_list )? RIGHT_PAREN !) ) ; $ANTLR start "assert_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:481:1: assert_clause : ASSERT ^ rel BY ! cond ( COMMA ! QUOTEDSTRING )? ; $ANTLR start "bracket_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:866:1: bracket_expr : ( LEFT_BRACKET real_arg ( COMMA real_arg )* RIGHT_BRACKET ( projection )* -> ^( FUNC_EVAL TOMAP ( real_arg )+ ) ( projection )* | LEFT_BRACKET keyvalue ( COMMA keyvalue )* RIGHT_BRACKET ( projection )* -> ^( MAP_VAL ( keyvalue )+ ) ( projection )* | LEFT_BRACKET RIGHT_BRACKET ( projection )* -> ^( MAP_VAL ) ( projection )* ); $ANTLR start "cache_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:394:1: cache_clause : CACHE ^ LEFT_PAREN ! path_list RIGHT_PAREN !; $ANTLR start "cast_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:691:1: cast_expr : ( scalar | MINUS cast_expr -> ^( NEG cast_expr ) | col_ref_without_identifier ( projection )* | invoker_func ( projection )* | identifier_plus ( projection )* | identifier_plus ( func_name_suffix )? LEFT_PAREN ( real_arg ( COMMA real_arg )* )? RIGHT_PAREN ( projection )* -> ^( FUNC_EVAL identifier_plus ( func_name_suffix )? ( real_arg )* ) ( projection )* | func_name_without_columns LEFT_PAREN ( real_arg ( COMMA real_arg )* )? RIGHT_PAREN ( projection )* -> ^( FUNC_EVAL func_name_without_columns ( real_arg )* ) ( projection )* | CASE ( ( WHEN )=> WHEN cond THEN expr ( WHEN cond THEN expr )* ( ELSE expr )? END ( projection )* -> ^( CASE_COND ^( WHEN ( cond )+ ) ^( THEN ( expr )+ ) ) ( projection )* | expr WHEN rhs_operand THEN rhs_operand ( WHEN rhs_operand THEN rhs_operand )* ( ELSE rhs_operand )? END ( projection )* -> ^( CASE_EXPR ^( CASE_EXPR_LHS expr ) ( ^( CASE_EXPR_RHS rhs_operand ) )+ ) ( projection )* ) | paren_expr | curly_expr | bracket_expr ); $ANTLR start "cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:469:1: cmd : EXECCOMMAND ^ ( ship_clause | cache_clause | input_clause | output_clause | error_clause )* ; $ANTLR start "col_range" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:884:1: col_range : (c1= col_ref DOUBLE_PERIOD (c2= col_ref )? -> ^( COL_RANGE $c1 DOUBLE_PERIOD ( $c2)? ) | DOUBLE_PERIOD col_ref -> ^( COL_RANGE DOUBLE_PERIOD col_ref ) ); $ANTLR start "col_ref" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:881:1: col_ref : ( col_ref_without_identifier | identifier_plus ); $ANTLR start "col_ref_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:945:1: col_ref_list : ( col_ref | ( LEFT_PAREN col_ref ( COMMA col_ref )* RIGHT_PAREN ) ) -> ( col_ref )+ ; $ANTLR start "col_ref_without_identifier" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:878:1: col_ref_without_identifier : ( GROUP | DOLLARVAR ); $ANTLR start "cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:623:1: cond : and_cond ( OR ^ and_cond )* ; $ANTLR start "cross_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:555:1: cross_clause : CROSS ^ rel_list ( partition_clause )? ; $ANTLR start "cube_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:595:1: cube_clause : CUBE rel BY cube_rollup_list ( COMMA cube_rollup_list )* -> ^( CUBE rel ^( BY ( cube_rollup_list )+ ) ) ; $ANTLR start "cube_rollup_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:598:1: cube_rollup_list : ( CUBE | ROLLUP ) ^ LEFT_PAREN ! real_arg ( COMMA ! real_arg )* RIGHT_PAREN !; $ANTLR start "curly_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:862:1: curly_expr : ( LEFT_CURLY real_arg ( COMMA real_arg )* RIGHT_CURLY ( projection )* -> ^( FUNC_EVAL TOBAG ( real_arg )+ ) ( projection )* | LEFT_CURLY RIGHT_CURLY ( projection )* -> ^( BAG_VAL ) ( projection )* ); Make a deep copy of the given node $ANTLR start "define_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:361:1: define_clause : DEFINE ^ IDENTIFIER ( cmd | func_clause | macro_clause ) ; $ANTLR start "distinct_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:546:1: distinct_clause : DISTINCT ^ rel ( partition_clause )? ; $ANTLR start "eid" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:1037:1: eid : ( eid_without_columns | IDENTIFIER | GROUP | CUBE | TRUE | FALSE | INT | LONG | FLOAT | DOUBLE | NULL | NOT | FLATTEN | BAG | TUPLE | MAP ); $ANTLR start "eid_without_columns" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:978:1: eid_without_columns : ( rel_str_op | IMPORT | REGISTER | RETURNS | DEFINE | LOAD | FILTER | FOREACH | ROLLUP | ORDER | DISTINCT | COGROUP | JOIN | CROSS | UNION | SPLIT | INTO | IF | ALL | AS | BY | USING | INNER | OUTER | PARALLEL | PARTITION | AND | OR | GENERATE | ASC | DESC | BOOL | BIGINTEGER | BIGDECIMAL | DATETIME | CHARARRAY | BYTEARRAY | IS | STREAM | THROUGH | STORE | MAPREDUCE | SHIP | CACHE | INPUT | OUTPUT | STDERROR | STDIN | STDOUT | LIMIT | SAMPLE | LEFT | RIGHT | FULL | REALIAS | BOOL_COND | ASSERT ); $ANTLR start "error_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:403:1: error_clause : STDERROR ^ LEFT_PAREN ! ( QUOTEDSTRING ( LIMIT ! INTEGER )? )? RIGHT_PAREN !; $ANTLR start "explicit_bag_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:326:1: explicit_bag_type : BAG ! implicit_bag_type ; $ANTLR start "explicit_bag_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:329:1: explicit_bag_type_cast : BAG LEFT_CURLY ( explicit_tuple_type_cast )? RIGHT_CURLY -> ^( BAG_TYPE_CAST ( explicit_tuple_type_cast )? ) ; $ANTLR start "explicit_field_def" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:444:1: explicit_field_def : ( identifier_plus ( COLON type )? -> ^( FIELD_DEF identifier_plus ( type )? ) | explicit_type -> ^( FIELD_DEF_WITHOUT_IDENTIFIER explicit_type ) ); $ANTLR start "explicit_map_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:335:1: explicit_map_type : MAP ! implicit_map_type ; $ANTLR start "explicit_tuple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:312:1: explicit_tuple_type : TUPLE ! implicit_tuple_type ; $ANTLR start "explicit_tuple_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:315:1: explicit_tuple_type_cast : TUPLE LEFT_PAREN ( explicit_type_cast ( COMMA explicit_type_cast )* )? RIGHT_PAREN -> ^( TUPLE_TYPE_CAST ( explicit_type_cast )* ) ; $ANTLR start "explicit_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:341:1: explicit_type : ( simple_type | explicit_tuple_type | explicit_bag_type | explicit_map_type ); $ANTLR start "explicit_type_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:350:1: explicit_type_cast : ( simple_type | explicit_map_type | explicit_tuple_type_cast | explicit_bag_type_cast ); $ANTLR start "expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:682:1: expr : multi_expr ( ( PLUS | MINUS ) ^ multi_expr )* ; $ANTLR start "field_def" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:448:1: field_def : ( explicit_field_def | implicit_type -> ^( FIELD_DEF_WITHOUT_IDENTIFIER implicit_type ) ); $ANTLR start "field_def_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:452:1: field_def_list : field_def ( COMMA ! field_def )* ; $ANTLR start "filter_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:484:1: filter_clause : FILTER ^ rel BY ! cond ; $ANTLR start "flatten_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:601:1: flatten_clause : FLATTEN ^ LEFT_PAREN ! expr RIGHT_PAREN !; $ANTLR start "flatten_generated_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:609:1: flatten_generated_item : ( flatten_clause ( generate_as_clause )? | real_arg ( generate_as_clause )? ); $ANTLR start "foreach_plan_complex" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:253:1: foreach_plan_complex : LEFT_CURLY nested_blk RIGHT_CURLY -> ^( FOREACH_PLAN_COMPLEX nested_blk ) ; $ANTLR start "foreach_plan_simple" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:256:1: foreach_plan_simple : GENERATE flatten_generated_item ( COMMA flatten_generated_item )* -> ^( FOREACH_PLAN_SIMPLE ^( GENERATE ( flatten_generated_item )+ ) ) ; $ANTLR start "foreach_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:247:1: foreach_statement : ( FAT_ARROW FOREACH rel ( foreach_plan_complex | ( foreach_plan_simple ( parallel_clause )? SEMI_COLON ) ) -> ^( STATEMENT IDENTIFIER[\"____RESERVED____\"] ^( FOREACH rel ( foreach_plan_complex )? ( foreach_plan_simple )? ) ( parallel_clause )? ) | ( identifier_plus EQUAL )? FOREACH rel ( foreach_plan_complex | ( foreach_plan_simple ( parallel_clause )? SEMI_COLON ) ) -> ^( STATEMENT ( identifier_plus )? ^( FOREACH rel ( foreach_plan_complex )? ( foreach_plan_simple )? ) ( parallel_clause )? ) ); $ANTLR start "func_args" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:425:1: func_args : func_args_string ( COMMA func_args_string )* -> ( func_args_string )+ ; $ANTLR start "func_args_string" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:422:1: func_args_string : ( QUOTEDSTRING | MULTILINE_QUOTEDSTRING ); $ANTLR start "func_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:409:1: func_clause : ( func_name -> ^( FUNC_REF func_name ) | func_name LEFT_PAREN ( func_args )? RIGHT_PAREN -> ^( FUNC func_name ( func_args )? ) ); $ANTLR start "func_name" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:419:1: func_name : eid ( ( PERIOD | DOLLAR ) eid )* ; $ANTLR start "func_name_suffix" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:688:1: func_name_suffix : ( ( DOLLAR | PERIOD ) eid )+ ; $ANTLR start "func_name_without_columns" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:416:1: func_name_without_columns : eid_without_columns ( ( PERIOD | DOLLAR ) eid )* ; $ANTLR start "general_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:239:1: general_statement : ( FAT_ARROW ( ( op_clause ( parallel_clause )? ) | nested_op_clause ) -> ^( STATEMENT IDENTIFIER[\"____RESERVED____\"] ( op_clause )? ( parallel_clause )? ( nested_op_clause )? ) | ( identifier_plus EQUAL )? ( ( op_clause ( parallel_clause )? ) | nested_op_clause ) -> ^( STATEMENT ( identifier_plus )? ( op_clause )? ( parallel_clause )? ( nested_op_clause )? ) ); $ANTLR start "generate_as_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:606:1: generate_as_clause : AS ! ( ( LEFT_PAREN ! field_def_list RIGHT_PAREN !) | explicit_field_def ) ; delegates $ANTLR start "group_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:429:1: group_clause : ( GROUP | COGROUP ) ^ group_item_list ( USING ! QUOTEDSTRING )? ( partition_clause )? ; $ANTLR start "group_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:436:1: group_item : rel ( join_group_by_clause | ALL | ANY ) ( INNER | OUTER )? ; $ANTLR start "group_item_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:432:1: group_item_list : group_item ( COMMA group_item )* -> ( group_item )+ ; $ANTLR start "identifier_plus" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:441:1: identifier_plus : ( IDENTIFIER | reserved_identifier_whitelist -> IDENTIFIER[$reserved_identifier_whitelist.text] ); $ANTLR start "implicit_bag_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:322:1: implicit_bag_type : ( LEFT_CURLY NULL COLON ( tuple_type )? RIGHT_CURLY -> ^( BAG_TYPE ( tuple_type )? ) | LEFT_CURLY ( ( identifier_plus COLON )? tuple_type )? RIGHT_CURLY -> ^( BAG_TYPE ( identifier_plus )? ( tuple_type )? ) ); $ANTLR start "implicit_map_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:332:1: implicit_map_type : LEFT_BRACKET ( ( identifier_plus COLON )? type )? RIGHT_BRACKET -> ^( MAP_TYPE ( identifier_plus )? ( type )? ) ; $ANTLR start "implicit_tuple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:309:1: implicit_tuple_type : LEFT_PAREN ( field_def_list )? RIGHT_PAREN -> ^( TUPLE_TYPE ( field_def_list )? ) ; $ANTLR start "implicit_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:344:1: implicit_type : ( implicit_tuple_type | implicit_bag_type | implicit_map_type ); $ANTLR start "import_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:355:1: import_clause : IMPORT ^ QUOTEDSTRING ; $ANTLR start "inline_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:300:1: inline_clause : inline_return_clause identifier_plus inline_param_clause -> ^( MACRO_INLINE identifier_plus inline_return_clause inline_param_clause ) ; $ANTLR start "inline_param_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:296:1: inline_param_clause : LEFT_PAREN ( parameter ( COMMA parameter )* )? RIGHT_PAREN -> ^( PARAMS ( parameter )* ) ; $ANTLR start "inline_return_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:280:1: inline_return_clause : ( identifier_plus EQUAL -> ^( RETURN_VAL identifier_plus ) | identifier_plus ( COMMA identifier_plus )+ EQUAL -> ^( RETURN_VAL ( identifier_plus )+ ) | -> ^( RETURN_VAL ) ); $ANTLR start "input_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:397:1: input_clause : INPUT ^ LEFT_PAREN ! stream_cmd_list RIGHT_PAREN !; $ANTLR start "invoker_func" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:776:1: invoker_func : INVOKE ( AMPERSAND | LEFT_PAREN real_arg RIGHT_PAREN ) (packageName= identifier_plus PERIOD )* methodName= identifier_plus LEFT_PAREN ( real_arg ( COMMA real_arg )* )? RIGHT_PAREN -> ^( INVOKER_FUNC_EVAL IDENTIFIER[Joiner.on(\".\").join(packageStr)] IDENTIFIER[methodStr] IDENTIFIER[staticStr] ( real_arg )* ) ; $ANTLR start "join_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:559:1: join_clause : JOIN ^ join_sub_clause ( USING ! join_type )? ( partition_clause )? ; $ANTLR start "join_group_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:574:1: join_group_by_clause : BY ^ real_arg ; $ANTLR start "join_item" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:568:1: join_item : rel join_group_by_clause -> ^( JOIN_ITEM rel join_group_by_clause ) ; $ANTLR start "join_sub_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:565:1: join_sub_clause : join_item ( ( ( LEFT | RIGHT | FULL ) ( OUTER )? COMMA ! join_item ) | ( ( COMMA ! join_item )+ ) ) ; $ANTLR start "join_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:562:1: join_type : QUOTEDSTRING ; $ANTLR start "keyvalue" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:900:1: keyvalue : QUOTEDSTRING POUND literal -> ^( KEY_VAL_PAIR QUOTEDSTRING literal ) ; $ANTLR start "limit_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:506:1: limit_clause : LIMIT ^ rel expr ; $ANTLR start "literal" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:916:1: literal : ( scalar | literal_map | literal_bag | literal_tuple ); $ANTLR start "literal_bag" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:908:1: literal_bag : ( LEFT_CURLY literal_tuple ( COMMA literal_tuple )* RIGHT_CURLY -> ^( BAG_VAL ( literal_tuple )+ ) | LEFT_CURLY RIGHT_CURLY -> ^( BAG_VAL ) ); $ANTLR start "literal_map" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:903:1: literal_map : ( LEFT_BRACKET keyvalue ( COMMA keyvalue )* RIGHT_BRACKET -> ^( MAP_VAL ( keyvalue )+ ) | LEFT_BRACKET RIGHT_BRACKET -> ^( MAP_VAL ) ); $ANTLR start "literal_tuple" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:912:1: literal_tuple : ( LEFT_PAREN literal ( COMMA literal )* RIGHT_PAREN -> ^( TUPLE_VAL ( literal )+ ) | LEFT_PAREN RIGHT_PAREN -> ^( TUPLE_VAL ) ); $ANTLR start "load_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:406:1: load_clause : LOAD ^ QUOTEDSTRING ( USING ! func_clause )? ( as_clause )? ; $ANTLR start "macro_body_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:273:1: macro_body_clause : macro_content -> ^( MACRO_BODY ) ; $ANTLR start "macro_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:276:1: macro_clause : macro_param_clause macro_return_clause macro_body_clause -> ^( MACRO_DEF macro_param_clause macro_return_clause macro_body_clause ) ; $ANTLR start "macro_content" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:261:1: macro_content : LEFT_CURLY ( macro_content |~ ( LEFT_CURLY | RIGHT_CURLY ) )* RIGHT_CURLY ; $ANTLR start "macro_param_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:264:1: macro_param_clause : LEFT_PAREN ( identifier_plus ( COMMA identifier_plus )* )? RIGHT_PAREN -> ^( PARAMS ( identifier_plus )* ) ; $ANTLR start "macro_return_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:268:1: macro_return_clause : RETURNS ( ( identifier_plus ( COMMA identifier_plus )* ) | VOID ) -> ^( RETURN_VAL ( identifier_plus )* ) ; $ANTLR start "map_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:338:1: map_type : ( implicit_map_type | explicit_map_type ); $ANTLR start "mr_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:490:1: mr_clause : MAPREDUCE ^ QUOTEDSTRING ( LEFT_PAREN ! path_list RIGHT_PAREN !)? store_clause load_clause ( EXECCOMMAND )? ; $ANTLR start "multi_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:685:1: multi_expr : cast_expr ( ( STAR | DIV | PERCENT ) ^ cast_expr )* ; $ANTLR start "nested_blk" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:921:1: nested_blk : ( nested_command SEMI_COLON )* GENERATE flatten_generated_item ( COMMA flatten_generated_item )* SEMI_COLON -> ( nested_command )* ^( GENERATE ( flatten_generated_item )+ ) ; $ANTLR start "nested_command" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:925:1: nested_command : ( ( identifier_plus EQUAL col_ref PERIOD col_ref_list {...}?)=> ( identifier_plus EQUAL nested_proj ) -> ^( NESTED_CMD identifier_plus nested_proj ) | identifier_plus EQUAL expr -> ^( NESTED_CMD_ASSI identifier_plus expr ) | identifier_plus EQUAL nested_op -> ^( NESTED_CMD identifier_plus nested_op ) ); $ANTLR start "nested_cross" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:961:1: nested_cross : CROSS ^ nested_op_input_list ; $ANTLR start "nested_distinct" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:955:1: nested_distinct : DISTINCT ^ nested_op_input ; $ANTLR start "nested_filter" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:949:1: nested_filter : FILTER ^ nested_op_input BY ! cond ; $ANTLR start "nested_foreach" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:964:1: nested_foreach : FOREACH nested_op_input GENERATE flatten_generated_item ( COMMA flatten_generated_item )* -> ^( FOREACH nested_op_input ^( GENERATE ( flatten_generated_item )+ ) ) ; $ANTLR start "nested_limit" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:958:1: nested_limit : LIMIT ^ nested_op_input ( ( INTEGER SEMI_COLON )=> INTEGER | expr ) ; $ANTLR start "nested_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:933:1: nested_op : ( nested_filter | nested_sort | nested_distinct | nested_limit | nested_cross | nested_foreach ); $ANTLR start "nested_op_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:234:1: nested_op_clause : ( LEFT_PAREN ! op_clause ( parallel_clause )? RIGHT_PAREN !| LEFT_PAREN FOREACH rel ( foreach_plan_complex | ( foreach_plan_simple ( parallel_clause )? ) ) RIGHT_PAREN -> ^( FOREACH rel ( foreach_plan_complex )? ( foreach_plan_simple )? ) ( parallel_clause )? ); $ANTLR start "nested_op_input" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:968:1: nested_op_input : ( col_ref | nested_proj ); $ANTLR start "nested_op_input_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:971:1: nested_op_input_list : nested_op_input ( COMMA nested_op_input )* -> ( nested_op_input )+ ; $ANTLR start "nested_proj" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:941:1: nested_proj : col_ref PERIOD col_ref_list -> ^( NESTED_PROJ col_ref col_ref_list ) ; $ANTLR start "nested_sort" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:952:1: nested_sort : ORDER ^ nested_op_input BY ! order_by_clause ( USING ! func_clause )? ; $ANTLR start "not_cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:629:1: not_cond : ( NOT ^)? unary_cond ; $ANTLR start "op_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:370:1: op_clause : ( define_clause | load_clause | group_clause | cube_clause | store_clause | filter_clause | distinct_clause | limit_clause | sample_clause | order_clause | rank_clause | cross_clause | join_clause | union_clause | stream_clause | mr_clause ); $ANTLR start "order_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:533:1: order_by_clause : ( STAR ( ASC | DESC )? | order_col_list ); $ANTLR start "order_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:530:1: order_clause : ORDER ^ rel BY ! order_by_clause ( USING ! func_clause )? ; $ANTLR start "order_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:541:1: order_col : ( col_range ( ASC | DESC )? | col_ref ( ASC | DESC )? | LEFT_PAREN ! col_ref ( ASC | DESC )? RIGHT_PAREN !); $ANTLR start "order_col_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:537:1: order_col_list : order_col ( COMMA order_col )* -> ( order_col )+ ; $ANTLR start "output_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:400:1: output_clause : OUTPUT ^ LEFT_PAREN ! stream_cmd_list RIGHT_PAREN !; $ANTLR start "parallel_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:367:1: parallel_clause : PARALLEL ^ INTEGER ; $ANTLR start "parameter" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:286:1: parameter : ( IDENTIFIER | INTEGER | DOUBLENUMBER | BIGDECIMALNUMBER | BIGINTEGERNUMBER | QUOTEDSTRING | DOLLARVAR ); $ANTLR start "paren_expr" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:793:1: paren_expr : LEFT_PAREN ! try_implicit_map_cast ; $ANTLR start "partition_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:549:1: partition_clause : PARTITION ^ BY ! func_name ; $ANTLR start "path_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:391:1: path_list : QUOTEDSTRING ( COMMA QUOTEDSTRING )* -> ( QUOTEDSTRING )+ ; $ANTLR start "previous_rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:475:1: previous_rel : ARROBA ; $ANTLR start "projection" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:871:1: projection : ( PERIOD ( col_ref | LEFT_PAREN col_ref ( COMMA col_ref )* RIGHT_PAREN ) -> ^( PERIOD ( col_ref )+ ) | POUND ^ ( QUOTEDSTRING | NULL ) ); $ANTLR start "query" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:212:1: query : ( statement )* EOF -> ^( QUERY ( statement )* ) ; $ANTLR start "rank_by_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:518:1: rank_by_clause : ( STAR ( ASC | DESC )? | rank_list ); $ANTLR start "rank_by_statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:515:1: rank_by_statement : BY ^ rank_by_clause ( DENSE )? ; $ANTLR start "rank_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:512:1: rank_clause : RANK ^ rel ( rank_by_statement )? ; $ANTLR start "rank_col" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:526:1: rank_col : ( col_range ( ASC | DESC )? | col_ref ( ASC | DESC )? ); $ANTLR start "rank_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:522:1: rank_list : rank_col ( COMMA rank_col )* -> ( rank_col )+ ; $ANTLR start "real_arg" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:618:1: real_arg : ( expr | STAR | col_range ); $ANTLR start "realias_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:364:1: realias_clause : identifier_plus EQUAL identifier_plus -> ^( REALIAS identifier_plus identifier_plus ) ; $ANTLR start "register_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:358:1: register_clause : REGISTER ^ QUOTEDSTRING ( USING identifier_plus AS identifier_plus )? ; $ANTLR start "rel" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:472:1: rel : ( identifier_plus | previous_rel | nested_op_clause ); $ANTLR start "rel_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:552:1: rel_list : rel ( COMMA rel )* -> ( rel )+ ; $ANTLR start "rel_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:1056:1: rel_op : ( rel_str_op | NUM_OP_EQ | NUM_OP_NE | NUM_OP_GT | NUM_OP_GTE | NUM_OP_LT | NUM_OP_LTE ); $ANTLR start "rel_str_op" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:1065:1: rel_str_op : ( STR_OP_EQ | STR_OP_NE | STR_OP_GT | STR_OP_LT | STR_OP_GTE | STR_OP_LTE | STR_OP_MATCHES ); $ANTLR start "reserved_identifier_whitelist" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:1074:1: reserved_identifier_whitelist : ( RANK | CUBE | IN | WHEN | THEN | ELSE | END ); $ANTLR start "rhs_operand" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:679:1: rhs_operand : expr ; $ANTLR start "sample_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:509:1: sample_clause : SAMPLE ^ rel expr ; $ANTLR start "scalar" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:888:1: scalar : ( INTEGER | LONGINTEGER | FLOATNUMBER | DOUBLENUMBER | BIGINTEGERNUMBER | BIGDECIMALNUMBER | QUOTEDSTRING | NULL | TRUE | FALSE ); $ANTLR start "schema" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:215:1: schema : field_def_list EOF ; $ANTLR start "ship_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:388:1: ship_clause : SHIP ^ LEFT_PAREN ! ( path_list )? RIGHT_PAREN !; $ANTLR start "simple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:306:1: simple_type : ( BOOLEAN | INT | LONG | FLOAT | DOUBLE | DATETIME | BIGINTEGER | BIGDECIMAL | CHARARRAY | BYTEARRAY ); $ANTLR start "split_branch" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:496:1: split_branch : identifier_plus IF cond -> ^( SPLIT_BRANCH identifier_plus cond ) ; $ANTLR start "split_branches" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:502:1: split_branches : ( COMMA ! split_branch ( split_branches )? | COMMA ! split_otherwise ); $ANTLR start "split_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:493:1: split_clause : SPLIT ^ rel INTO ! split_branch split_branches ; $ANTLR start "split_otherwise" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:499:1: split_otherwise : identifier_plus OTHERWISE ( ALL )? -> ^( OTHERWISE identifier_plus ( ALL )? ) ; $ANTLR start "statement" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:220:1: statement : ( SEMI_COLON !| general_statement SEMI_COLON !| split_clause SEMI_COLON !| inline_clause SEMI_COLON !| import_clause SEMI_COLON !| realias_clause SEMI_COLON !| register_clause SEMI_COLON !| assert_clause SEMI_COLON !| foreach_statement ); $ANTLR start "store_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:478:1: store_clause : STORE ^ rel INTO ! QUOTEDSTRING ( USING ! func_clause )? ; $ANTLR start "stream_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:487:1: stream_clause : STREAM ^ rel THROUGH ! ( EXECCOMMAND | identifier_plus ) ( as_clause )? ; $ANTLR start "stream_cmd" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:466:1: stream_cmd : ( STDIN | STDOUT | QUOTEDSTRING ) ^ ( USING ! func_clause )? ; $ANTLR start "stream_cmd_list" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:463:1: stream_cmd_list : stream_cmd ( COMMA stream_cmd )* -> ( stream_cmd )+ ; $ANTLR end "reserved_identifier_whitelist" $ANTLR start synpred1_QueryParser $ANTLR end synpred1_QueryParser $ANTLR start synpred2_QueryParser $ANTLR end synpred2_QueryParser $ANTLR start synpred3_QueryParser $ANTLR end synpred4_QueryParser Delegated rules $ANTLR end synpred3_QueryParser $ANTLR start synpred4_QueryParser $ANTLR start "try_implicit_map_cast" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:836:1: try_implicit_map_cast : ( ( implicit_map_type RIGHT_PAREN cast_expr )=> implicit_map_type RIGHT_PAREN cast_expr -> ^( CAST_EXPR implicit_map_type cast_expr ) | after_left_paren ); $ANTLR start "tuple_type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:319:1: tuple_type : ( implicit_tuple_type | explicit_tuple_type ); $ANTLR start "type" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:347:1: type : ( explicit_type | implicit_type ); $ANTLR start "unary_cond" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:632:1: unary_cond : exp1= expr ( ( IS ( NOT )? NULL -> ^( NULL $exp1 ( NOT )? ) ) | ( IN LEFT_PAREN ( rhs_operand ( COMMA rhs_operand )* ) RIGHT_PAREN -> ^( IN ^( IN_LHS expr ) ( ^( IN_RHS rhs_operand ) )+ ) ) | ( rel_op exp2= expr -> ^( rel_op $exp1 $exp2) ) | ( -> ^( BOOL_COND expr ) ) ) ; $ANTLR start "union_clause" /usr/local/pig-0.17.0/src/org/apache/pig/parser/QueryParser.g:592:1: union_clause : UNION ^ ( ONSCHEMA )? rel_list ;
MacroDef node has two child nodes: 1. name 2. MACRO_DEF (PARAMS, RETURN_VAL, MACRO_BODY)

Refer to ANTLRStringStream class for definitions of n, p, and data.







Method invoked on every tuple during foreach evaluation (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
Method invoked on every tuple during FOREACH evaluation. If source string or search replacement map is empty or null, source string is returned.

java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
Allocate a buffer for numSamples elements, populate it with the first numSamples tuples, and continue scanning rest of the input. For every ith next() call, we generate a random number r s.t. 0<=r<i, and if r<numSamples we insert the new tuple into our buffer at position r. This gives us a random sample of the tuples in the partition.

(non-Javadoc) @see org.apache.pig.data.DataBag#add(org.apache.pig.data.Tuple) (non-Javadoc) @see org.apache.pig.data.DataBag#addAll(org.apache.pig.data.DataBag) (non-Javadoc) @see org.apache.pig.data.DataBag#clear() (non-Javadoc) @see java.lang.Comparable#compareTo(java.lang.Object) This has to be defined since DataBag implements Comparable although, in this case we cannot really compare. (non-Javadoc) @see org.apache.pig.impl.util.Spillable#getMemorySize() (non-Javadoc) @see org.apache.pig.data.DataBag#isDistinct() (non-Javadoc) @see org.apache.pig.data.DataBag#isSorted() (non-Javadoc) @see org.apache.pig.data.DataBag#iterator() (non-Javadoc) @see org.apache.pig.data.DataBag#markStale(boolean) (non-Javadoc) @see org.apache.hadoop.io.Writable#readFields(java.io.DataInput) (non-Javadoc) @see org.apache.pig.data.DataBag#size() (non-Javadoc) @see org.apache.pig.impl.util.Spillable#spill() (non-Javadoc) @see org.apache.hadoop.io.Writable#write(java.io.DataOutput)
Java level API

Updates curSplitIndex , just increment if splitIndexes is null, else get next split in splitIndexes
Used by DatumWriter.  Applications should not call. Gets the value of the 'booleanValue' field. Gets the value of the 'bytesValue' field. Gets the value of the 'doubleValue' field. Gets the value of the 'floatValue' field. Gets the value of the 'intValue' field. Gets the value of the 'key' field. Gets the value of the 'longValue' field. Gets the value of the 'nullValue' field. Creates a new RecordPojo RecordBuilder Creates a new RecordPojo RecordBuilder by copying an existing Builder Creates a new RecordPojo RecordBuilder by copying an existing RecordPojo instance Used by DatumReader.  Applications should not call. Sets the value of the 'booleanValue' field. Sets the value of the 'bytesValue' field. Sets the value of the 'doubleValue' field. Sets the value of the 'floatValue' field. Sets the value of the 'intValue' field. Sets the value of the 'key' field. Sets the value of the 'longValue' field. Sets the value of the 'nullValue' field.





@Override

This function determines the type of pattern we are working with The return value of the function determines the type we are expecting





Get the ith MapKeysInfo structure   Merge with RequiredFields r2. Merge both required fields and required map keys Merge another map key into existing required map keys list Merge a MapKeysInfo structure to existing required map keys list   Set the index of all fields to i  Set a MapKeysInfo structure to the required map keys list Set the ith MapKeysInfo structure


Test whether two ResourceSchemas are the same.  Two schemas are said to be the same if they are both null or have the same number of fields and for each field the name, type are the same.  For fields that have may have schemas (i.e. tuples) both schemas be equal.  Field descriptions are not used in testing equality. Get all field names. Get field schema for each field Get order for sort keys. Get the sort keys for this data. Get the version of this schema. Set all the fields.  If fields are not currently null the new fields will be silently ignored. Set the order for each sort key.  If order is not currently null, new order will be silently ignored. Set the sort keys for htis data.  If sort keys are not currently null the new sort keys will be silently ignored.
equals() and hashCode() overridden mostly for ease of testing you shouldn't encounter a situation in which you need to .equals() two sets of statistics on different objects "in the wild" returns average record size in bytes. This number can be explicitly specified by statistics, or if absent, computed using totalbytes/totalrecords. Will return null if can't be computed.  Set average record size in bytes Sets the size in bytes  Probably more in here


Begin traversing the graph.
Begin traversing the graph.



The add method accepts a varargs argument; each argument can be either a random object, a DataBag, or a RubyArray. In the case of a random object, that object will be converted to a Pig object and put into a Tuple. In the case of a RubyArray, it will be treated as a Tuple and added. In the case of a DataBag, it will iterate over the DataBag and add all of the elements to the element encapsulated by RubyDataBag. This method deletes all of the entries in the underlying DataBag. This method returns a copy of the encapsulated DataBag. TODO see if a deepcopy is necessary as well (and consider adding to DataBag and Tuple) This method registers the class with the given runtime. It is not necessary to do this here, but it is simpler to associate the methods necessary to register the class with the class itself, so on the Library side it is possible to just specify "RubyDataBag.define(runtime)". This is an implementation of the each method which opens up the Enumerable interface, and makes it very convenient to iterate over the elements of a DataBag. Note that currently, due to a deficiency in JRuby, it is not possible to call each without a block given. TODO let them specify which element will be returned, or if it will just iterate over each ie a true flatten This is a convenience method which will run the given block on the first element of each tuple contained. The initialize method is the method used on the Ruby side to construct the RubyDataBag object. The default is just an empty bag. The initialize method can optionally receive a DataBag. In the case of a RubyDataBag, a RubyDataBag will be returned that directly encapsulates it. This method returns a string representation of the RubyDataBag. If given an optional argument, then if that argument is true, the contents of the bag will also be printed. This returns whether the encapsulated DatBag is distinct, per the distinct setting. This method returns whether or not the encapsulated DataBag is empty. This returns whether the encapsulated DatBag is distinct, per the sorted setting. This returns the size of the encapsulated DataBag.
This calls to the append function of the underlying DataByteArray. This accepts the same types that set accepts. This calls to the static method compare of DataByteArray. Given two DataByteArrays, it will call it on the underlying bytes. This calls the compareTo method of the encapsulated DataByteArray. Registers the DataByteArray class with the Ruby runtime. Overrides equality leveraging DataByteArray's equality. Overrides ruby's hash leveraging DataByteArray's hash. This is the default initializer, which returns an empty DataByteArray. Given a String or a set of bytes[], initializes the encapsulated DataByteArray using {@link DataByteArray#set}. In the case of a DataByteArray, will copy the underlying bytes. Given two RubyDataByteArrays, initializes the encapsulated DataByteArray to be a concatentation of the copied bits of the first and second. This method accepts as an argument anything that could be passed to set (ie a RubyDataByteArray, RubyString, or byte array). It then adds those values together. This method calls the set method of the underlying DataByteArray with one exception: if given a RubyDataByteArray, it will set the encapsulated DataByteArray to be equal.
This method provides addition semantics, without modifying the original Schema. This method can be given any number of arguments, much as with the constructor. This method provides addition semantics, modifying the original Schema in place. This method can be given any number of arguments, much as with the constructor. This is a ruby method which takes an array of arguments and constructs a Bag schema from them. The name will be set automatically. This is a ruby method which takes a name and an array of arguments and constructs a Bag schema from them.  This method registers the class with the given runtime. This is a helper method which recursively searches for an alias in the Schema encapsulated by RubySchema. This is necessary because findFieldSchema uses canonicalName, not name. Given a field name this string will search the RubySchema for a FieldSchema with that name and return it encapsulated in a Schema. This method will fix any name conflicts in a schema. It's important to note that this will change the Schema object itself. It will deal with any collisions in things named tuple_#, bag_#, map_#, or val_#, as these are generally names generated by Util.getSchemaFromString. In the case of another name conflict, it will not be changed, as that name conflict was created by the user. This method allows access into the Schema nested in the encapsulated Schema. For example, if the encapsulated Schema is a bag Schema, this allows the user to access the schema of the interior Tuple. This is the ruby method which allows people to access elements of the RubySchema object. It can be given either a single numeric index, or a Range object to specify a range of indices. It's important to note that the Schema object returned from this references the Schema stored internally, so if the user wants to make changes without affecting this object, it must be cloned. This is a version of [] which allows the range to be specified as such: [1,2]. This is a helper method to pull out the native Java type from the ruby object. This method allows the user to see the name of the alias of the FieldSchema of the encapsulated Schema. This method only works if the Schema has one FieldSchema. Given a field name, this will return the index of it in the schema. The ruby initializer accepts any number of arguments. With no arguments, it will return an empty Schema object. It can accept any number of arguments. To understand the valid arguments, see the documentation for {@link #rubyArgToSchema}. This is a helper method to generate a RubySchema of the given type without an alias. This is a ruby method which takes an array of arguments and constructs a Map schema from them. The name will be set automatically. This is a ruby method which takes a name and an array of arguments and constructs a Map schema from them. This is a static helper method to create a null aliased bag Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased Boolean Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased bytearray Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased chararray Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased datetime Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased double Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased float Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased int Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased long Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased map Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a static helper method to create a null aliased tuple Schema. This is useful in cases where you do not want the output to have an explicit name, which {@link Utils#getSchemaFromString} will assign. This is a helper function which converts objects into Schema objects. The valid options are as follows: <p> A RubyString, which will have {@link Utils#getSchemaFromString} called on it, and it will be added. <p> A RubySchema, which will be added directly. IMPORTANT NOTE: since this API abstracts away from the distinction between Schema/FieldSchema, its important to understand how a Schema is added to another. In this case, the FieldSchema is pulled directly out of the given Schema. Thus, where in Pig a Schema.FieldSchema might be passed around, internally to this class, generally a Schema will be passed around encapsulating it. <p> A list will create the Schema for a Tuple whose elements will be the elements of the list. Each element will be subjected to the same rules applied here. <p> A hash in the form of:<br> <code>{"name:tuple"=>["x:int","y:int,z:int"], "name2:bag"=>["a:chararray"]}</code><br> The keys must be a tuple, bag, or map, and the value must be an array. This allows the users to set an index or a range of values to a specified RubySchema. The first argument must be a Fixnum or Range, and the second argument may optionally be a Fixnum. The given index (or range of indices) will be replaced by a RubySchema instantiated based on the remaining arguments. This method sets the name of a RubySchema to the name given. It's important to note that if the RubySchema represents anything other than a tuple, databag, or map then an error will be thrown. This method allows the user to set the name of the alias of the FieldSchema of the encapsulated Schema. This method only works if the Schema has one FieldSchema. This is just a convenience method which sets the name of the internalSchema to the argument that was given.  The toString method just leverages Schema's printing. This is a ruby method which takes an array of arguments and constructs a Tuple schema from them. The name will be set automatically. This is a ruby method which takes a name and an array of arguments and constructs a Tuple schema from them.
Build the pattern that this rule will look for Get the transformer for this rule.  Abstract because the rule may want to choose how to instantiate the transformer. This should never return a cached transformer, it should always return a fresh one with no state. Return the pattern to be matched for this rule Check if two operators match each other, we want to find matches that don't share nodes Search for all the sub-plans that matches the pattern defined by this rule. See class description above for limitations on the patterns supported.

java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()


(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()



Wrapper around Java's String.split (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping() The outputSchema of STRSPLIT cannot be set as DataType.chararry otherwise in some cases, it will cause error. For example, when stringsize() is called.
Wrapper around Java's String.split
Method invoked on every tuple during foreach evaluation (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()

search for PartionSkewedKeys and update input file name it is always under a POForEach operator in reduce plan




Recursively compare two schemas to check if the input schema can be cast to the cast schema Make a deep copy of a schema. * For two schemas to be equal, they have to be deeply equal. Use Schema.equals(Schema schema, Schema other, boolean relaxInner, boolean relaxAlias) if relaxation of aliases is a requirement. Recursively compare two schemas for equality Look for a FieldSchema instance in the schema hierarchy which has the given canonical name.  Given a field number, find the associated FieldSchema. Given an alias name, find the associated FieldSchema. Given an alias name, find the associated FieldSchema. If exact name is not found see if any field matches the part of the 'namespaced' alias. eg. if given alias is nm::a , and schema is (a,b). It will return FieldSchema of a. if given alias is nm::a and schema is (nm2::a, b), it will return null Utility function that calls schema.getFiled(alias), and converts {@link FrontendException} to {@link SchemaMergeException} Given an alias, find the associated position of the field schema. Given an alias, find the associated position of the field schema. It uses getFieldSubNameMatch to look for subName matches as well. no-op if indentLevel is negative.<br> otherwise, print newline and 4*indentLevel spaces.  * Merge this schema with the other schema * Merge two aliases. If one of aliases is null, return the other. Otherwise check the precedence condition Schema will not be merged if types are incompatible, as per DataType.mergeType(..) For Tuples and Bags, SubSchemas have to be equal be considered compatible Aliases are assumed to be same for both If one of the aliases is of form 'nm::str1', and other is of the form 'str1', this returns str1 * Recursively prefix merge two schemas * Recursively prefix merge two schemas * Recursively merge two schemas * Recursively merge two schemas Merges two schemas using their column aliases (unlike mergeSchema(..) functions which merge using positions) Schema will not be merged if types are incompatible, as per DataType.mergeType(..) For Tuples and Bags, SubSchemas have to be equal be considered compatible Merges collection of schemas using their column aliases (unlike mergeSchema(..) functions which merge using positions) Schema will not be merged if types are incompatible, as per DataType.mergeType(..) For Tuples and Bags, SubSchemas have to be equal be considered compatible Reconcile this schema with another schema.  The schema being reconciled with should have the same number of columns.  The use case is where a schema already exists but may not have alias and or type information.  If an alias exists in this schema and a new one is given, then the new one will be used.  Similarly with types, though this needs to be used carefully, as types should not be lightly changed. Recursively set NULL type to the specifid type in a schema  Find the number of fields in the schema. This is used for building up output string type can only be BAG or TUPLE
* The logic here is to check if we have duplicate alias in each schema



Check if schema is valid (ready to be part of a final logical plan)
TODO also need to implement the raw comparator TODO could generate a faster getAll in the code This only accounts for the size of members (adjusting for word alignment). It also includes the size of the object itself, since this never affects word boundaries. This is a mechanism which allows the SchemaTupleFactory to get around having to use reflection. The generated code will return a generator which will be created via reflection, but after which can generate SchemaTuples at native speed. This method will return the identifier that the generated code was generated with. This is useful because when the classes are resolved generically, this let's us know the identifier, which is used when serlializing and deserializing tuples. this is ok because we only cast to T after checking This method is responsible for writing everything contained by the Tuple. Note that the base SchemaTuple class does not have an implementation (but is not abstract) so that the generated code can call this method via super without worrying about whether it is abstract or not, as there may be classes in between in the inheritance tree (such as AppendableSchemaTuple).
This method copies all of the generated classes from the distributed cache to a local directory, and then seeks to resolve them and cache their respective SchemaTupleFactories. This method fetches the SchemaTupleFactory that generates the SchemaTuple registered with the given identifier. IMPORTANT: if no such SchemaTupleFactory is available, this returns null. This method fetches the SchemaTupleFactory that can create Tuples of the given Schema (ignoring aliases) and appendability. IMPORTANT: if no such SchemaTupleFactory is available, this returns null. This method fetches the SchemaTupleFactory that can create Tuples of the given Schema and appendability. IMPORTANT: if no such SchemaTupleFactory is available, this returns null. Once all of the files are copied from the distributed cache to the local temp directory, this will attempt to resolve those files and add their information.
This method takes generated code, and compiles it down to a class file. It will output the generated class file to the static temporary directory for generated code. Note that the compiler will use the classpath that Pig is instantiated with, as well as the generated directory. TODO in the future, we can use ASM to generate the bytecode directly. This class actually generates the code for a given Schema. This method generates the actual SchemaTuple for the given Schema.
This method is the publicly facing method which returns a SchemaTupleFactory which will generate the SchemaTuple associated with the given identifier. This method is primarily for internal use in cases where the problem SchemaTuple is known based on the identifier associated with it (such as when deserializing). This method is the publicly facing method which returns a SchemaTupleFactory which will generate SchemaTuples of the given Schema. Note that this method returns null if such a given SchemaTupleFactory does not exist, instead of throwing an error. The GenContext is used to specify the context in which we are requesting a SchemaTupleFactory. This is necessary so that the use of SchemaTuple can be controlled -- it is possible that someone wants a factory that generates code in the context of joins, but wants to disable such use for udfs. We could make this faster by caching the result, but I doubt it will be called in any great volume. This method inspects a Schema to see whether or not a SchemaTuple implementation can be generated for the types present. Currently, bags and maps are not supported.
This must be called when the code has been generated and the generated code needs to be shipped to the cluster, so that it may be used by the mappers and reducers. This is a method which caches a PigContext object that has had relevant key values set by SchemaTupleBackend. This is necessary because in some cases, multiple cycles of jobs might run in the JVM, but the PigContext object may be shared, so we want to make sure to undo any changes we have made to it. This method "registers" a Schema to be generated. It allows a portions of the code to register a Schema for generation without knowing whether code generation is enabled. A unique ID will be passed back that can be used internally to refer to generated SchemaTuples (such as in the case of serialization and deserialization). The context is necessary to allow the client to restrict where generated code can be used. This allows the frontend/backend process to be repeated if on the same JVM (as in testing).
Create a new tuple schema according one array: the type of fields, the tuple name is t, and the bag name is b. Create a bag schema according the bag name,tuple name and two list: name of fields, type of fields Create a new tuple schema according two arrays: names of field,types of fields. The default tuple name is t, and the bag is b. Create a new tuple schema according one list: types of fields, the default names of fields are f0,f1,f2..., and the tuple is t, the bag name is b. Create a bag schema according two list: name of fields, type of fields, and the default bag name is b, the default tuple name is t. Create a new tuple schema according one array: types of fields, the default names of fields are f0,f1,f2..., and the tuple name is t. Create a new tuple schema according the tuple name and two arrays: names of fields, types of fields Create a new tuple schema according the tuple name and two list: names of fields, types of fields Create a new tuple schema according the two arrays: names of fields, types of fields, the default tuple name is t. Create a new tuple schema according one list: types of fields, the default names of fields are f0,f1,f2..., and the tuple name is t. Create a new tuple schema according the two list: names of fields, types of fields, the default tuple name is t.
This method gives a name to the column.
Gets instance of the scriptEngine for the given scripting language Figures out the jar location from the class Returns a map from local variable names to their values Gets the collection of {@link PigStats} after the script is run. open a stream load a script locally or in the classpath Not needed as a general abstraction so far /** * Loads the script in the interpreter. * @param script */ protected abstract void load(InputStream script) throws IOException; Gets ScriptEngine classname or keyword for the scripting language  Actually runs the script file. This method will be implemented by individual script engines. Registers scripting language functions as Pig functions with given namespace Runs a script file.




(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()

Return true if we saw physical operators other than project in the plan Build sort key structure from POLocalRearrange

Find the MRPlan of the physicalPlan which containing currentLR Backward search all the physicalOperators which precede currentLR until the previous POLocalRearrange is found or the root of physicalPlan is found. Find the ReducePlan of the physicalPlan which containing currentLR Forward search all the physicalOperators which succeed currentLR until the next POLocalRearrange is found or the leave of physicalPlan is found. Secondary key sort optimization is enabled in group + foreach nested situation, like TestAccumlator#testAccumWithSort After calling SecondaryKeyOptimizerUtil.applySecondaryKeySort, the POSort in the POForeach will be deleted in the spark plan. Sort function can be implemented in secondary key sort even though POSort is deleted in the spark plan.

Return true if we saw physical operators other than project in the plan Build sort key structure from POLocalRearrange



get secret keys and tokens and store them into TokenCache

Seeks to a given offset as specified by whence flags. If whence is SEEK_SET, offset is added to beginning of stream If whence is SEEK_CUR, offset is added to current position inside stream If whence is SEEK_END, offset is added to end of file position Returns current offset



Start. Reset buffer when finished. Get token literal value. Get the suffix. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Reinitialise. Method to adjust line and column numbers for the start of a token. Backup a number of characters. Get token beginning column number. Get token beginning line number.  Get token end column number. Get token end line number.  Read a character.




Override this method if you want to customize how the node dumps out its children. You can override these two methods in subclasses of SimpleNode to customize the way the node appears when the tree is dumped.  If your output uses more than one line you should override toString(String), otherwise overriding toString() is probably all you need to do.

(non-Javadoc) @see org.apache.pig.data.DataBag#add(org.apache.pig.data.Tuple) NOTE: It is the user's responsibility to ensure only a single Tuple is ever added into a SingleTupleBag (non-Javadoc) @see org.apache.pig.data.DataBag#addAll(org.apache.pig.data.DataBag) (non-Javadoc) @see org.apache.pig.data.DataBag#clear() (non-Javadoc) @see java.lang.Comparable#compareTo(java.lang.Object) (non-Javadoc) @see org.apache.pig.impl.util.Spillable#getMemorySize() (non-Javadoc) @see org.apache.pig.data.DataBag#isDistinct() (non-Javadoc) @see org.apache.pig.data.DataBag#isSorted() (non-Javadoc) @see org.apache.pig.data.DataBag#iterator() (non-Javadoc) @see org.apache.pig.data.DataBag#markStale(boolean) (non-Javadoc) @see org.apache.hadoop.io.Writable#readFields(java.io.DataInput) (non-Javadoc) @see org.apache.pig.data.DataBag#size() (non-Javadoc) @see org.apache.pig.impl.util.Spillable#spill() Write the bag into a string. (non-Javadoc) @see org.apache.hadoop.io.Writable#write(java.io.DataOutput)
Memory size of objects are rounded to multiple of 8 bytes
use parallelism from keyDist or the default parallelism to create user defined partitioner do all kinds of Join (inner, left outer, right outer, full outer) Utility function. 1. Get parallelism 2. build a key distribution map from the broadcasted key distribution file


(non-Javadoc) @see java.lang.Object#equals(java.lang.Object)    (non-Javadoc) @see java.lang.Object#hashCode() (non-Javadoc) @see java.lang.Object#toString()

(non-Javadoc) @see java.lang.Object#equals(java.lang.Object)  (non-Javadoc) @see java.lang.Object#hashCode()  (non-Javadoc) @see java.lang.Object#toString()



Sort contents of mContents and write them to disk

build a POPoissonSampleSpark operator for SkewedJoin's sampling job Add POBroadcastSpark operator to broadcast key distribution for SkewedJoin's sampling job Compiles the physicalPlan below op into a Spark Operator and stores it in curSparkOp. Sets up the indexing job for single-stage cogroups. Create a sampling job to collect statistics by sampling an input file. The sequence of operations is as following: <li>Transform input sample tuples into another tuple.</li> <li>Add an extra field &quot;all&quot; into the tuple </li> <li>Package all tuples into one bag </li> <li>Add constant field for number of reducers. </li> <li>Sorting the bag </li> <li>Invoke UDF with the number of reducers and the sorted bag.</li> <li>Data generated by UDF is stored into a file.</li> Create Sampling job for skewed join. Returns a temporary DFS Path The merge of a list of plans into a single physicalPlan  currently use regular join to replace skewedJoin Skewed join currently works with two-table inner join. More info about pig SkewedJoin, See https://wiki.apache.org/pig/PigSkewedJoinSpec









Creates new SparkCounters instance for the job, initializes aggregate warning counters if required We store the value of udf.import.list in SparkEngineConf#properties Later we will serialize it in SparkEngineConf#writeObject and deserialize in SparkEngineConf#readObject. More detail see PIG-4920 Only one SparkContext may be active per JVM (SPARK-2243). When multiple threads start SparkLaucher, the static member sparkContext should be initialized only once You can use this in unit tests to stop the SparkContext between tests.




If enable multiquery optimizer, in some cases, the predecessor(from) of a physicalOp(to) will be the leaf physicalOperator of previous sparkOperator.More detail see PIG-4675





SparkPlan can have many SparkOperators. Each SparkOperator can have multiple POStores We currently collect stats once for every POStore, But do not want to collect input stats for every POStore e.g. After multiQuery optimization, the sparkOperator may look like this: POLoad_1             (PhysicalPlan) ...POStore_A \          / ...POSplit /          \ POLoad_2            (PhysicalPlan) ...POStore_B






createIndexerSparkNode is a utility to create an indexer spark node with baseSparkOp
Requests that an object return an estimate of its in memory size. Instructs an object to spill whatever it can to disk and release references to any data structures it spills.
Configure thresholds for memory usage/collection threshold exceeded notifications. Uses memoryThresholdFraction and collectionMemoryThresholdFraction to configure thresholds for heap sizes less than 1GB and unusedMemoryThreshold for bigger heaps. Register a spillable to be tracked. No need to unregister, the tracking will stop when the spillable is GCed. @StaticDataCleanup





LoadMetaData  StoreFunc LoadFunc reset the store and get the Data object to access it reset the store and get the Data object to access it  StoreMetaData
Transform bytes from a byte array up to the specified length to a <code>Tuple</code> Transform a <code>String</code> into a byte representing the field delimiter. Serialize an object to an {@link OutputStream} in the field-delimited form. --------------------------------------------------------------- private methods Transform a line of <code>Text</code> to a <code>Tuple</code>



Set the schema for data to be stored.  This will be called on the front end during planning if the store is associated with a schema. A Store function should implement this function to check that a given schema is acceptable to it.  For example, it can check that the correct partition keys are included; a storage function to be written directly to an OutputFormat can make sure the schema will translate in a well defined way.  Default implementation is a no-op. This method will be called by Pig if the job which contains this store fails. Implementations can clean up output locations in this method to ensure that no incorrect/incomplete results are left in the output location. The default implementation  deletes the output location if it is a {@link FileSystem} location. Default implementation for {@link #cleanupOnFailure(String, Job)} and {@link #cleanupOnSuccess(String, Job)}.  This removes a file from HDFS. This method will be called by Pig if the job which contains this store is successful, and some cleanup of intermediate resources is required. Implementations can clean up output locations in this method to ensure that no incorrect/incomplete results are left in the output location. Return the OutputFormat associated with StoreFunc.  This will be called on the front end during planning and on the backend during execution. Initialize StoreFunc to write data.  This will be called during execution on the backend before the call to putNext. Write a tuple to the data store. This method is called by the Pig runtime in the front end to convert the output location to an absolute path if the location is relative. The StoreFunc implementation is free to choose how it converts a relative location to an absolute location since this may depend on what the location string represent (hdfs path or some other data source). This method will be called by Pig both in the front end and back end to pass a unique signature to the {@link StoreFunc} which it can use to store information in the {@link UDFContext} which it needs to store between various method invocations in the front end and back end. This method will be called before other methods in {@link StoreFunc}.  This is necessary because in a Pig Latin script with multiple stores, the different instances of store functions need to be able to find their (and only their) data in the UDFContext object.  The default implementation is a no-op. Communicate to the storer the location where the data needs to be stored. The location string passed to the {@link StoreFunc} here is the return value of {@link StoreFunc#relToAbsPathForStoreLocation(String, Path)} This method will be called in the frontend and backend multiple times. Implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls. {@link #checkSchema(ResourceSchema)} will be called before any call to {@link #setStoreLocation(String, Job)}. TODO When dropping support for JDK 7 move this as a default method to StoreFuncInterface DAG execution engines like Tez support optimizing union by writing to output location in parallel from tasks of different vertices. Commit is called once all the vertices in the union are complete. This eliminates need to have a separate phase to read data output from previous phases, union them and write out again. Enabling the union optimization requires the OutputFormat to 1) Support creation of different part file names for tasks of different vertices. Conflicting filenames can create data corruption and loss. For eg: If task 0 of vertex1 and vertex2 both create filename as part-r-00000, then one of the files will be overwritten when promoting from temporary to final location leading to data loss. FileOutputFormat has mapreduce.output.basename config which enables naming files differently in different vertices. Classes extending FileOutputFormat and those prefixing file names with mapreduce.output.basename value will not encounter conflict. Cases like HBaseStorage which write to key value store and do not produce files also should not face any conflict. 2) Support calling of commit once at the end takes care of promoting temporary files of the different vertices into the final location. For eg: FileOutputFormat commit algorithm handles promoting of files produced by tasks of different vertices into final output location without issues if there is no file name conflict. In cases like HBaseStorage, the TableOutputCommitter does nothing on commit. If custom OutputFormat used by the StoreFunc does not support the above two criteria, then false should be returned. Union optimization will be disabled for the StoreFunc. Default implementation returns null and in that case planner falls back to {@link PigConfiguration.PIG_TEZ_OPT_UNION_SUPPORTED_STOREFUNCS} and {@link PigConfiguration.PIG_TEZ_OPT_UNION_UNSUPPORTED_STOREFUNCS} settings to determine if the StoreFunc supports it. Issue a warning.  Warning messages are aggregated and reported to the user.
Call {@code StoreFunc#putNext(Tuple)} and handle errors
Set the schema for data to be stored.  This will be called on the front end during planning if the store is associated with a schema. A Store function should implement this function to check that a given schema is acceptable to it.  For example, it can check that the correct partition keys are included; a storage function to be written directly to an OutputFormat can make sure the schema will translate in a well defined way. This method will be called by Pig if the job which contains this store fails. Implementations can clean up output locations in this method to ensure that no incorrect/incomplete results are left in the output location This method will be called by Pig if the job which contains this store is successful, and some cleanup of intermediate resources is required. Implementations can clean up output locations in this method to ensure that no incorrect/incomplete results are left in the output location Return the OutputFormat associated with StoreFuncInterface.  This will be called on the front end during planning and on the backend during execution. Initialize StoreFuncInterface to write data.  This will be called during execution before the call to putNext. Write a tuple to the data store. This method is called by the Pig runtime in the front end to convert the output location to an absolute path if the location is relative. The StoreFuncInterface implementation is free to choose how it converts a relative location to an absolute location since this may depend on what the location string represent (hdfs path or some other data source). The static method {@link LoadFunc#getAbsolutePath} provides a default implementation for hdfs and hadoop local file system and it can be used to implement this method. This method will be called by Pig both in the front end and back end to pass a unique signature to the {@link StoreFuncInterface} which it can use to store information in the {@link UDFContext} which it needs to store between various method invocations in the front end and back end.  This is necessary because in a Pig Latin script with multiple stores, the different instances of store functions need to be able to find their (and only their) data in the UDFContext object. Communicate to the storer the location where the data needs to be stored. The location string passed to the {@link StoreFuncInterface} here is the return value of {@link StoreFuncInterface#relToAbsPathForStoreLocation(String, Path)} This method will be called in the frontend and backend multiple times. Implementations should bear in mind that this method is called multiple times and should ensure there are no inconsistent side effects due to the multiple calls. {@link #checkSchema(ResourceSchema)} will be called before any call to {@link #setStoreLocation(String, Job)}.
The wrapped StoreMetadata object must be set before method calls are made on this object. Typically, this is done with via constructor, but often times the wrapped object can not be properly initialized until later in the lifecycle of the wrapper object.
Returns a method in the call stack at the given depth. Depth 0 will return the method that called this getMethodName, depth 1 the method that called it, etc... The wrapped StoreFuncInterface object must be set before method calls are made on this object. Typically, this is done with via constructor, but often times the wrapped object can not be properly initialized until later in the lifecycle of the wrapper object.
Store schema of the data being written Store statistics about the data being written.
Allow a StoreFunc to specify a list of files it would like placed in the distributed cache. The default implementation returns null. Allow a StoreFunc to specify a list of files located locally and would like to ship to backend (through distributed cache). Check for {@link FuncUtils} for utility function to facilitate it The default implementation returns null.

Create and add the widgets. Create the GUI and show it.  For thread safety, this method should be invoked from the event-dispatching thread.
Given a byte array from a streaming executable, produce a tuple. This will be called on both the front end and the back end during execution.

Attach a {@link HandleSpec} to a given {@link Handle} Add a file to be cached on execute nodes on the cluster. The file is assumed to be available at the shared filesystem. Add a file to be shipped to the cluster. Users can use this to distribute executables and other necessary files to the clusters. Get the list of files which need to be cached on the execute nodes. Get the parsed command arguments. Get the command to be executed. Get specifications for the given <code>Handle</code>. Get the input specification of the <code>StreamingCommand</code>. Get the directory where the log-files of the command are persisted. Get the maximum number of tasks whose stderr logs files are persisted. Get the specification of the primary output of the <code>StreamingCommand</code>. Should the stderr of the managed process be persisted? Get whether files for this command should be shipped or not. Get the list of files which need to be shipped to the cluster. Set the command line arguments for the <code>StreamingCommand</code>. Set the executable for the <code>StreamingCommand</code>. Set the input specification for the <code>StreamingCommand</code>. Set the directory where the log-files of the command are persisted. Set the maximum number of tasks whose stderr logs files are persisted. Set the specification for the primary output of the <code>StreamingCommand</code>. Specify if the stderr of the managed process should be persisted. Set whether files should be shipped or not.
Check if file is in the list paths to be skipped


Need to make sure the user's file is available. If jar hasn't been exploded, just copy the udf file to its path relative to the controller file and update file cache path appropriately. Find the path to the controller file for the streaming language. First check path to job jar and if the file is not found (like in the case of running hadoop in standalone mode) write the necessary files to temporary files and return that path.




Create an external process for StreamingCommand command. Set up the run-time environment of the managed process.




return each line from input as one field in the tuple in each getNext()




Compares the two bag fields from input Tuple and returns a new bag composed of elements of first bag not in the second bag.



@Override



Get the progress within the split
(non-Javadoc) @see org.apache.hadoop.mapreduce.RecordWriter#close(org.apache.hadoop.mapreduce.TaskAttemptContext) (non-Javadoc) @see org.apache.hadoop.mapreduce.RecordWriter#write(java.lang.Object, java.lang.Object)

(non-Javadoc) @see org.apache.pig.EvalFunc#outputSchema(org.apache.pig.impl.logicalLayer.schema.Schema) If all the columns in the tuple are of same type, then set the bag schema to bag of tuple with column of this type
(non-Javadoc) @see org.apache.pig.EvalFunc#outputSchema(org.apache.pig.impl.logicalLayer.schema.Schema) If all the columns in the tuple are of same type, then set the bag schema to bag of tuple with column of this type


(non-Javadoc) @see java.lang.Object#equals(java.lang.Object) (non-Javadoc) @see java.lang.Object#hashCode()



TextLoader does not support conversion to Bag TextLoader does not support conversion to Boolean Cast data from bytes to chararray value. TextLoader does not support conversion to DateTime TextLoader does not support conversion to Double TextLoader does not support conversion to Float TextLoader does not support conversion to Integer TextLoader does not support conversion to Long TextLoader does not support conversion to Tuple
Adds the plan for DISTINCT. Note that the PODistinct is not actually added to the plan, but rather is implemented by the action of the local rearrange, shuffle and project operations. Add a sampler to the sort input The front-end method that the user calls to compile the plan. Assumes that all submitted plans have a Store operators as the leaf. Compiles the plan below op into a Tez Operator and stores it in curTezOp. Sets up the indexing job for map-side cogroups. Get LocalRearrange for POSort input Returns a POPackage with default packager. This method shouldn't be used if special packager such as LitePackager and CombinerPackager is needed. Segment a single DAG into a DAG graph Create a sampling job to collect statistics by sampling input data. The sequence of operations is as following: <li>Add an extra field &quot;all&quot; into the tuple </li> <li>Package all tuples into one bag </li> <li>Add constant field for number of reducers. </li> <li>Sorting the bag </li> <li>Invoke UDF with the number of reducers and the sorted bag.</li> <li>Data generated by UDF is transferred via a broadcast edge.</li> Returns a temporary DFS Path Merges the TezOperators in the compiledInputs into a single merged TezOperator. Care is taken to remove the TezOperators that are merged from the TezPlan and their connections moved over to the merged map TezOperator. Merge is implemented as a sequence of binary merges. merge(PhyPlan finPlan, List<PhyPlan> lst) := finPlan, merge(p) foreach p in lst The merge of a list of plans into a single plan Since merge-join works on two inputs there are exactly two TezOper predecessors identified  as left and right. Right input generates index on-the-fly. This consists of two Tez vertexes. The first vertex generates index, and the second vertex sort them. Left input contains POMergeJoin which do the actual join. First right Tez oper is identified as rightTezOpr, second is identified as rightTezOpr2 Left Tez oper is identified as curTezOper. 1) RightTezOpr: It can be preceded only by POLoad. If there is anything else in physical plan, that is yanked and set as inner plans of joinOp. 2) LeftTezOper:  add the Join operator in it. We also need to segment the DAG into two, because POMergeJoin depends on the index file which loads with DefaultIndexableLoader. It is possible to convert the index as a broadcast input, but that is costly because a lot of logic is built into DefaultIndexableLoader. We can revisit it later.

Used with POValueOutputTez Get a plain POForEach: ForEach X generate flatten($1) simpleConnectTwoVertex is a utility to end a vertex equivalent to map and start vertex equivalent to reduce in a tez operator: 1. op1 is open 2. op2 is blank POPackage to start a reduce vertex 3. POLocalRearrange/POPackage are trivial 4. User need to connect op1 to op2 themselves
Updates the statistics after a DAG is finished.
Hack to turn off relocalization till TEZ-2192 is fixed. Return EdgeProperty that connects two vertices.




Add to the list of inputs to skip download if already available in vertex cache Attach the inputs to the operator. Also ensure reader.next() is called to force fetch the input so that all inputs are fetched and memory released before memory is allocated for outputs







In the statement "CACHE('/input/data.txt#alias.txt')" we'll map the resource name alias.txt to the actual resource path in the remote FS at '/input/data.txt'. Add extra plan-specific local resources already present in the remote FS Add extra plan-specific local resources from the source FS In the statement "SHIP('/home/foo')" we'll map the resource name foo to the file that has been copied to the staging directory in the remote FS. Get the plan-specific resources This method is used in PigGraceShuffleVertexManager to get a list of grandparents. Also need to exclude grandparents which also a parent (a is both parent and grandparent in the diagram below) a   ->    c \  b  /  Move everything below a given operator to the new operator plan.  The specified operator will be moved and will be the root of the new operator plan
Union is the only operator that uses alias vertex (VertexGroup) now. But more operators could be added to the list in the future.





Updates the statistics after a DAG is finished.
Add the Pig jar and the UDF jars as AM resources (all DAG's in the planContainer will use them for simplicity). This differs from MR Pig, where they are added to the job jar.





Add files already present in the remote FS as local resources. Allow the resource name to be different from the file name to to support resource aliasing in a CACHE statement (and to allow the same file to be aliased with multiple resource names). Add files from the source FS as local resources. The resource name will be the same as the file name. This method is only used by test code to reset state.









ISO-8601 format and JDBC timestamp format are similar but not the same. Strict ISO-8601 specifies a 'T' between the date portion and the time portion: 2015-05-29T10:41:30.123 ISO-8601 allows a space instead of a 'T' as a looser variant. This variant is often adopted because it increases human readability. The JDBC timestamp format uses the ' ' space variant. 2015-05-29 10:41:30.123 Hive & Impala are database-oriented and generate JDBC timestamps with a ' ' space. We would like to accept both 'T' and ' ' space formats. org.joda.time.format.ISODateTimeFormatter requires the 'T'. The cleanest way to get joda-time to accept both is to convert the ' ' space to a a 'T' before feeding the string to the ISODateTimeFormatter.



(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping() This is needed to make sure that both bytearrays and chararrays can be passed as arguments This method gives a name to the column.




An optional attribute value of the Token. Tokens which are not used as syntactic sugar will often contain meaningful values that will be used later on by the compiler or interpreter. This attribute value is often different from the image. Any subclass of Token that actually wants to return a non-null value can override this method as appropriate. Returns a new Token object, by default. However, if you want, you can create and return subclass objects based on the value of ofKind. Simply add the cases to the switch for all those special cases. For example, if you have a subclass of Token called IDToken that you want to create if ofKind is ID, simply add something like : case MyParserConstants.ID : return new IDToken(ofKind, image); to the following switch statement. Then you can cast matchedToken variable to the appropriate type and use sit in your lexical actions. Returns the image.
Returns a detailed message for the Error when it is thrown by the token manager to indicate a lexical error. Parameters : EOFSeen     : indicates if EOF caused the lexical error curLexState : lexical state in which this error occurred errorLine   : line number when the error occurred errorColumn : column number when the error occurred errorAfter  : prefix that was seen before this error occurred curchar     : the offending character Note: You can customize the lexical error message by modifying this method. Replaces unprintable characters by their escaped (or unicode escaped) equivalents in the given string You can also modify the body of this method to customize your error messages. For example, cases like LOOP_DETECTED and INVALID_LEXICAL_STATE are not of end-users concern, so you can return something like : "Internal Error : Please file a bug report .... " from this method for such cases in the release version of your parser.
Get a class containing the Pig plans.  For now it just contains the new logical plan.  At some point in the future it should contain the MR plan as well. Register a script without running it.  This method is not compatible with {@link #registerQuery(String)}, {@link #registerScript(String)}, {@link #store(String, String)} or {@link #openIterator(String)}.  It can be used with {@link #getPlans()} and {@link #runPlan(LogicalPlan, String)} in this class only. The proper control flow is for the caller to call registerNoRun() and then getPlans() to get a copy of the plans.  The user can then modify the logical plan.  It can then be returned via runPlan(), which will execute the plan. Given a (modified) new logical plan, run the script.
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
check if the transform should be done.  If this is being called then the pattern matches, but there may be other criteria that must be met as well. Report what parts of the tree were transformed.  This is so that listeners can know which part of the tree to visit and modify schemas, annotations, etc.  So any nodes that were removed need will not be in this plan, only nodes that were added or moved. Transform the tree

@see org.apache.pig.LoadFunc#getInputFormat() @see org.apache.pig.StoreFuncInterface#getOutputFormat()
(non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
Append a field to a tuple.  This method is not efficient as it may force copying of existing data in order to grow the data structure. Whenever possible you should construct your Tuple with {@link TupleFactory#newTuple(int)} and then fill in the values with {@link #set(int, Object)}, rather than construct it with {@link TupleFactory#newTuple()} and append values. Get the value in a given field. Get all of the fields in the tuple as a list. Determine the size of tuple in memory.  This is used by data bags to determine their memory size.  This need not be exact, but it should be a decent estimation. Find the type of a given field. Find out if a given field is null. Make this tuple reference the contents of another.  This method does not copy the underlying data.   It maintains references to the data from the original tuple (and possibly even to the data structure holding the data). Set the value in a given field.  This should not be called unless the tuple was constructed by {@link TupleFactory#newTuple(int)} with an argument greater than the fieldNum being passed here.  This call will not automatically expand the tuple size.  That is if you called {@link TupleFactory#newTuple(int)} with a 2, it is okay to call this function with a 1, but not with a 2 or greater. Find the size of the tuple.  Used to be called arity(). Write a tuple of values into a string. The output will be the result of calling toString on each of the values in the tuple.
Get a reference to the singleton factory. This method is used to inspect whether the Tuples created by this factory will be of a fixed size when they are created. In practical terms, this means whether they support append or not. Create an empty tuple.  This should be used as infrequently as possible, use newTuple(int) instead. Create a tuple with size fields.  Whenever possible this is preferred over the null constructor, as the constructor can preallocate the size of the container holding the fields.  Once this is called, it is legal to call Tuple.set(x, object), where x &lt; size. Create a tuple with a single element.  This is useful because of the fact that bags (currently) only take tuples, we often end up sticking a single element in a tuple in order to put it in a bag. Create a tuple from the provided list of objects.  The underlying list will be copied. Create a tuple from a provided list of objects, keeping the provided list.  The new tuple will take over ownership of the provided list. Provided for testing purposes only.  This function should never be called by anybody but the unit tests. Return the actual class representing a tuple that the implementing factory will be returning.  This is needed because Hadoop needs to know the exact class we will be using for input and output. Return the actual class implementing the raw comparator for tuples that the factory will be returning. Ovverride this to allow Hadoop to speed up tuple sorting. The actual returned class should know the serialization details for the tuple. The default implementation (PigTupleDefaultRawComparator) will serialize the data before comparison
Default implementation of format of tuple (each filed is delimited by tab)

Checks if one of the compared tuples had a null field. This method is meaningful only when {@link #compare(byte[],int,int,byte[],int,int)} has returned a zero value (i.e. tuples are determined to be equal).

This is a simple utility function that make word-level ngrams from a set of words This function splits a search query string into a set of non-empty words



Add casts to promote numeric type to larger of two input numeric types of the {@link BinaryExpression}  binOp . If one of the inputs is numeric and other bytearray, cast the bytearray type to other numeric type. If both inputs are bytearray, cast them to double. Tries to find the schema supported by one of funcSpecs which can be obtained by inserting a set of casts to the input schema Tries to find the schema supported by one of funcSpecs which can be obtained by inserting a set of casts to the input schema Checks to see if any field of the input schema is a byte array * Helper for collecting warning when casting is inserted to the plan (implicit casting) Finds if there is an exact match between the schema supported by one of the funcSpecs and the input schema s. Here an exact match for all fields is attempted. Finds if there is an exact match between the schema supported by one of the funcSpecs and the input schema s Finds if there is an exact match between the schema supported by one of the funcSpecs and the input schema s. Here first exact match for all non byte array fields is first attempted and if there is exactly one candidate, it is chosen (since the bytearray(s) can just be cast to corresponding type(s) in the candidate) Computes a modified version of manhattan distance between the two schemas: s1 & s2. Here the value on the same axis are preferred over values that change axis as this means that the number of casts required will be lesser on the same axis. However, this function ceases to be a metric as the triangle inequality does not hold. Each schema is an s1.size() dimensional vector. The ordering for each axis is as defined by castLookup. Unallowed casts are returned a dist of INFINITY. Gets the positions in the schema which are byte arrays add cast to convert the input of exp {@link LogicalExpression} arg to type toType Check if the fieldSch is a bag with empty tuple schema ************************************************************************* Compare two schemas for equality for argument matching purposes. This is a more relaxed form of Schema.equals wherein first the Datatypes of the field schema are checked for equality. Then if a field schema in the udf schema is for a complex type AND if the inner schema is NOT null, check for schema equality of the inner schemas of the UDF field schema and input field schema For Basic Types: 0) Casting to itself is always ok 1) Casting from number to number is always ok 2) ByteArray to anything is ok 3) number to chararray is ok For Composite Types: Recursively traverse the schemas till you get a basic type {@link RegexExpression} expects CharArray as input Itself always returns Boolean
This can be used to get the merged type of output join col only when the join/cogroup col is of atomic type This can be used to get the merged type of output join col only when the join col is of atomic type Create combined group-by/join column schema based on join/cogroup expression plans for all inputs. This implementation is based on the assumption that all the inputs have the same join col tuple arity. Cast the single output operator of innerPlan to toType * For casting insertion for relational operators only if it's necessary Currently this only does "shallow" casting   COGroup All group by cols from all inputs have to be of the same type * Return concatenated of all fields from all input operators If one of the inputs have no schema then we cannot construct the output schema. * LODistinct, output schema should be the same as input * The schema of filter output will be the same as filter input LOJoin visitor * The schema of rank output will be the same as input, plus a rank field. * The schema of sort output will be the same as sort input. * The schema of split output will be the same as split input (non-Javadoc) @see org.apache.pig.newplan.logical.relational.LogicalRelationalNodesVisitor#visit(org.apache.pig.newplan.logical.relational.LOUnion) The output schema of LOUnion is the merge of all input schemas. Operands on left side always take precedance on aliases. We allow type promotion here
Get the underlying class for a type, or null if the type is a variable type. Incremented counters will use this as the counter group. Typically this works fine, since the subclass name is enough to identify the UDF. In some cases though (i.e. a UDF wrapper that is a facade to a number of different transformation functions), a more specific group name is needed. Get the actual type arguments a child class has used to extend a generic base class. Increment Hadoop counters for bad inputs which are either null or too small. Increment Hadoop counters for bad inputs which are either null or too small.
Adds the JobConf to this singleton.  Will be called on the backend by the Map and Reduce functions so that UDFs can obtain the JobConf on the backend. Make a shallow copy of the context. Populate the udfConfs field.  This function is intended to be called by Map.configure or Reduce.configure on the backend. It assumes that addJobConf has already been called. Get the System Properties (Read only) as on the client machine from where Pig was launched. This will include command line properties passed at launch time Get the JobConf.  This should only be called on the backend.  It will return null on the frontend.  Get a properties object that is specific to this UDF. Note that if a given UDF is called multiple times in a script, they will all be provided the same configuration object.  It is up to the UDF to make sure the multiple instances do not stomp on each other. It is guaranteed that this properties object will be separate from that provided to any other UDF. Note that this can only be used to share information across instantiations of the same function in the front end and between front end and back end.  It cannot be used to share information between instantiations (that is, between map and/or reduce instances) on the back end at runtime. Get a properties object that is specific to this UDF. Note that if a given UDF is called multiple times in a script, and each instance passes different arguments, then each will be provided with different configuration object. This can be used by loaders to pass their input object path or URI and separate themselves from other instances of the same loader.  Constructor arguments could also be used, as they are available on both the front and back end. Note that this can only be used to share information across instantiations of the same function in the front end and between front end and back end.  It cannot be used to share information between instantiations (that is, between map and/or reduce instances) on the back end at runtime. internal pig use only - should NOT be called from user code Convenience method for UDF code to check where it runs (see PIG-2576) Internal pig use Serialize the UDF specific information into an instance of JobConf.  This function is intended to be called on the front end in preparation for sending the data to the backend. internal pig use only - should NOT be called from user code internal pig use only - should NOT be called from user code internal pig use only - should NOT be called from user code @StaticDataCleanup





java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
Upper-cases an input string. (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping() This method gives a name to the column.







Get the expression that this unary expression operators on.
Get child expression of this expression Get the contained expression. Set the contained expression explicitly.  This is mostly for testing. Set the contained expression to the be the input value.



Clone plan of union and merge it into the predecessor operator Connects the unionOp predecessor to the store vertex groups and the output vertex groups and disconnects it from the unionOp. Connect the split operator to the successors of the union operators and update the edges. Also change the inputs of the successor from the union operator to the split operator. Connect the predecessors of the union which are not members of the union (usually FRJoin replicated table orSkewedJoin sample) to the Split op which is the only member of the union. Disconnect those predecessors from the union. Replace the output keys of those predecessors with the split operator key instead of the union operator key. Connect vertexgroup operator to successor operator in the plan. Copy the output edge between union operator and successor to between predecessors and successor. Predecessor output key and output edge points to successor so that we have all the edge configuration, but they are connected to the vertex group in the plan.




TODO need to fix this to use the updated code, it currently won't copy properly if called before it's done (maybe that's ok?)
Sanity check of whether this number is a valid integer or long.
If schema argument has fields where a bag does not contain a tuple schema, it inserts a tuple schema. It does so for all inner levels. eg bag({int}) => bag({(int)}) Returns a LinkedList of operators contained within the physical plan which implement the supplied class, in dependency order. Returns an empty LinkedList of no such operators exist. This function translates the new LogicalSchema into old Schema format required by PhysicalOperators
Thanks, HBase Find a jar that contains a class of the same name, if any. It will return a jar file, even if that is not the first thing on the class path that has a class with the same name. Looks first on the classpath and then in the <code>packagedClasses</code> map. If org.apache.hadoop.util.JarFinder is available (0.23+ hadoop), finds the Jar for a class or creates it if it doesn't exist. If the class is in a directory in the classpath, it creates a Jar on the fly with the contents of the directory and returns the path to that Jar. If a Jar is created, it is created in the system temporary directory. Otherwise, returns an existing jar that contains a class of the same name. Maintains a mapping from jar contents to the tmp jar created. Returns the full path to the Jar containing the class. It always return a JAR. Add entries to <code>packagedClasses</code> corresponding to class files contained in <code>jar</code>.










Returns the current contents of the buffer. Data is only valid to {@link #getLength()}. Returns the length of the valid data currently in the buffer

Creates a tuple from a matched string



input should contain: 1) xml 2) xpath 3) optional cache xml doc flag 4) optional ignore namespace flag Usage: 1) XPath(xml, xpath) 2) XPath(xml, xpath, false) 3) XPath(xml, xpath, false, false) Returns argument schemas of the UDF. Validates values of the input parameters.
Returns a new the xPathString by adding additional parameters in the existing xPathString for ignoring the namespace during compilation. input should contain: 1) xml 2) xpath 3) optional cache xml doc flag 4) optional ignore namespace flag The optional fourth parameter (IGNORE_NAMESPACE), if set true will remove the namespace from xPath For example xpath /html:body/html:div will be considered as /body/div Usage: 	1) XPathAll(xml, xpath) 2) XPathAll(xml, xpath, false) 3) XPathAll(xml, xpath, false, false) Returns argument schemas of the UDF. Validates values of the input parameters.


java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()
java level API (non-Javadoc) @see org.apache.pig.EvalFunc#getArgToFuncMapping()


