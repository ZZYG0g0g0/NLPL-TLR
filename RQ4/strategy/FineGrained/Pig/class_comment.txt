ABS implements a binding to the Java function {@link java.lang.Math#abs(double) Math.abs(double)} for computing the absolute value of the argument. The returned value will be a double which is absolute value of the input.
ACOS implements a binding to the Java function {@link java.lang.Math#acos(double) Math.acos(double)} for computing the arc cosine of value of the argument. The returned value will be a double which is the arc cosine of the value of  input.
Find the number of fields in a tuple.  Expected input is a tuple, output is an integer.
ASIN implements a binding to the Java function {@link java.lang.Math#asin(double) Math.asin(double)} for computing the arc sine of value of the argument. The returned value will be a double which is the arc sine of the value of  input.
ATAN implements a binding to the Java function {@link java.lang.Math#atan(double) Math.atan(double)} for computing the arc tangent of value of the argument. The returned value will be a double which is the arc tangent of the value of  input.
math.ATAN2 implements a binding to the Java function {@link java.lang.Math#atan2(double,double) Math.atan2(double,double)}. Given a tuple with two data atom x and y it will returns the angle theta from the conversion of rectangular coordinates (x, y) to polar coordinates (r, theta). <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>atan2_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.ATAN2(float1); </code></dd> </dl>
Generates the average of a set of values. This class implements {@link org.apache.pig.Algebraic}, so if possible the execution will performed in a distributed fashion. <p> AVG can operate on any numeric type.  It can also operate on bytearrays, which it will cast to doubles.    It expects a bag of tuples of one record each.  If Pig knows from the schema that this function will be passed a bag of integers or longs, it will use a specially adapted version of AVG that uses integer arithmetic for summing the data.  The return type of AVG will always be double, regardless of the input type. <p> AVG implements the {@link org.apache.pig.Accumulator} interface as well. While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation
A LoadStoreFunc for retrieving data from and storing data to Accumulo A Key/Val pair will be returned as tuples: (key, colfam, colqual, colvis, timestamp, value). All fields except timestamp are DataByteArray, timestamp is a long. Tuples can be written in 2 forms: (key, colfam, colqual, colvis, value) OR (key, colfam, colqual, value)
This class provides a convenient base for Tuple implementations. This makes it easier to provide default implementations as the Tuple interface is evolved.

This class is for testing of accumulator udfs
This interface is used during Reduce phrase to process tuples in batch mode. It is used by POPackage when all of the UDFs can be called in accumulative mode. Tuples are not pulled all at once, instead, each time, only a specified number of tuples are pulled out of iterator and put in an buffer. Then this buffer is wrapped into a bag to be passed to the operators in reduce plan. The purpose of doing this is to reduce memory usage and avoid spilling.
An interface that allows UDFs that take a bag to accumulate tuples in chunks rather than take the whole set at once.  This is intended for UDFs that do not need to see all of the tuples together but cannot be used with the combiner.  This lowers the memory needs, avoiding the need to spill large bags, and thus speeds up the query.  An example is something like session analysis. It cannot be used with the combiner because all it's inputs must first be ordered.  But it does not need to see all the tuples at once.  UDF implementors might also choose to implement this interface so that if other UDFs in the FOREACH implement it it can be used.
This annotation is to be used on a Groovy method which is part of an Accumulator UDF to indicate that it will act as the 'accumulate' method of the UDF. The value of the annotation is the name of the Accumulator UDF it belongs to. The annotated method MUST NOT be static and MUST accept a single groovy.lang.Tuple as parameter. Its return value, if any, is ignored.

This annotation is to be used on a Groovy method which is part of an Accumulator UDF to indicate that it will act as the 'cleanup' method of the UDF. The value of the annotation is the name of the Accumulator UDF it belongs to. The annotated method MUST NOT be static and MUST NOT accept any parameters. Its return value, if any, is ignored.
This class is used to provide a free implementation of the EvalFunc exec function given implementation of the Accumulator interface. Instead of having to provide a redundant implementation, this provides the base exec function for free, given that the methods associated with the Accumulator interface are implemented. For information on how to implement Accumulator, see {@link Accumulator}.
This annotation is to be used on a Groovy method which is part of an Accumulator UDF to indicate that it will act as the 'getValue' method of the UDF. The value of the annotation is the name of the Accumulator UDF it belongs to. The annotated method MUST NOT be static and MUST NOT accept any parameters. Its return value will be that of the Accumulator UDF.
A visitor to optimize plans that determines if a vertex plan can run in accumulative mode.

A LoadStoreCaster implementation which stores most type implementations as bytes generated from the toString representation with a UTF8 Charset. Pulled some implementations from the Accumulo Lexicoder implementations in 1.6.0.
Basic PigStorage implementation that uses Accumulo as the backing store. <p> When writing data, the first entry in the {@link Tuple} is treated as the row in the Accumulo key, while subsequent entries in the tuple are handled as columns in that row. {@link Map}s are expanded, placing the map key in the column family and the map value in the Accumulo value. Scalars are placed directly into the value with an empty column qualifier. If the columns argument on the constructor is omitted, null or the empty String, no column family is provided on the Keys created for Accumulo </p> <p> When reading data, if aggregateColfams is true, elements in the same row and column family are aggregated into a single {@link Map}. This will result in a {@link Tuple} of length (unique_column_families + 1) for the given row. If aggregateColfams is false, column family and column qualifier are concatenated (separated by a colon), and placed into a {@link Map}. This will result in a {@link Tuple} with two entries, where the latter element has a number of elements equal to the number of columns in the given row. </p>


<p>AddDuration returns the result of a DateTime object plus a Duration object</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Duration Format: http://en.wikipedia.org/wiki/ISO_8601#Durations</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dr:chararray); DESCRIBE ISOin; ISOin: {dt: datetime,dr: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,PT1S) (2008-02-06T02:06:02.000Z,PT1M) (2007-03-05T03:05:03.000Z,P1D) ... dtadd = FOREACH ISOin GENERATE AddDuration(dt, dr) AS dt1; DESCRIBE dtadd; dtadd: {dt1: datetime} DUMP dtadd; (2009-01-07T01:07:02.000Z) (2008-02-06T02:07:02.000Z) (2007-03-06T03:05:03.000Z) </pre>
Add Operator

An interface to declare that an EvalFunc's calculation can be decomposed into intitial, intermediate, and final steps. More formally, suppose we have to compute an function f over a bag X. In general, we need to know the entire X before we can make any progress on f. However, some functions are <i>algebraic</i> e.g. SUM. In these cases, you can apply some initital function f_init on subsets of X to get partial results. You can then combine partial results from different subsets of X using an intermediate function f_intermed. To get the final answers, several partial results can be combined by invoking a final f_final function. For the function SUM, f_init, f_intermed, and f_final are all SUM. See the code for builtin AVG to get a better idea of how algebraic works. When eval functions implement this interface, Pig will attempt to use MapReduce's combiner. The initial funciton will be called in the map phase and be passed a single tuple.  The intermediate function will be called 0 or more times in the combiner phase.  And the final function will be called once in the reduce phase.  It is important that the results be the same whether the intermediate function is called 0, 1, or more times.  Hadoop makes no guarantees about how many times the combiner will be called in a job.
Core logic for applying an SUM function to a bag of BigDecimals. This is a modified version of AlgebraicDoubleMathBase
Core logic for applying an SUM function to a bag of BigIntegers. This is a modified version of AlgebraicDoubleMathBase
Core logic for applying an accumulative/algebraic math function to a bag of doubles.
Core logic for applying an accumulative/algebraic math function to a bag of doubles.
This class is used to provide a free implementation of the Accumulator interface and EvalFunc class in the case of an Algebraic function. Instead of having to provide redundant implementations for Accumulator and EvalFunc, implementing the getInitial, getIntermed, and getFinal methods (which implies implementing the static classes they reference) will give you an implementation of each of those for free. <br><br> One key thing to note is that if a subclass of AlgebraicEvalFunc wishes to use any constructor arguments, it MUST call super(args). <br><br> IMPORTANT: the implementation of the Accumulator interface that this class provides is good, but it is simulated. For maximum efficiency, it is important to manually implement the accumulator interface. See {@link Accumulator} for more information on how to do so.
This annotation is to be used on a Groovy method which is part of an Algebraic UDF to indicate that it will act as the exec method of the 'Final' class of the UDF. The value of the annotation is the name of the Algebraic UDF it belongs to. The annotated method MUST NOT accept any parameters. Its return value will be that of the Algebraic UDF.
Core logic for applying an accumulative/algebraic math function to a bag of Floats.
This annotation is to be used on a Groovy method which is part of an Algebraic UDF to indicate that it will act as the exec method of the 'Initial' class of the UDF. The value of the annotation is the name of the Algebraic UDF it belongs to. The annotated method MUST accept a single groovy.lang.Tuple parameter, and MUST return a groovy.lang.Tuple or Object[].
Core logic for applying an accumulative/algebraic math function to a bag of doubles.
This annotation is to be used on a Groovy method which is part of an Algebraic UDF to indicate that it will act as the exec method of the 'Intermed' class of the UDF. The value of the annotation is the name of the Algebraic UDF it belongs to. The annotated method MUST accept a single groovy.lang.Tuple parameter, and MUST return a groovy.lang.Tuple or Object[].
Core logic for applying an accumulative/algebraic math function to a bag of Longs.
The purpose of this class is to hold some of the common code shared among the typed Basic*Funcs (BasicDoubleFunc, BasicIntegerFunc, etc).
Grammar file for Pig tree parser (visitor for default data type insertion). NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.
A visitor that walks a logical plan and then applies a given LogicalExpressionVisitor to all expressions it encounters.

The AllLoader provides the ability to point pig at a folder that contains files in multiple formats e.g. PlainText, Gz, Bz, Lzo, HiveRC etc and have the LoadFunc(s) automatically selected based on the file extension. <br/> <b>How this works:<b/><br/> The file extensions are mapped in the pig.properties via the property file.extension.loaders. <p/> <b>file.extension.loaders format</b> <ul> <li>[file extension]:[loader func spec]</li> <li>[file-extension]:[optional path tag]:[loader func spec]</li> <li>[file-extension]:[optional path tag]:[sequence file key value writer class name]:[loader func spec]</li> </ul> <p/> The file.extension.loaders property associate pig loaders with file extensions, if a file does not have an extension the AllLoader will look at the first three bytes of a file and try to guess its format bassed on: <ul> <li>[ -119, 76, 90 ] = lzo</li> <li>[ 31, -117, 8 ] = gz</li> <li>[ 66, 90, 104 ] = bz2</li> <li>[ 83, 69, 81 ] = seq</li> </ul> <br/> The loader associated with that extension will then be used. <p/> <b>Path partitioning</b> The AllLoader supports hive style path partitioning e.g. /log/type1/daydate=2010-11-01<br/> "daydate" will be considered a partition key and filters can be written against this.<br/> Note that the filter should go into the AllLoader contructor e.g.<br/> a = LOAD 'input' using AllLoader('daydate<\"2010-11-01\"')<br/> <b>Path tags</b> AllLoader supports configuring different loaders for the same extension based on there file path.<br/> E.g.<br/> We have the paths /log/type1, /log/type2<br/> For each of these directories we'd like to use different loaders.<br/> So we use setup our loaders:<br/> file.extension.loaders:gz:type1:MyType1Loader, gz:type2:MyType2Loader<br/> <p/> <b>Sequence files<b/> Sequence files also support using the Path tags for loader selection but has an extra configuration option that relates to the Key Class used to write the Sequence file.<br/> E.g. for HiveRC this value is: org.apache.hadoop.hive.ql.io.RCFile so we can setup our sequence file formatting:<br/> file.extension.loaders:seq::org.apache.hadoop.hive.ql.io.RCFile: MyHiveRCLoader, seq::DefaultSequenceFileLoader<br/> <p/> <b>Schema</b> The JsoneMetadata schema loader is supported and the schema will be loaded using this loader.<br/> In case this fails, the schema can be loaded using the default schema provided.

A visitor that walks the logical plan and calls the same method on every type of node.  Subclasses can extend this and implement the execute method, and this method will be called on every node in the graph.

Boolean and expression.



Grammar file for Pig tree parser (visitor for printing Pig script from Ast). NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.
Grammar file for Pig tree parser (visitor for default data type insertion). NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.
This is used to generate synthetic data Synthetic data generation is done by making constraint tuples for each operator as we traverse the plan and try to replace the constraints with values as far as possible. We only deal with simple conditions right now
RecordReader for Avro files
Class that implements the Pig bag interface, wrapping an Avro array. Designed to reduce data copying.
Wrapper for map objects, so we can translate UTF8 objects to Strings if we encounter them.
RecordReader for Avro files
RecordWriter for Avro objects.
This class converts Avro schema to Pig schema
This class creates two maps out of a given Avro schema. And it supports looking up avro schemas using either type name or field name. 1. map[type name] = > avro schema 2. map[field name] => avro schema
Pig UDF for reading and writing Avro data.
Utility classes for AvroStorage; contains static methods for converting between Avro and Pig objects.
Adapt an {@link FSDataInputStream} to {@link SeekableInput}.
Simple logging utils of this package
Static methods for converting from Avro Schema object to Pig Schema objects, and vice versa.
This is utility class for this package
Object that wraps an Avro object in a tuple.

Base class for both the compress and decompress classes. Holds common arrays, and static data.








Factory for constructing different types of bags. This class is abstract so that users can override the bag factory if they desire to provide their own that returns their implementation of a bag.  If the property pig.data.bag.factory.name is set to a class name and pig.data.bag.factory.jar is set to a URL pointing to a jar that contains the above named class, then getInstance() will create an instance of the named class using the indicated jar.  Otherwise, it will create an instance of DefaultBagFactory.

This method should never be used directly, use {@link SIZE}.
Flatten a bag into a string.  This UDF will use the character '_' as the default delimiter if one is not provided. Example: bag = {(a),(b),(c)} BagToString(bag) -> 'a_b_c' BagToString(bag, '+') -> 'a+b+c' bag = {(a,b), (c,d), (e,f)} BagToString(bag) --> 'a_b_c_d_e_f' If input bag is null, this UTF will return null;
Flatten a bag into a tuple.  This UDF performs only flattening at the first level, it doesn't recursively flatten nested bags. Example: {(a),(b),(c)} --> (a,b,c) {(a,b), (c,d), (e,f)} --> (a,b,c,d,e,f); If input bag is null, this UDF will return null;
base class for math udfs


This method should never be used directly, use {@link AVG}.
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MIN}.
This method should never be used directly, use {@link SUM}.
Max and min seeds cannot be defined to BigDecimal as the value could go as large as The computer allows. This wrapper is used to provide seed for MIN and MAX functions in AlgrebraicBigDecimalMathBase.java


This method should never be used directly, use {@link AVG}.
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MIN}.
This method should never be used directly, use {@link SUM}.
Max and min seeds cannot be defined to BigInteger as the value could go as large as The computer allows. This wrapper is used to provide seed for MIN and MAX functions in AlgrebraicBigIntegerMathBase.java

<dl> <dt><b>Syntax:</b></dt> <dd><code>String Bin(arithmetic_expression, string1, ,..., stringN, sentinelN, default_string)</code>.</dd> <dt><b>Logic:</b></dt> <dd><code>if      (arithmetic_expression<=sentinel1) return string1; <br> ...... <br> else if (arithmetic_expression<=sentinelN) return stringN; <br> else                                       return default_string; <br></code> <br> arithmetic_expression can only be numeric types.</dd> </dl>
<dl> <dt><b>Syntax:</b></dt> <dd><code>String BinCond(boolean_expression1, mapping_string1, ..., boolean_expressionN, mapping_stringN, other_string)</code>.</dd> <dt><b>Logic:</b></dt> <dd><code>if      (boolean_expression1) return mapping_string1; <br> ...... <br> else if (boolean_expressionN) return mapping_stringN; <br> else                          return other_string; <br> <br></code></dd> </dl>

A class to handle reading and writing of intermediate results of data types. The serialization format used by this class more efficient than what was used in DataReaderWriter . The format used by the functions in this class is subject to change, so it should be used ONLY to store intermediate results within a pig query.
This tuple has a faster (de)serialization mechanism. It to be used for storing intermediate data between Map and Reduce and between MR jobs. This is for internal pig use only. The serialization format can change, so do not use it for storing any persistant data (ie in load/store functions).
Default implementation of TupleFactory.
Load and store data in a binary format.  This class is used by Pig to move data between MapReduce jobs.  Use of this function for storing user data is supported.


Treats keys as offset in file and value as line.

This is a base class for all binary comparison operators. Supports the use of operand type instead of result type as the result type is always boolean. All comparison operators fetch the lhs and rhs operands and compare them for each type using different comparison methods based on what comparison is being implemented.
Superclass for all binary expressions
A base class for all Binary expression operators. Supports the lhs and rhs operators which are used to fetch the inputs and apply the appropriate operation with the appropriate type.
This Filter handles black and whitelisting of Pig commands.
This validator walks through the list of operators defined in {@link PigConfiguration#PIG_BLACKLIST} and {@link PigConfiguration#PIG_WHITELIST} and checks whether the operation is permitted. In case these properties are not defined (default), we let everything pass as usual.
Use a Bloom filter build previously by BuildBloom.  You would first build a bloom filter in a group all job.  For example: in a group all job.  For example: define bb BuildBloom('jenkins', '100', '0.1'); A = load 'foo' as (x, y); B = group A all; C = foreach B generate bb(A.x); store C into 'mybloom'; The bloom filter can be on multiple keys by passing more than one field (or the entire bag) to BuildBloom. The resulting file can then be used in a Bloom filter as: define bloom Bloom(mybloom); A = load 'foo' as (x, y); B = load 'bar' as (z); C = filter B by bloom(z); D = join C by z, A by x; It uses {@link org.apache.hadoop.util.bloom.BloomFilter}. You can also pass the Bloom filter from BuildBloom directly to Bloom UDF as a scalar instead of storing it to file and loading again. This is simpler if the Bloom filter will not be reused and needs to be discarded after the run of the script. define bb BuildBloom('jenkins', '100', '0.1'); A = load 'foo' as (x, y); B = group A all; C = foreach B generate bb(A.x) as bloomfilter; D = load 'bar' as (z); E = filter D by Bloom(C.bloomfilter, z); F = join E by z, A by x;

This represents an instance of a bound pipeline.


Build a bloom filter for use later in Bloom.  This UDF is intended to run in a group all job.  For example: define bb BuildBloom('jenkins', '100', '0.1'); A = load 'foo' as (x, y); B = group A all; C = foreach B generate bb(A.x); store C into 'mybloom'; The bloom filter can be on multiple keys by passing more than one field (or the entire bag) to BuildBloom. The resulting file can then be used in a Bloom filter as: define bloom Bloom('mybloom'); A = load 'foo' as (x, y); B = load 'bar' as (z); C = filter B by bloom(z); D = join C by z, A by x; It uses {@link org.apache.hadoop.util.bloom.BloomFilter}.
A Base class for BuildBloom and its Algebraic implementations.


CBRT implements a binding to the Java function {@link java.lang.Math#cbrt(double) Math.cbrt(double)} for computing the cube root of the argument. The returned value will be a double which is cube root value of the input.
An input stream that decompresses from the BZip2 format (without the file header chars) to be read as any other stream.
An output stream that compresses into the BZip2 format (without the file header chars) into another stream.
CEIL implements a binding to the Java function {@link java.lang.Math#ceil(double) Math.ceil(double)}. Given a single data atom it  Returns the smallest (closest to negative infinity) double value that is greater than or equal  to the argument and is equal to a mathematical integer.
Generates the concatenation of two or more arguments.  It can be used with two or more bytearrays or two or more chararrays (but not a mixture of the two).
Computes the correlation between sets of data.  The returned value will be a bag which will contain a tuple for each combination of input schema and inside tuple we will have two schema name and correlation between those  two schemas. A = load 'input.xml' using PigStorage(':');<br/> B = group A all;<br/> D = foreach B generate group,COR(A.$0,A.$1,A.$2);<br/>
COS implements a binding to the Java function {@link java.lang.Math#cos(double) Math.cos(double)}. Given a single data atom it  Returns the cosine value of the input
COSH implements a binding to the Java function {@link java.lang.Math#cosh(double) Math.cosh(double)}. Given a single data atom it  Returns the hyperbolic cosine value of the input
Generates the count of the number of values in a bag.  This count does not include null values, and thus matches SQL semantics for COUNT(a) (where a is field) but not for COUNT(*) (where * in SQL indicates all). <p> This class implements {@link org.apache.pig.Algebraic}, so if possible the execution will performed in a distributed fashion. <p> There are no restrictions as to the data types inside the bag to be counted. <p> COUNT implements the {@link org.apache.pig.Accumulator} interface as well. While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation.
Generates the count of the values of the first field of a tuple. This class is different from COUNT in that it counts all NULL values and as such implements SQL COUNT(*) semantics. Generates the count of the number of values in a bag.  This count does include null values, and thus matches SQL semantics for COUNT(*) (where in SQL indicates all) but not for COUNT(a) (where a is * field). <p> This class implements {@link org.apache.pig.Algebraic}, so if possible the execution will performed in a distributed fashion. <p> There are no restrictions as to the data types inside the bag to be counted. <p> COUNT_STAR implements the {@link org.apache.pig.Accumulator} interface as well. While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation.
Computes the covariance between sets of data.  The returned value will be a bag which will contain a tuple for each combination of input schema and inside tuple we will have two schema name and covariance between those  two schemas. A = load 'input.xml' using PigStorage(':');<br/> B = group A all;<br/> D = foreach B generate group,COV(A.$0,A.$1,A.$2);<br/>
A simple class the hold and calculate the CRC for sanity checking of the data.
CSV loading and storing with support for multi-line fields, and escaping of delimiters and double quotes within fields; uses CSV conventions of Excel 2007. Arguments allow for control over: Which field delimiter to use (default = ',') Whether line breaks are allowed inside of fields (YES_MULTILINE = yes, NO_MULTILINE = no, default = no) How line breaks are to be written when storing (UNIX = LF, WINDOWS = CRLF, NOCHANGE = system default, default = system default) What to do with header rows (first line of each file): On load: READ_INPUT_HEADER = read header rows, SKIP_INPUT_HEADER = do not read header rows, default = read header rows On store: WRITE_OUTPUT_HEADER = write a header row, SKIP_OUTPUT_HEADER = do not write a header row, default = do not write a header row Usage: STORE x INTO '<destFileName>' USING org.apache.pig.piggybank.storage.CSVExcelStorage( [DELIMITER[, {YES_MULTILINE | NO_MULTILINE}[, {UNIX | WINDOWS | NOCHANGE}[, {READ_INPUT_HEADER, SKIP_INPUT_HEADER, WRITE_OUTPUT_HEADER, SKIP_OUTPUT_HEADER}]]]] ); Linebreak settings are only used during store; during load, no conversion is performed. WARNING: A danger with enabling multiline fields during load is that unbalanced double quotes will cause slurping up of input until a balancing double quote is found, or until something breaks. If you are not expecting newlines within fields it is therefore more robust to use NO_MULTILINE, which is the default for that reason. This is Adreas Paepcke's <paepcke@cs.stanford.edu> CSVExcelStorage with a few modifications.
A load function based on PigStorage that implements part of the CSV "standard" This loader properly supports double-quoted fields that contain commas and other double-quotes escaped with backslashes. The following fields are all parsed as one tuple, per each line "the man, he said ""hello""" "one,two,three" This version supports pig 0.7+
A visitor to walk the logical plan and give canonical names fields.

Find uid lineage information. Set the load function in CastExpression if it needs to convert bytearray to another type.

A dummy class to ensure  PigContext's classloader's 'consistency' after registering jars.


Encapsulates all the file system operations. <p>Mainly used for copying data to the test cluster.

This interface implemented by a {@link LoadFunc} implementations indicates to Pig that it has the capability to load data such that all instances of a key will occur in same split.

Extracts necessary information from a user provide column "specification": colf[[*]:[colq[*]]] Removes any trailing asterisk on colfam or colqual, and appropriately sets the {#link Column.Type}
Logical plan visitor which will convert all column alias references to column indexes, using the underlying anonymous expression plan visitor.
Representing one sort key. Sort key may be compound if we sort on multiple columns, if that is the case, then this sort key contains multiple ColumnChainInfo
Super class for all column expressions, including projection, constants, and deferences.
Represent one column inside order key, this is a direct mapping from POProject
This Rule prunes columns and map keys and set to loader. This rule depends on MapKeysPruneHelper to calculate what keys are required for a loader, and ColumnPruneHelper to calculate the required columns for a loader. Then it combines the map keys and columns info to set into the loader.
Helper class used by ColumnMapKeyPrune to figure out what columns can be pruned. It doesn't make any changes to the operator plan

CombinedLogLoader is used to load logs based on Apache's combined log format, based on a format like LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined The log filename ends up being access_log from a line like CustomLog logs/combined_log combined Example: raw = LOAD 'combined_log' USING org.apache.pig.piggybank.storage.apachelog.CombinedLogLoader AS (remoteAddr, remoteLogname, user, time, method, uri, proto, status, bytes, referer, userAgent);
This class goes through the physical plan are replaces GlobalRearrange with ReduceBy where there are algebraic operations.

The package operator that packages the globally rearranged tuples into output format after the combiner stage.  It differs from POPackage in that it does not use the index in the NullableTuple to find the bag to put a tuple in.  Instead, the inputs are put in a bag corresponding to their offset in the tuple.

CommonLogLoader is used to load logs based on Apache's common log format, based on a format like LogFormat "%h %l %u %t \"%r\" %>s %b" common The log filename ends up being access_log from a line like CustomLog logs/access_log common Example: raw = LOAD 'access_log' USING org.apache.pig.piggybank.storage.apachelog.CommongLogLoader AS (remoteAddr, remoteLogname, user, time, method, uri, proto, bytes);
An interface for custom order by comparator function.
This is an interface for all comparison operators. Supports the use of operand type instead of result type as the result type is always boolean.
* This class is used for collecting all messages (error + warning) in compilation process. These messages are reported back to users at the end of compilation. iterator() has to be called after CompilationMessageCollector is fully populated otherwise the state is undefined.


A class to add util functions that gets used by LogToPhyTranslator and MRCompiler


Borrowed from jline.console.internal.ConsoleReaderInputStream. However, we cannot use ConsoleReaderInputStream directly since: 1. ConsoleReaderInputStream is not public 2. ConsoleReaderInputStream has a bug which does not deal with UTF-8 correctly

A constant
This method should never be used directly, use {@link SIZE}.






Produces a DataBag with all combinations of the argument tuple members as in a data cube. Meaning, (a, b, c) will produce the following bag: <pre> { (a, b, c), (null, null, null), (a, b, null), (a, null, c), (a, null, null), (null, b, c), (null, null, c), (null, b, null) } </pre> <p> The "all" marker is null by default, but can be set to an arbitrary string by invoking a constructor (via a DEFINE). The constructor takes a single argument, the string you want to represent "all". <p> Usage goes something like this: <pre>{@code events = load '/logs/events' using EventLoader() as (lang, event, app_id); cubed = foreach x generate FLATTEN(piggybank.CubeDimensions(lang, event, app_id)) as (lang, event, app_id), measure; cube = foreach (group cubed by (lang, event, app_id) parallel $P) generate flatten(group) as (lang, event, app_id), COUNT_STAR(cubed), SUM(measure); store cube into 'event_cube';}</pre> <p> <b>Note</b>: doing this with non-algebraic aggregations on large data can result in very slow reducers, since one of the groups is going to get <i>all</i> the records in your relation.

CustomFormatToISO converts arbitrary date formats to ISO format. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> <li>Jodatime custom date formats: http://joda-time.sourceforge.net/api-release/org/joda/time/format/DateTimeFormat.html</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE CustomFormatToISO org.apache.pig.piggybank.evaluation.datetime.convert.CustomFormatToISO(); CustomIn = LOAD 'test3.tsv' USING PigStorage('\t') AS (dt:chararray); DESCRIBE CustomIn; CustomIn: {dt: chararray} DUMP CustomIn; (10-1-2010) toISO = FOREACH CustomIn GENERATE CustomFormatToISO(dt, "MM-dd-YYYY") AS ISOTime:chararray; DESCRIBE toISO; toISO: {ISOTime: chararray} DUMP toISO; (2010-10-01T00:00:00.000Z) ... </pre>

<dl> <dt><b>Syntax:</b></dt> <dd><code>String Decode(expression, value1, mapping_string1, ..., valueN, mapping_stringN, other_string)</code>.</dd> <dt><b>Logic:</b></dt> <dd><code>if      (expression==value1) return mapping_string1; <br> ...... <br> else if (expression==valueN) return mapping_stringN; <br> else                         return other_string;<br></code> <br> expression can be any simple types</dd> </dl>
DIFF takes two bags as arguments and compares them.   Any tuples that are in one bag but not the other are returned.  If the fields are not bags then they will be returned if they do not match, or an empty bag will be returned if the two records match. <p> The implementation assumes that both bags being passed to this function will fit entirely into memory simultaneously.  If that is not the case the UDF will still function, but it will be <strong>very</strong> slow.
@bgen(jjtree)
Token literal values and constants. Generated by org.javacc.parser.OtherFilesGen#start()
Token Manager.


A collection of Tuples.  A DataBag may or may not fit into memory. DataBag extends spillable, which means that it registers with a memory manager.  By default, it attempts to keep all of its contents in memory. If it is asked by the memory manager to spill to disk (by a call to spill()), it takes whatever it has in memory, opens a spill file, and writes the contents out.  This may happen multiple times.  The bag tracks all of the files it's spilled to. <p> DataBag provides an Iterator interface, that allows callers to read through the contents.  The iterators are aware of the data spilling. They have to be able to handle reading from files, as well as the fact that data they were reading from memory may have been spilled to disk underneath them. <p> The DataBag interface assumes that all data is written before any is read.  That is, a DataBag cannot be used as a queue.  If data is written after data is read, the results are undefined.  This condition is not checked on each add or read, for reasons of speed.  Caveat emptor. <p> Since spills are asynchronous (the memory manager requesting a spill runs in a separate thread), all operations dealing with the mContents Collection (which is the collection of tuples contained in the bag) have to be synchronized.  This means that reading from a DataBag is currently serialized.  This is ok for the moment because pig execution is currently single threaded.  A ReadWriteLock was experimented with, but it was found to be about 10x slower than using the synchronize keyword. If pig changes its execution model to be multithreaded, we may need to return to this issue, as synchronizing reads will most likely defeat the purpose of multi-threading execution. <p> DataBags come in several types, default, sorted, and distinct.  The type must be chosen up front, there is no way to convert a bag on the fly. Default data bags do not guarantee any particular order of retrieval for the tuples and may contain duplicate tuples.  Sorted data bags guarantee that tuples will be retrieved in order, where "in order" is defined either by the default comparator for Tuple or the comparator provided by the caller when the bag was created.  Sorted bags may contain duplicates. Distinct bags do not guarantee any particular order of retrieval, but do guarantee that they will not contain duplicate tuples.
An implementation of byte array.  This is done as an object because we need to be able to implement compareTo, toString, hashCode, and some other methods.
A tool to generate data for performance testing.
This class was used to handle reading and writing of intermediate results of data types. Now that functionality is in {@link BinInterSedes} This class could also be used for storing permanent results, it used by BinStorage through DefaultTuple class.
DataStorage provides an abstraction of a generic container. Special instances of it can be a file system.

A class of static final values used to encode data type and a number of static helper functions for manipulating data objects.  The data type values could be done as an enumeration, but it is done as byte codes instead to save creating objects.
DateExtractor has four different constructors which each allow for different functionality. The incomingDateFormat ("dd/MMM/yyyy:HH:mm:ss Z" by default) is used to match the date string that gets passed in from the log. The outgoingDateFormat ("yyyy-MM-dd" by default) is used to format the returned string. Different constructors exist for each combination; please use the appropriate respective constructor. Note that any data that exists in the SimpleDateFormat schema can be supported. For example, if you were starting with the default incoming format and wanted to extract just the year, you would use the single string constructor DateExtractor("yyyy"). From pig latin you will need to use aliases to use a non-default format, like define MyDateExtractor org.apache.pig.piggybank.evaluation.util.apachelogparser.DateExtractor("yyyy-MM"); A = FOREACH row GENERATE DateExtractor(dayTime); If a string cannot be parsed, null will be returned and an error message printed to stderr. By default, the DateExtractor uses the GMT timezone. You can use the three-parameter constructor to override the timezone.
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MAX}.
Writable for DateTime values.
<p>DaysBetween returns the number of days between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>
Default implementation of DataBag.  This is the an abstract class used as a parent for all three of the types of data bags.
Default implementation of BagFactory.
An unordered collection of Tuples (possibly) with multiples.  The tuples are stored in a List, since there is no concern for order or distinctness.
Used by MergeJoin . Takes an index on sorted data consisting of sorted tuples of the form (key1,key2..., position,splitIndex) as input. For key given in seekNear(Tuple) finds the splitIndex that can contain the key and initializes ReadToEndLoader to read from that splitIndex onwards , in the sequence of splits in the index
DefaultInputHandler handles the input for the Pig-Streaming executable in a synchronous manner by feeding it input via its <code>stdin</code>.
FileOutputHandler handles the output from the Pig-Streaming executable in an synchronous manner by reading its output via its <code>stdout</code>.
A default implementation of Tuple. This class will be created by the DefaultTupleFactory.
This class was being used to create new tuple instances in some udfs and other places in pig code, even though the tuplefactory creation function (getInstance()) is static in TupleFactory (ie DefaultTuple could not override it) A typical call is - DefaultTupleFactory.getInstance().newTuple(..); So that such external udfs don't break, a DefaultTupleFactory is present. Don't use this in your code, use TupleFactory directly instead.
A walker to walk graphs in dependency order.  It is guaranteed that a node will not be visited until all of its predecessors have been visited.  This is equivalent to doing a topilogical sort on the graph and then visiting the nodes in order.
Do a depth first traversal of the graph.
get one or elements out of a tuple or a bag in case of Tuple( a#2:int, b#3:bag{ b_a#4:int, b_b#5:float }, c#6:int ) # 1 (the number after # represents the uid) Dereference ( 0 ) --> a:int - dereference of single column in a tuple gives the field Dereference ( 0,2 ) --> Tuple(a#2:int, c#6:int) #7 - dereference of more than one column gives a tuple Dereference ( 1 ) --> Dereference ( 1 ) --> b:bag{b_b#5:float}#8 - dereference of a bag gives a bag
<dl> <dt><b>Syntax:</b></dt> <dd><code>int DiffDate(String yyyymmdd1, String yyyymmdd2)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>date string in "yyyymmdd" format</code>.</dd> <dt><b>Output:</b></dt> <dd><code>(date1-date2) in days, can be negative</code>.</dd> </dl>

Class containing some generic printing methods to print example data in a simple/tabular form
Find the distinct set of tuples in a bag. This is a blocking operator. All the input is put in the hashset implemented in DistinctDataBag which also provides the other DataBag interfaces.
A special implementation of combiner used only for distinct.  This combiner does not even parse out the records.  It just throws away duplicate values in the key in order to minimize the data being sent to the reduce.

An unordered collection of Tuples with no multiples.  Data is stored without duplicates as it comes in.  When it is time to spill, that data is sorted and written to disk.  It must also be sorted upon the first read, otherwise if a spill happened after that the iterators would have no way to find their place in the new file.  The data is stored in a HashSet.  When it is time to sort it is placed in an ArrayList and then sorted.  Dispite all these machinations, this was found to be faster than storing it in a TreeSet.

Divide Operator
This represents an edge in DOT format. An edge in DOT can have attributes but we're not interested
* This represents graph structure in DOT format
* This class is responsible for loading textual Dot graph into object representation.
This class can print a logical plan in the DOT format. It uses clusters to illustrate nesting. If "verbose" is off, it will skip any nesting.
This class can print an MR plan in the DOT format. It uses clusters to illustrate nesting. If "verbose" is off, it will skip any nesting in the associated physical plans.
* This represents a node in DOT format
This class can print a physical plan in the DOT format. It uses clusters to illustrate nesting. If "verbose" is off, it will skip any nesting.
This class puts everything that is needed to dump a plan in a format readable by graphviz's dot algorithm. Out of the box it does not print any nested plans.
This class can print Spark plan in the DOT format. It uses clusters to illustrate nesting. If "verbose" is off, it will skip any nesting in the associated physical plans.

This method should never be used directly, use {@link AVG}.
base class for math udfs that return Double value
math.copySign implements a binding to the Java function {@link java.lang.Math#copySign(double,double) Math.copySign(double,double)}. Given a tuple with two data atom Returns the first floating-point argument with the sign of the second floating-point argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>copySign_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.copySign(float1); </code></dd> </dl>
base class for math udfs that return Double value
math.getExponent implements a binding to the Java function {@link java.lang.Math#getExponent(double) Math.getExponent(double)}. Given a single data atom it returns the unbiased exponent used in the representation of a double <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>int</code> </dd> <dt><b>Return Schema:</b></dt> <dd>getExponent_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.getExponent(float1); </code></dd> </dl>
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MIN}.
math.nextAfter implements a binding to the Java function {@link java.lang.Math#nextAfter(double,double) Math.nextAfter(double,double)}. Given a tuple with two data atom it Returns the floating-point number adjacent to the first argument in the direction of the second argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>nextAfter_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.nextAfter(float1); </code></dd> </dl>
math.NEXTUP implements a binding to the Java function {@link java.lang.Math#nextUp(double) Math.nextUp(double)}. Given a single data atom it return the floating-point value adjacent to input in the direction of positive infinity <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>NEXTUP_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.NEXTUP(float1); </code></dd> </dl>
Given a single data atom it Returns the closest long to the argument.
ROUND_TO safely rounds a number to a given precision by using an intermediate BigDecimal. The too-often seen trick of doing (1000.0 * ROUND(x/1000)) is not only hard to read but also fails to produce numerically accurate results. Given a single data atom and number of digits, it returns a double extending to the given number of decimal places. The result is a multiple of ten to the power given by the digits argument: a negative value zeros out correspondingly many places to the left of the decimal point: ROUND_TO(0.9876543, 3) is 0.988; ROUND_TO(0.9876543, 0) is 1.0; and ROUND_TO(1234.56, -2) is 1200.0. The optional mode argument specifies the {@link java.math.RoundingMode rounding mode}; by default, {@link java.math.RoundingMode#HALF_EVEN 'HALF_EVEN'} is used.
math.SIGNUM implements a binding to the Java function {@link java.lang.Math#signum(double) Math.signum(double)}. Given a single data atom it Returns the signum function of the argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>SIGNUM_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.SIGNUM(float1); </code></dd> </dl>
This method should never be used directly, use {@link SUM}.
math.ULP implements a binding to the Java function {@link java.lang.Math#ulp(double) Math.ulp(double)}. Given a single data atom it Returns the size of an ulp of the argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>ULP_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.ULP(float1); </code></dd> </dl>
Writable for Double values.



This is a loader for data produced by DumpStore. Line format: (field1, field2, ... fieldn)\n
This is a loader for data produced by DumpStore. Line format: (field1, field2, ... fieldn)\n this class is only used in negative tests just based on DumpLoader so we don't need to provide any other special implementation here
This is a loader for data produced by DumpStore. Line format: (field1, field2, ... fieldn)\n
This is a loader for data produced by DumpStore. Line format: (field1, field2, ... fieldn)\n



Pig UDF to test input <code>tuple.get(0)</code> against <code>tuple.get(1)</code> to determine if the first argument ends with the string in the second.
Given a single data atom it returns the Euler's number e raised to the power of input
math.EXPM1 implements a binding to the Java function {@link java.lang.Math#expm1(double) Math.expm1(double)}. Given a single data atom it returns exp(input)+1 <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>expm1_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.expm1(float1); </code></dd> </dl>
DataStorageElementDescriptor provides methods necessary to manage an element in a DataStorage.
This class encapsulates the statistics collected from an embedded Pig script. It includes a collection of {@link SimplePigStats} corresponding to the instances of Pig scripts run by the given embedded script.
EmptyPigStats encapsulates dummy statistics of a fetch task, since during a fetch no MR jobs are executed
This visitor visits the MRPlan and does the following for each MROper: If the map plan or the reduce plan of the MROper has an end of all input flag present in it, this marks in the MROper whether the map has an end of all input flag set or if the reduce has an end of all input flag set.
Equality test expression.

Compares two Strings ignoring case considerations.
These methods are used to generate equivalence classes given the operator name and the output from the operator For example, it gives out 2 eq. classes for filter, one that passes the filter and one that doesn't
The interface that handles errors thrown by the {@link StoreFuncInterface#putNext(Tuple)}
A {@link StoreFunc} should implement this interface to enable handling errors during {@code StoreFunc#putNext(Tuple)}
The class is used to implement functions to be applied to fields in a dataset. The function is applied to each Tuple in the set. The programmer should not make assumptions about state maintained between invocations of the exec() method since the Pig runtime will schedule and localize invocations based on information provided at runtime.  The programmer also should not make assumptions about when or how many times the class will be instantiated, since it may be instantiated multiple times in both the front and back end.
* This matcher only does exact key matching
This class is used to generate example tuples for the ILLUSTRATE purpose
Example tuple adds 2 booleans to Tuple synthetic say whether the tuple was generated synthetically omittable is for future use in case we want to attach weights to tuples that have been displayed earlier
An interface that captures a unit of work against an item where an exception might be thrown.

Abstraction on a job that the execution engine runs. It allows the front-end to retrieve information on job status and manage a running job.
The type of query execution. Pig will cycle through all implementations of ExecType and choose the first one that matches the Properties passed in. This ExecType then dictates the ExecutionEngine used for processing and other behaviour throughout Pig. Any implementing classes should be noted in the META-INF/services folder titled org.apache.pig.ExecType as per the Java ServiceLoader specification.

{@link ExecutableManager} manages an external executable which processes data in a Pig query. The <code>ExecutableManager</code> is responsible for startup/teardown of the external process and also for managing it. It feeds input records to the executable via it's <code>stdin</code>, collects the output records from the <code>stdout</code> and also diagnostic information from the <code>stdout</code>.
The main interface bridging the front end and back end of Pig. This allows Pig to be ran on multiple Execution Engines, and not being limited to only Hadoop MapReduce. The ExecutionEngines must support the following methods as these are all the access points for the Pig frontend for processing. Traditionally there is one ExecutionEngine created per processing job, but this is not necessary. The ExecutionEngine instance comes from the ExecType, and it can choose to reuse ExecutionEngine instances. All specifications for methods are listed below as well as expected behavior, and the ExecutionEngine must conform to these.

A class to communicate Filter expressions to LoadFuncs.
A base class for all types of expressions. All expression operators must extend this class.
The excite query log timestamp format is YYMMDDHHMMSS This function extracts the hour, HH
This class is similar to MaxTupleBy1stField except that it allows you to specify with field to use (instead of just using 1st field) and to specify ascending or descending. The first parameter in the constructor specifies which field to use and the second parameter to the constructor specifies which extremal to retrieve. Strings prefixed by "min", "least", "desc", "small" and "-", irrespective of capitalization and leading white spacing, specifies the computation of the minimum and all other strings means maximum; <h2>Example1: Invoking the UDF</h2> e.g. Using this udf: <br /> <code> define myMin ExtremalTupleByNthField( '4', 'min' ); T = group G ALL; R = foreach T generate myMin(G); </code> is equivalent to:<br /> <code> T = order G by $3 asc; R = limit G 1; </code> Note above 4 indicates the field with index 3 in the tuple. The 4th field can be any comparable type, so you can use float, int, string, or even tuples. By default constructor, this UDF behaves as MaxTupleBy1stField in that it chooses the max tuple by the comparable in the first field. <h2>Example 2: Default behavior</h2> This class also has a one parameter constructor that specifies the index and takes the max tuple from the bag. <code> define myMax ExtremalTupleByNthField( '3' ); T = group G ALL; R = foreach T generate myMax(G); </code> is equivalent to:<br /> <code> T = order G by $2 desc; R = limit G 1; </code> <h2>Example 3: Choosing a Large Bag or Tuple</h2> Another possible use case is the choosing of larger or smaller bags/tuples. In pig, bags and tuples are comparable and the comparison is based on size. <code> define biggestBag ExtremalTupleByNthField('1', max); R = group TABLE by (key1, key2); G = cogroup L by key1, R by group.key1; V = foreach G generate L, biggestBag(R); </code> This results in each L(eft) bag associated with only the largest bag from the R(ight) table. If all bags in R are of equal size, the comparator continues on to perform element-wise comparison. In case of a complete tie in the comparison, which result is returned is nondeterministic. But because this class is able to compare any comparable we are able to specify a secondary key. <h2>Example 4: Secondary Sort Key</h2> <code> define biggestBag ExtremalTupleByNthField('1', max); G = cogroup L by key1, M by key1, R by key1; V = foreach G generate FLATTEN(L), biggestBag(R.($0, $1, $2, $5)) as best_result_by_0, biggestBag(R.($3, $1, $2, $5)) as best_result_by_3, biggestBag(M.($0, $2)) as best_misc_data; </code> this will generate two sets of results and misc data based on two separate criterion. Since all tuples in the bags have the same size (4, 4, 2 respectively), the tuple comparator continues on and compares the members of tuples until it finds one. best_result_by_0 and best_result_by3 are ordered by 1st and 4th member of the tuples. Within each group, ties are broken by second and third field. Finally, note that the udf implements both Algebraic and Accumulator, so it is relatively efficient because it's a one-pass algorithm.
define MyFilterSet util.FILTERFROMFILE('/user/pig/filterfile'); A = load 'mydata' using PigStorage() as ( a, b ); B = filter A by MyFilterSet(a);
FLOOR implements a binding to the Java function {@link java.lang.Math#floor(double) Math.floor(double)}. Given a single data atom it returns the largest (closest to positive infinity) double value that is less than or equal to the argument and is equal to a mathematical integer.




A dummy counter handling context for fetch tasks
This class is responsible for executing the fetch task, saving the result to disk and do the necessary cleanup afterwards.
FetchOptimizer determines whether the entire physical plan is fetchable, meaning that the task's result can be directly read (fetched) from the underlying storage rather than creating MR jobs. During the check {@link FetchablePlanVisitor} is used to walk through the plan.
This class is used to have a POStore write the output to the underlying storage via a output collector/record writer in case of a fetch task. It sets up dummy context objects which otherwise would be initialized by the Hadoop job itself.
A dummy ProgressableReporter used for fetch tasks


Class that computes the size of output for file-based systems.
FileInputHandler handles the input for the Pig-Streaming executable in an asynchronous manner by feeding it input via an external file specified by the user.
This class provides an implementation of OrderedLoadFunc interface which can be optionally re-used by LoadFuncs that use FileInputFormat, by having this as a super class Since we haven't done outer join for merge join yet
This class extends ArrayList<File> to add a finalize() that calls delete on the files . This helps in getting rid of the finalize() in the classes such as DefaultAbstractBag, and they can be freed up without waiting for finalize to be called. Only if those classes have spilled to disk, there will be a (this) class that needs to be finalized. CAUTION: if you assign a new value for a variable of this type, the files (if any) in the old object it pointed to will be scheduled for deletion. To avoid that call .clear() before assigning a new value.

FileOutputHandler handles the output from the Pig-Streaming executable in an asynchronous manner by reading it from an external file specified by the user.
A simple class that specifies a file name and storage function which is used to read/write it
This class represents a relative position in a file.  It records a filename and an offset.  This allows Pig to order FileSplits. Since we haven't done outer join for merge join yet
This Rule moves Filter Above Foreach. It checks if uid on which filter works on is present in the predecessor of foreach. If so it transforms it.

Converter that converts an RDD to a filtered RRD using POFilter
Extracts filter predicates for interfaces implementing {@code LoadPredicatePushdown}



Uses a fixed length array and will not grow in size dynamically like the {@link java.io.ByteArrayOutputStream}.
A fixed-width file loader. Takes a string argument specifying the ranges of each column in a unix 'cut'-like format. Ex: '-5, 10-12, 14, 20-' Ranges are comma-separated, 1-indexed (for ease of use with 1-indexed text editors), and inclusive. A single-column field at position n may be specified as either 'n-n' or simply 'n'. A second optional argument specifies whether to skip the first row of the input file, assuming it to be a header. As Pig may combine multiple input files each with their own header into a single split, FixedWidthLoader makes sure to skip any duplicate headers as will. 'SKIP_HEADER' skips the row; anything else and the default behavior ('USE_HEADER') is not to skip it. A third optional argument specifies a Pig schema to load the data with. Automatically trims whitespace from numeric fields. Note that if fewer fields are specified in the schema than are specified in the column spec, only the fields in the schema will be used. Warning: fields loaded as char/byte arrays will trim all leading and trailing whitespace from the field value as it is indistiguishable from the spaces that separate different fields. All datetimes are converted to UTC when loaded. Column spec idea and syntax parser borrowed from Russ Lankenau's implementation at https://github.com/rlankenau/fixed-width-pig-loader
Stores Pig records in a fixed-width file format. Takes a string argument specifying the ranges of each column in a unix 'cut'-like format. Ex: '-5, 10-12, 14, 20-' Ranges are comma-separated, 1-indexed (for ease of use with 1-indexed text editors), and inclusive. A single-column field at position n may be specified as either 'n-n' or simply 'n'. A second optional argument specifies whether to write a header record with the names of each field. 'WRITE_HEADER' writes a header record; 'NO_HEADER' and the default does not write one. All datetimes are stored in UTC. Column spec idea and syntax parser borrowed from Russ Lankenau's FixedWidthLoader implementation at https://github.com/rlankenau/fixed-width-pig-loader

This method should never be used directly, use {@link AVG}.
math.copySign implements a binding to the Java function {@link java.lang.Math#copySign(float,float) Math.copySign(float,float)}. Given a tuple with two data atom Returns the first floating-point argument with the sign of the second floating-point argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two float</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>float</code> </dd> <dt><b>Return Schema:</b></dt> <dd>copySign_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.copySign(float1); </code></dd> </dl>
math.getExponent implements a binding to the Java function {@link java.lang.Math#getExponent(double) Math.getExponent(double)}. Given a single data atom it returns the unbiased exponent used in the representation of a double <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>float</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>int</code> </dd> <dt><b>Return Schema:</b></dt> <dd>getExponent_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.getExponent(float1); </code></dd> </dl>
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MIN}.
math.nextAfter implements a binding to the Java function {@link java.lang.Math#nextAfter(double,double) Math.nextAfter(double,double)}. Given a tuple with two data atom it Returns the floating-point number adjacent to the first argument in the direction of the second argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing float and double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>float</code> </dd> <dt><b>Return Schema:</b></dt> <dd>nextAfter_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.nextAfter(float1); </code></dd> </dl>
math.NEXTUP implements a binding to the Java function {@link java.lang.Math#nextUp(double) Math.nextUp(double)}. Given a single data atom it return the floating-point value adjacent to input in the direction of positive infinity <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Float</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Float</code> </dd> <dt><b>Return Schema:</b></dt> <dd>NEXTUP_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.NEXTUP(float1); </code></dd> </dl>
ROUND implements a binding to the Java function {@link java.lang.Math#round(float) Math.round(float)}. Given a single data atom it Returns the closest long to the argument.
ROUND_TO safely rounds a number to a given precision by using an intermediate BigDecimal. The too-often seen trick of doing (1000.0 * ROUND(x/1000)) is not only hard to read but also fails to produce numerically accurate results. Given a single data atom and number of digits, it returns a float extending to the given number of decimal places. The result is a multiple of ten to the power given by the digits argument: a negative value zeros out correspondingly many places to the left of the decimal point: ROUND_TO(0.9876543, 3) is 0.988; ROUND_TO(0.9876543, 0) is 1.0; and ROUND_TO(1234.56, -2) is 1200.0. The optional mode argument specifies the {@link java.math.RoundingMode rounding mode}; by default, {@link java.math.RoundingMode#HALF_EVEN HALF_EVEN} is used.
math.SIGNUM implements a binding to the Java function {@link java.lang.Math#signum(float) Math.signum(float)}. Given a single data atom it Returns the signum function of the argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Float</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Float</code> </dd> <dt><b>Return Schema:</b></dt> <dd>SIGNUM_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.SIGNUM(float1); </code></dd> </dl>
This method should never be used directly, use {@link SUM}.
math.ULP implements a binding to the Java function {@link java.lang.Math#ulp(float) Math.ulp(float)}. Given a single data atom it Returns the size of an ulp of the argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Float</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Float</code> </dd> <dt><b>Return Schema:</b></dt> <dd>ULP_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.ULP(float1); </code></dd> </dl>

Convert that is able to convert an RRD to another RRD using a POForEach



Class to represent a UDF specification. Encapsulates the class name and the arguments to the constructor.

A convenience typedef that ties into both google's {@code Function} and {@code ExceptionalFunction}.

EvalFunc that wraps an implementation of the Function interface, which is passed as a String in the constructor. When resolving the Function class, the Pig UDF package import list is used. <P> The Function must have a default no-arg constructor, which will be used. For Functions that take args in the constructor, initialize the function in a subclass of this one and call <code>super(function)</code>. <P> Example: <code>DEFINE myUdf FunctionWrapperEvalFunc('MyFunction')</code>
built-in grouping function; permits system to choose any grouping.





The generic Invoker class does all the common grunt work of setting up an invoker. Class-specific non-generic extensions of this class are needed for Pig to know what type of return to expect from exec, and to find the appropriate classes through reflection. All they have to do is implement the constructors that call into super(). Note that the no-parameter constructor is <b>required</b>, if seemingly nonsensical, for Pig to do its work. <p> The Invoker family of udfs understand the following class names (all case-independent): <li>String <li>Long <li>Float <li>Double <li>Int <p> Invokers can also work with array arguments, represented in Pig as DataBags of single-tuple elements. Simply refer to <code>string[]</code>, for example. <p> This UDF allows one to dynamically invoke Java methods that return a <code>T</code> <p> Usage of the Invoker family of UDFs (adjust as appropriate): <p> <pre> {@code -- invoking a static method DEFINE StringToLong InvokeForLong('java.lang.Long.valueOf', 'String') longs = FOREACH strings GENERATE StringToLong(some_chararray); -- invoking a method on an object DEFINE StringConcat InvokeForString('java.lang.String.concat', 'String String', 'false') concatenations = FOREACH strings GENERATE StringConcat(str1, str2);} </pre> <p> The first argument to the constructor is the full path to desired method.<br> The second argument is a list of classes of the method parameters.<br> If the method is not static, the first element in this list is the object to invoke the method on.<br> The second argument is optional (a no-argument static method is assumed if it is not supplied).<br> The third argument is the keyword "static" (or "true") to signify that the method is static. <br> The third argument is optional, and true by default.<br> <p>
GetDay extracts the day of a month from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
GetHour extracts the hour of a day from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
UDF to get memory size of a tuple and extracts number of rows value from special tuple created by PoissonSampleLoader It is used by skewed join.
GetSecond extracts the millisecond of a second from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
GetMinute extracts the minute of an hour from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
GetMonth extracts the month of a year from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
GetSecond extracts the second of a minute from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
GetMonth extracts the week of a week year from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
GetMonth extracts the week year from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>
GetYear extracts the year from a DateTime object. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime); DESCRIBE ISOin; ISOin: {dt: datetime} DUMP ISOin; (2009-01-07T01:07:01.071Z) (2008-02-06T02:06:02.062Z) (2007-03-05T03:05:03.053Z) ... truncated = FOREACH ISO in GENERATE GetYear(dt) AS year, GetMonth(dt) as month, GetWeek(dt) as week, GetWeekYear(dt) as weekyear, GetDay(dt) AS day, GetHour(dt) AS hour, GetMinute(dt) AS min, GetSecond(dt) AS sec; GetMillSecond(dt) AS milli; DESCRIBE truncated; truncated: {year: int,month: int,week: int,weekyear: int,day: int,hour: int,min: int,sec: int,milli: int} DUMP truncated; (2009,1,2,2009,7,1,7,1,71) (2008,2,6,2008,6,2,6,2,62) (2007,3,10,2007,5,3,5,3,53) </pre>











Rule: If a LOCogroup is 'group all', set the parallelism to 1, or in general - if the group-by expression is just a constant then set parallelism to 1 LogicalExpressionSimplifier could be used to convert an expression with constants into a single ConstantExpression
Interface to be implemented by classes that group data in memory



A HBase implementation of LoadFunc and StoreFunc. <P> Below is an example showing how to load data from HBase: <pre>{@code raw = LOAD 'hbase://SampleTable' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage( 'info:first_name info:last_name friends:* info:*', '-loadKey true -limit 5') AS (id:bytearray, first_name:chararray, last_name:chararray, friends_map:map[], info_map:map[]);}</pre> This example loads data redundantly from the info column family just to illustrate usage. Note that the row key is inserted first in the result schema. To load only column names that start with a given prefix, specify the column name with a trailing '*'. For example passing <code>friends:bob_*</code> to the constructor in the above example would cause only columns that start with <i>bob_</i> to be loaded. <P> Note that when using a prefix like <code>friends:bob_*</code>, explicit HBase filters are set for all columns and prefixes specified. Querying HBase with many filters can cause performance degredation. This is typically seen when mixing one or more prefixed descriptors with a large list of columns. In that case better perfomance will be seen by either loading the entire family via <code>friends:*</code> or by specifying explicit column descriptor names. <P> Below is an example showing how to store data into HBase: <pre>{@code copy = STORE raw INTO 'hbase://SampleTableCopy' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage( 'info:first_name info:last_name friends:* info:*');}</pre> Note that STORE will expect the first value in the tuple to be the row key. Scalars values need to map to an explicit column descriptor and maps need to map to a column family name. In the above examples, the <code>friends</code> column family data from <code>SampleTable</code> will be written to a <code>buddies</code> column family in the <code>SampleTableCopy</code> table.



A class of helper methods for converting from pig data types to hadoop data types, and vice versa.




Support for logging in using a kerberos keytab file. <br/> Kerberos is a authentication system that uses tickets with a limited valitity time.<br/> As a consequence running a pig script on a kerberos secured hadoop cluster limits the running time to at most the remaining validity time of these kerberos tickets. When doing really complex analytics this may become a problem as the job may need to run for a longer time than these ticket times allow.<br/> A kerberos keytab file is essentially a Kerberos specific form of the password of a user. <br/> It is possible to enable a Hadoop job to request new tickets when they expire by creating a keytab file and make it part of the job that is running in the cluster. This will extend the maximum job duration beyond the maximum renew time of the kerberos tickets.<br/> <br/> Usage: <ol> <li>Create a keytab file for the required principal.<br/> <p>Using the ktutil tool you can create a keytab using roughly these commands:<br/> <i>addent -password -p niels@EXAMPLE.NL -k 1 -e rc4-hmac<br/> addent -password -p niels@EXAMPLE.NL -k 1 -e aes256-cts<br/> wkt niels.keytab</i></p> </li> <li>Set the following properties (either via the .pigrc file or on the command line via -P file)<br/> <ul> <li><i>java.security.krb5.conf</i><br/> The path to the local krb5.conf file.<br/> Usually this is "/etc/krb5.conf"</li> <li><i>hadoop.security.krb5.principal</i><br/> The pricipal you want to login with.<br/> Usually this would look like this "niels@EXAMPLE.NL"</li> <li><i>hadoop.security.krb5.keytab</i><br/> The path to the local keytab file that must be used to authenticate with.<br/> Usually this would look like this "/home/niels/.krb/niels.keytab"</li> </ul></li> </ol> NOTE: All paths in these variables are local to the client system starting the actual pig script. This can be run without any special access to the cluster nodes.


math.HYPOT implements a binding to the Java function {@link java.lang.Math#hypot(double,double) Math.hypot(double,double)}. Given a tuple with two data atom Returns sum of squares of two arguments. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>HYPOT_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.HYPOT(float1); </code></dd> </dl>
{@link HadoopExecutableManager} is a specialization of {@link ExecutableManager} and provides HDFS-specific support for secondary outputs, task-logs etc. <code>HadoopExecutableManager</code> provides support for  secondary outputs of the managed process and also persists the logs of the tasks on HDFS.

Factory to create an {@link InputHandler} or {@link OutputHandler} depending on the specification of the {@link StreamingCommand}.
<dl> <dt><b>Syntax:</b></dt> <dd><code>long HashFNV(String string_to_hash, [int mod])</code>.</dd> </dl>
<dl> Implementation for HashFNV which takes 1 parameter </dl>
<dl> Implementation for HashFNV which takes 2 parameter </dl>



Loader for Hive RC Columnar files.<br/> Supports the following types:<br/> * <table> <tr> <th>Hive Type</th> <th>Pig Type from DataType</th> </tr> <tr> <td>string</td> <td>CHARARRAY</td> </tr> <tr> <td>int</td> <td>INTEGER</td> </tr> <tr> <td>bigint or long</td> <td>LONG</td> </tr> <tr> <td>float</td> <td>float</td> </tr> <tr> <td>double</td> <td>DOUBLE</td> </tr> <tr> <td>boolean</td> <td>BOOLEAN</td> </tr> <tr> <td>byte</td> <td>BYTE</td> </tr> <tr> <td>array</td> <td>TUPLE</td> </tr> <tr> <td>map</td> <td>MAP</td> </tr> </table> <p/> <b>Partitions</b><br/> The input paths are scanned by the loader for [partition name]=[value] patterns in the subdirectories.<br/> If detected these partitions are appended to the table schema.<br/> For example if you have the directory structure:<br/> <pre> /user/hive/warehouse/mytable /year=2010/month=02/day=01 </pre> The mytable schema is (id int,name string).<br/> The final schema returned in pig will be (id:int, name:chararray, year:chararray, month:chararray, day:chararray).<br/> <p/> Usage 1: <p/> To load a hive table: uid bigint, ts long, arr ARRAY<string,string>, m MAP<String, String> <br/> <code> <pre> a = LOAD 'file' USING HiveColumnarLoader("uid bigint, ts long, arr array<string,string>, m map<string,string>"); -- to reference the fields b = FOREACH GENERATE a.uid, a.ts, a.arr, a.m; </pre> </code> <p/> Usage 2: <p/> To load a hive table: uid bigint, ts long, arr ARRAY<string,string>, m MAP<String, String> only processing dates 2009-10-01 to 2009-10-02 in a <br/> date partitioned hive table.<br/> <b>Old Usage</b><br/> <b>Note:</b> The partitions can be filtered by using pig's FILTER operator.<br/> <code> <pre> a = LOAD 'file' USING HiveColumnarLoader("uid bigint, ts long, arr array<string,string>, m map<string,string>", "2009-10-01:2009-10-02"); -- to reference the fields b = FOREACH GENERATE a.uid, a.ts, a.arr, a.m; </pre> </code> <br/> <b>New Usage</b/><br/> <code> <pre> a = LOAD 'file' USING HiveColumnarLoader("uid bigint, ts long, arr array<string,string>, m map<string,string>"); f = FILTER a BY daydate>='2009-10-01' AND daydate >='2009-10-02'; </pre> </code> <p/> Usage 3: <p/> To load a hive table: uid bigint, ts long, arr ARRAY<string,string>, m MAP<String, String> only reading column uid and ts for dates 2009-10-01 to 2009-10-02.<br/ <br/> <b>Old Usage</b><br/> <b>Note:<b/> This behaviour is now supported in pig by LoadPushDown adding the columns needed to be loaded like below is ignored and pig will automatically send the columns used by the script to the loader.<br/> <code> <pre> a = LOAD 'file' USING HiveColumnarLoader("uid bigint, ts long, arr array<string,string>, m map<string,string>"); f = FILTER a BY daydate>='2009-10-01' AND daydate >='2009-10-02'; -- to reference the fields b = FOREACH a GENERATE uid, ts, arr, m; </pre> </code> <p/> <b>Issues</b> <p/> <u>Table schema definition</u><br/> The schema definition must be column name followed by a space then a comma then no space and the next column name and so on.<br/> This so column1 string, column2 string will not work, it must be column1 string,column2 string <p/> <u>Partitioning</u><br/> Partitions must be in the format [partition name]=[partition value]<br/> Only strings are supported in the partitioning.<br/> Partitions must follow the same naming convention for all sub directories in a table<br/> For example:<br/> The following is not valid:<br/> <pre> mytable/hour=00 mytable/day=01/hour=00 </pre>

HiveRCInputFormat used by HiveColumnarLoader as the InputFormat; <p/> Reasons for implementing a new InputFormat sub class:<br/> <ul> <li>The current RCFileInputFormat uses the old InputFormat mapred interface, and the pig load store design used the new InputFormat mapreduce classes.</li> <li>The splits are calculated by the InputFormat, HiveColumnarLoader supports date partitions, the filtering is done here.</li> </ul>

This class delegates the work to the RCFileRecordReader<br/>
Implements helper methods for:<br/> <ul> <li>Parsing the hive table schema string.</li> <li>Converting from hive to pig types</li> <li>Converting from pig to hive types</li> </ul>
Use Hive UDAF or GenericUDAF. Example: define avg HiveUDAF('avg'); A = load 'mydata' as (name:chararray, num:double); B = group A by name; C = foreach B generate group, avg(A.num);
Use Hive UDF or GenericUDF. Example: define sin HiveUDF('sin'); A = load 'mydata' as (num:double); B = foreach A generate sin(num); HiveUDF takes an optional second parameter if the Hive UDF require constant parameters define in_file HiveUDF('in_file', '(null, "names.txt")');

Use Hive GenericUDTF. Example: define explode HiveUDTF('explode'); A = load 'mydata' as (a0:{(b0:chararray)}); B = foreach A generate flatten(explode(a0));

HostExtractor takes a url and returns the host. For example, http://sports.espn.go.com/mlb/recap?gameId=281009122 leads to sports.espn.go.com Pig latin usage looks like host = FOREACH row GENERATE org.apache.pig.piggybank.evaluation.util.apachelogparser.HostExtractor(referer);
<p>HoursBetween returns the number of hours between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>
math.IEEEremainder implements a binding to the Java function {@link java.lang.Math#IEEEremainder(double,double) Math.IEEEremainder(double,double)}. Given a tuple with two data atom it Returns the remainder operation on two arguments as prescribed by the IEEE 754 standard. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>IEEEremainder_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.IEEEremainder(float1); </code></dd> </dl>
INDEXOF implements eval function to search for a string Example: A = load 'mydata' as (name); B = foreach A generate INDEXOF(name, ",");
This UDF accepts a Map as input with values of any primitive data type. <br /> UDF swaps keys with values and returns the new inverse Map. <br /> Note in case original values are non-unique, the resulting Map would <br /> contain String Key -> DataBag of values. Here the bag of values is composed <br /> of the original keys having the same value. <br /> <pre> Note: 1. UDF accepts Map with Values of primitive data type 2. UDF returns Map<String,DataBag> <code> grunt> cat 1data [open#1,1#2,11#2] [apache#2,3#4,12#24] <br /> grunt> a = load 'data' as (M:[int]); grunt> b = foreach a generate INVERSEMAP($0); grunt> dump b; ([2#{(1),(11)},apache#{(open)}]) ([hadoop#{(apache),(12)},4#{(3)}]) </code> </pre>
<p>ISODaysBetween returns the number of days between two ISO8601 datetimes as a Long</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOYearsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOYearsBetween(); DEFINE ISOMonthsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMonthsBetween(); DEFINE ISODaysBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISODaysBetween(); DEFINE ISOHoursBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOHoursBetween(); DEFINE ISOMinutesBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMinutesBetween(); DEFINE ISOSecondsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOSecondsBetween(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE ISOYearsBetween(dt, dt2) AS years, ISOMonthsBetween(dt, dt2) AS months, ISODaysBetween(dt, dt2) AS days, ISOHoursBetween(dt, dt2) AS hours, ISOMinutesBetween(dt, dt2) AS mins, ISOSecondsBetween(dt, dt2) AS secs; DESCRIBE diffs; diffs: {years: long,months: long,days: long,hours: long,mins: long,secs: long} DUMP diffs; (0L,11L,341,8185L,491107L,29466421L) (0L,0L,5,122L,7326L,439562L) (0L,-10L,-332,-7988L,-479334L,-28760097L) </pre>
ISOHelper provides helper methods for the other classes in this package. <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> This class is public so that it can be tested in TestTruncateDateTime. Otherwise, it would have "package" visibility.
<p>ISOHoursBetween returns the number of hours between two ISO8601 datetimes as a Long</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/ <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601 </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOYearsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOYearsBetween(); DEFINE ISOMonthsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMonthsBetween(); DEFINE ISODaysBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISODaysBetween(); DEFINE ISOHoursBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOHoursBetween(); DEFINE ISOMinutesBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMinutesBetween(); DEFINE ISOSecondsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOSecondsBetween(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE ISOYearsBetween(dt, dt2) AS years, ISOMonthsBetween(dt, dt2) AS months, ISODaysBetween(dt, dt2) AS days, ISOHoursBetween(dt, dt2) AS hours, ISOMinutesBetween(dt, dt2) AS mins, ISOSecondsBetween(dt, dt2) AS secs; DESCRIBE diffs; diffs: {years: long,months: long,days: long,hours: long,mins: long,secs: long} DUMP diffs; (0L,11L,341,8185L,491107L,29466421L) (0L,0L,5,122L,7326L,439562L) (0L,-10L,-332,-7988L,-479334L,-28760097L) </pre>
<p>ISOMinutesBetween returns the number of minutes between two ISO8601 datetimes as a Long</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOYearsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOYearsBetween(); DEFINE ISOMonthsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMonthsBetween(); DEFINE ISODaysBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISODaysBetween(); DEFINE ISOHoursBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOHoursBetween(); DEFINE ISOMinutesBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMinutesBetween(); DEFINE ISOSecondsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOSecondsBetween(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE ISOYearsBetween(dt, dt2) AS years, ISOMonthsBetween(dt, dt2) AS months, ISODaysBetween(dt, dt2) AS days, ISOHoursBetween(dt, dt2) AS hours, ISOMinutesBetween(dt, dt2) AS mins, ISOSecondsBetween(dt, dt2) AS secs; DESCRIBE diffs; diffs: {years: long,months: long,days: long,hours: long,mins: long,secs: long} DUMP diffs; (0L,11L,341,8185L,491107L,29466421L) (0L,0L,5,122L,7326L,439562L) (0L,-10L,-332,-7988L,-479334L,-28760097L) </pre>
<p>ISOMonthsBetween returns the number of months between two ISO8601 datetimes as a Long</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOYearsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOYearsBetween(); DEFINE ISOMonthsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMonthsBetween(); DEFINE ISODaysBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISODaysBetween(); DEFINE ISOHoursBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOHoursBetween(); DEFINE ISOMinutesBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMinutesBetween(); DEFINE ISOSecondsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOSecondsBetween(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE ISOYearsBetween(dt, dt2) AS years, ISOMonthsBetween(dt, dt2) AS months, ISODaysBetween(dt, dt2) AS days, ISOHoursBetween(dt, dt2) AS hours, ISOMinutesBetween(dt, dt2) AS mins, ISOSecondsBetween(dt, dt2) AS secs; DESCRIBE diffs; diffs: {years: long,months: long,days: long,hours: long,mins: long,secs: long} DUMP diffs; (0L,11L,341,8185L,491107L,29466421L) (0L,0L,5,122L,7326L,439562L) (0L,-10L,-332,-7988L,-479334L,-28760097L) </pre>
<p>ISOSecondsBetween returns the number of seconds between two ISO8601 datetimes as a Long</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOYearsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOYearsBetween(); DEFINE ISOMonthsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMonthsBetween(); DEFINE ISODaysBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISODaysBetween(); DEFINE ISOHoursBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOHoursBetween(); DEFINE ISOMinutesBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOMinutesBetween(); DEFINE ISOSecondsBetween org.apache.pig.piggybank.evaluation.datetime.diff.ISOSecondsBetween(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE ISOYearsBetween(dt, dt2) AS years, ISOMonthsBetween(dt, dt2) AS months, ISODaysBetween(dt, dt2) AS days, ISOHoursBetween(dt, dt2) AS hours, ISOMinutesBetween(dt, dt2) AS mins, ISOSecondsBetween(dt, dt2) AS secs; DESCRIBE diffs; diffs: {years: long,months: long,days: long,hours: long,mins: long,secs: long} DUMP diffs; (0L,11L,341,8185L,491107L,29466421L) (0L,0L,5,122L,7326L,439562L) (0L,-10L,-332,-7988L,-479334L,-28760097L) </pre>
ISOToDay truncates an ISO8601 datetime string to the precision of the day field <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOToYear org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToYear(); DEFINE ISOToMonth org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMonth(); DEFINE ISOToWeek org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToWeek(); DEFINE ISOToDay org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToDay(); DEFINE ISOToHour org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToHour(); DEFINE ISOToMinute org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMinute(); DEFINE ISOToSecond org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToSecond(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... truncated = FOREACH ISOin GENERATE ISOToYear(dt) AS year, ISOToMonth(dt) as month, ISOToWeek(dt) as week, ISOToDay(dt) AS day, ISOToHour(dt) AS hour, ISOToMinute(dt) AS min, ISOToSecond(dt) as sec; DESCRIBE truncated; truncated: {year: chararray,month: chararray,week: chararray,day: chararray,hour: chararray,min: chararray,sec: chararray} DUMP truncated; (2009-01-01T00:00:00.000Z,2009-01-01T00:00:00.000Z,2009-01-05T00:00:00.000Z,2009-01-07T00:00:00.000Z,2009-01-07T01:00:00.000Z,2009-01-07T01:07:00.000Z,2009-01-07T01:07:01.000Z) (2008-01-01T00:00:00.000Z,2008-02-01T00:00:00.000Z,2008-02-04T00:00:00.000Z,2008-02-06T00:00:00.000Z,2008-02-06T02:00:00.000Z,2008-02-06T02:06:00.000Z,2008-02-06T02:06:02.000Z) (2007-01-01T00:00:00.000Z,2007-03-01T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T03:00:00.000Z,2007-03-05T03:05:00.000Z,2007-03-05T03:05:03.000Z) </pre>
ISOToHour truncates an ISO8601 datetime string to the precision of the hour field <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOToYear org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToYear(); DEFINE ISOToMonth org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMonth(); DEFINE ISOToWeek org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToWeek(); DEFINE ISOToDay org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToDay(); DEFINE ISOToHour org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToHour(); DEFINE ISOToMinute org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMinute(); DEFINE ISOToSecond org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToSecond(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... truncated = FOREACH ISOin GENERATE ISOToYear(dt) AS year, ISOToMonth(dt) as month, ISOToWeek(dt) as week, ISOToDay(dt) AS day, ISOToHour(dt) AS hour, ISOToMinute(dt) AS min, ISOToSecond(dt) as sec; DESCRIBE truncated; truncated: {year: chararray,month: chararray,week: chararray,day: chararray,hour: chararray,min: chararray,sec: chararray} DUMP truncated; (2009-01-01T00:00:00.000Z,2009-01-01T00:00:00.000Z,2009-01-05T00:00:00.000Z,2009-01-07T00:00:00.000Z,2009-01-07T01:00:00.000Z,2009-01-07T01:07:00.000Z,2009-01-07T01:07:01.000Z) (2008-01-01T00:00:00.000Z,2008-02-01T00:00:00.000Z,2008-02-04T00:00:00.000Z,2008-02-06T00:00:00.000Z,2008-02-06T02:00:00.000Z,2008-02-06T02:06:00.000Z,2008-02-06T02:06:02.000Z) (2007-01-01T00:00:00.000Z,2007-03-01T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T03:00:00.000Z,2007-03-05T03:05:00.000Z,2007-03-05T03:05:03.000Z)
ISOToMinute truncates an ISO8601 datetime string to the precision of the minute field <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOToYear org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToYear(); DEFINE ISOToMonth org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMonth(); DEFINE ISOToWeek org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToWeek(); DEFINE ISOToDay org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToDay(); DEFINE ISOToHour org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToHour(); DEFINE ISOToMinute org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMinute(); DEFINE ISOToSecond org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToSecond(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... truncated = FOREACH ISOin GENERATE ISOToYear(dt) AS year, ISOToMonth(dt) as month, ISOToWeek(dt) as week, ISOToDay(dt) AS day, ISOToHour(dt) AS hour, ISOToMinute(dt) AS min, ISOToSecond(dt) as sec; DESCRIBE truncated; truncated: {year: chararray,month: chararray,week: chararray,day: chararray,hour: chararray,min: chararray,sec: chararray} DUMP truncated; (2009-01-01T00:00:00.000Z,2009-01-01T00:00:00.000Z,2009-01-05T00:00:00.000Z,2009-01-07T00:00:00.000Z,2009-01-07T01:00:00.000Z,2009-01-07T01:07:00.000Z,2009-01-07T01:07:01.000Z) (2008-01-01T00:00:00.000Z,2008-02-01T00:00:00.000Z,2008-02-04T00:00:00.000Z,2008-02-06T00:00:00.000Z,2008-02-06T02:00:00.000Z,2008-02-06T02:06:00.000Z,2008-02-06T02:06:02.000Z) (2007-01-01T00:00:00.000Z,2007-03-01T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T03:00:00.000Z,2007-03-05T03:05:00.000Z,2007-03-05T03:05:03.000Z) </pre>
ISOToMonth truncates an ISO8601 datetime string to the precision of the month field <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOToYear org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToYear(); DEFINE ISOToMonth org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMonth(); DEFINE ISOToWeek org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToWeek(); DEFINE ISOToDay org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToDay(); DEFINE ISOToHour org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToHour(); DEFINE ISOToMinute org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMinute(); DEFINE ISOToSecond org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToSecond(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... truncated = FOREACH ISOin GENERATE ISOToYear(dt) AS year, ISOToMonth(dt) as month, ISOToWeek(dt) as week, ISOToDay(dt) AS day, ISOToHour(dt) AS hour, ISOToMinute(dt) AS min, ISOToSecond(dt) as sec; DESCRIBE truncated; truncated: {year: chararray,month: chararray,week: chararray,day: chararray,hour: chararray,min: chararray,sec: chararray} DUMP truncated; (2009-01-01T00:00:00.000Z,2009-01-01T00:00:00.000Z,2009-01-05T00:00:00.000Z,2009-01-07T00:00:00.000Z,2009-01-07T01:00:00.000Z,2009-01-07T01:07:00.000Z,2009-01-07T01:07:01.000Z) (2008-01-01T00:00:00.000Z,2008-02-01T00:00:00.000Z,2008-02-04T00:00:00.000Z,2008-02-06T00:00:00.000Z,2008-02-06T02:00:00.000Z,2008-02-06T02:06:00.000Z,2008-02-06T02:06:02.000Z) (2007-01-01T00:00:00.000Z,2007-03-01T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T03:00:00.000Z,2007-03-05T03:05:00.000Z,2007-03-05T03:05:03.000Z) </pre>
ISOToSecond truncates an ISO8601 datetime string to the precision of the second field <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOToYear org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToYear(); DEFINE ISOToMonth org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMonth(); DEFINE ISOToWeek org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToWeek(); DEFINE ISOToDay org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToDay(); DEFINE ISOToHour org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToHour(); DEFINE ISOToMinute org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMinute(); DEFINE ISOToSecond org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToSecond(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... truncated = FOREACH ISOin GENERATE ISOToYear(dt) AS year, ISOToMonth(dt) as month, ISOToWeek(dt) as week, ISOToDay(dt) AS day, ISOToHour(dt) AS hour, ISOToMinute(dt) AS min, ISOToSecond(dt) as sec; DESCRIBE truncated; truncated: {year: chararray,month: chararray,week: chararray,day: chararray,hour: chararray,min: chararray,sec: chararray} DUMP truncated; (2009-01-01T00:00:00.000Z,2009-01-01T00:00:00.000Z,2009-01-05T00:00:00.000Z,2009-01-07T00:00:00.000Z,2009-01-07T01:00:00.000Z,2009-01-07T01:07:00.000Z,2009-01-07T01:07:01.000Z) (2008-01-01T00:00:00.000Z,2008-02-01T00:00:00.000Z,2008-02-04T00:00:00.000Z,2008-02-06T00:00:00.000Z,2008-02-06T02:00:00.000Z,2008-02-06T02:06:00.000Z,2008-02-06T02:06:02.000Z) (2007-01-01T00:00:00.000Z,2007-03-01T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T03:00:00.000Z,2007-03-05T03:05:00.000Z,2007-03-05T03:05:03.000Z) </pre>
<p>ISOToUnix converts ISO8601 datetime strings to Unix Time Longs</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> <li>Unix Time: http://en.wikipedia.org/wiki/Unix_time</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOToUnix org.apache.pig.piggybank.evaluation.datetime.convert.ISOToUnix(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... toUnix = FOREACH ISOin GENERATE ISOToUnix(dt) AS unixTime:long; DESCRIBE toUnix; toUnix: {unixTime: long} DUMP toUnix; (1231290421000L) (1202263562000L) (1173063903000L) ... </pre>
ISOToWeek truncates an ISO8601 datetime string to the precision of the day field, for the first day of the week of the datetime.  This 'rounds' to the week's monday, see http://joda-time.sourceforge.net/cal_iso.html <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE ISOToYear org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToYear(); DEFINE ISOToMonth org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMonth(); DEFINE ISOToWeek org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToWeek(); DEFINE ISOToDay org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToDay(); DEFINE ISOToHour org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToHour(); DEFINE ISOToMinute org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToMinute(); DEFINE ISOToSecond org.apache.pig.piggybank.evaluation.datetime.truncate.ISOToSecond(); ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:chararray, dt2:chararray); DESCRIBE ISOin; ISOin: {dt: chararray,dt2: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... truncated = FOREACH ISOin GENERATE ISOToYear(dt) AS year, ISOToMonth(dt) as month, ISOToWeek(dt) as week, ISOToDay(dt) AS day, ISOToHour(dt) AS hour, ISOToMinute(dt) AS min, ISOToSecond(dt) as sec; DESCRIBE truncated; truncated: {year: chararray,month: chararray,week: chararray,day: chararray,hour: chararray,min: chararray,sec: chararray} DUMP truncated; (2009-01-01T00:00:00.000Z,2009-01-01T00:00:00.000Z,2009-01-05T00:00:00.000Z,2009-01-07T00:00:00.000Z,2009-01-07T01:00:00.000Z,2009-01-07T01:07:00.000Z,2009-01-07T01:07:01.000Z) (2008-01-01T00:00:00.000Z,2008-02-01T00:00:00.000Z,2008-02-04T00:00:00.000Z,2008-02-06T00:00:00.000Z,2008-02-06T02:00:00.000Z,2008-02-06T02:06:00.000Z,2008-02-06T02:06:02.000Z) (2007-01-01T00:00:00.000Z,2007-03-01T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T00:00:00.000Z,2007-03-05T03:00:00.000Z,2007-03-05T03:05:00.000Z,2007-03-05T03:05:03.000Z) </pre>


Returns the input
Returns the input, used by DuplicateForEachColumnRewrite to rewrite duplicate columns in ForEach. Not intended for external use


Dummy implementation of StatusReporter for illustrate mode
Class used by physical operators to generate example tuples for the ILLUSTRATE purpose
The class used to (re)attach illustrators to physical operators


* This matcher allows matching of different keys but both key sets have to be in the same increasing order Example: Plan1:  0-Load 1-Distinct 2-Split 3-SplitOutput 4-SplitOutput 5-Union Plan2: 10-Load 14-Distinct 15-Split 100-SplitOutput 110-SplitOutput 9999-Union Note: All the keys have to be in the same scope.
This class is intended for use by LoadFunc implementations which have an internal index for sorted data and can use the index to support merge join in Pig. Interaction with the index is abstracted away by the methods in this interface which the Pig runtime will call in a particular sequence to get the records it needs to perform the merge based join. The sequence of calls made from the Pig runtime are: <ol> <li>{@link LoadFunc#setUDFContextSignature(String)} <li>{@link IndexableLoadFunc#initialize(Configuration)} <li>{@link LoadFunc#setLocation(String, org.apache.hadoop.mapreduce.Job)} <li>{@link IndexableLoadFunc#seekNear(Tuple)} <li>{@link LoadFunc#getNext} called multiple times to retrieve data and perform the join <li>{@link IndexableLoadFunc#close} </ol> Set to evolving because we don't have this working with outer join yet, and we may need to change it some for that.
IndexedKey records the index and key info. This is used as key for JOINs. It addresses the case where key is either empty (or is a tuple with one or more empty fields). In this case, we must respect the SQL standard as documented in the equals() method.
<code>IndexedStorage</code> is a form of <code>PigStorage</code> that supports a per record seek.  <code>IndexedStorage</code> creates a separate (hidden) index file for every data file that is written.  The format of the index file is: <pre> | Header     | | Index Body | | Footer     | </pre> The Header contains the list of record indices (field numbers) that represent index keys. The Index Body contains a <code>Tuple</code> for each record in the data. The fields of the <code>Tuple</code> are: <ul> <li> The index key(s) <code>Tuple</code> </li> <li> The number of records that share this index key. </li> <li> Offset into the data file to read the first matching record. </li> </ul> The Footer contains sequentially: <ul> <li> The smallest key(s) <code>Tuple</code> in the index. </li> <li> The largest key(s) <code>Tuple</code> in the index. </li> <li> The offset in bytes to the start of the footer </li> </ul> <code>IndexStorage</code> implements <code>IndexableLoadFunc</code> and can be used as the 'right table' in a PIG 'merge' or 'merge-sparse' join. <code>IndexStorage</code> does not require the data to be globally partitioned & sorted by index keys.  Each partition (separate index) must be locally sorted. Also note IndexStorage is a loader to demonstrate "merge-sparse" join.
{@link InputHandler} is responsible for handling the input to the Pig-Streaming external command. The managed executable could be fed input in a {@link InputType#SYNCHRONOUS} manner via its <code>stdin</code> or in an {@link InputType#ASYNCHRONOUS} manner via an external file which is subsequently read by the executable.


Class that estimates the number of reducers based on input size. Number of reducers is based on two properties: <ul> <li>pig.exec.reducers.bytes.per.reducer - how many bytes of input per reducer (default is 1000*1000*1000)</li> <li>pig.exec.reducers.max - constrain the maximum number of reducer task (default is 999)</li> </ul> If using a loader that implements LoadMetadata the reported input size is used, otherwise attempt to determine size from the filesystem. <p> e.g. the following is your pig script <pre> a = load '/data/a'; b = load '/data/b'; c = join a by $0, b by $0; store c into '/tmp'; </pre> and the size of /data/a is 1000*1000*1000, and the size of /data/b is 2*1000*1000*1000 then the estimated number of reducer to use will be (1000*1000*1000+2*1000*1000*1000)/(1000*1000*1000)=3
This class encapsulates the runtime statistics of a user specified input.
ABS implements a binding to the Java function {@link java.lang.Math#abs(double) Math.abs(int)} for computing the absolute value of the argument. The returned value will be an int which is absolute value of the input.
This method should never be used directly, use {@link AVG}.
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MIN}.
This method should never be used directly, use {@link SUM}.
A record reader used to read data written using {@link InterRecordWriter} It uses the default InterSedes object for deserialization.
A record reader used to write data compatible with {@link InterRecordWriter} It uses the default InterSedes object for serialization.
A class to handle reading and writing of intermediate results of data types. The serialization format used by this class more efficient than what was used in DataReaderWriter . The format used by the functions in this class is subject to change, so it should be used ONLY to store intermediate results within a pig query.
Used to get hold of the single instance of InterSedes . In future, the subclass of InterSedes that gets instantiated would be configurable.
LOAD FUNCTION FOR PIG INTERNAL USE ONLY! This load function is used for storing intermediate data between MR jobs of a pig query. The serialization format of this load function can change in newer versions of pig, so this should NOT be used to store any persistent data.
Annotation to inform users of a package, class or method's intended audience.
Annotation to inform users of how much to rely on a particular package, class or method not changing over time.

An unordered collection of Tuples with no multiples.  Data is stored without duplicates as it comes in.  When it is time to spill, that data is sorted and written to disk.  The data is stored in a HashSet.  When it is time to sort it is placed in an ArrayList and then sorted.  Dispite all these machinations, this was found to be faster than storing it in a TreeSet. This bag spills pro-actively when the number of tuples in memory reaches a limit
This class is an empty extension of Map<Object, Object>.  It only exists so that DataType.findType() can distinguish an internal map type that maps object to object from an external map type that is string to object.
An ordered collection of Tuples (possibly) with multiples.  Data is stored unsorted as it comes in, and only sorted when it is time to dump it to a file or when the first iterator is requested.  Experementation found this to be the faster than storing it sorted to begin with. We allow a user defined comparator, but provide a default comparator in cases where the user doesn't specify one. This bag is not registered with SpillableMemoryManager. It calculates the number of tuples to hold in memory and spill pro-actively into files.










TODO need to add support for ANY Pig type! TODO statically cache the generated code based on the input Strings
This UDF is used to check whether the String input is a Double. Note this function checks for Double range. If range is not important, use IsNumeric instead if you would like to check if a String is numeric. Also IsNumeric performs slightly better compared to this function.
Determine whether a bag or map is empty.


This UDF is used to check whether the String input is a Float. Note this function checks for Float range. If range is not important, use IsNumeric instead if you would like to check if a String is numeric. Also IsNumeric performs slightly better compared to this function.
This UDF is used to check whether the String input is an Integer. Note this function checks for Integer range -2,147,483,648 to 2,147,483,647. If range is not important, use IsNumeric instead if you would like to check if a String is numeric. Also IsNumeric performs slightly better compared to this function.
This UDF is used to check whether the String input is a Long. Note this function checks for Long range. If range is not important, use IsNumeric instead if you would like to check if a String is numeric. Also IsNumeric performs slightly better compared to this function.

This UDF is used to check if a String is numeric. Note this UDF is different from IsInt in 2 ways, 1. Does not check for Integer range 2. Runs faster as this UDF uses Regex match and not Integer.parseInt(String) This UDF is expected to perform slightly better than isInt, isLong, isFloat, isDouble. However, primary goal of this UDF is NOT performance but rather to check "numeric"-ness of a String. Use this UDF if you do not care for the type (int, long, float, double) and would just like to check if its numeric in nature. It does NOT check for Range of int, long, double, parse. This function will return true when the String is larger than the range of any numeric data type (int, long, double, float). Use specific functions (IsInt, IsFloat, IsLong, IsDouble) if range is important.

This class provides a much more intuitive way to write Accumulator UDFs.<br> For example, you could express IsEmpty as follows: <pre><code>public class IsEmpty extends IteratingAccumulatorEvalFunc<Boolean> { public Boolean exec(Iterator<Tuple> iter) throws IOException { return !iter.hashNext(); } }</code></pre> Count could be implemented as follows: <pre><code>public class Count extends IteratingAccumulatorEvalFunc<Long> { public Long exec(Iterator<Tuple> iter) throws IOException { long ct = 0; for (; iter.hasNext(); iter.next()) { ct++; } return ct; } }</code></pre>




Pig entry point from javascript

This class is used to manage JVM Reuse in case of execution engines like Tez and Spark. Static data members of a UDF, LoadFunc or StoreFunc class need to be reset or object references made null or reinitialized when the jvm container is reused for new tasks. Example usage to perform static data cleanup in a UDF as follows. public class MyUDF extends EvalFunc<Tuple> { private static int numInvocations = 0; private static Reporter reporter; static { // Register this class for static data cleanup JVMReuseManager.getInstance().registerForStaticDataCleanup(MyUDF.class); } // Write a public static method that performs the cleanup // and annotate it with @StaticDataCleanup

An implementation of interface CharStream, where the stream is assumed to contain only ASCII characters (with java-like unicode escape processing).

This is compiler class that takes an MROperPlan and converts it into a JobControl object with the relevant dependency info maintained. The JobControl Object is made up of Jobs each of which has a JobConf. The MapReduceOper corresponds to a Job and the getJobCong method returns the JobConf that is configured as per the MapReduceOper <h2>Comparator Design</h2> <p> A few words on how comparators are chosen.  In almost all cases we use raw comparators (the one exception being when the user provides a comparison function for order by).  For order by queries the PigTYPERawComparator functions are used, where TYPE is Int, Long, etc.  These comparators are null aware and asc/desc aware.  The first byte of each of the NullableTYPEWritable classes contains info on whether the value is null. Asc/desc is written as an array into the JobConf with the key pig.sortOrder so that it can be read by each of the comparators as part of their setConf call. <p> For non-order by queries, PigTYPEWritableComparator classes are used. These are all just type specific instances of WritableComparator.



This class encapsulates the runtime statistics of a MapReduce job. Job statistics is collected when job is completed.
Collapse LocalRearrange,GlobalRearrange,Package to POJoinGroupSpark to reduce unnecessary map operations to optimize join/group. Detail see PIG-4797


This class provides a bridge between Ruby classes that extend AccumulatorPigUdf and their execution in Pig. This class passes a Bag of data to the Ruby "exec" function, and ultimate gets the value by calling "get" on the class instance that receives methods.
This class provides the bridge between Ruby classes that extend the AlgebraicPigUdf "interface" by implementing an initial, intermed, and final method. Unlike EvalFuncs and Accumulators, the type must be known at compile time (ie it can't return Object), as Pig inspects the type and ensures that it is valid. This is why class specific shells are provided at the bottom. This class leverages AlgebraicEvalFunc to provide the Accumulator and EvalFunc implementations.
This class serves at the bridge between Ruby methods that are registered with and extend PigUdf, and their execution in Pig. An instance of the containing class is created, and their method name will be called against that instance. If they have a schema function associated, then when outputSchema is called, that function will be given the input Schema and the output will be given to Pig.
Implementation of the script engine for Jruby, which facilitates the registration of scripts as UDFs, and also provides information (via the nested class RubyFunctions) on the registered functions.

{@link ScriptEngine} implementation for JavaScript
A loader for data stored using {@link JsonStorage}.  This is not a generic JSON loader. It depends on the schema being stored with the data when conceivably you could write a loader that determines the schema from the JSON.
Reads and Writes metadata using JSON in metafiles next to the data.
A JSON Pig store function.  Each Pig tuple is stored on one line (as one value for TextOutputFormat) so that it can be read easily using TextInputFormat.  Pig tuples are mapped to JSON objects.  Pig bags are mapped to JSON arrays.  Pig maps are also mapped to JSON objects.  Maps are assumed to be string to string.  A schema is stored in a side file to deal with mapping between JSON and Pig types. The schema file share the same format as the one we use in PigStorage.
Python implementation of a Pig UDF Performs mappings between Python & Pig data structures
Implementation of the script engine for Jython

This UDF takes a Map and returns a Bag containing the keyset. <br /> <pre> <code> grunt> cat data [open#apache,1#2,11#2] [apache#hadoop,3#4,12#hadoop] grunt> a = load 'data' as (M:[]); grunt> b = foreach a generate KEYSET($0); grunt> dump b; ({(open),(1),(11)}) ({(3),(apache),(12)}) </code> </pre>
A visitor to figure out the type of the key for the map plan this is needed when the key is null to create an appropriate NullableXXXWritable object
Pulled class from Hive on Spark

















string.INSTR implements eval function to search for the last occurrence of a string<br> Returns null on error<br> Example: <code> register pigudfs.jar; A = load 'mydata' as (name); B = foreach A generate string.LASTINDEXOF(name, ","); dump B; </code>
string.INSTR implements eval function to search for the last occurrence of a string Returns null on error Example: <code> A = load 'mydata' as (name); B = foreach A generate LASTINDEXOF(name, ","); </code>
string.LENGTH implements eval function to find length of a string Example: register piggybank.jar; A = load 'mydata' as (name); B = foreach A generate string.LENGTH(name); dump B;


CUBE operator implementation for data cube computation. <p> Cube operator syntax <pre> {@code alias = CUBE rel BY { CUBE | ROLLUP}(col_ref) [, { CUBE | ROLLUP }(col_ref) ...];} alias - output alias CUBE - operator rel - input relation BY - operator CUBE | ROLLUP - cube or rollup operation col_ref - column references or * or range in the schema referred by rel </pre> </p> <p> The cube computation and rollup computation using UDFs {@link org.apache.pig.builtin.CubeDimensions} and {@link org.apache.pig.builtin.RollupDimensions} can be represented like below <pre> {@code events = LOAD '/logs/events' USING EventLoader() AS (lang, event, app_id, event_id, total); eventcube = CUBE events BY CUBE(lang, event), ROLLUP(app_id, event_id); result = FOREACH eventcube GENERATE FLATTEN(group) as (lang, event), COUNT_STAR(cube), SUM(cube.total); STORE result INTO 'cuberesult';} </pre> In the above example, CUBE(lang, event) will generate all combinations of aggregations {(lang, event), (lang, ), ( , event), ( , )}. For n dimensions, 2^n combinations of aggregations will be generated. Similarly, ROLLUP(app_id, event_id) will generate aggregations from the most detailed to the most general (grandtotal) level in the hierarchical order like {(app_id, event_id), (app_id, ), ( , )}. For n dimensions, n+1 combinations of aggregations will be generated. The output of the above example query will have the following combinations of aggregations {(lang, event, app_id, event_id), (lang, , app_id, event_id), ( , event, app_id, event_id), ( , , app_id, event_id), (lang, event, app_id, ), (lang, , app_id, ), ( , event, app_id, ), ( , , app_id, ), (lang, event, , ), (lang, , , ), ( , event, , ), ( , , , )} Total number of combinations will be ( 2^n * (n+1) ) Since cube and rollup clause use null to represent "all" values of a dimension, if the dimension values contain null values it will be converted to "unknown" before computing cube or rollup. </p>



LOG implements a binding to the Java function {@link java.lang.Math#log(double) Math.log(double)}. Given a single data atom it returns the natural logarithm (base e)  of a double
LOG10 implements a binding to the Java function {@link java.lang.Math#log10(double) Math.log10(double)}. Given a single data atom it returns the base 10 logarithm of a double
math.LOG1P implements a binding to the Java function {@link java.lang.Math#log1p(double) Math.log1p(double)}. Given a single data atom it Returns the natural logarithm of the sum of the argument and 1. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>LOG1P_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.LOG1P(float1); </code></dd> </dl>

Operator to map the data into the inner plan of LOForEach It can only be used in the inner plan of LOForEach



/** * @return the load */ public LOLoad getLoad() { return load; }  /** * @param load the load to set */ public void setLoad(LOLoad load) { this.load = load; }  /** * @return the store */ public LOStore getStore() { return store; }  /** * @param store the store to set */ public void setStore(LOStore store) { this.store = store; }
RANK operator implementation. Operator Syntax: <pre> {@code alias = RANK rel ( BY (col_ref) (ASC|DESC)? ( DENSE )? )?;} alias - output alias RANK - operator rel - input relation BY - operator col_ref - STAR or Column References or a range in the schema of rel DENSE - dense rank means a sequential value without gasp among different tuple values. </pre>






LOWER implements eval function to convert a string to lower case Example: A = load 'mydata' as (name); B = foreach A generate LOWER(name);

Returns a string, with only leading whitespace omitted. Implements a binding to the Java function {@link java.lang.String#trim() String.trim()}.
Provides core processing implementation for the backend of Pig if ExecutionEngine chosen decides to delegate it's work to this class. Also contains set of utility methods, including ones centered around Hadoop.
lower-case the first character of a string



A collection of static functions for use by the pigmix map reduce tasks.



An ordered collection of Tuples (possibly) with multiples.  Data is stored in a priority queue as it comes in, and only sorted when iterator is requested. LimitedSortedDataBag is not spillable. We allow a user defined comparator, but provide a default comparator in cases where the user doesn't specify one.
Create mapping between uid and Load FuncSpec when the LogicalExpression associated with it is known to hold an unmodified element of data returned by the load function. This information is used to find the LoadCaster to be used to cast bytearrays into other other types.



This package operator is a specialization of POPackage operator used for the specific case of the order by query. See JIRA 802 for more details.
An interface that provides cast implementations for load functions.  For casts between bytearray objects and internal types, Pig relies on the load function that loaded the data to provide the cast.  This is because Pig does not understand the binary representation of the data and thus cannot cast it.  This interface provides functions to cast from bytearray to each of Pig's internal types. Because we still don't have the map casts quite right
Converter that loads data via POLoad and converts it to RRD&lt;Tuple>. Abuses the interface a bit in that there is no input RRD to convert in this case. Instead input is the source path of the POLoad.
A LoadFunc loads data into Pig.  It can read from an HDFS file or other source. LoadFunc is tightly coupled to Hadoop's {@link org.apache.hadoop.mapreduce.InputFormat}. LoadFunc's sit atop an InputFormat and translate from the keys and values of Hadoop to Pig's tuples. <p> LoadFunc contains the basic features needed by the majority of load functions.  For more advanced functionality there are separate interfaces that a load function can implement.  See {@link LoadCaster}, {@link LoadMetadata}, {@link LoadPushDown}, {@link OrderedLoadFunc}, {@link CollectableLoadFunc}, and {@link IndexableLoadFunc}.
Contains the logic for finding a LoadFunc based on the definition of: <ul> <li>file.extension.loaders</li> <li>file.format.loaders</li> </ul>
Convenience class to extend when decorating a class that extends LoadFunc and implements LoadMetadata.
Convenience class to extend when decorating a LoadFunc. Subclasses must call the setLoadFunc with an instance of LoadFunc before other methods can be called. Not doing so will result in an IllegalArgumentException when the method is called.
This interface defines how to retrieve metadata related to data to be loaded. If a given loader does not implement this interface, it will be assumed that it is unable to provide metadata about the associated data.
This interface defines how a loader can support predicate pushdown. If a given loader implements this interface, pig will pushdown predicates based on type of operations supported by the loader on given set of fields. This interface is private in Pig 0.14 and will be made public in Pig 0.15 after PIG-4093. It is to be used only by builtin LoadFunc implementations till it is made public as PIG-4093 will cause API changes to this interface and make it backward incompatible.
This interface defines how to communicate to Pig what functionality can be pushed into the loader.  If a given loader does not implement this interface it will be assumed that it is unable to accept any functionality for push down.
This is just a union interface of LoadCaster and StoreCaster, made available for simplicity.


LocalExecType is the ExecType for local mode in Hadoop Mapreduce.
Main class that launches pig for Map Reduce


This is a pig loader that can load Apache HTTPD access logs written in (almost) any Apache HTTPD LogFormat.<br/> Basic usage: <br/> Simply feed the loader your (custom) logformat specification and it will tell you which fields can be extracted from this logformat.<br/> For example: <pre> -- Specify any existing file as long as it exists. -- It won't be read by the loader when no fields are requested. Example = LOAD 'test.pig' USING org.apache.pig.piggybank.storage.apachelog.LogFormatLoader( '%h %l %u %t "%r" %>s %b "%{Referer}i" "%{User-Agent}i"' ); DUMP Example; </pre> The output of this command is a (huge) example (yes actual pig code) which demonstrates how all possible fields can be extracted. In normal use cases this example will be trimmed down to request only the fields your application really needs. This loader implements pushdown projection so there is no need to worry too much about the fields you leave in. This loader supports extracting things like an individual cookie or query string parameter regardless of the position it has in the actual log line. In addition to the logformat specification used in your custom config this parser also understands the standard formats:<pre> common combined combinedio referer agent </pre> So this works also: <pre> Example = LOAD 'test.pig' USING org.apache.pig.piggybank.storage.apachelog.LogFormatLoader('common'); DUMP Example; </pre> This class is simply a wrapper around <a href="https://github.com/nielsbasjes/logparser" >https://github.com/nielsbasjes/logparser</a> so more detailed documentation can be found there.


Logical representation of expression operators.  Expression operators have a data type and a uid.  Uid is a unique id for each expression.
A plan containing LogicalExpressionOperators.
A visitor for expression plans.
LogicalPlan is the logical view of relational operations Pig will execute for a given script.  Note that it contains only relational operations. All expressions will be contained in LogicalExpressionPlans inside each relational operator.

This class provides information regarding the LogicalPlan. Make sure to avoid exposing LogicalPlan itself. Only data regarding the logical plan could be exposed but none of Pig internals (plans, operators etc) should be.
Grammar file for Pig tree parser (for schema alias validation). NOTE: THIS FILE IS BASED ON QueryParser.g, SO IF YOU CHANGE THAT FILE, YOU WILL PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THIS FILE AS WELL.

A visitor mechanism printing out the logical plan.
Implementors of this interface would define validations based on logical operators within a Pig script. The validations could be called from {@link HExecutionEngine#compile(org.apache.pig.newplan.logical.relational.LogicalPlan, java.util.Properties)}
A visitor for logical plans.
Logical representation of relational operators.  Relational operators have a schema.
Schema, from a logical perspective.


This method should never be used directly, use {@link AVG}.
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MIN}.
This method should never be used directly, use {@link SUM}.
<dl> <dt><b>Syntax:</b></dt> <dd><code>int lookupInFiles(String expression,... <comma separated filelist>)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>files are text files on DFS</code>.</dd> <dt><b>Output:</b></dt> <dd><code>if any file contains expression, return 1, otherwise, 0</code>.</dd> </dl>
Generates the maximum of a set of values. This class implements {@link org.apache.pig.Algebraic}, so if possible the execution will performed in a distributed fashion. <p> MAX can operate on any numeric type and on chararrays.  It can also operate on bytearrays, which it will cast to doubles.    It expects a bag of tuples of one record each.  If Pig knows from the schema that this function will be passed a bag of integers or longs, it will use a specially adapted version of MAX that uses integer arithmetic for comparing the data.  The return type of MAX will match the input type. <p> MAX implements the {@link org.apache.pig.Accumulator} interface as well. While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation.
Generates the minimum of a set of values. This class implements {@link org.apache.pig.Algebraic}, so if possible the execution will performed in a distributed fashion. <p> MIN can operate on any numeric type and on chararrays.  It can also operate on bytearrays, which it will cast to doubles.    It expects a bag of tuples of one record each.  If Pig knows from the schema that this function will be passed a bag of integers or longs, it will use a specially adapted version of MIN that uses integer arithmetic for comparing the data.  The return type of MIN will match the input type. <p> MIN implements the {@link org.apache.pig.Accumulator} interface as well. While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation.
The compiler that compiles a given physical plan into a DAG of MapReduce operators which can then be converted into the JobControl structure. Is implemented as a visitor of the PhysicalPlan it is compiling. Currently supports all operators except the MR Sort operator Uses a predecessor based depth first traversal. To compile an operator, first compiles the predecessors into MapReduce Operators and tries to merge the current operator into one of them. The goal being to keep the number of MROpers to a minimum. It also merges multiple Map jobs, created by compiling the inputs individually, into a single job. Here a new map job is created and then the contents of the previous map plans are added. However, any other state that was in the previous map plans, should be manually moved over. So, if you are adding something new take care about this. Ex of this is in requestedParallelism Only in case of blocking operators and splits, a new MapReduce operator is started using a store-load combination to connect the two operators. Whenever this happens care is taken to add the MROper into the MRPlan and connect it appropriately.


MRExecType is the ExecType for distributed mode in Hadoop Mapreduce.

An {@link MROpPlanVisitor} that gathers the paths for all intermediate data from a {@link MROperPlan}
This class encapsulates the runtime statistics of a MapReduce job. Job statistics is collected when job is completed.
A visitor for the MROperPlan class
A Plan used to create the plan of Map Reduce Operators which can be converted into the Job Control object. This is necessary to capture the dependencies among jobs
A utility class for Pig Statistics
A visitor mechanism printing out the logical plan.
ScriptStates encapsulates settings for a Pig script that runs on a hadoop cluster. These settings are added to all MR jobs spawned by the script and in turn are persisted in the hadoop job xml. With the properties already in the job xml, users who want to know the relations between the script and MR jobs can derive them from the job xmls.



Main class for Pig engine.

This filter Marks every Load Operator which has a Map with MAP_MARKER_ANNOTATION. The annotation value is <code>Map<Integer,Set<String>><code> where Integer is the column number of the field and Set is the set of Keys in this field ( field is a map field only ). It does this for only the top level schema in load. Algorithm: Traverse the Plan in ReverseDependency order ( ie. Sink to Source ) For LogicalRelationalOperators having MapLookupExpression in their expressionPlan collect uid and keys related to it. This is retained in the visitor For ForEach having nested LogicalPlan use the same visitor hence there is no distinction required At Sources find all the uids provided by this source and annotate this LogicalRelationalOperator ( load ) with <code>Map<Integer,Set<String>></code> containing only the column numbers that this LogicalRelationalOperator generates NOTE: This is a simple Map Pruner. If a map key is mentioned in the script then this pruner assumes you need the key. This pruner is not as optimized as column pruner ( which removes a column if it is mentioned but never used )


A class of utility static methods to be used in the hadoop map reduce backend verification code: debug purpose only public String inputSplitToString(ArrayList<ComparableSplit> splits) throws IOException, InterruptedException { StringBuilder st = new StringBuilder(); st.append("Number of splits :" + splits.size()+"\n"); long len = 0; for (ComparableSplit split: splits) len += split.getSplit().getLength(); st.append("Total Length = "+ len +"\n"); for (int i = 0; i < splits.size(); i++) { st.append("Input split["+i+"]:\n   Length = "+ splits.get(i).getSplit().getLength()+"\n  Locations:\n"); for (String location :  splits.get(i).getSplit().getLocations()) st.append("    "+location+"\n"); st.append("\n-----------------------\n"); } return st.toString(); }
Main class that launches pig for Map Reduce
An operator model for a Map Reduce job. Acts as a host to the plans that will execute in map, reduce and optionally combine phases. These will be embedded in the MROperPlan in order to capture the dependencies amongst jobs.
This class is used to have a POStore write to DFS via a output collector/record writer. It sets up a modified job configuration to force a write to a specific subdirectory of the main output directory. This is done so that multiple output directories can be used in the same job.
Spark Partitioner that wraps a custom partitioner that implements org.apache.hadoop.mapreduce.Partitioner interface. Since Spark's shuffle API takes a different parititioner class (@see org.apache.spark.Partitioner) compared to MapReduce, we need to wrap custom partitioners written for MapReduce inside this Spark Partitioner. MR Custom partitioners are expected to implement getPartition() with specific arguments: public int getPartition(PigNullableWritable key, Writable value, int numPartitions) For an example of such a partitioner,

This method should never be used directly, use {@link SIZE}.
MaxTupleBy1stField UDF returns a tuple with max value of the first field in a given bag. Caveat: first field assumed to have type 'long'. You may need to enforece this via schema when loading data, as sown in sample usage below. Sample usage: A = load 'test.tsv' as (first: long, second, third); B = GROUP A by second; C = FOREACH B GENERATE group, MaxTupleBy1stField(A);




Merge Join indexer is used to generate on the fly index for doing Merge Join efficiently. It samples first record from every block of right side input. and returns tuple in the following format : (key0, key1,...,position,splitIndex) These tuples are then sorted before being written out to index file on HDFS.

Evaluates various metrics
<p>MilliSecondsBetween returns the number of milliseconds between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>
This class builds a single instance of itself with the Singleton design pattern. While building the single instance, it sets up a mini cluster that actually consists of a mini DFS cluster and a mini MapReduce cluster on the local machine and also sets up the environment for Pig to run on top of the mini cluster.
Starts an on-demand mini cluster that requires no set up. <p>It can be useful if you don't want to restart the cluster between each run of test and don't want to set up a real cluster. <p>CLASSPATH needs to contain: pig.jar and piggybank.jar <pre> export CLASSPATH=/path/pig.jar:/path/piggybank.jar java org.apache.pig.pigunit.MiniClusterRunner </pre> <p>Possible improvements <ul> <li>add a main in MiniCluster</li> <li>make MiniCluster configurable (number of maps...)</li> <li>make MiniCluster use a default properties for chosing the hadoop conf dir (e.g. minicluster.conf.dir) instead of always using System.getProperty("user.home"), "pigtest/conf/"</li> <li>use CLI option</li> <li>make a shell wrapper</li> </ul>
This class builds a single instance of itself with the Singleton design pattern. While building the single instance, it sets up a mini cluster that actually consists of a mini DFS cluster and a mini MapReduce cluster on the local machine and also sets up the environment for Pig to run on top of the mini cluster. This class is the base class for MiniCluster, which has slightly difference among different versions of hadoop. MiniCluster implementation is located in $PIG_HOME/shims.
<p>MinutesBetween returns the number of minutes between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>

Mod Operator
Describes how the execution of a UDF should be monitored, and what to do if it times out. <p> NOTE: does not work with UDFs that implement the Accumulator interface <p> Setting a default value will cause it to be used instead of null when the UDF times out. The appropriate value among in, long, string, etc, is used. The default fields of these annotations are arrays for Java reasons I won't bore you with. <p> Set them as if they were primitives: <code>@MonitoredUDF( intDefault=5 )</code> <p> There is currently no way to provide a default ByteArray, Tuple, Map, or Bag. Null will always be used for those. <p> Currently this annotation is ignored when the Accumulator interface is used.
MonitoredUDF is used to watch execution of a UDF, and kill it if the UDF takes an exceedingly long time. Null is returned if the UDF times out. Optionally, UDFs can implement the provided interfaces to provide custom logic for handling errors and default values.
<p>MonthsBetween returns the number of months between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>



An implementation of multi-map.  We can't use Apache commons MultiValueMap because it isn't serializable.  And we don't want to use MultiHashMap, as it is marked deprecated. This class can't extend Map, because it needs to change the semantics of put, so that you give it one key and one value, and it either creates a new entry with the key and a new collection of value (if the is not yet in the map) or adds the values to the existing collection for the key (if the key is already in the map).
An optimizer that merges all or part splittee MapReduceOpers into splitter MapReduceOper. <p> The merge can produce a MROperPlan that has fewer MapReduceOpers than MapReduceOpers in the original MROperPlan. <p> The MRCompler generates multiple MapReduceOpers whenever it encounters a split operator and connects the single splitter MapReduceOper to one or more splittee MapReduceOpers using store/load operators: <p> ---- POStore (in splitter) -... ---- |        |    ...    | |        |    ...    | POLoad  POLoad ...  POLoad (in splittees) |        |           | <p> This optimizer merges those MapReduceOpers by replacing POLoad/POStore combination with POSplit operator.
MultiQueryOptimizer for spark

The package operator that packages the globally rearranged tuples into output format as required by multi-query de-multiplexer. <p> This operator is used when merging multiple Map-Reduce splittees into a Map-only splitter during multi-query optimization. The package operators of the reduce plans of the splittees form an indexed package list inside this operator. When this operator receives an input, it extracts the index from the key and calls the corresponding package to get the output data. <p> Due to the recursive nature of multi-query optimization, this operator may be contained in another multi-query packager. <p> The successor of this operator must be a PODemux operator which knows how to consume the output of this operator.
The UDF is useful for splitting the output data into a bunch of directories and files dynamically based on user specified key field in the output tuple. Sample usage: <code> A = LOAD 'mydata' USING PigStorage() as (a, b, c); STORE A INTO '/my/home/output' USING MultiStorage('/my/home/output','0', 'bz2', '\\t'); </code> Parameter details:- ========== <b>/my/home/output </b>(Required) : The DFS path where output directories and files will be created. <b> 0 </b>(Required) : Index of field whose values should be used to create directories and files( field 'a' in this case). <b>'bz2' </b>(Optional) : The compression type. Default is 'none'. Supported types are:- 'none', 'gz' and 'bz2' <b> '\\t' </b>(Optional) : Output field separator. Let 'a1', 'a2' be the unique values of field 'a'. Then output may look like this /my/home/output/a1/a1-0000 /my/home/output/a1/a1-0001 /my/home/output/a1/a1-0002 ... /my/home/output/a2/a2-0000 /my/home/output/a2/a2-0001 /my/home/output/a2/a2-0002 The prefix '0000*' is the task-id of the mapper/reducer task executing this store. In case user does a GROUP BY on the field followed by MultiStorage(), then its imperative that all tuples for a particular group will go exactly to 1 reducer. So in the above case for e.g. there will be only 1 file each under 'a1' and 'a2' directories. If the output is compressed,then the sub directories and the output files will be having the extension. Say for example in the above case if bz2 is used one file will look like ;/my/home/output.bz2/a1.bz2/a1-0000.bz2 Key field can also be a comma separated list of indices e.g. '0,1' - in this case storage will be multi-level: /my/home/output/a1/b1/a1-b1-0000 /my/home/output/a1/b2/a1-b2-0000 There is also an option to leave key values out of storage, see isRemoveKeys.

Multiply Operator
MyRegExLoader extends RegExLoader, allowing regular expressions to be passed by argument through pig latin via a line like A = LOAD 'file:test.txt' USING org.apache.pig.piggybank.storage.MyRegExLoader('(\\d+)!+(\\w+)~+(\\w+)'); which would parse lines like 1!!!one~i 2!!two~~ii 3!three~~~iii into arrays like {1, "one", "i"}, {2, "two", "ii"}, {3, "three", "iii"}


math.NEXTUP implements a binding to the Java function {@link java.lang.Math#nextUp(double) Math.nextUp(double)}. Given a single data atom it return the floating-point value adjacent to input in the direction of positive infinity <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>NEXTUP_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.NEXTUP(float1); </code></dd> </dl>
This function divides a search query string into wrods and extracts n-grams with up to _ngramSizeLimit length. Example 1: if query = "a real nice query" and _ngramSizeLimit = 2, the query is split into: a, real, nice, query, a real, real nice, nice query Example 2: if record = (u1, h1, pig hadoop) and _ngramSizeLimit = 2, the record is split into: (u1, h1, pig), (u1, h1, hadoop), (u1, h1, pig hadoop)

NativeSparkOperator:



Additional utility functions for new logical plan

All AST nodes must implement this interface.  It provides basic machinery for constructing the parent and child relationships between nodes.
Generates IDs as long values in a thread safe manner. Each thread has its own generated IDs.
* This is a common interface for graph vertex mapping logic. Though I call it Matcher so that people don't get confused with mapper in MapReduce.


Test data bag factory, for testing that we can properly provide a non default bag factory.
Marker interface to distinguish LoadFunc implementations that don't use file system sources.

An unordered collection of Tuples (possibly) with multiples.  The tuples are stored in an ArrayList, since there is no concern for order or distinctness. The implicit assumption is that the user of this class is storing only those many tuples as will fit in memory - no spilling will be done on this bag to disk.
This function removes search queries that are URLs (as defined by _urlPattern). This function also removes empty queries.
<p>A non-deterministic UDF is one that can produce different results when invoked on the same input. Examples of non-deterministic behavior might be, for example, getCurrentTime() or random().</p> <p>Certain Pig optimizations depend on UDFs being deterministic. It is therefore very important for correctness that non-deterministic UDFs be annotated as such.</p>
For historical reasons splits will always produce filters that pass everything through unchanged. This optimizer removes these. <p/> The condition we look for is POFilters with a constant boolean (true) expression as it's plan.

An optimizer that removes unnecessary temp stores (Stores generated by the MRCompiler to bridge different jobs - even though a real store is produced from the same data). The pattern looks like this: ------------- Split --------- |                           | Store(InterStorage)         Store(StoreFunc) Followed by a load of the tmp store in a dependent MapReduceOper. This optmizer removes the store, collapses the split if only one branch remains and adjusts the loads to load from the real store. The situation is produced by something we do to the logical plan in PigServer. There we change: PreOp | Store Load | PostOp To: PreOp | / \ /   \ PostOp  Store If there is a job boundary between pre and post we will end up in this case.
NotEquality test expression.












NullablePartitionWritable is an adaptor class around PigNullableWritable that adds a partition index to the class.


This class can be used when data type is 'Unknown' and there is a need for PigNullableWritable object.





An interface that defines graph operations on plans.  Plans are modeled as graphs with restrictions on the types of connections and operations allowed.

Class to represent a view of a plan. The view contains a subset of the plan. All the operators returned from the view are the same objects to the operators in its base plan. It is used to represent match results.
This class transforms LogicalPlan to its textual representation


Boolean OR Expression
A load function and store function for ORC file. An optional constructor argument is provided that allows one to customize advanced behaviors. A list of available options is below: <ul> <li><code>-s, --stripeSize</code> Set the stripe size for the file <li><code>-r, --rowIndexStride</code> Set the distance between entries in the row index <li><code>-b, --bufferSize</code> The size of the memory buffers used for compressing and storing the stripe in memory <li><code>-p, --blockPadding</code> Sets whether the HDFS blocks are padded to prevent stripes from straddling blocks <li><code>-c, --compress</code> Sets the generic compression that is used to compress the data. Valid codecs are: NONE, ZLIB, SNAPPY, LZO <li><code>-v, --version</code> Sets the version of the file that will be written </ul>




Implementing this interface indicates to Pig that a given loader can be used for MergeJoin. It does not mean the data itself is ordered, but rather that the splits returned by the underlying InputFormat can be ordered to match the order of the data they are loading.  For example, files splits have a natural order (that of the file they are from) while splits of RDBMS does not (since tables have no inherent order). The position as represented by the WritableComparable object is stored in the index created by a MergeJoin sampling MapReduce job to get an ordered sequence of splits. It is necessary to read splits in order during a merge join to assure data is being read according to the sort order. Since we haven't done outer join for merge join yet

{@link OutputHandler} is responsible for handling the output of the Pig-Streaming external command. The output of the managed executable could be fetched in a {@link OutputType#SYNCHRONOUS} manner via its <code>stdout</code> or in an {@link OutputType#ASYNCHRONOUS} manner via an external file to which the process wrote its output.
An EvalFunc can annotated with an <code>OutputSchema</code> to tell Pig what the expected output is. This can be used in place of {@link EvalFunc#outputSchema(Schema)} <p> The default implementation of {@link EvalFunc#outputSchema(Schema)} will look at this annotation and return an interpreted schema, if the annotation is present. <p> Implementing a custom {@link EvalFunc#outputSchema(Schema)} will override the annotation (unless you deal with it explicitly, or by calling <code>super.outputSchema(schema)</code>). <p> Here's an example of a complex schema declared in an annotation: <code>@OutputSchema("y:bag{t:tuple(len:int,word:chararray)}")</code>

This class encapsulates the runtime statistics of an user specified output.
Given an aggregate function, a bag, and possibly a window definition, produce output that matches SQL OVER.  It is the reponsibility of the caller to have already ordered the bag as required by their operation. The aggregate, and window definition are passed in the constructor.  The bag is passed to exec each time. <p>Usage: Over(bag, function_to_call[, window_start, window_end[, function specific args]]) <p>bag - The bag to be called.  Most functions assume this is a bag with tuples of a single field. <p>function_to_call - Can be one of the following: <ul> <li>count</li> <li>sum(double) <li>sum(float)</li> <li>sum(int)</li> <li>sum(long)</li> <li>sum(bytearray)</li> <li>sum(bigdecimal)</li> <li>avg(double)</li> <li>avg(float)</li> <li>avg(long)</li> <li>avg(int)</li> <li>avg(bytearray)</li> <li>avg(bigdecimal)</li> <li>min(double)</li> <li>min(float)</li> <li>min(long)</li> <li>min(int)</li> <li>min(chararray)</li> <li>min(bytearray)</li> <li>min(bigdecimal)</li> <li>max(double)</li> <li>max(float)</li> <li>max(long)</li> <li>max(int)</li> <li>max(chararray)</li> <li>max(bytearray)</li> <li>max(bigdecimal)</li> <li>row_number</li> <li>first_value</li> <li>last_value</li> <li>lead</li> <li>lag</li> <li>rank</li> <li>dense_rank</li> <li>ntile</li> <li>percent_rank</li> <li>cume_dist</li> </ul> <p>window_start - optional - Record to start window on for the function.  -1 indicates 'unbounded preceding', i.e. the beginning of the bag.  A positive integer indicates that number of records before the current record.  0 indicates the current record.  If not specified -1 is the default. <p>window_end - optional - Record to end window on for the function.  -1 indicates 'unbounded following', i.e. the end of the bag.  A positive integer indicates that number of records after the current record.  0 indicates teh current record.  If not specified 0 is the default. <p>function_specific_args - maybe optional - The following functions accept require additional arguments: <ul> <li>lead - two optional arguments, first number of records ahead of current to lead, second default value when lead extends beyond the end of the window frame.</li> <li>lag - two optional arguments, first number of records behind of current to lag, second default value when lag extends beyond the beginning of the window frame.</li> <li>rank - one required, the number of the field the bag is ordered by</li> <li>dense_rank - one required, the number of the field the bag is ordered by</li> <li>ntile - one required, the number of buckets to split the data into</li> <li>percent_rank - one required, the number of the field the bag is ordered by</li> <li>cume_dist - one required, the number of the field the bag is ordered by</li> </ul> <p>Example Usage: <p>To do a cumulative sum: <p><pre> A = load 'T' AS (si:chararray, i:int, d:long, f:float, s:chararray); C = foreach (group A by si) { Aord = order A by d; generate flatten(Stitch(Aord, Over(Aord.f, 'sum(float)'))); } D = foreach C generate s, $5;</pre> <p> This is equivalent to the SQL statement <p><tt>select s, sum(f) over (partition by si order by d) from T;</tt> <p>To find the record 3 ahead of the current record, using a window between the current row and 3 records ahead and a default value of 0. <p><pre> A = load 'T' AS (si:chararray, i:int, d:long, f:float, s:chararray); C = foreach (group A by si) { Aord = order A by i; generate flatten(Stitch(Aord, Over(Aord.i, 'lead', 0, 3, 3, 0))); } D = foreach C generate s, $9;</pre> <p> This is equivalent to the SQL statement <p><tt>select s, lead(i, 3, 0) over (partition by si order by i rows between current row and 3 following) over T;</tt> <p>Over accepts a constructor argument specifying the name and type, colon-separated, of its return schema. If the argument option is 'true' use the inner-search, take the name and type of bag and return a schema with alias+'_over' and the same type</p> <p><pre> DEFINE IOver org.apache.pig.piggybank.evaluation.Over('state_rk:int'); cities = LOAD 'cities' AS (city:chararray, state:chararray, pop:int); -- Decorate each city with its population rank within the state it belongs to: ranked = FOREACH(GROUP cities BY state) { c_ord = ORDER cities BY pop DESC; GENERATE FLATTEN(Stitch(c_ord, IOver(c_ord, 'rank', -1, -1, 2))); -- beginning (-1) to end (-1) on third field (2) }; DESCRIBE ranked; -- ranked: {stitched::city: chararray,stitched::state: chararray,stitched::pop: int,stitched::state_rk: int} DUMP ranked; -- ... -- (Nashville,Tennessee,609644,2) -- (Houston,Texas,2145146,1) -- (San Antonio,Texas,1359758,2) -- (Dallas,Texas,1223229,3) -- (Austin,Texas,820611,4) -- ... </pre></p>
A {@link StoreFunc} should implement this interface to enable overwriting its store/output location if it already exists.
Boolean and operator.



This operator writes out the key value for the hash join reduce operation similar to POLocalRearrangeTez. In addition, it also writes out the bloom filter constructed from the join keys in the case of bloomjoin map strategy or join keys themselves in case of reduce strategy. Using multiple bloom filters partitioned by the hash of the key allows for parallelism. It also allows us to have lower false positives with smaller vector sizes.
This is just a cast that converts DataByteArray into either String or Integer. Just added it for testing the POUnion. Need the full operator implementation.
This is just a cast that converts DataByteArray into either String or Integer. Just added it for testing the POUnion. Need the full operator implementation.
The collected group operator is a special operator used when users give the hint 'using "collected"' in a group by clause. It implements a map-side group that collects all records for a given key into a buffer. When it sees a key change it will emit the key and bag for records it had buffered. It will assume that all keys for a given record are collected together and thus there is not need to buffer across keys.
This operator is part of the RANK operator implementation. It adds a local counter and a unique task id to each tuple. There are 2 modes of operations: regular and dense. The local counter is depends on the mode of operation. With regular rank is considered duplicate rows while assigning numbers to distinct values groups. With dense rank counts the number of distinct values, without considering duplicate rows. Depending on if it is considered. the entire tuple (row number) or a by a set of columns (rank by). This Physical Operator relies on some specific MR class, available at PigMapReduceCounter.
POCounterStatsTez is used to group counters from previous vertex POCounterTez tasks

Recover this class for nested cross operation.
The MapReduce Demultiplexer operator. <p> This operator is used when merging multiple Map-Reduce splittees into a Map-only splitter during multi-query optimization. The reduce physical plans of the splittees become the inner plans of this operator. <p> Due to the recursive nature of multi-query optimization, this operator may be contained in another demux operator. <p> The predecessor of this operator must be a POMultiQueryPackage operator which passes the index (indicating which inner reduce plan to run) along with other data to this operator.
Find the distinct set of tuples in a bag. This is a blocking operator. All the input is put in the hashset implemented in DistinctDataBag which also provides the other DataBag interfaces.
The operator models the join keys using the Local Rearrange operators which are configured with the plan specified by the user. It also sets up one Hashtable per replicated input which maps the Key(k) stored as a Tuple to a DataBag which holds all the values in the input having the same key(k) The getNext() reads an input from its predecessor and separates them into key & value. It configures a foreach operator with the databags obtained from each Hashtable for the key and also with the value for the fragment input. It then returns tuples returned by this foreach operator. We intentionally skip type checking in backend for performance reasons

POFRJoinTez is used on the backend to load replicated table from Tez ShuffleUnorderedKVInput and load fragmented table from data pipeline.
This is an implementation of the Filter operator. It has an Expression Plan that decides whether the input tuple should be filtered or passed through. To avoid many function calls, the filter operator, stores the Comparison Operator that is the root of the Expression Plan and uses its getNext directly. Since the filter is supposed to return tuples only, getNext is not supported on any other data type.
We intentionally skip type checking in backend for performance reasons
Dummy operator to test MRCompiler. This will be a local operator and its getNext methods have to be implemented We intentionally skip type checking in backend for performance reasons
POGlobalRearrange for spark mode
POIdentityInOutTez is used to pass through tuples as is to next vertex from previous vertex's POLocalRearrangeTez. For eg: In case of Order By, the partition vertex which just applies the WeightedRangePartitioner on the previous vertex data uses POIdentityInOutTez.

Collapse POLocalRearrange,POGlobalRearrange and POPackage to POJoinGroupSpark to reduce unnecessary map operations in the join/group

The load operator which is used in two ways: 1) As a local operator it can be used to load files 2) In the Map Reduce setting, it is used to create jobs from MapReduce operators which keep the loads and stores in the Map and Reduce Plans till the job is created
The local rearrange operator is a part of the co-group implementation. It has an embedded physical plan that generates tuples of the form (grpKey,(indxed inp Tuple)).
POLocalRearrangeTez is used to write to a Tez OrderedPartitionedKVOutput (shuffle) or UnorderedKVOutput (broadcast)



This operator implements merge join algorithm to do map side joins. Currently, only two-way joins are supported. One input of join is identified as left and other is identified as right. Left input tuples are the input records in map. Right tuples are read from HDFS by opening right stream. This join doesn't support outer join. Data is assumed to be sorted in ascending order. It will fail if data is sorted in descending order.


Boolean not operator.

A specialized version of POForeach with the difference that in getNext(), it knows that "input" has already been attached by its input operator which SHOULD be POJoinPackage We intentionally skip type checking in backend for performance reasons
Boolean or operator.
The package operator that packages the globally rearranged tuples into output format as required by co-group. This is last stage of processing co-group. This operator has a slightly different format than other operators in that, it takes two things as input. The key being worked on and the iterator of bags that contain indexed tuples that just need to be packaged into their appropriate output bags based on the index.
This visitor visits the MRPlan and does the following for each MROper - visits the POPackage in the reduce plan and finds the corresponding POLocalRearrange(s) (either in the map plan of the same oper OR reduce plan of predecessor MROper). It then annotates the POPackage with information about which columns in the "value" are present in the "key" and will need to stitched in to the "value"
Do partial aggregation in map plan. Inputs are buffered up in a hashmap until a threshold is reached; then the combiner functions are fed these buffered up inputs, and results stored in a secondary map. Once that map fills up or all input has been seen, results are piped out into the next operator (caller of getNext()).
The partition rearrange operator is a part of the skewed join implementation. It has an embedded physical plan that generates tuples of the form (inpKey,reducerIndex,(indxed inp Tuple)).
The partition rearrange operator is a part of the skewed join implementation. It has an embedded physical plan that generates tuples of the form (inpKey,reducerIndex,(indxed inp Tuple)).


A specialized local rearrange operator which behaves like the regular local rearrange in the getNext() as far as getting its input and constructing the "key" out of the input. It then returns a tuple with two fields - the key in the first position and the "value" inside a bag in the second position. This output format resembles the format out of a Package. This output will feed to a foreach which expects this format.
TODO FIX private PrintStream mStream = null; public POPrinter(Map<OperatorKey, ExecPhysicalOperator> opTable, PrintStream ps) { super(opTable); mStream = ps; } public void visitMapreduce(POMapreduce mr) { mStream.println("MAPREDUCE"); printHeader(mr); mStream.println("Map: "); visitSpecs(mr.toMap); if (mr.toCombine != null) { mStream.println("Combine: "); //visitSpecs(mr.toCombine); mr.toCombine.visit(new EvalSpecPrinter(mStream)); } if (mr.toReduce != null) { mStream.println("Reduce: "); mr.toReduce.visit(new EvalSpecPrinter(mStream)); } if (mr.groupFuncs != null) { mStream.println("Grouping Funcs: "); visitSpecs(mr.groupFuncs); } mStream.print("Input Files: "); Iterator<FileSpec> i = mr.inputFileSpecs.iterator(); while (i.hasNext()) { mStream.print(i.next().getFileName()); if (i.hasNext()) mStream.print(", "); } mStream.println(); if (mr.outputFileSpec != null) { mStream.println("Output File: " + mr.outputFileSpec.getFileName()); } if (mr.partitionFunction != null) { mStream.println("Partition Function: " + mr.partitionFunction.getName()); } super.visitMapreduce(mr); } public void visitLoad(POLoad load) { mStream.println("LOAD"); printHeader(load); super.visitLoad(load); } public void visitSort(POSort s) { mStream.println("SORT"); printHeader(s); super.visitSort(s); } public void visitStore(POStore s) { mStream.println("STORE"); printHeader(s); super.visitStore(s); } private void visitSpecs(List<EvalSpec> specs) { Iterator<EvalSpec> j = specs.iterator(); while (j.hasNext()) { j.next().visit(new EvalSpecPrinter(mStream)); } } private void printHeader(PhysicalOperator po) { mStream.println("Object id: " + po.hashCode()); mStream.print("Inputs: "); for (int i = 0; i < po.inputs.length; i++) { if (i != 0) mStream.print(", "); mStream.print(po.inputs[i].hashCode()); } mStream.println(); }
Implements the overloaded form of the project operator. Projects the specified column from the input tuple. However, if asked for tuples when the input is a bag, the overloaded form is invoked and the project streams the tuples through instead of the bag.
This operator is part of the RANK operator implementation. Reads the output tuple from POCounter and the cumulative sum previously calculated. Here is read the task identifier in order to get the corresponding cumulative sum, and the local counter at the tuple. These values are summed and prepended to the tuple.

This operator is used to read tuples from a databag in memory. Used mostly for testing. It'd also be useful for the example generator
ReduceBy operator that maps to Sparks ReduceBy. Extends ForEach and adds packager, secondary sort and partitioner support.

Implements a specialized form of POProject which is used *ONLY* in the following case: This project is Project(*) introduced after a relational operator to supply a bag as output (as an expression). This project is either providing the bag as input to a successor expression operator or is itself the leaf in a inner plan If the predecessor relational operator sends an EOP then send an empty bag first to signal "empty" output and then send an EOP NOTE: A Project(*) of return type BAG whose predecessor is from an outside plan (i.e. not in the same inner plan as the project) will NOT lead us here. So a query like: a = load 'baginp.txt' as (b:bag{t:tuple()}); b = foreach a generate $0; dump b; will go through a regular project (without the following flag)



POShuffledValueInputTez is used read tuples from a Tez Intermediate output from a shuffle edge To be used with POValueOutputTez. TODO: To be removed after PIG-3775 and TEZ-661
POSimpleTezLoad is used on the backend to read tuples from a Tez MRInput
The PhysicalOperator that represents a skewed join. It must have two inputs. This operator does not do any actually work, it is only a place holder. When it is translated into MR plan, a POSkewedJoin is translated into a sampling job and a join job.
This implementation is applicable for both the physical plan and for the local backend, as the conversion of physical to mapreduce would see the SORT operator and take necessary steps to convert it to a quantile and a sort job. This is a blocking operator. The sortedDataBag accumulates Tuples and sorts them only when there an iterator is started. So all the tuples from the input operator should be accumulated and filled into the dataBag. The attachInput method is not applicable here. We intentionally skip type checking in backend for performance reasons
This operator is a variation of PODistinct, the input to this operator must be sorted already.
The MapReduce Split operator. <p> The assumption here is that the logical to physical translation will create this dummy operator with just the filename using which the input branch will be stored and used for loading Also the translation should make sure that appropriate filter operators are configured as outputs of this operator using the conditions specified in the LOSplit. So LOSplit will be converted into: |        |           | Filter1  Filter2 ... Filter3 |        |    ...    | |        |    ...    | ---- POSplit -... ---- This is different than the existing implementation where the POSplit writes to sidefiles after filtering and then loads the appropriate file. <p> The approach followed here is as good as the old approach if not better in many cases because of the availability of attachinInputs. An optimization that can ensue is if there are multiple loads that load the same file, they can be merged into one and then the operators that take input from the load can be stored. This can be used when the mapPlan executes to read the file only once and attach the resulting tuple as inputs to all the operators that take input from this load. In some cases where the conditions are exclusive and some outputs are ignored, this approach can be worse. But this leads to easier management of the Split and also allows to reuse this data stored from the split job whenever necessary.
POStatus is a set of flags used to communicate the status of Pig's operator pipeline to consumers.
The store operator which is used in two ways: 1) As a local operator it can be used to store files 2) In the Map Reduce setting, it is used to create jobs from MapReduce operators which keep the loads and stores in the Map and Reduce Plans till the job is created
This class is used to specify the actual behavior of the store operator just when ready to start execution.
POStoreTez is used to write to a Tez MROutput

The union operator that combines the two inputs into a single stream. Note that this doesn't eliminate duplicate tuples. The Operator will also be added to every map plan which processes more than one input. This just pulls out data from the piepline using the proposed single threaded shared execution model. By shared execution I mean, one input to the Union operator is called once and the execution moves to the next non-drained input till all the inputs are drained.
We intentionally skip type checking in backend for performance reasons

POValueInputTez is used read tuples from a Tez Intermediate output from a 1-1 edge

math.POW implements a binding to the Java function {@link java.lang.Math#pow(double,double) Math.pow(double,double)}. Given a tuple with two data atom it Returns the the value of the first argument raised to the power of the second argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>POW_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.POW(float1); </code></dd> </dl>


Copy of C++ STL pair container.


warnings in by code generated by javacc cannot be fixed here, so suppressing all warnings for this class. But this does not help in supressing Warnings in other classes generated by this .jj file
Token literal values and constants. Generated by org.javacc.parser.OtherFilesGen#start()
Token Manager.


Wrapper class which will delegate calls to parquet.pig.ParquetLoader
Wrapper class which will delegate calls to parquet.pig.ParquetStorer
This exception is thrown when parse errors are encountered. You can explicitly create objects of this exception type by calling the method generateParseException in the generated parser. You can modify this class to customize your error reporting mechanisms so long as you retain the public fields.



This is a rewrite of {@code PColFilterExtractor} Extracts partition filters for interfaces implementing LoadMetaData

Partition reducers for skewed keys. This is used in skewed join during sampling process. It figures out how many reducers required to process a skewed key without causing spill and allocate this number of reducers to this key. This UDF outputs a map which contains 2 keys: <li>&quot;totalreducers&quot;: the value is an integer wich indicates the number of total reducers for this join job </li> <li>&quot;partition.list&quot;: the value is a bag which contains a list of tuples with each tuple representing partitions for a skewed key. The tuple has format of &lt;join key&gt;,&lt;min index of reducer&gt;, &lt;max index of reducer&gt; </li> For example, a join job configures 10 reducers, and the sampling process finds out 2 skewed keys, &quot;swpv&quot; needs 4 reducers and &quot;swps&quot; needs 2 reducers. The output file would be like following: {totalreducers=10, partition.list={(swpv,0,3), (swps,4,5)}} The name of this file is set into next MR job which does the actual join. That job uses this information to partition skewed keys properly

VertexManagerPlugin used by sorting job of order by and skewed join. What is does is to set parallelism of the sorting vertex according to numParallelism specified by the predecessor vertex. The complex part is the PigOrderByEdgeManager, which specify how partition in the previous setting routes to the new vertex setting
Implements the logic for:<br/> <ul> <li>Listing partition keys and values used in an hdfs path</li> <li>Filtering of partitions from a pig filter operator expression</li> </ul> <p/> <b>Restrictions</b> <br/> Function calls are not supported by this partition helper and it can only handle String values.<br/> This is normally not a problem given that partition values are part of the hdfs folder path and is given a<br/> determined value that would not need parsing by any external processes.<br/>
Its convenient sometimes to partition logs by date values or other e.g. country, city etc.<br/> A daydate partitioned hdfs directory might look something like:<br/> <pre> /logs/repo/mylog/ daydate=2010-01-01 daydate=2010-01-02 </pre> This class accepts a path like /logs/repo/mylog and return a map of the partition keys
Used for finding/representing a pattern in the plan This class represents a node in the pattern
Used for finding/representing a pattern in the plan This class represents the pattern Finds only a single matching pattern This is finding a sub-graph( represented by pattern) in the graph(plan)


Sets the parent plan for all Physical Operators. Note: parentPlan is a bit of a misnomer. We actually want all the operators to point to the same plan - not necessarily the one they're a member of.
The visitor class for the Physical Plan. To use this, create the visitor with the plan to be visited. Call the visit() method to traverse the plan in a depth first fashion. This class also visits the nested plans inside the operators. One has to extend this class to modify the nature of each visit and to maintain any relevant state information between the visits to two different operators.
This is the base class for all operators. This supports a generic way of processing inputs which can be overridden by operators extending this class. The input model assumes that it can either be taken from an operator or can be attached directly to this operator. Also it is assumed that inputs to an operator are always in the form of a tuple. For this pipeline rework, we assume a pull based model, i.e, the root operator is going to call getNext with the appropriate type which initiates a cascade of getNext calls that unroll to create input for the root operator to work on. Any operator that extends the PhysicalOperator, supports a getNext with all the different types of parameter types. The concrete implementation should use the result type of its input operator to decide the type of getNext's parameter. This is done to avoid switch/case based on the type as much as possible. The default is assumed to return an erroneus Result corresponding to an unsupported operation on that type. So the operators need to implement only those types that are supported.
The base class for all types of physical plans. This extends the Operator Plan.
This visitor visits the physical plan and resets it for next MRCompilation
The class being used in scripts to interact with Pig

An avro GenericDatumReader which reads in avro data and converts them to pig data: tuples, bags, etc.
An avro GenericDatumWriter to write pig data as Avro data.
The InputFormat for avro data.
The OutputFormat for avro data.
This is an implementation of record reader which reads in avro data and convert them into <NullWritable, Writable> pairs.
The RecordWriter used to output pig results as avro data





Interface defining Pig commands and a {@link PigCommandFilter#validate(Command)} method to operate on it


Container for static configuration strings, defaults, etc. This is intended just for keys that can be set by users, not for keys that are generally used within pig.


A helper class to deal with Hadoop counters in Pig.  They are stored within the singleton PigStatusReporter instance, but are null for some period of time at job startup, even after Pig has been invoked.  This class buffers counters, trying each time to get a valid Reporter and flushing stored counters each time it does.
The following enum will contain the general counters that pig uses.


All exceptions in Pig are encapsulated in the <code>PigException</code> class. Details such as the source of the error, error message, error code, etc. are contained in this class. The default values for the attributes are: errorSource = BUG errorCode = 0 retriable = false detailedMessage = null location = null


warnings in by code generated by javacc cannot be fixed here, so suppressing all warnings for this class. But this does not help in supressing Warnings in other classes generated by this .jj file
Token literal values and constants. Generated by org.javacc.parser.OtherFilesGen#start()
Token Manager.

This class is the base class for PigMapBase, which has slightly difference among different versions of hadoop. PigMapBase implementation is located in $PIG_HOME/shims.
This class is the static Mapper &amp; Reducer classes that are used by Pig to execute Pig Map Reduce jobs. Since there is a reduce phase, the leaf is bound to be a POLocalRearrange. So the map phase has to separate the key and tuple and collect it into the output collector. The shuffle and sort phase sorts these keys &amp; tuples and creates key, List&lt;Tuple&gt; and passes the key and iterator to the list. The deserialized POPackage operator is used to package the key, List&lt;Tuple&gt; into pigKey, Bag&lt;Tuple&gt; where pigKey is of the appropriate pig type and then the result of the package is attached to the reduce plan which is executed if its not empty. Either the result of the reduce plan or the package res is collected into the output collector. The index of the tuple (that is, which bag it should be placed in by the package) is packed into the key.  This is done so that hadoop sorts the keys in order of index for join. This class is the base class for PigMapReduce, which has slightly difference among different versions of hadoop. PigMapReduce implementation is located in $PIG_HOME/shims.

A singleton class that implements the PigLogger interface for use in map reduce context. Provides ability to aggregate warning messages
Private, internal constants for use by Pig itself. Please see {@link org.apache.pig.PigConstants} if looking for public constants.




extends the hadoop JobControl to remove the hardcoded sleep(5000) as most of this is private we have to use reflection See {@link https://svn.apache.org/repos/asf/hadoop/common/branches/branch-0.23.1/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/jobcontrol/JobControl.java}
This class provides the ability to present to Ruby a library that was written in Java. In JRuby, there are two ways to present a library to ruby: one is to implement it in ruby as a module or class, and the other is to implement it in Java and then register it with the runtime. For the Pig datatypes we provide Ruby implementations for, it was easier to implement them in Java and provide a Ruby interface via the annotations that Jruby provides. Additionally, this class provides static object conversion functionality to and from Pig and JRuby.
An interface to allow aggregation of messages



This class is the static Mapper class used by Pig to execute Pig map only jobs. It gets a TargetedTuple as input to the map function. Using the targets in it, attaches the tuple as input to the target operators. The map plan is then executed. The result is then collected into the output collector provide by Hadoop which is configured to write to the store file. There is a reporter running in a separate thread that keeps reporting progress to let the job tracker know that we are alive. The sleep time for the reporter thread can be configured per job via the "pig.reporter.sleep.time". By default it uses a 10 sec sleep time


A base class for all types that pig uses to move data between map and reduce.  It implements WritableComparable so that compareTo etc. can be called.  It also wraps a WritableComparable 'value'.  This is set by each different type to be an object of its specific type. It also provides a getIndex() and setIndex() calls that are used to get and set the index.  These can be used by LocalRearrange, the partitioner, and Package to determine the index. Index and the null indicator are packed into one byte to save space. Put in to make the compiler not complain about WritableComparable being a generic type.
A specialization of the default FileOutputCommitter to allow pig to inturn delegate calls to the OutputCommiter(s) of the StoreFunc(s)' OutputFormat(s).
The better half of PigInputFormat which is responsible for the Store functionality. It is the exact mirror image of PigInputFormat having RecordWriter instead of a RecordReader.



A load function for the performance tests.

Should be implemented by an object that wants to receive notifications from {@link PigRunner}.
Pig's progress indicator.  An implemenation of this interface is passed to UDFs to allow them to send heartbeats.  By default Hadoop will kill a task if it does not receive a heartbeat every 600 seconds.  Any operation that may take more than this should call progress on a regular basis.
Subclass of Antlr RecognitionException which should be the parent class of all parser related exception classes. We need this layer because of the line number problem in tree parser in Antlr-3.2. You will need a token where the exception occurs in order to instantiate an instance of this class.
A wrapper around the actual RecordReader and loadfunc - this is needed for two reasons 1) To intercept the initialize call from hadoop and initialize the underlying actual RecordReader with the right Context object - this is achieved by looking up the Context corresponding to the input split this Reader is supposed to process 2) We need to give hadoop consistent key-value types - text and tuple respectively - so PigRecordReader will call underlying Loader's getNext() to get the Tuple value - the key is null text since key is not used in input to map() in Pig.
Interface to implement when you want to use a custom approach to estimating the number of reducers for a job.
A utility to help run PIG scripts within a Java program.
This class contains functions to convert Pig schema to Avro. It consists of two sets of methods: 1. Convert a Pig schema to Avro schema; 2. Validate whether a Pig schema is compatible with a given Avro schema. Notice that the Avro schema doesn't need to cover all fields in Pig schema, and the missing fields are converted using methods in set 1.

Token literal values and constants. Generated by org.javacc.parser.OtherFilesGen#start()
Token Manager.

Utility class that handles secondary key for sorting.

A class for Java programs to connect to Pig. Typically a program will create a PigServer instance. The programmer then registers queries using registerQuery() and retrieves results using openIterator() or store(). After doing so, the shutdown() method should be called to free any resources used by the current PigServer instance. Not doing so could result in a memory leak.
The main split class that maintains important information about the input split. The reason this class implements Configurable is so that Hadoop will call {@link Configurable#setConf(Configuration)} on the backend so we can use the Configuration to create the SerializationFactory to deserialize the wrapped InputSplit.
PigStats encapsulates the statistics collected from a running script. It includes status of the execution, the DAG of its Hadoop jobs, as well as information about outputs and inputs of the script.
Interface to implement when you want to customize the way of computing the size of output in PigStats. Since not every storage is file-based (e.g. HBaseStorage), the output size cannot always be computed as the total size of output files.
A utility class for Pig Statistics

A load function that parses a line of input into fields using a character delimiter. The default delimiter is a tab. You can specify any character as a literal ("a"), a known escape character ("\\t"), or a dec or hex value ("\\u001", "\\x0A"). <p> An optional second constructor argument is provided that allows one to customize advanced behaviors. A list of available options is below: <ul> <li><code>-schema</code> Reads/Stores the schema of the relation using a hidden JSON file. <li><code>-noschema</code> Ignores a stored schema during loading. <li><code>-tagFile</code> Appends input source file name to beginning of each tuple. <li><code>-tagPath</code> Appends input source file path to beginning of each tuple. </ul> <p> <h3>Schemas</h3> If <code>-schema</code> is specified, a hidden ".pig_schema" file is created in the output directory when storing data. It is used by PigStorage (with or without -schema) during loading to determine the field names and types of the data without the need for a user to explicitly provide the schema in an <code>as</code> clause, unless <code>-noschema</code> is specified. No attempt to merge conflicting schemas is made during loading. The first schema encountered during a file system scan is used. If the schema file is not present while '-schema' option is used during loading, it results in an error. <p> In addition, using <code>-schema</code> drops a ".pig_headers" file in the output directory. This file simply lists the delimited aliases. This is intended to make export to tools that can read files with header lines easier (just cat the header to your data). <p> <h3>Source tagging</h3> If<code>-tagFile</code> is specified, PigStorage will prepend input split name to each Tuple/row. Usage: A = LOAD 'input' using PigStorage(',','-tagFile'); B = foreach A generate $0; The first field (0th index) in each Tuple will contain input file name. If<code>-tagPath</code> is specified, PigStorage will prepend input split path to each Tuple/row. Usage: A = LOAD 'input' using PigStorage(',','-tagPath'); B = foreach A generate $0; The first field (0th index) in each Tuple will contain input file path <p> Note that regardless of whether or not you store the schema, you <b>always</b> need to specify the correct delimiter to read your data. If you store reading delimiter "#" and then load using the default delimiter, your data will not be parsed correctly. <h3>Compression</h3> Storing to a directory whose name ends in ".bz2" or ".gz" or ".lzo" (if you have installed support for LZO compression in Hadoop) will automatically use the corresponding compression codec.<br> <code>output.compression.enabled</code> and <code>output.compression.codec</code> job properties also work. <p> Loading from directories ending in .bz2 or .bz works automatically; other compression formats are not auto-detected on loading.
Same as PigStorage with no default constructor - used in testing POCast with a loader function which has no default constructor
This Load/Store Func reads/writes metafiles that allow the schema and aliases to be determined at load time, saving one from having to manually enter schemas for pig-generated datasets. It also creates a ".pig_headers" file that simply lists the delimited aliases. This is intended to make export to tools that can read files with header lines easier (just cat the header to your data).



The default implementation of {@link PigStreamingBase}. It converts tuples into <code>fieldDel</code> separated lines and <code>fieldDel</code> separated lines into tuples.
The interface is used for the custom mapping of - a {@link Tuple} to a byte array. The byte array is fed to the stdin of the streaming process. - a byte array, received from the stdout of the streaming process, to a {@link Tuple}. This interface is designed to provide a common protocol for data exchange between Pig runtime and streaming executables. Typically, a user implements this interface for a particular type of stream command and specifies the implementation class in the Pig DEFINE statement.




Should be implemented by an object that wants to receive notifications from {@link PigRunner}.
The interface used for the custom mapping of a {@link Tuple} to a byte array. The byte array is fed to the stdin of the streaming process. This interface, together with {@link StreamToPig}, is designed to provide a common protocol for data exchange between Pig runtime and streaming executables. Typically, a user implements this interface for a particular type of stream command and specifies the implementation class in the Pig DEFINE statement.


Interface for incrementing warning counters
An enum to enumerate the warning types in Pig same as above but for custom UDF warnings only, see PIG-2207

This class dumps a nested plan to a print stream. It does not walk the graph in any particular fashion it merely iterates over all operators and edges and calls a corresponding dump function. If a node of the plan has nested plans this will be dumped when the node is handled.



Utility class with a few helper functions to deal with physical plans.
The core class of the optimizer.  The basic design of this class is that it is provided a list of RuleSets.  RuleSets represent all of the optimizer rules that can be run together.  The rules in the RuleSet will be run repeatedly until either no rule in the RuleSet passes check and calls transform or until maxIter iterations (default 500) has been made over the RuleSet.  Then the next RuleSet will be moved to.  Once finished, a given RuleSet is never returned to. Each rule is has two parts:  a pattern and and associated transformer. Transformers have two important functions:   check(), and transform(). The pattern describes a pattern of node types that the optimizer will look to match.  If that match is found anywhere in the plan, then check() will be called.  check() allows the rule to look more in depth at the matched pattern and decide whether the rule should be run or not.  For example, one might design a rule to push filters above join that would look for the pattern filter(join) (meaning a filter followed by a join). But only certain types of filters can be pushed.  The check() function would need to decide whether the filter that it found was pushable or not. If check() returns true, the rule is said to have matched, and transform() is then called.  This function is responsible for making changes in the logical plan.  Once transform is complete PlanPatcher.patchUp will be called to do any necessary cleanup in the plan, such as resetting schemas, etc.

* This abstract class is a base for plan equality comparer
An interface to describe listeners that are notified when a plan is modified.  The purpose of these listeners is to make modifications to annotations on the plan after the plan is modified.  For example, if there is a plan that has ... -&gt; Join -&gt; Filter -&gt; ... which is transformed by pushing the filter before the join, then the input schema of the filter will mostly likely change.  A schema listener can be used to make these changes.

A visitor mechanism for navigating and operating on a plan of Operators.  This class contains the logic to traverse the plan.  It does not visit individual nodes.  That is left to implementing classes (such as LOVisitor).

This is a UDF which allows the user to specify a string prefix, and then filter for the columns in a relation that begin with that prefix. Example: 1) Prefix a = load 'a' as (x, y); b = load 'b' as (x, y); c = join a by x, b by x; DEFINE pluck PluckTuple('a::'); d = foreach c generate FLATTEN(pluck(*)); describe c; c: {a::x: bytearray,a::y: bytearray,b::x: bytearray,b::y: bytearray} describe d; d: {plucked::a::x: bytearray,plucked::a::y: bytearray} 2) Regex a = load 'a' as (x, y); b = load 'b' as (x, y); c = join a by x, b by x; DEFINE pluck PluckTuple('.*::y'); d = foreach c generate FLATTEN(pluck(*)); describe c; c: {a::x: bytearray,a::y: bytearray,b::x: bytearray,b::y: bytearray} describe d; d: {plucked::a::y: bytearray,plucked::a::y: bytearray}

See "Skewed Join sampler" in http://wiki.apache.org/pig/PigSampler

We traverse the expression plan bottom up and separate it into two plans - pushdownExprPlan, plan that can be pushed down to the loader and - filterExprPlan, remaining plan that needs to be evaluated by pig If the predicate is not removable then filterExprPlan will not have the pushdownExprPlan removed.


Base class for simple Pig UDFs that are functions of primitive types IN to OUT. Handles marshalling objects, basic error checking, etc. Extend this class and implement the <pre>OUT exec(IN input)</pre> method when writting a UDF that operates on only the first input (of expected type IN) from the Tuple.

Expand project-star or project-range when used as udf argument. This is different from {@link ProjectStarExpander} because in those cases, the project star gets expanded as new {@link LogicalExpressionPlan}. In case of project-star or project-range within udf, it should get expanded only as multiple inputs to this udf, no addtional {@link LogicalExpressionPlan}s are created. The expansion happens only if input schema is not null
Projection of columns in an expression.
A visitor to walk operators that contain a nested plan and translate project( * ) operators to a list of projection operators, i.e., project( * ) -> project(0), project(1), ... project(n-2), project(n-1) If input schema is null, project(*) is not expanded. It also expands project range ( eg $1 .. $5). It won't expand project-range-to-end (eg $3 ..) if the input schema is null.
Util function(s) for project-(star/range) expansion
A struct detailing how a projection is altered by an operator.
A PlanTransformListener that will patch up references in projections.

It's generally a good idea to do flattens as late as possible as they tend to generate more rows (and so more I/O). This optimization swaps the order of SORTs, CROSSes and JOINs that come after FOREACH..GENERATE..FLATTENs. FILTERs are re-ordered by the {@link FilterAboveForeach} rule so are ignored here.


Lexer file for Pig Parser
Parser file for Pig Parser NOTE: THIS FILE IS THE BASE FOR A FEW TREE PARSER FILES, such as AstValidator.g, SO IF YOU CHANGE THIS FILE, YOU WILL PROBABLY NEED TO MAKE CORRESPONDING CHANGES TO THOSE FILES AS WELL.

Pig's implementation class of file stream, used to make ANTLR case insensitive while preserving case.

Pig's Implementation class for String stream, used to make ANTLR case insensitive while preserving case.

Return a random double value.  Whatever arguments are passed to this UDF are ignored.
Given an RDD and a PhysicalOperater, and implementation of this class can convert the RDD to another RDD.
<dl> <dt><b>Syntax:</b></dt> <dd><code>String RegexExtract(String expression, String regex, int match_index)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>expression</code>-<code>source string</code>.</dd> <dd><code>regex</code>-<code>regular expression</code>.</dd> <dd><code>match_index</code>-<code>index of the group to extract</code>.</dd> <dt><b>Output:</b></dt> <dd><code>extracted group, if fail, return null</code>.</dd> <dt><b>Matching strategy:</b></dt> <dd>Try to only match the first sequence by using {@link Matcher#find()} instead of {@link Matcher#matches()} (default useMatches=false).</dd> <dd><code>DEFINE NON_GREEDY_EXTRACT REGEX_EXTRACT('true');</code></dd> </dl>
<dl> <dt><b>Syntax:</b></dt> <dd><code>String RegexExtractAll(String expression, String regex)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>expression</code>-<code>source string</code>.</dd> <dd><code>regex</code>-<code>regular expression</code>.</dd> <dt><b>Output:</b></dt> <dd><code>A tuple of matched strings</code>.</dd> <dt><b>Matching strategy:</b></dt> <dd>Trying to match the entire input by using {@link Matcher#matches()} instead of {@link Matcher#find()} (default useMatches=true).</dd> <dd><code>DEFINE GREEDY_EXTRACT REGEX_EXTRACT_ALL('false');</code></dd> </dl>
Search and find all matched characters in a string with a given regular expression. Example: a = LOAD 'mydata' AS (name:chararray); b = FOREACH A GENERATE REGEX_SEARCH(name, 'regEx'); input tuple: the first field is a string on which performs regular expression matching; the second field is the regular expression;
REPLACE implements eval function to replace part of a string. Example:<code> A = load 'mydata' as (name); B = foreach A generate REPLACE(name, 'blabla', 'bla'); The first argument is a string on which to perform the operation. The second argument is treated as a regular expression. The third argument is the replacement string. This is a wrapper around Java's String.replaceAll(String, String);
REPLACE_MULTI implements eval function to replace all occurrences of search keys with replacement values. Search - Replacement values are specified in Map Example:<code> input_data = LOAD 'input_data' as (name); -- name = 'Hello World!' replaced_name = FOREACH input_data GENERATE REPLACE_MULTI ( name, [ ' '#'_', '!'#'', 'e'#'a', 'o'#'oo' ] ); -- replaced_name = Halloo_Woorld </code> The first argument is the source string on which REPLACE_MULTI operation is performed. The second argument is a map having search key - replacement value pairs.
math.RINT implements a binding to the Java function {@link java.lang.Math#rint(double) Math.rint(double)}. Given a single data atom it Returns the double value that is closest in value to the argument and is equal to a mathematical integer. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>RINT_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.RINT(float1); </code></dd> </dl>
ROUND implements a binding to the Java function {@link java.lang.Math#round(double) Math.round(double)}. Given a single data atom it Returns the closest long to the argument.
ROUND_TO safely rounds a number to a given precision by using an intermediate BigDecimal. The too-often seen trick of doing (1000.0 * ROUND(x/1000)) is not only hard to read but also fails to produce numerically accurate results. Given a single data atom and number of digits, it returns a double extending to the given number of decimal places. The result is a multiple of ten to the power given by the digits argument: a negative value zeros out correspondingly many places to the left of the decimal point: ROUND_TO(0.9876543, 3) is 0.988; ROUND_TO(0.9876543, 0) is 1.0; and ROUND_TO(1234.56, -2) is 1200.0. The optional mode argument specifies the {@link java.math.RoundingMode rounding mode}; by default, {@link java.math.RoundingMode#HALF_EVEN HALF_EVEN} is used.
Returns a string, with only tailing whitespace omitted. Implements a binding to the Java function {@link java.lang.String#trim() String.trim()}.
A loader that samples the data. It randomly samples tuples from input. The number of tuples to be sampled has to be set before the first call to getNext(). see documentation of getNext() call.

This bag does not store the tuples in memory, but has access to an iterator typically provided by Hadoop. Use this when you already have an iterator over tuples and do not want to copy over again to a new bag.
ReadScalars reads a line from a file and returns it as its value. The file is only read once, and the same line is returned over and over again. This is useful for incorporating a result from an agregation into another evaluation.

This is wrapper Loader which wraps a real LoadFunc underneath and allows to read a file completely starting a given split (indicated by a split index which is used to look in the List<InputSplit> returned by the underlying InputFormat's getSplits() method). So if the supplied split index is 0, this loader will read the entire file. If it is non zero it will read the partial file beginning from that split to the last split. The call sequence to use this is: 1) construct an object using the constructor 2) Call getNext() in a loop till it returns null
to test a generated Avro Java class

RegExLoader is an abstract class used to parse logs based on a regular expression. There is a single abstract method, getPattern which needs to return a Pattern. Each group will be returned as a different DataAtom. Look to org.apache.pig.piggybank.storage.apachelog.CommonLogLoader for example usage.
Regex Operator
<dl> <dt><b>Syntax:</b></dt> <dd><code>String RegexExtract(String expression, String regex, int match_index)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>expression</code>-<code>source string</code>.</dd> <dd><code>regex</code>-<code>regular expression</code>.</dd> <dd><code>match_index</code>-<code>index of the group to extract</code>.</dd> <dt><b>Output:</b></dt> <dd><code>extracted group, if fail, return null</code>.</dd> </dl>
<dl> <dt><b>Syntax:</b></dt> <dd><code>String RegexExtractAll(String expression, String regex)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>expression</code>-<code>source string</code>.</dd> <dd><code>regex</code>-<code>regular expression</code>.</dd> <dt><b>Output:</b></dt> <dd><code>A tuple of matched strings</code>.</dd> </dl>

General interface for regexComparison

<dl> <dt><b>Syntax:</b></dt> <dd><code>int RegexMatch(String expression, String regex)</code>.</dd> <dt><b>Output:</b></dt> <dd><code>return 1 if expression contains regex, 0 otherwise</code>.</dd> </dl>

Please see {@link TestRegisteredJarVisibility} for information about this class.
Please see {@link TestRegisteredJarVisibility} for information about this class.

A struct detailing how a projection is altered by an operator.


A represenation of a schema used to communicate with load and store functions.  This is separate from {@link Schema}, which is an internal Pig representation of a schema.
An class that represents statistics about data to be loaded or stored.  It is marked unstable because Pig does very little statistics collection at this point.  If and when that functionality is added it is expected that this interface will change.

reverses a string.
Visit a plan in the reverse of the dependency order.  That is, every node after every node that depends on it is visited.  Thus this is equivalent to doing a reverse topilogical sort on the graph and then visiting it in order.
Visit a plan in the reverse of the dependency order.  That is, every node after every node that depends on it is visited.  Thus this is equivalent to doing a reverse topilogical sort on the graph and then visiting it in order.

Produces a DataBag with hierarchy of values (from the most detailed level of aggregation to most general level of aggregation) of the specified dimensions For example, (a, b, c) will produce the following bag: <pre> { (a, b, c), (a, b, null), (a, null, null), (null, null, null) } </pre>
This partitioner should be used with extreme caution and only in cases where the order of output records is guaranteed to be same. If the order of output records can vary on retries which is mostly the case, map reruns due to shuffle fetch failures can lead to data being partitioned differently and result in incorrect output due to loss or duplication of data. Refer PIG-5041 for more details. This will be removed in the next release as it is risky to use in most cases.
TODO: need to fix the enumerator piece! TODO: need to fix the flatten semantics This provides a Ruby-esque way to interact with DataBag objects. It encapsulates a bag object, and provides an easy to use interface. One difference between the Ruby and the the Java API on DataBag is that in Ruby you iterate on the bag directly. <p> The RubyDataBag class  uses JRuby's API for the defintion Ruby class using Java code. The comments in this class will more extensively explain the annotations for those not familiar with JRuby. <p> In JRuby, the annotations are provided for convenience, and are detected and used by the "defineAnnotatedMethods" method. The JRubyClass annotation sets the class name as it will be seen in the Ruby runtime, and alows you to include any modules. In the case of the RubyDataBag, within Ruby we just want it to be called DataBag, and we want it to be enumerable.
This class presents a native Ruby object for interacting with and manipulating DataByteArray objects. For more information on what the annotations mean, see {@link RubyDataBag}.
TODO implement all of the merge functions This class encapsulated a native Schema object, and provides a more convenient interface for manipulating Schemas. It hides the Schema/FieldSchema distinction from the user, and tries to present a cleaner, more Ruby-esque API to the user. For general information on JRuby's API definition annotations, see {@link RubyDataBag}.
Rules describe a pattern of operators.  They also reference a Transformer. If the pattern of operators is found one or more times in the provided plan, then the optimizer will use the associated Transformer to transform the plan. Note: the pattern matching logic implemented here has a limitation that it assumes that all the leaves in the pattern are siblings. See more detailed description here - https://issues.apache.org/jira/browse/PIG-1742 If new rules use patterns that don't work with this limitation, the pattern match logic will need to be updated.

math.SCALB implements a binding to the Java function {@link java.lang.Math#scalb(double,int) Math.scalb(double,int)}. Given a tuple with two data atom x and y ,it Return x pow(2,y) rounded as if performed by a single correctly rounded floating-point multiply to a member of the double value set. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two numeric values [Double, Integer]</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>SCALB_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.SCALB(float1); </code></dd> </dl>
math.SIGNUM implements a binding to the Java function {@link java.lang.Math#signum(double) Math.signum(double)}. Given a single data atom it Returns the signum function of the argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>SIGNUM_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.SIGNUM(float1); </code></dd> </dl>
SIN implements a binding to the Java function {@link java.lang.Math#sin(double) Math.sin(double)}. Given a single data atom it Returns the sine of the argument.
SINH implements a binding to the Java function {@link java.lang.Math#sinh(double) Math.sinh(double)}. Given a single data atom it Returns the hyperbolic sine of the argument.
Generates the size of the argument passed to it.  For bytearrays this means the number of bytes.  For charrays the number of characters.  For bags the number of tuples, for tuples the number of fields, and for maps the number of keyvalue pairs.  For all other types the value of 1 is always returned.
Formatted strings using java.util.Formatter See http://docs.oracle.com/javase/7/docs/api/java/util/Formatter.html ex: SPRINTF('%2$10s %1$-17s %2$,10d %2$8x %3$10.3f %4$1TFT%<tT%<tz', city, pop_2011, (float)(pop_2011/69.0f), (long)(pop_2011 * 1000000L)); -'   8244910 New York           8,244,910   7dceae 119491.453 2231-04-09T23:46:40-0500'
SQRT implements a binding to the Java function {@link java.lang.Math#sqrt(double) Math.sqrt(double)}. Given a single data atom it Returns the square root of the argument.
Pig UDF to test input <code>tuple.get(0)</code> against <code>tuple.get(1)</code> to determine if the first argument starts with the string in the second.
Wrapper around Java's String.split<br> input tuple: first column is assumed to have a string to split;<br> the optional second column is assumed to have the delimiter or regex to split on;<br> if not provided, it's assumed to be '\s' (space)<br> the optional third column may provide a limit to the number of results.<br> If limit is not provided, 0 is assumed, as per Java's split().
Wrapper around Java's String.split<br> input tuple: first column is assumed to have a string to split;<br> the optional second column is assumed to have the delimiter or regex to split on;<br> if not provided, it's assumed to be '\s' (space)<br> the optional third column may provide a limit to the number of results.<br> If limit is not provided, 0 is assumed, as per Java's split().
SUBSTRING implements eval function to get a part of a string. Example:<code> A = load 'mydata' as (name); B = foreach A generate SUBSTRING(name, 10, 12); </code> First argument is the string to take a substring of.<br> Second argument is the index of the first character of substring.<br> Third argument is the index of the last character of substring.<br> if the last argument is past the end of the string, substring of (beginIndex, length(str)) is returned.
Generates the sum of a set of values. This class implements {@link org.apache.pig.Algebraic}, so if possible the execution will performed in a distributed fashion. <p> SUM can operate on any numeric type.  It can also operate on bytearrays, which it will cast to doubles.  It expects a bag of tuples of one record each.  If Pig knows from the schema that this function will be passed a bag of integers or longs, it will use a specially adapted version of SUM that uses integer arithmetic for summing the data.  The return type of SUM is double for float, double, or bytearray arguments and long for int or long arguments. <p> SUM implements the {@link org.apache.pig.Accumulator} interface as well. While this will never be the preferred method of usage it is available in case the combiner can not be used for a given calculation.
Abstract class that specifies the interface for sample loaders
A visitor to optimize plans that have a sample job that immediately follows a load/store only MR job.  These kinds of plans are generated for order bys, and will soon be generated for joins that need to sample their data first.  These can be changed so that the RandomSampleLoader subsumes the loader used in the first job and then removes the first job.


This validator checks the correctness of use of scalar variables in logical operators. It assesses the validity of the expression by making sure there is no projection in it. Currently it works for Limit and Sample (see PIG-1926)
Logical plan visitor which handles scalar projections. It will find or create a LOStore and a soft link between the store operator to a scalar expression. It will also sync the file name between the store and scalar expression.
The Schema class encapsulates the notion of a schema for a relational operator. A schema is a list of columns that describe the output of a relational operator. Each column in the relation is represented as a FieldSchema, a static class inside the Schema. A column by definition has an alias, a type and a possible schema (if the column is a bag or a tuple). In addition, each column in the schema has a unique auto generated name used for tracking the lineage of the column in a sequence of statements. The lineage of the column is tracked using a map of the predecessors' columns to the operators that generate the predecessor columns. The predecessor columns are the columns required in order to generate the column under consideration.  Similarly, a reverse lookup of operators that generate the predecessor column to the predecessor column is maintained.



A PlanTransformListener for the logical optimizer that will patch up schemas after a plan has been transformed.

A SchemaTuple is a type aware tuple that is much faster and more memory efficient. In our implementation, given a Schema, code generation is used to extend this class. This class provides a broad range of functionality that minimizes the complexity of the code that must be generated. The odd looking generic signature allows for certain optimizations, such as "setSpecific(T t)", which allows us to do much faster sets and comparisons when types match (since the code is generated, there is no other way to know).

This class encapsulates the generation of SchemaTuples, as well as some logic around shipping code to the distributed cache.
This is an implementation of TupleFactory that will instantiate SchemaTuple's. This class has nothing to do with the actual generation of code, and instead simply encapsulates the classes which allow for efficiently creating SchemaTuples.
This class is to be used at job creation time. It provides the API that lets code register Schemas with pig to be generated. It is necessary to register these Schemas and reducers.
A utility class for simplify the schema creation, especially for bag and tuple schema. Currently, it only support simple schema creation, nested tuple and bag is not supported
For each n-gram, we have a set of (hour, count) pairs. This function reads the set and retains those hours with above above mean count, and calculates the score of each retained hour as the multiplier of the count of the hour over the standard deviation. A score greater than 1.0 indicates the frequency of this n-gram in this particular hour is at least one standard deviation away from the average frequency among all hours
Base class for various scripting implementations
Context for embedded Pig script.
ScriptStates encapsulates settings for a Pig script that runs on a hadoop cluster. These settings are added to all MR jobs spawned by the script and in turn are persisted in the hadoop job xml. With the properties already in the job xml, users who want to know the relations between the script and MR jobs can derive them from the job xmls.
This class helps a scripting UDF capture user output by managing when to capture output and where the output is written to. For illustrate, we will only capture output for the last run (with the final set of data) and we need to keep track of the file containing that output for returning w/ the illustrate results. For runs, all standard output is written to the user logs.
SearchEngineExtractor takes a url string and extracts the search engine. For example, given http://www.google.com/search?hl=en&safe=active&rls=GGLG,GGLG:2005-24,GGLG:en&q=purpose+of+life&btnG=Search then Google would be extracted. From pig latin, usage looks something like searchEngine = FOREACH row GENERATE org.apache.pig.piggybank.evaluation.util.apachelogparser.SearchEngineExtractor(referer); Supported search engines include abacho.com, alice.it, alltheweb.com, altavista.com, aolsearch.aol.com, as.starware.com, ask.com, blogs.icerocket.com, blogsearch.google.com, blueyonder.co.uk, busca.orange.es, buscador.lycos.es, buscador.terra.es, buscar.ozu.es, categorico.it, cuil.com, excite.com, excite.it, fastweb.it, feedster.com, godado.com, godado.it, google.ad, google.ae, google.af, google.ag, google.am, google.as, google.at, google.az, google.ba, google.be, google.bg, google.bi, google.biz, google.bo, google.bs, google.bz, google.ca, google.cc, google.cd, google.cg, google.ch, google.ci, google.cl, google.cn, google.co.at , google.co.bi, google.co.bw, google.co.ci, google.co.ck, google.co.cr, google.co.gg, google.co.gl, google.co.gy, google.co.hu, google.co.id, google.co.il, google.co.im, google.co.in, google.co.it, google.co.je, google.co.jp, google.co.ke, google.co.kr, google.co.ls, google.co.ma, google.co.mu, google.co.mw, google.co.nz, google.co.pn, google.co.th, google.co.tt, google.co.ug, google.co.uk, google.co.uz, google.co.ve, google.co.vi, google.co.za, google.co.zm, google.co.zw, google.com, google.com.af, google.com.ag, google.com.ai, google.com.ar, google.com.au, google.com.az, google.com.bd, google.com.bh, google.com.bi, google.com.bn, google.com.bo, google.com.br, google.com.bs, google.com.bz, google.com.cn, google.com.co, google.com.cu, google.com.do, google.com.ec, google.com.eg, google.com.et, google.com.fj, google.com.ge, google.com.gh, google.com.gi, google.com.gl, google.com.gp, google.com.gr, google.com.gt, google.com.gy, google.com.hk, google.com.hn, google.com.hr, google.com.jm, google.com.jo, google.com.kg, google.com.kh, google.com.ki, google.com.kz, google.com.lk, google.com.lv, google.com.ly, google.com.mt, google.com.mu, google.com.mw, google.com.mx, google.com.my, google.com.na, google.com.nf, google.com.ng, google.com.ni, google.com.np, google.com.nr, google.com.om, google.com.pa, google.com.pe, google.com.ph, google.com.pk, google.com.pl, google.com.pr, google.com.pt, google.com.py, google.com.qa, google.com.ru, google.com.sa, google.com.sb, google.com.sc, google.com.sg, google.com.sv, google.com.tj, google.com.tr, google.com.tt, google.com.tw, google.com.uy, google.com.uz, google.com.ve, google.com.vi, google.com.vn, google.com.ws, google.cz, google.de, google.dj, google.dk , google.dm , google.ec, google.ee, google.es, google.fi, google.fm, google.fr, google.gd, google.ge, google.gf, google.gg, google.gl, google.gm, google.gp, google.gr, google.gy, google.hk, google.hn, google.hr, google.ht, google.hu, google.ie, google.im, google.in, google.info, google.is, google.it, google.je, google.jo, google.jobs, google.jp, google.kg, google.ki, google.kz, google.la, google.li, google.lk, google.lt, google.lu, google.lv, google.ma, google.md, google.mn, google.mobi, google.ms, google.mu, google.mv, google.mw, google.net, google.nf, google.nl, google.no, google.nr, google.nu, google.off.ai, google.ph, google.pk, google.pl, google.pn, google.pr, google.pt, google.ro, google.ru, google.rw, google.sc, google.se, google.sg, google.sh, google.si, google.sk, google.sm, google.sn, google.sr, google.st, google.tk, google.tm, google.to, google.tp, google.tt, google.tv, google.tw, google.ug, google.us, google.uz, google.vg, google.vn, google.vu, google.ws, gps.virgin.net, hotbot.com, ilmotore.com, ithaki.net, kataweb.it, libero.it, lycos.it, mamma.com, megasearching.net, mirago.co.uk, netscape.com, search.aol.co.uk, search.arabia.msn.com, search.bbc.co.uk, search.conduit.com, search.icq.com, search.live.com, search.lycos.co.uk, search.lycos.com, search.msn.co.uk, search.msn.com, search.myway.com, search.mywebsearch.com, search.ntlworld.com, search.orange.co.uk, search.prodigy.msn.com, search.sweetim.com, search.virginmedia.com, search.yahoo.co.jp, search.yahoo.com, search.yahoo.jp, simpatico.ws, soso.com, suche.fireball.de, suche.t-online.de, suche.web.de, technorati.com, tesco.net, thespider.it, tiscali.co.uk, uk.altavista.com, uk.ask.com, uk.search.yahoo.com Thanks to Spiros Denaxas for his URI::ParseSearchString, which is the basis for the lookups.
This small UDF takes a search engine URL (Google/Yahoo/AOL/Live) containing the search query and extracts it. The URL is assumed to be encoded. The query is normalized, converting it to lower-case, removing punctuations, removing extra spaces.
SearchTermExtractor takes a url string and extracts the search terms. For example, given http://www.google.com/search?hl=en&safe=active&rls=GGLG,GGLG:2005-24,GGLG:en&q=purpose+of+life&btnG=Search then purpose of life would be extracted. From pig latin, usage looks something like searchTerm = FOREACH row GENERATE org.apache.pig.piggybank.evaluation.util.apachelogparser.SearchTermExtractor(referer); Supported search engines include alltheweb.com, altavista.com, aolsearch.aol.com, arianna.libero.it, as.starware.com, ask.com, blogs.icerocket.com, blueyonder.co.uk, busca.orange.es, buscador.lycos.es, buscador.terra.es, buscar.ozu.es, categorico.it, cerca.lycos.it, cuil.com, excite.it, godado.com, godado.it, gps.virgin.net, hotbot.com, ilmotore.com, it.altavista.com, ithaki.net, libero.it, lycos.es, lycos.it, mamma.com, megasearching.net, mirago.co.uk, netscape.com, ozu.es, ricerca.alice.it, search.aol.co.uk, search.bbc.co.uk, search.conduit.com, search.icq.com, search.live.com, search.lycos.co.uk, search.lycos.com, search.msn.co.uk, search.msn.com, search.myway.com, search.mywebsearch.com, search.ntlworld.com, search.orange.co.uk, search.sweetim.com, search.virginmedia.com, simpatico.ws, soso.com, suche.fireball.de, suche.web.de, terra.es, tesco.net, thespider.it, tiscali.co.uk, uk.altavista.com, uk.ask.com Thanks to Spiros Denaxas for his URI::ParseSearchString, which is the basis for the lookups.


Secondary key sort optimization for spark mode



Provide utility functions which is used by ReducedByConverter and JoinGroupSparkConverter.
<p>SecondsBetween returns the number of seconds between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>
This class duplicates some security related private methods from org.apache.hadoop.mapreduce.JobSubmitter for Tez.

Unix-like API for an input stream that supports random access.
Class to hold code common to self spilling bags such as InternalCachedBag
Store tuples (BinSedesTuples, specifically) using sequence files to leverage sequence file's compression features. Replacement for previous use of {@code TFileStorage}, which had some edge cases that were not properly handled there.
A Loader for Hadoop-Standard SequenceFiles. able to work with the following types as keys or values: Text, IntWritable, LongWritable, FloatWritable, DoubleWritable, BooleanWritable, ByteWritable
An implementation of interface CharStream, where the stream is assumed to contain only ASCII characters (without unicode processing).





SimplePigStats encapsulates the statistics collected from a running script. It includes status of the execution, the DAG of its MR jobs, as well as information about outputs and inputs of the script.
A simple performant implementation of the DataBag interface which only holds a single tuple. This will be used from POPreCombinerLocalRearrange and wherever else a single Tuple non-serializable DataBag is required.
Utility functions for estimating size of objects of pig types

This class is used by skewed join. For the partitioned table, the skewedpartitioner reads the key distribution data from the sampler file and returns the reducer index in a round robin fashion. For ex: if the key distribution file contains (k1, 5, 3) as an entry, reducers from 5 to 3 are returned in a round robin manner.

A class representing information about a sort column in {@link SortInfo}

Class to communicate sort column information based on order by statment's sort columns and schema

Represent a set of sort keys. All the sort keys are equivalent. Eg: C = cogroup A by ($0, $1), B by ($0, $1) This query have two sort keys, (A.$0, A.$1) and (B.$0, B.$1)
An ordered collection of Tuples (possibly) with multiples.  Data is stored unsorted as it comes in, and only sorted when it is time to dump it to a file or when the first iterator is requested.  Experementation found this to be the faster than storing it sorted to begin with. We allow a user defined comparator, but provide a default comparator in cases where the user doesn't specify one.
Common functionality for proactively spilling bags that need to keep the data sorted.

The compiler that compiles a given physical physicalPlan into a DAG of Spark operators
Create a new SparkCompilerException with null as the error message.



The object of SparkEngineConf is to solve the initialization problem of PigContext.properties.get("udf.import.list"), UDFContext#udfConfs, UDFContext#clientSysProps in spark mode. These variables can not be serialized because they are ThreadLocal variables. In MR mode, they are serialized in JobConfiguration in JobControlCompiler#getJob and deserialized by JobConfiguration in PigGenericMapBase#setup. But there is no setup() in spark like what in mr, so these variables can be correctly deserialized before spark programs call them. Here we use following solution to solve: these variables are saved in SparkEngineConf#writeObject and available and then initialized in SparkEngineConf#readObject in spark executor thread.




Main class that launches pig for Spark
SparkLocalExecType is the ExecType for local mode in Spark.

A visitor for the SparkOperPlan class
A Plan used to create the physicalPlan of Spark Operators
An operator model for a Spark job. Acts as a host to the plans that will execute in spark.
This visitor visits the SparkPlan and does the following for each SparkOperator - visits the POPackage in the plan and finds the corresponding POLocalRearrange(s). It then annotates the POPackage with information about which columns in the "value" are present in the "key" and will need to stitched in to the "value"

singleton class like PigContext
Record reader for Spark mode - handles SparkPigSplit
Wrapper class for PigSplits in Spark mode Spark only counts HDFS bytes read if we provide a FileSplit, so we have to wrap PigSplits and have the wrapper class extend FileSplit

Just like PigStatusReporter which will create/reset Hadoop counters, SparkPigStatusReporter will create/reset Spark counters. Note that, it is not suitable to make SparkCounters as a Singleton, it will be created/reset for a given pig script or a Dump/Store action in Grunt mode.
A visitor mechanism printing out the logical plan.
sort the sample data and convert the sample data to the format (all,{(sampleEle1),(sampleEle2),...})
ScriptStates encapsulates settings for a Pig script that runs on a hadoop cluster. These settings are added to all Spark jobs spawned by the script and in turn are persisted in the hadoop job xml. With the properties already in the job xml, users who want to know the relations between the script and Spark jobs can derive them from the job xmls.




This class Tracks the tenured pool and a list of Spillable objects. When memory gets low, this class will start requesting Spillable objects to free up memory. <p> Low memory is defined as more than 50% of the tenured pool being allocated. Spillable objects are tracked using WeakReferences so that the objects can be GCed even though this class has a reference to them.
Wrapper around Java's String.split<br> input tuple: first column is assumed to have a string to split;<br> the optional second column is assumed to have the delimiter or regex to split on;<br> if not provided, it's assumed to be '\s' (space)<br> the optional third column may provide a limit to the number of results.<br> If limit is not provided, 0 is assumed, as per Java's split().



Given a set of bags, stitch them together tuple by tuple.  That is, assuming the bags have row numbers join them by row number.  So given two bags <p> {(1, 2), (3, 4)} and <p> {(5, 6), (7, 8)} the result will be <p> {(1, 2, 5, 6), (3, 4, 7, 8)} In general it is assumed that each bag has the same number of tuples. The implementation uses the first bag to determine the number of tuples placed in the output.  If bags beyond the first have fewer tuples then the resulting tuples will have fewer fields.  Nulls will not be filled in. <p>Any number of bags can be passed to this function.
A convenient mock Storage for unit tests <pre> PigServer pigServer = new PigServer(ExecType.LOCAL); Data data = resetData(pigServer); data.set("foo", tuple("a"), tuple("b"), tuple("c"), tuple(map("d","e", "f","g")), tuple(bag(tuple("h"),tuple("i"))) ); pigServer.registerQuery("A = LOAD 'foo' USING mock.Storage();"); pigServer.registerQuery("STORE A INTO 'bar' USING mock.Storage();"); List<Tuple> out = data.get("bar"); assertEquals(tuple("a"), out.get(0)); assertEquals(tuple("b"), out.get(1)); assertEquals(tuple("c"), out.get(2)); assertEquals(tuple(map("f", "g", "d", "e" )), out.get(3)); assertEquals(tuple(bag(tuple("h"),tuple("i"))), out.get(4)); </pre> With Schema: <pre> PigServer pigServer = new PigServer(ExecType.LOCAL); Data data = resetData(pigServer); data.set("foo", "blah:chararray", tuple("a"), tuple("b"), tuple("c") ); pigServer.registerQuery("A = LOAD 'foo' USING mock.Storage();"); pigServer.registerQuery("B = FOREACH A GENERATE blah as a, blah as b;"); pigServer.registerQuery("STORE B INTO 'bar' USING mock.Storage();"); assertEquals(schema("a:chararray,b:chararray"), data.getSchema("bar")); List<Tuple> out = data.get("bar"); assertEquals(tuple("a", "a"), out.get(0)); assertEquals(tuple("b", "b"), out.get(1)); assertEquals(tuple("c", "c"), out.get(2)); </pre>
This util class provides methods that are shared by storage class {@link PigStorage} and streaming class {@link PigStreaming}

An interface that provides methods for converting Pig internal types to byte[]. It is intended to be used by StoreFunc implementations. Because we still don't have the map casts quite right
Converter that takes a POStore and stores it's content.
StoreFuncs take records from Pig's processing and store them into a data store.  Most frequently this is an HDFS file, but it could also be an HBase instance, RDBMS, etc.
This class is used to decorate the {@code StoreFunc#putNext(Tuple)}. It handles errors by calling {@code OutputErrorHandler#handle(String, long, Throwable)} if the {@link StoreFunc} implements {@link ErrorHandling}
StoreFuncs take records from Pig's processing and store them into a data store.  Most frequently this is an HDFS file, but it could also be an HBase instance, RDBMS, etc.
Convenience class to extend when decorating a class that implements both StoreFunc and StoreMetadata. It's not abstract so that it will fail to compile if new methods get added to either interface.
Convenience class to extend when decorating a StoreFunc. It's not abstract so that it will fail to compile if new methods get added to StoreFuncInterface. Subclasses must call the setStoreFunc with an instance of StoreFuncInterface before other methods can be called. Not doing so will result in an IllegalArgumentException when the method is called.
This interface defines how to write metadata related to data to be stored. If a given store function does not implement this interface, it will be assumed that it is unable to record metadata about the associated data.
This interface allow StoreFunc to specify resources needed in distributed cache. The resources can be on dfs (getCacheFiles) or locally (getShipFiles)


The interface is used for the custom mapping of a byte array, received from the stdout of the streaming process, to a {@link Tuple}. This interface, together with {@link PigToStream}, is designed to provide a common protocol for data exchange between Pig runtime and streaming executables. The method <code>getLoadCaster</code> is called by Pig to convert the fields in the byte array to typed fields of the Tuple based on a given schema. Typically, user implements this interface for a particular type of stream command and specifies the implementation class in the Pig DEFINE statement.

{@link StreamingCommand} represents the specification of an external command to be executed in a Pig Query. <code>StreamingCommand</code> encapsulates all relevant details of the command specified by the user either directly via the <code>STREAM</code> operator or indirectly via a <code>DEFINE</code> operator. It includes details such as input/output/error specifications and also files to be shipped to the cluster and files to be cached.
Check and set files to be automatically shipped for the given StreamingCommand Auto-shipping rules: 1. If the command begins with either perl or python assume that the binary is the first non-quoted string it encounters that does not start with dash - subject to restrictions in (2). 2. Otherwise, attempt to ship the first string from the command line as long as it does not come from /bin, /user/bin, /user/local/bin. It will determine that by scanning the path if an absolute path is provided or by executing "which". The paths can be made configurable via "set stream.skippath <paths>" option.








This method should never be used directly, use {@link CONCAT}.
This method should never be used directly, use {@link MAX}.
This method should never be used directly, use {@link MIN}.
This method should never be used directly, use {@link SIZE}.

this class is only used in negative tests just based on StringStore so we don't need to provide any other special implementation here


Given a string, this UDF replaces a substring given its starting index and length with the given replacement string. If the the last argument is null, the specified part of the string gets deleted. B = FOREACH A GENERATE Stuff($0, 10, 4, 'Pie') If $0 is "Chocolate Cake" then the UDF will return "Chocolate Pie"
SUBTRACT takes two bags as arguments and returns a new bag composed of tuples of first bag not in the second bag.<br> If null, bag arguments are replaced by empty bags. <p> The implementation assumes that both bags being passed to this function will fit entirely into memory simultaneously. </br> If that is not the case the UDF will still function, but it will be <strong>very</strong> slow.
<p>SubtractDuration returns the result of a DateTime object plus a Duration object</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Duration Format: http://en.wikipedia.org/wiki/ISO_8601#Durations</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dr:chararray); DESCRIBE ISOin; ISOin: {dt: datetime,dr: chararray} DUMP ISOin; (2009-01-07T01:07:01.000Z,PT1S) (2008-02-06T02:06:02.000Z,PT1M) (2007-03-05T03:05:03.000Z,P1D) ... dtsubtract = FOREACH ISOin GENERATE SubtractDuration(dt, dr) AS dt1; DESCRIBE dtsubtract; dtsubtract: {dt1: datetime} DUMP dtsubtract; (2009-01-07T01:07:00.000Z) (2008-02-06T02:05:02.000Z) (2007-03-04T03:05:03.000Z) </pre>
Subtract Operator



TAN implements a binding to the Java function {@link java.lang.Math#tan(double) Math.tan(double)}. Given a single data atom it Returns the tangent of the argument.
TANH implements a binding to the Java function {@link java.lang.Math#tanh(double) Math.tanh(double)}. Given a single data atom it Returns the hyperbolic tangent of the argument.
A record reader used to read data written using {@link InterRecordWriter} It uses the default InterSedes object for deserialization.
A record reader used to write data compatible with {@link InterRecordWriter} It uses the default InterSedes object for serialization.
LOAD FUNCTION FOR PIG INTERNAL USE ONLY! This load function is used for storing intermediate data between MR jobs of a pig query. The serialization format of this load function can change in newer versions of pig, so this should NOT be used to store any persistent data.
This class takes a list of items and puts them into a bag T = foreach U generate TOBAG($0, $1, $2); It's like saying this: T = foreach U generate {($0), ($1), ($2)} All arguments that are not of tuple type are inserted into a tuple before being added to the bag. This is because bag is always a bag of tuples. Output schema: The output schema for this udf depends on the schema of its arguments. If all the arguments have same type and same inner schema (for bags/tuple columns), then the udf output schema would be a bag of tuples having a column of the type and inner-schema (if any) of the arguments. If the arguments are of type tuple/bag, then their inner schemas should match, though schema field aliases may differ. If these conditions are not met the output schema will be a bag with null inner schema. example 1 grunt> describe a; a: {a0: int,a1: int} grunt> b = foreach a generate TOBAG(a0,a1); grunt> describe b; b: {{int}} example 2 grunt> describe a; a: {a0: (x: int),a1: (x: int)} grunt> b = foreach a generate TOBAG(a0,a1); grunt> describe b; b: {{(x: int)}} example 3 grunt> describe a; a: {a0: (x: int),a1: (y: int)} -- note that the inner schemas have matching types but different field aliases. -- the aliases of the first argument (a0) will be used in output schema: grunt> b = foreach a generate TOBAG(a0,a1); grunt> describe b; b: {{(x: int)}} example 4 grunt> describe a; a: {a0: (x: int),a1: (x: chararray)} -- here the inner schemas do not match, so output schema is not well defined: grunt> b = foreach a generate TOBAG(a0,a1); grunt> describe b; b: {{NULL}}
This class takes a list of items and puts them into a bag T = foreach U generate TOBAG($0, $1, $2); It's like saying this: T = foreach U generate {($0), ($1), ($2)} All arguments that are not of tuple type are inserted into a tuple before being added to the bag. This is because bag is always a bag of tuples. Output schema: The output schema for this udf depends on the schema of its arguments. If all the arguments have same type and same inner schema (for bags/tuple columns), then the udf output schema would be a bag of tuples having a column of the type and inner-schema (if any) of the arguments. If the arguments are of type tuple/bag, then their innerschmea, including the alias names should match. If these conditions are not met the output schema will be a bag with null inner schema. example 1 grunt> describe a; a: {a0: int,a1: int} grunt> b = foreach a generate TOBAG(a0,a1); grunt> describe b; b: {{int}} example 2 grunt> describe a; a: {a0: (x: int),a1: (x: int)} grunt> b = foreach a generate TOBAG(a0,a1); grunt> describe b; b: {{(x: int)}} example 3 grunt> describe a; a: {a0: (x: int),a1: (y: int)} -- note that the inner schema is different because the alises (x & y) are different grunt> b = foreach a generate TOBAG(a0,a1); grunt> describe b; b: {{NULL}}
Given a chararray as an argument, this method will split the chararray and return a bag with a tuple for each chararray that results from the split. The string is split on space, double quote, comma, open parend, close parend, and asterisk (star).
This class makes a map out of the parameters passed to it T = foreach U generate TOMAP($0, $1, $2, $3); It generates a map $0->1, $2->$3 This UDF also accepts a bag with 'pair' tuples (i.e. tuples with a 'key' and a 'value').

A tuple composed with the operators to which it needs be attached

This is an interface which, if implemented, allows an Accumulator function to signal that it can terminate early. Certain classes of UDF to do not need access to an entire set of data in order to finish processing. A model example is {org.apache.pig.builtin.IsEmpty}. IsEmpty can be Accumulative as if it receives even one line, it knows that it is not empty. Another example might be a UDF which does streaming analysis, and once a given stream matches a criteria, can terminate without needing any further analysis.
This load function simply creates a tuple for each line of text that has a single chararray field that contains the line of text.
The compiler that compiles a given physical plan into a DAG of Tez operators which can then be converted into the JobControl structure.


TezDAGStats encapsulates the statistics collected from a Tez DAG. It includes status of the execution, the Tez Vertices, as well as information about outputs and inputs of the DAG.
A visitor to construct DAG out of Tez plan.
Descriptor for Tez edge. It holds combine plan as well as edge properties.

TezExecType is the ExecType for distributed mode in Tez.

This interface is implemented by PhysicalOperators that can have Tez inputs attached directly to the operator.
Wrapper class that encapsulates Tez DAG. This class mediates between Tez DAGs and JobControl.
This is compiler class that takes a TezOperPlan and converts it into a JobControl object with the relevant dependency info maintained. The JobControl object is made up of TezJobs each of which has a JobConf.
Main class that launches pig for Tez
TezExecType is the ExecType for distributed mode in Tez.

A visitor for the TezOperPlan class
Estimate the parallelism of the vertex using: 1. parallelism of the predecessors 2. bloating factor of the physical plan of the predecessor Since currently it is only possible to reduce the parallelism estimation is exaggerated and will rely on Tez runtime to decrease the parallelism
A Plan used to create the plan of Tez operators which can be converted into the Job Control object. This is necessary to capture the dependencies among jobs.
An operator model for a Tez job. Acts as a host to the plans that will execute in Tez vertices.
This interface is implemented by PhysicalOperators that can have Tez outputs attached directly to the operator.
A port of the POPackageAnnotator from MR to Tez.



TezPigScriptStats encapsulates the statistics collected from a running script executed in Tez mode. It includes status of the execution, the Tez DAGs launched for the script, as well as information about outputs and inputs of the script. TezPigScriptStats encapsulates multiple TezDAGStats. TezDAGStats encapsulates multiple TezVertexStats





A visitor to print out the Tez plan.


ScriptStates encapsulates settings for a Pig script that runs on a hadoop cluster. These settings are added to all Tez jobs spawned by the script and in turn are persisted in the hadoop job xml. With the properties already in the job xml, users who want to know the relations between the script and Tez jobs can derive them from the job xmls.

This interface is implemented by PhysicalOperators that can need to access TezProcessorContext of a Tez task.



TezVertexStats encapsulates the statistics collected from a Tez Vertex. It includes status of the execution as well as information about outputs and inputs of the Vertex.

<p>ToDate converts the ISO or the customized string or the Unix timestamp to the DateTime object.</p> <p>ToDate is overloaded.</p> <dl> <dt><b>Syntax:</b></dt> <dd><code>DateTime ToDate(Long millis)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>the milliseconds</code>.</dd> <dt><b>Output:</b></dt> <dd><code>the DateTime object</code>.</dd> </dl> <dl> <dt><b>Syntax:</b></dt> <dd><code>DateTime ToDate(String dtStr)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>the ISO format date time string</code>.</dd> <dt><b>Output:</b></dt> <dd><code>the DateTime object</code>.</dd> </dl> <dl> <dt><b>Syntax:</b></dt> <dd><code>DateTime ToDate(String dtStr, String format)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>dtStr: the string that represents a date time</code>.</dd> <dd><code>format: the format string</code>.</dd> <dt><b>Output:</b></dt> <dd><code>the DateTime object</code>.</dd> </dl> <dl> <dt><b>Syntax:</b></dt> <dd><code>DateTime ToDate(String dtStr, String format, String timezone)</code>.</dd> <dt><b>Input:</b></dt> <dd><code>dtStr: the string that represents a date time</code>.</dd> <dd><code>format: the format string</code>.</dd> <dd><code>timezone: the timezone string</code>.</dd> <dt><b>Output:</b></dt> <dd><code>the DateTime object</code>.</dd> </dl>
This method should never be used directly, use {@link ToDate}.
This method should never be used directly, use {@link ToDate}.
This method should never be used directly, use {@link ToDate}.
This function converts the input into lowercase and removes leading and trailing white spaces.
<p> ToMilliSeconds converts the DateTime to the number of milliseconds that have passed since January 1, 1970 00:00:00.000 GMT. </p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> <li>Unix Time: http://en.wikipedia.org/wiki/Unix_time</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... toMilliSeconds = FOREACH ISOin GENERATE ToMilliSeconds(dt) AS unixTime:long; DESCRIBE toMilliSeconds; toMilliSeconds: {unixTime: long} DUMP toMilliSeconds; (1231290421000L) (1202263562000L) (1173063903000L) ... </pre>
<p>ToString converts the DateTime object of the ISO or the customized string.</p>
This class makes a tuple out of the parameter T = foreach U generate TOTUPLE($0, $1, $2); It generates a tuple containing $0, $1, and $2
<p>ToUnixTime converts the DateTime to the Unix Time Long</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> <li>Unix Time: http://en.wikipedia.org/wiki/Unix_time</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (dt:datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... toUnix = FOREACH ISOin GENERATE ToUnixTime(dt) AS unixTime:long; DESCRIBE toUnix; toUnix: {unixTime: long} DUMP toUnix; (1231290421L) (1202263562L) (1173063903L) ... </pre>
Describes the input token stream.
Token Manager Error.
ToolsPigServer is a subclass of PigServer intended only for Pig tools.  Users should not use this interface, as we make no promises about its stability or continued existence.
Top UDF accepts a bag of tuples and returns top-n tuples depending upon the tuple field value of type long. Both n and field number needs to be provided to the UDF. The UDF iterates through the input bag and just retains top-n tuples by storing them in a priority queue of size n+1 where priority is the long field. This is efficient as priority queue provides constant time - O(1) removal of the least element and O(log n) time for heap restructuring. The UDF is especially helpful for turning the nested grouping operation inside out and retaining top-n in a nested group. Assumes all tuples in the bag contain an element of the same type in the compared column. Sample usage: DEFINE TOP_ASC TOP("ASC") DEFINE TOP_DESC TOP("DESC") A = LOAD 'test.tsv' as (first: chararray, second: chararray); B = GROUP A BY (first, second); C = FOREACH B generate FLATTEN(group), COUNT(*) as count; D = GROUP C BY first; // again group by first topResults = FOREACH D { result = TOP_ASC(10, 1, C); // and retain top 10 occurrences of 'second' in first GENERATE FLATTEN(result); topDescResults = FOREACH D { result = TOP_DESC(10, 1, C); // and retain top 10 occurrences of 'second' in first GENERATE FLATTEN(result); * }


Pig Store/Load Function for Trevni.
Returns a string, with leading and trailing whitespace omitted. Implements a binding to the Java function {@link java.lang.String#trim() String.trim()}.
An ordered list of Data.  A tuple has fields, numbered 0 through (number of fields - 1).  The entry in the field can be any datatype, or it can be null. <p> Tuples are constructed only by a {@link TupleFactory}.  A {@link DefaultTupleFactory} is provided by the system.  If users wish to use their own type of Tuple, they should also provide an implementation of {@link TupleFactory} to construct their types of Tuples. Put in to make the compiler not complain about WritableComparable being a generic type.
A factory to construct tuples.  This class is abstract so that users can override the tuple factory if they desire to provide their own that returns their implementation of a tuple.  If the property pig.data.tuple.factory.name is set to a class name and pig.data.tuple.factory.jar is set to a URL pointing to a jar that contains the above named class, then {@link #getInstance()} will create a an instance of the named class using the indicated jar.  Otherwise, it will create an instance of {@link DefaultTupleFactory}.
Default implementation of format of Tuple. Dump and PigDump use this default implementation

This interface is intended to compare Tuples. The semantics of Tuple comparison must take into account null values in different ways. According to SQL semantics nulls are not equal. But for other Pig/Latin statements nulls must be grouped together. This interface allows to check if there are null fields in the tuples compared using this comparator. This method is meaningful only when the tuples are determined to be equal by the {@link #compare(byte[],int,int,byte[],int,int)} method.
This method should never be used directly, use {@link SIZE}.





All the getType() of these operators always return BAG. We just have to :- 1) Check types of inputs, expression plans 2) Compute output schema with type information (At the moment, the parser does only return GetSchema with correct aliases) 3) Insert casting if necessary
Base class for Pig UDFs that are functions from Tuples to generic type OUT. Handles marshalling objects, basic error checking, etc. Also infers outputSchema and provides a function to verify the input Tuple. <P></P> Extend this class and implement the <pre>OUT exec(Tuple input)</pre> method when writting a UDF that operates on multiple inputs from the Tuple.





find udf jars which will be downloaded with spark job on every nodes
math.ULP implements a binding to the Java function {@link java.lang.Math#ulp(double) Math.ulp(double)}. Given a single data atom it Returns the size of an ulp of the argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>ULP_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.ULP(float1); </code></dd> </dl>
UPPER implements eval function to convert a string to upper case Example: A = load 'mydata' as (name); B = foreach A generate UPPER(name);

upper-case the first character of a string




This is a base class for all unary comparison operators. Supports the use of operand type instead of result type as the result type is always boolean.
Superclass for all unary expressions



A visitor that modifies the logical plan (if necessary) for union-onschema functionality. It runs logical plan validator so that the correct schema of its inputs is available. It inserts foreach statements in its input if the input operator schema does not match the schema created by merging all input schemas. Migrated from the old UnionOnSchemaSetter class.
Optimizes union by removing the intermediate union vertex and making the successor get input from the predecessor vertices directly using VertexGroup. This should be run after MultiQueryOptimizer so that it handles cases like union followed by split and then store. For eg: 1) Union followed by store Vertex 1 (Load), Vertex 2 (Load) -> Vertex 3 (Union + Store) will be optimized to Vertex 1 (Load + Store), Vertex 2 (Load + Store). Both the vertices will be writing output to same store location directly which is supported by Tez. 2) Union followed by groupby Vertex 1 (Load), Vertex 2 (Load) -> Vertex 3 (Union + POLocalRearrange) -> Vertex 4 (Group by) will be optimized to Vertex 1 (Load + POLR), Vertex 2 (Load + POLR) -> Vertex 4 (Group by)
UniqueID generates a unique id for each records in the job. This unique id is stable in task retry. Any arguments to the function are ignored. Example: A = load 'mydata' as (name); B = foreach A generate name, UniqueID(); UniqueID takes the form "index-sequence"
<p>UnixToISO converts Unix Time Long datetimes to ISO8601 datetime strings</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> <li>Unix Time: http://en.wikipedia.org/wiki/Unix_time</li> </ul> <br /> <pre> Example usage: REGISTER /Users/me/commiter/piggybank/java/piggybank.jar ; REGISTER /Users/me/commiter/piggybank/java/lib/joda-time-1.6.jar ; DEFINE UnixToISO org.apache.pig.piggybank.evaluation.datetime.convert.UnixToISO(); UnixIn = LOAD 'test2.tsv' USING PigStorage('\t') AS (dt:long); DESCRIBE UnixIn; UnixIn: {dt: long} DUMP UnixIn; (1231290421000L) (1233885962000L) (1236222303000L) toISO = FOREACH UnixIn GENERATE UnixToISO(dt) AS ISOTime:chararray; DESCRIBE toISO; toISO: {ISOTime: chararray} DUMP toISO; (2009-01-07T01:07:01.000Z) (2009-02-06T02:06:02.000Z) (2009-03-05T03:05:03.000Z) ... </pre>



This abstract class provides standard conversions between utf8 encoded data and pig data types.  It is intended to be extended by load and store functions (such as {@link PigStorage}).


This UDF takes a Map and returns a Bag containing the values from map. <br /> Note that output tuple contains all values, not just unique ones. <br /> For obtaining unique values from map, use VALUESET instead. <br /> <pre> <code> grunt> cat data [open#apache,1#2,11#2] [apache#hadoop,3#4,12#hadoop] grunt> a = load 'data' as (M:[]); grunt> b = foreach a generate VALUELIST($0); grunt> dump b; ({(apache),(2),(2)}) ({(4),(hadoop),(hadoop)}) </code> </pre>
This UDF takes a Map and returns a Tuple containing the value set. <br /> Note, this UDF returns only unique values. For all values, use <br /> VALUELIST instead. <br /> <pre> <code> grunt> cat data [open#apache,1#2,11#2] [apache#hadoop,3#4,12#hadoop] grunt> a = load 'data' as (M:[]); grunt> b = foreach a generate VALUELIST($0); ({(apache),(2)}) ({(4),(hadoop)}) </code> </pre>


<p>WeeksBetween returns the number of weeks between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>


Super class for all rules that operates on the whole plan. It doesn't look for a specific pattern. An example of such kind rule is ColumnPrune.


A reusable byte buffer implementation <p>This saves memory over creating a new byte[] and ByteArrayOutputStream each time data is written. <p>Typical usage is something like the following:<pre> WritableByteArray buffer = new WritableByteArray(); while (... loop condition ...) { buffer.reset(); ... write to buffer ... byte[] data = buffer.getData(); int dataLength = buffer.getLength(); ... write data to its ultimate destination ... } </pre>

Parses an XML input file given a specified identifier of tags to be loaded. The output is a bag of XML elements where each element is returned as a chararray containing the text of the matched XML element including the start and tags as well as the data between them. In case of nesting elements of the matching tags, only the top level one is returned.
A visitor mechanism printing out the logical plan.


XPath is a function that allows for text extraction from xml
XPathAll is a function that allows for Tuple extraction from xml

<p>YearsBetween returns the number of years between two DateTime objects</p> <ul> <li>Jodatime: http://joda-time.sourceforge.net/</li> <li>ISO8601 Date Format: http://en.wikipedia.org/wiki/ISO_8601</li> </ul> <br /> <pre> Example usage: ISOin = LOAD 'test.tsv' USING PigStorage('\t') AS (datetime, dt2:datetime); DESCRIBE ISOin; ISOin: {dt: datetime,dt2: datetime} DUMP ISOin; (2009-01-07T01:07:01.000Z,2008-02-01T00:00:00.000Z) (2008-02-06T02:06:02.000Z,2008-02-01T00:00:00.000Z) (2007-03-05T03:05:03.000Z,2008-02-01T00:00:00.000Z) ... diffs = FOREACH ISOin GENERATE YearsBetween(dt, dt2) AS years, MonthsBetween(dt, dt2) AS months, WeeksBetween(dt, dt2) AS weeks, DaysBetween(dt, dt2) AS days, HoursBetween(dt, dt2) AS hours, MinutesBetween(dt, dt2) AS mins, SecondsBetween(dt, dt2) AS secs; MilliSecondsBetween(dt, dt2) AS millis; DESCRIBE diffs; diffs: {years: long,months: long,weeks: long,days: long,hours: long,mins: long,secs: long,millis: long} DUMP diffs; (0L,11L,48L,341L,8185L,491107L,29466421L,29466421000L) (0L,0L,0L,5L,122L,7326L,439562L,439562000L) (0L,-10L,-47L,-332L,-7988L,-479334L,-28760097L,-28760097000L) </pre>
math.copySign implements a binding to the Java function {@link java.lang.Math#copySign(double,double) Math.copySign(double,double)}. Given a tuple with two data atom Returns the first floating-point argument with the sign of the second floating-point argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>copySign_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.copySign(float1); </code></dd> </dl>
math.getExponent implements a binding to the Java function {@link java.lang.Math#getExponent(double) Math.getExponent(double)}. Given a single data atom it returns the unbiased exponent used in the representation of a double <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>int</code> </dd> <dt><b>Return Schema:</b></dt> <dd>getExponent_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.getExponent(float1); </code></dd> </dl>
math.nextAfter implements a binding to the Java function {@link java.lang.Math#nextAfter(double,double) Math.nextAfter(double,double)}. Given a tuple with two data atom it Returns the floating-point number adjacent to the first argument in the direction of the second argument. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Tuple containing two double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>nextAfter_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.nextAfter(float1); </code></dd> </dl>
math.toDegrees implements a binding to the Java function {@link java.lang.Math#toDegrees(double) Math.toDegrees(double)}. Given a single data atom it Converts an angle measured in radians to an approximately equivalent angle measured in degrees. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>toDegrees_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.toDegrees(float1); </code></dd> </dl>
math.toRadians implements a binding to the Java function {@link java.lang.Math#toRadians(double) Math.toRadians(double)}. Given a single data atom it Converts an angle measured in degrees to an approximately equivalent angle measured in radians. <dl> <dt><b>Parameters:</b></dt> <dd><code>value</code> - <code>Double</code>.</dd> <dt><b>Return Value:</b></dt> <dd><code>Double</code> </dd> <dt><b>Return Schema:</b></dt> <dd>toRadians_inputSchema</dd> <dt><b>Example:</b></dt> <dd><code> register math.jar;<br/> A = load 'mydata' using PigStorage() as ( float1 );<br/> B = foreach A generate float1, math.toRadians(float1); </code></dd> </dl>
